{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saulgrimaldo1/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from w266_common import utils, vocabulary\n",
    "import time\n",
    "import datetime\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "tokenizer = TweetTokenizer()\n",
    "x_data = []\n",
    "x_contexts = []\n",
    "labels = []\n",
    "sentences  = []\n",
    "contexts = []\n",
    "with open('merged_data_v3.csv', 'r') as csvfile:\n",
    "    linereader = csv.reader(csvfile, delimiter = '|')\n",
    "    for i, row in enumerate(linereader):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        sentence, context, sarcasm = row\n",
    "        sentences.append(sentence)\n",
    "        contexts.append(context)\n",
    "        x_tokens = utils.canonicalize_words(tokenizer.tokenize(sentence))\n",
    "        context_tokens = utils.canonicalize_words(tokenizer.tokenize(context))\n",
    "        x_data.append(x_tokens)\n",
    "        x_contexts.append(context_tokens)\n",
    "        labels.append(int(sarcasm))\n",
    "\n",
    "\n",
    "#rng = np.random.RandomState(5)\n",
    "#rng.shuffle(x_data)  # in-place\n",
    "#train_split_idx = int(0.7 * len(labels))\n",
    "#test_split_idx  = int(0.9 * len(labels))\n",
    "\n",
    "shuffle_indices = np.random.permutation(np.arange(len(labels)))\n",
    "train_split_idx = int(0.7 * len(labels))\n",
    "test_split_idx  = int(0.9 * len(labels))\n",
    "\n",
    "train_indices = shuffle_indices[:train_split_idx]\n",
    "validation_indices = shuffle_indices[train_split_idx:test_split_idx]\n",
    "test_indices = shuffle_indices[test_split_idx:]\n",
    "\n",
    "\n",
    "train_sentences = np.array(x_data)[train_indices]\n",
    "train_contexts = np.array(x_contexts)[train_indices]\n",
    "train_labels= np.array(labels)[train_indices] \n",
    "validation_sentences = np.array(x_data)[validation_indices]\n",
    "validation_labels = np.array(labels)[validation_indices]\n",
    "validation_contexts = train_contexts = np.array(x_contexts)[validation_indices]\n",
    "test_sentences = np.array(x_data)[test_indices]  \n",
    "test_contexts = train_contexts = np.array(x_contexts)[test_indices]\n",
    "test_labels = np.array(labels)[test_indices]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6108,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_labels(raw_label_set, size):\n",
    "    label_set = []\n",
    "    for label in raw_label_set:\n",
    "        labels = [0] * size\n",
    "        labels[label] = 1\n",
    "        label_set.append(labels)\n",
    "    return np.array(label_set)\n",
    "\n",
    "expanded_train_labels = transform_labels(train_labels, 2)\n",
    "expanded_validation_labels = transform_labels(validation_labels,2)\n",
    "expanded_test_labels = transform_labels(test_labels,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4, 5, 6, 7, 8, 8, 1, 5, 6, 7]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,3,4,5,6,7,8,8,1,5,6,7]\n",
    "a + [\"<PADDING>\"]*0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'angry',\n",
       " 'man',\n",
       " 'is',\n",
       " 'angry',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PaddingAndTruncating:\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def pad_or_truncate(self, sentence):\n",
    "        sen_len = len(sentence)\n",
    "        paddings = self.max_len - sen_len\n",
    "        if paddings >=0:\n",
    "            return sentence + [\"<PADDING>\"] * paddings\n",
    "        return sentence[0:paddings]\n",
    "        \n",
    "PadAndTrunc = PaddingAndTruncating(10)\n",
    "        \n",
    "        \n",
    "PadAndTrunc.pad_or_truncate([\"the\",\"angry\",\"man\",\"is\",\"angry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10,   5,   7, ...,  15,   3,   3],\n",
       "       [  5,  85,   2, ...,  12,   3,   3],\n",
       "       [ 85, 213,  30, ...,   4,  14,   3],\n",
       "       ..., \n",
       "       [ 10,   5,   7, ...,   3,   3,   3],\n",
       "       [739,  73,  96, ...,   3,   3,   3],\n",
       "       [ 10,   5,   7, ...,   3,   3,   3]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.contrib import learn\n",
    "\n",
    "#max_len = max([len(sent) for sent  in train_sentences])\n",
    "PadAndTrunc =PaddingAndTruncating(30)\n",
    "train_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, train_sentences))\n",
    "train_context_padded = list(map(PadAndTrunc.pad_or_truncate, train_contexts))\n",
    "validation_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, validation_sentences))\n",
    "validation_context_padded = list(map(PadAndTrunc.pad_or_truncate, validation_contexts))\n",
    "test_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, test_sentences))\n",
    "test_context_padded = list(map(PadAndTrunc.pad_or_truncate, test_contexts))\n",
    "\n",
    "vocabulary_size = 2500\n",
    "vocab = vocabulary.Vocabulary(utils.flatten(list(train_sentences_padded) + list(train_context_padded)), vocabulary_size)\n",
    "train_s = np.array(list(map(vocab.words_to_ids, train_sentences_padded)))\n",
    "train_c = np.array(list(map(vocab.words_to_ids, train_context_padded)))\n",
    "validation_s = np.array(list(map(vocab.words_to_ids, validation_sentences_padded)))\n",
    "validation_c = np.array(list(map(vocab.words_to_ids, validation_context_padded)))\n",
    "test_s = np.array(list(map(vocab.words_to_ids, test_sentences_padded)))\n",
    "test_c = np.array(list(map(vocab.words_to_ids, test_context_padded)))\n",
    "train_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv-maxpool-3'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 3\n",
    "(\"conv-maxpool-%s\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size, \n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x1 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x1\")\n",
    "        self.input_x2 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x2\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars1 = tf.nn.embedding_lookup(self.W, self.input_x1)\n",
    "            self.embedded_chars_expanded1 = tf.expand_dims(self.embedded_chars1, -1)\n",
    "            self.embedded_chars2 = tf.nn.embedding_lookup(self.W, self.input_x2)\n",
    "            self.embedded_chars_expanded2 = tf.expand_dims(self.embedded_chars2, -1)\n",
    "\n",
    "        # Create a convolution + avgpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.variable_scope(\"conv-avgpool-%s\" % i) as scope:\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "              \n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded1,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h1 = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"tanh1\")\n",
    "               \n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.avg_pool(\n",
    "                    h1,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                \n",
    "                pooled_outputs.append(pooled)\n",
    "                #pooled_outputs.append(pooled)\n",
    "                scope.reuse_variables()\n",
    "                 \n",
    "                \n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded2,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h2 = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"tanh2\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled2 = tf.nn.avg_pool(\n",
    "                    h2,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled2)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "\n",
    "        num_filters_total = num_filters * len(filter_sizes) * 2\n",
    "\n",
    "        self.h_pool = tf.concat(pooled_outputs, 1)\n",
    "\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "        \n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "           \n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.labels = tf.argmax(self.input_y, 1)\n",
    "            TP = tf.count_nonzero(self.predictions * self.labels)\n",
    "            TN = tf.count_nonzero((self.predictions - 1) * (self.labels - 1))\n",
    "            FP = tf.count_nonzero(self.predictions * (self.labels - 1))\n",
    "            FN = tf.count_nonzero((self.predictions - 1) * self.labels)\n",
    "            correct_predictions = tf.equal(self.predictions, self.labels)\n",
    "            self.precision = TP / (TP + FP)\n",
    "            self.recall = TP / (TP + FN)\n",
    "            self.f1_score = 2 * self.precision * self.recall / (self.precision + self.recall)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=100\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.75\n",
      "DROPOUT_KEEP_PROB=0.4\n",
      "EMBEDDING_DIM=50\n",
      "EVALUATE_EVERY=20\n",
      "FILTER_SIZES=1\n",
      "L2_REG_LAMBDA=0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.neg\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=100\n",
      "NUM_FILTERS=50\n",
      "POSITIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.pos\n",
      "\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/W:0/grad/hist is illegal; using conv-avgpool-0/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/W:0/grad/sparsity is illegal; using conv-avgpool-0/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/b:0/grad/hist is illegal; using conv-avgpool-0/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/b:0/grad/sparsity is illegal; using conv-avgpool-0/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523943868\n",
      "\n",
      "2018-04-17T05:44:29.151471: step 1, loss 0.787715, acc 0.51\n",
      "2018-04-17T05:44:29.176807: step 2, loss 0.729545, acc 0.39\n",
      "2018-04-17T05:44:29.197444: step 3, loss 0.724706, acc 0.49\n",
      "2018-04-17T05:44:29.217784: step 4, loss 0.776687, acc 0.43\n",
      "2018-04-17T05:44:29.238677: step 5, loss 0.784469, acc 0.53\n",
      "2018-04-17T05:44:29.259265: step 6, loss 0.67691, acc 0.57\n",
      "2018-04-17T05:44:29.279779: step 7, loss 0.6996, acc 0.56\n",
      "2018-04-17T05:44:29.300586: step 8, loss 0.697774, acc 0.57\n",
      "2018-04-17T05:44:29.316975: step 9, loss 0.652201, acc 0.630137\n",
      "2018-04-17T05:44:29.337646: step 10, loss 0.63589, acc 0.67\n",
      "2018-04-17T05:44:29.361165: step 11, loss 0.620476, acc 0.65\n",
      "2018-04-17T05:44:29.381278: step 12, loss 0.705227, acc 0.53\n",
      "2018-04-17T05:44:29.401547: step 13, loss 0.679343, acc 0.57\n",
      "2018-04-17T05:44:29.421873: step 14, loss 0.669196, acc 0.56\n",
      "2018-04-17T05:44:29.442223: step 15, loss 0.652896, acc 0.61\n",
      "2018-04-17T05:44:29.462312: step 16, loss 0.614326, acc 0.7\n",
      "2018-04-17T05:44:29.483222: step 17, loss 0.636654, acc 0.66\n",
      "2018-04-17T05:44:29.499665: step 18, loss 0.676643, acc 0.671233\n",
      "2018-04-17T05:44:29.521079: step 19, loss 0.623404, acc 0.74\n",
      "2018-04-17T05:44:29.542151: step 20, loss 0.650055, acc 0.62\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:29.664399: step 20, loss 0.679967, acc 0.55702, rec 0.878824, pre 0.52717, f1 0.659021\n",
      "\n",
      "2018-04-17T05:44:29.688125: step 21, loss 0.638729, acc 0.69\n",
      "2018-04-17T05:44:29.709278: step 22, loss 0.602907, acc 0.72\n",
      "2018-04-17T05:44:29.730104: step 23, loss 0.6402, acc 0.67\n",
      "2018-04-17T05:44:29.751369: step 24, loss 0.607329, acc 0.68\n",
      "2018-04-17T05:44:29.773151: step 25, loss 0.626128, acc 0.56\n",
      "2018-04-17T05:44:29.794275: step 26, loss 0.667987, acc 0.66\n",
      "2018-04-17T05:44:29.812134: step 27, loss 0.588025, acc 0.69863\n",
      "2018-04-17T05:44:29.832383: step 28, loss 0.60996, acc 0.71\n",
      "2018-04-17T05:44:29.852827: step 29, loss 0.677164, acc 0.62\n",
      "2018-04-17T05:44:29.876913: step 30, loss 0.621917, acc 0.7\n",
      "2018-04-17T05:44:29.897350: step 31, loss 0.597303, acc 0.66\n",
      "2018-04-17T05:44:29.917501: step 32, loss 0.634257, acc 0.71\n",
      "2018-04-17T05:44:29.938157: step 33, loss 0.629087, acc 0.66\n",
      "2018-04-17T05:44:29.959266: step 34, loss 0.585703, acc 0.67\n",
      "2018-04-17T05:44:29.979662: step 35, loss 0.58864, acc 0.71\n",
      "2018-04-17T05:44:29.996855: step 36, loss 0.671465, acc 0.712329\n",
      "2018-04-17T05:44:30.018158: step 37, loss 0.582627, acc 0.67\n",
      "2018-04-17T05:44:30.038881: step 38, loss 0.619606, acc 0.66\n",
      "2018-04-17T05:44:30.059172: step 39, loss 0.57365, acc 0.72\n",
      "2018-04-17T05:44:30.083479: step 40, loss 0.698851, acc 0.54\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:30.181914: step 40, loss 0.786377, acc 0.489971, rec 0.981176, pre 0.48829, f1 0.652072\n",
      "\n",
      "2018-04-17T05:44:30.204613: step 41, loss 0.584625, acc 0.73\n",
      "2018-04-17T05:44:30.225532: step 42, loss 0.596414, acc 0.74\n",
      "2018-04-17T05:44:30.245983: step 43, loss 0.533889, acc 0.8\n",
      "2018-04-17T05:44:30.266222: step 44, loss 0.568923, acc 0.67\n",
      "2018-04-17T05:44:30.282999: step 45, loss 0.627639, acc 0.726027\n",
      "2018-04-17T05:44:30.308299: step 46, loss 0.592653, acc 0.72\n",
      "2018-04-17T05:44:30.329380: step 47, loss 0.537344, acc 0.77\n",
      "2018-04-17T05:44:30.350260: step 48, loss 0.645172, acc 0.61\n",
      "2018-04-17T05:44:30.370276: step 49, loss 0.54771, acc 0.77\n",
      "2018-04-17T05:44:30.390881: step 50, loss 0.605608, acc 0.74\n",
      "2018-04-17T05:44:30.411387: step 51, loss 0.652193, acc 0.68\n",
      "2018-04-17T05:44:30.431974: step 52, loss 0.634699, acc 0.66\n",
      "2018-04-17T05:44:30.452709: step 53, loss 0.529382, acc 0.82\n",
      "2018-04-17T05:44:30.469375: step 54, loss 0.51899, acc 0.739726\n",
      "2018-04-17T05:44:30.489716: step 55, loss 0.636641, acc 0.6\n",
      "2018-04-17T05:44:30.514544: step 56, loss 0.691926, acc 0.6\n",
      "2018-04-17T05:44:30.536058: step 57, loss 0.531845, acc 0.74\n",
      "2018-04-17T05:44:30.557425: step 58, loss 0.627239, acc 0.65\n",
      "2018-04-17T05:44:30.577645: step 59, loss 0.566215, acc 0.67\n",
      "2018-04-17T05:44:30.598437: step 60, loss 0.609024, acc 0.71\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:30.698653: step 60, loss 0.741315, acc 0.492837, rec 0.965882, pre 0.489565, f1 0.649782\n",
      "\n",
      "2018-04-17T05:44:30.725466: step 61, loss 0.512492, acc 0.82\n",
      "2018-04-17T05:44:30.746170: step 62, loss 0.593221, acc 0.73\n",
      "2018-04-17T05:44:30.762775: step 63, loss 0.524936, acc 0.780822\n",
      "2018-04-17T05:44:30.783692: step 64, loss 0.578831, acc 0.75\n",
      "2018-04-17T05:44:30.804222: step 65, loss 0.608459, acc 0.73\n",
      "2018-04-17T05:44:30.824525: step 66, loss 0.549469, acc 0.74\n",
      "2018-04-17T05:44:30.844941: step 67, loss 0.569376, acc 0.73\n",
      "2018-04-17T05:44:30.865397: step 68, loss 0.599375, acc 0.75\n",
      "2018-04-17T05:44:30.885527: step 69, loss 0.718499, acc 0.63\n",
      "2018-04-17T05:44:30.906352: step 70, loss 0.625688, acc 0.75\n",
      "2018-04-17T05:44:30.931276: step 71, loss 0.534061, acc 0.72\n",
      "2018-04-17T05:44:30.947804: step 72, loss 0.540297, acc 0.739726\n",
      "2018-04-17T05:44:30.968055: step 73, loss 0.591197, acc 0.71\n",
      "2018-04-17T05:44:30.988107: step 74, loss 0.488103, acc 0.78\n",
      "2018-04-17T05:44:31.007767: step 75, loss 0.502253, acc 0.8\n",
      "2018-04-17T05:44:31.027714: step 76, loss 0.556719, acc 0.75\n",
      "2018-04-17T05:44:31.047703: step 77, loss 0.630522, acc 0.71\n",
      "2018-04-17T05:44:31.067904: step 78, loss 0.59057, acc 0.77\n",
      "2018-04-17T05:44:31.088419: step 79, loss 0.581266, acc 0.7\n",
      "2018-04-17T05:44:31.108316: step 80, loss 0.523478, acc 0.8\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:31.209976: step 80, loss 0.963822, acc 0.491117, rec 0.990588, pre 0.488966, f1 0.654743\n",
      "\n",
      "2018-04-17T05:44:31.227977: step 81, loss 0.595044, acc 0.684932\n",
      "2018-04-17T05:44:31.249212: step 82, loss 0.606416, acc 0.67\n",
      "2018-04-17T05:44:31.269530: step 83, loss 0.710178, acc 0.57\n",
      "2018-04-17T05:44:31.290197: step 84, loss 0.707045, acc 0.62\n",
      "2018-04-17T05:44:31.311121: step 85, loss 0.576656, acc 0.72\n",
      "2018-04-17T05:44:31.332366: step 86, loss 0.488076, acc 0.85\n",
      "2018-04-17T05:44:31.352847: step 87, loss 0.564744, acc 0.76\n",
      "2018-04-17T05:44:31.373563: step 88, loss 0.510952, acc 0.79\n",
      "2018-04-17T05:44:31.395332: step 89, loss 0.495494, acc 0.81\n",
      "2018-04-17T05:44:31.416017: step 90, loss 0.51718, acc 0.739726\n",
      "2018-04-17T05:44:31.438046: step 91, loss 0.540745, acc 0.79\n",
      "2018-04-17T05:44:31.459550: step 92, loss 0.52016, acc 0.74\n",
      "2018-04-17T05:44:31.480733: step 93, loss 0.583832, acc 0.74\n",
      "2018-04-17T05:44:31.501506: step 94, loss 0.546855, acc 0.71\n",
      "2018-04-17T05:44:31.521782: step 95, loss 0.552878, acc 0.75\n",
      "2018-04-17T05:44:31.542136: step 96, loss 0.505056, acc 0.79\n",
      "2018-04-17T05:44:31.562087: step 97, loss 0.480184, acc 0.82\n",
      "2018-04-17T05:44:31.581924: step 98, loss 0.51787, acc 0.78\n",
      "2018-04-17T05:44:31.598128: step 99, loss 0.516586, acc 0.739726\n",
      "2018-04-17T05:44:31.621349: step 100, loss 0.417797, acc 0.87\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:31.720798: step 100, loss 0.793104, acc 0.49914, rec 0.975294, pre 0.492866, f1 0.654818\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523943868/checkpoints/model-100\n",
      "\n",
      "2018-04-17T05:44:31.792911: step 101, loss 0.514537, acc 0.82\n",
      "2018-04-17T05:44:31.813765: step 102, loss 0.583684, acc 0.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17T05:44:31.838826: step 103, loss 0.660751, acc 0.68\n",
      "2018-04-17T05:44:31.859883: step 104, loss 0.529219, acc 0.78\n",
      "2018-04-17T05:44:31.880754: step 105, loss 0.542582, acc 0.81\n",
      "2018-04-17T05:44:31.901431: step 106, loss 0.543616, acc 0.7\n",
      "2018-04-17T05:44:31.922884: step 107, loss 0.524705, acc 0.8\n",
      "2018-04-17T05:44:31.940227: step 108, loss 0.448085, acc 0.863014\n",
      "2018-04-17T05:44:31.962247: step 109, loss 0.601412, acc 0.77\n",
      "2018-04-17T05:44:31.983507: step 110, loss 0.495531, acc 0.78\n",
      "2018-04-17T05:44:32.004189: step 111, loss 0.486158, acc 0.8\n",
      "2018-04-17T05:44:32.024996: step 112, loss 0.492305, acc 0.75\n",
      "2018-04-17T05:44:32.050256: step 113, loss 0.506012, acc 0.85\n",
      "2018-04-17T05:44:32.071245: step 114, loss 0.534678, acc 0.73\n",
      "2018-04-17T05:44:32.091938: step 115, loss 0.549939, acc 0.73\n",
      "2018-04-17T05:44:32.112630: step 116, loss 0.497657, acc 0.73\n",
      "2018-04-17T05:44:32.129140: step 117, loss 0.558126, acc 0.767123\n",
      "2018-04-17T05:44:32.149716: step 118, loss 0.536376, acc 0.73\n",
      "2018-04-17T05:44:32.170760: step 119, loss 0.498853, acc 0.75\n",
      "2018-04-17T05:44:32.192340: step 120, loss 0.616824, acc 0.71\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:32.295583: step 120, loss 0.661505, acc 0.54957, rec 0.938824, pre 0.520888, f1 0.670025\n",
      "\n",
      "2018-04-17T05:44:32.318757: step 121, loss 0.592026, acc 0.79\n",
      "2018-04-17T05:44:32.340042: step 122, loss 0.554439, acc 0.73\n",
      "2018-04-17T05:44:32.361499: step 123, loss 0.461265, acc 0.83\n",
      "2018-04-17T05:44:32.383335: step 124, loss 0.552769, acc 0.78\n",
      "2018-04-17T05:44:32.404861: step 125, loss 0.499479, acc 0.78\n",
      "2018-04-17T05:44:32.422086: step 126, loss 0.48539, acc 0.876712\n",
      "2018-04-17T05:44:32.442899: step 127, loss 0.656324, acc 0.7\n",
      "2018-04-17T05:44:32.464151: step 128, loss 0.532557, acc 0.79\n",
      "2018-04-17T05:44:32.484595: step 129, loss 0.456423, acc 0.78\n",
      "2018-04-17T05:44:32.510120: step 130, loss 0.511142, acc 0.79\n",
      "2018-04-17T05:44:32.531069: step 131, loss 0.490287, acc 0.8\n",
      "2018-04-17T05:44:32.552060: step 132, loss 0.538849, acc 0.78\n",
      "2018-04-17T05:44:32.572981: step 133, loss 0.475186, acc 0.83\n",
      "2018-04-17T05:44:32.593463: step 134, loss 0.52082, acc 0.76\n",
      "2018-04-17T05:44:32.610140: step 135, loss 0.421466, acc 0.835616\n",
      "2018-04-17T05:44:32.631599: step 136, loss 0.45952, acc 0.87\n",
      "2018-04-17T05:44:32.652057: step 137, loss 0.492299, acc 0.83\n",
      "2018-04-17T05:44:32.672337: step 138, loss 0.443737, acc 0.81\n",
      "2018-04-17T05:44:32.693203: step 139, loss 0.463118, acc 0.78\n",
      "2018-04-17T05:44:32.716920: step 140, loss 0.551349, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:32.813107: step 140, loss 0.664416, acc 0.548424, rec 0.950588, pre 0.519949, f1 0.672213\n",
      "\n",
      "2018-04-17T05:44:32.834699: step 141, loss 0.471483, acc 0.82\n",
      "2018-04-17T05:44:32.855752: step 142, loss 0.408914, acc 0.85\n",
      "2018-04-17T05:44:32.876792: step 143, loss 0.512946, acc 0.82\n",
      "2018-04-17T05:44:32.893645: step 144, loss 0.534601, acc 0.767123\n",
      "2018-04-17T05:44:32.914768: step 145, loss 0.450245, acc 0.8\n",
      "2018-04-17T05:44:32.939477: step 146, loss 0.786341, acc 0.63\n",
      "2018-04-17T05:44:32.961009: step 147, loss 0.576521, acc 0.71\n",
      "2018-04-17T05:44:32.982645: step 148, loss 0.4837, acc 0.79\n",
      "2018-04-17T05:44:33.002952: step 149, loss 0.504754, acc 0.73\n",
      "2018-04-17T05:44:33.023219: step 150, loss 0.549057, acc 0.75\n",
      "2018-04-17T05:44:33.044194: step 151, loss 0.531893, acc 0.74\n",
      "2018-04-17T05:44:33.064575: step 152, loss 0.492267, acc 0.81\n",
      "2018-04-17T05:44:33.080914: step 153, loss 0.584974, acc 0.767123\n",
      "2018-04-17T05:44:33.101259: step 154, loss 0.442533, acc 0.79\n",
      "2018-04-17T05:44:33.121616: step 155, loss 0.468321, acc 0.84\n",
      "2018-04-17T05:44:33.145532: step 156, loss 0.481712, acc 0.8\n",
      "2018-04-17T05:44:33.165840: step 157, loss 0.541911, acc 0.69\n",
      "2018-04-17T05:44:33.186050: step 158, loss 0.527749, acc 0.77\n",
      "2018-04-17T05:44:33.206555: step 159, loss 0.539656, acc 0.73\n",
      "2018-04-17T05:44:33.226862: step 160, loss 0.499629, acc 0.76\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:33.321074: step 160, loss 0.963888, acc 0.491691, rec 0.990588, pre 0.48925, f1 0.654998\n",
      "\n",
      "2018-04-17T05:44:33.342992: step 161, loss 0.417743, acc 0.84\n",
      "2018-04-17T05:44:33.361896: step 162, loss 0.5079, acc 0.794521\n",
      "2018-04-17T05:44:33.382404: step 163, loss 0.449971, acc 0.82\n",
      "2018-04-17T05:44:33.403078: step 164, loss 0.448656, acc 0.82\n",
      "2018-04-17T05:44:33.423315: step 165, loss 0.487172, acc 0.73\n",
      "2018-04-17T05:44:33.443620: step 166, loss 0.480268, acc 0.81\n",
      "2018-04-17T05:44:33.463823: step 167, loss 0.453858, acc 0.84\n",
      "2018-04-17T05:44:33.483921: step 168, loss 0.458588, acc 0.8\n",
      "2018-04-17T05:44:33.504457: step 169, loss 0.389542, acc 0.86\n",
      "2018-04-17T05:44:33.524880: step 170, loss 0.563838, acc 0.78\n",
      "2018-04-17T05:44:33.541128: step 171, loss 0.579971, acc 0.780822\n",
      "2018-04-17T05:44:33.566975: step 172, loss 0.587644, acc 0.68\n",
      "2018-04-17T05:44:33.587208: step 173, loss 0.500847, acc 0.73\n",
      "2018-04-17T05:44:33.607913: step 174, loss 0.475779, acc 0.82\n",
      "2018-04-17T05:44:33.628069: step 175, loss 0.423914, acc 0.87\n",
      "2018-04-17T05:44:33.648645: step 176, loss 0.388014, acc 0.85\n",
      "2018-04-17T05:44:33.669030: step 177, loss 0.50055, acc 0.76\n",
      "2018-04-17T05:44:33.689591: step 178, loss 0.438335, acc 0.82\n",
      "2018-04-17T05:44:33.714592: step 179, loss 0.496579, acc 0.79\n",
      "2018-04-17T05:44:33.732300: step 180, loss 0.483878, acc 0.808219\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:33.831352: step 180, loss 1.09956, acc 0.488825, rec 0.992941, pre 0.487861, f1 0.654264\n",
      "\n",
      "2018-04-17T05:44:33.854003: step 181, loss 0.573013, acc 0.72\n",
      "2018-04-17T05:44:33.874745: step 182, loss 0.398211, acc 0.87\n",
      "2018-04-17T05:44:33.895033: step 183, loss 0.407372, acc 0.82\n",
      "2018-04-17T05:44:33.915617: step 184, loss 0.554332, acc 0.73\n",
      "2018-04-17T05:44:33.936669: step 185, loss 0.525985, acc 0.77\n",
      "2018-04-17T05:44:33.958509: step 186, loss 0.655573, acc 0.67\n",
      "2018-04-17T05:44:33.978928: step 187, loss 0.714966, acc 0.68\n",
      "2018-04-17T05:44:33.999418: step 188, loss 0.53987, acc 0.75\n",
      "2018-04-17T05:44:34.016657: step 189, loss 0.457688, acc 0.794521\n",
      "2018-04-17T05:44:34.041249: step 190, loss 0.40568, acc 0.87\n",
      "2018-04-17T05:44:34.062062: step 191, loss 0.413404, acc 0.85\n",
      "2018-04-17T05:44:34.083903: step 192, loss 0.447808, acc 0.83\n",
      "2018-04-17T05:44:34.105174: step 193, loss 0.445942, acc 0.85\n",
      "2018-04-17T05:44:34.126757: step 194, loss 0.476923, acc 0.75\n",
      "2018-04-17T05:44:34.147121: step 195, loss 0.456822, acc 0.78\n",
      "2018-04-17T05:44:34.167772: step 196, loss 0.462761, acc 0.8\n",
      "2018-04-17T05:44:34.189263: step 197, loss 0.528557, acc 0.84\n",
      "2018-04-17T05:44:34.208145: step 198, loss 0.475293, acc 0.767123\n",
      "2018-04-17T05:44:34.230574: step 199, loss 0.514509, acc 0.8\n",
      "2018-04-17T05:44:34.256515: step 200, loss 0.473196, acc 0.76\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:34.353940: step 200, loss 0.601871, acc 0.646991, rec 0.928235, pre 0.587054, f1 0.719234\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523943868/checkpoints/model-200\n",
      "\n",
      "2018-04-17T05:44:34.420288: step 201, loss 0.414881, acc 0.84\n",
      "2018-04-17T05:44:34.441877: step 202, loss 0.363486, acc 0.86\n",
      "2018-04-17T05:44:34.466841: step 203, loss 0.424309, acc 0.81\n",
      "2018-04-17T05:44:34.487945: step 204, loss 0.483932, acc 0.83\n",
      "2018-04-17T05:44:34.509999: step 205, loss 0.4396, acc 0.81\n",
      "2018-04-17T05:44:34.530825: step 206, loss 0.396251, acc 0.81\n",
      "2018-04-17T05:44:34.547459: step 207, loss 0.48052, acc 0.821918\n",
      "2018-04-17T05:44:34.568332: step 208, loss 0.429207, acc 0.86\n",
      "2018-04-17T05:44:34.588865: step 209, loss 0.471805, acc 0.79\n",
      "2018-04-17T05:44:34.609808: step 210, loss 0.476737, acc 0.79\n",
      "2018-04-17T05:44:34.630082: step 211, loss 0.435882, acc 0.84\n",
      "2018-04-17T05:44:34.650989: step 212, loss 0.438383, acc 0.81\n",
      "2018-04-17T05:44:34.677264: step 213, loss 0.544724, acc 0.73\n",
      "2018-04-17T05:44:34.701793: step 214, loss 0.54255, acc 0.72\n",
      "2018-04-17T05:44:34.735389: step 215, loss 0.487357, acc 0.78\n",
      "2018-04-17T05:44:34.751832: step 216, loss 0.459399, acc 0.767123\n",
      "2018-04-17T05:44:34.772038: step 217, loss 0.498822, acc 0.78\n",
      "2018-04-17T05:44:34.792680: step 218, loss 0.496863, acc 0.77\n",
      "2018-04-17T05:44:34.813046: step 219, loss 0.42496, acc 0.81\n",
      "2018-04-17T05:44:34.833237: step 220, loss 0.500309, acc 0.82\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:34.929533: step 220, loss 0.929879, acc 0.492837, rec 0.984706, pre 0.48976, f1 0.654162\n",
      "\n",
      "2018-04-17T05:44:34.951166: step 221, loss 0.424138, acc 0.83\n",
      "2018-04-17T05:44:34.971049: step 222, loss 0.44745, acc 0.89\n",
      "2018-04-17T05:44:34.991001: step 223, loss 0.377956, acc 0.82\n",
      "2018-04-17T05:44:35.011145: step 224, loss 0.413099, acc 0.86\n",
      "2018-04-17T05:44:35.027299: step 225, loss 0.460953, acc 0.767123\n",
      "2018-04-17T05:44:35.047804: step 226, loss 0.408274, acc 0.84\n",
      "2018-04-17T05:44:35.067749: step 227, loss 0.366878, acc 0.86\n",
      "2018-04-17T05:44:35.087980: step 228, loss 0.464524, acc 0.81\n",
      "2018-04-17T05:44:35.108142: step 229, loss 0.488074, acc 0.77\n",
      "2018-04-17T05:44:35.128458: step 230, loss 0.413429, acc 0.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17T05:44:35.151870: step 231, loss 0.505998, acc 0.76\n",
      "2018-04-17T05:44:35.172433: step 232, loss 0.433505, acc 0.87\n",
      "2018-04-17T05:44:35.192991: step 233, loss 0.477783, acc 0.8\n",
      "2018-04-17T05:44:35.209687: step 234, loss 0.472208, acc 0.794521\n",
      "2018-04-17T05:44:35.231043: step 235, loss 0.535786, acc 0.72\n",
      "2018-04-17T05:44:35.252677: step 236, loss 0.688999, acc 0.62\n",
      "2018-04-17T05:44:35.273453: step 237, loss 0.653346, acc 0.7\n",
      "2018-04-17T05:44:35.295432: step 238, loss 0.447335, acc 0.84\n",
      "2018-04-17T05:44:35.319200: step 239, loss 0.351795, acc 0.93\n",
      "2018-04-17T05:44:35.339548: step 240, loss 0.393656, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:35.437348: step 240, loss 0.563817, acc 0.725501, rec 0.876471, pre 0.665773, f1 0.756729\n",
      "\n",
      "2018-04-17T05:44:35.459164: step 241, loss 0.409059, acc 0.88\n",
      "2018-04-17T05:44:35.479815: step 242, loss 0.406653, acc 0.81\n",
      "2018-04-17T05:44:35.496495: step 243, loss 0.433361, acc 0.821918\n",
      "2018-04-17T05:44:35.517349: step 244, loss 0.449673, acc 0.79\n",
      "2018-04-17T05:44:35.537905: step 245, loss 0.490742, acc 0.81\n",
      "2018-04-17T05:44:35.559448: step 246, loss 0.39165, acc 0.84\n",
      "2018-04-17T05:44:35.581882: step 247, loss 0.32222, acc 0.91\n",
      "2018-04-17T05:44:35.604359: step 248, loss 0.443555, acc 0.78\n",
      "2018-04-17T05:44:35.625842: step 249, loss 0.535331, acc 0.78\n",
      "2018-04-17T05:44:35.650835: step 250, loss 0.563224, acc 0.74\n",
      "2018-04-17T05:44:35.671906: step 251, loss 0.43993, acc 0.78\n",
      "2018-04-17T05:44:35.688701: step 252, loss 0.379351, acc 0.835616\n",
      "2018-04-17T05:44:35.710957: step 253, loss 0.513722, acc 0.72\n",
      "2018-04-17T05:44:35.734598: step 254, loss 0.654877, acc 0.69\n",
      "2018-04-17T05:44:35.758109: step 255, loss 0.641036, acc 0.73\n",
      "2018-04-17T05:44:35.780147: step 256, loss 0.421895, acc 0.83\n",
      "2018-04-17T05:44:35.806191: step 257, loss 0.306615, acc 0.9\n",
      "2018-04-17T05:44:35.830041: step 258, loss 0.354182, acc 0.9\n",
      "2018-04-17T05:44:35.856482: step 259, loss 0.414566, acc 0.82\n",
      "2018-04-17T05:44:35.879051: step 260, loss 0.415808, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:35.984684: step 260, loss 0.833823, acc 0.498567, rec 0.982353, pre 0.492625, f1 0.656189\n",
      "\n",
      "2018-04-17T05:44:36.004094: step 261, loss 0.442807, acc 0.794521\n",
      "2018-04-17T05:44:36.027110: step 262, loss 0.481251, acc 0.83\n",
      "2018-04-17T05:44:36.049927: step 263, loss 0.392189, acc 0.84\n",
      "2018-04-17T05:44:36.074300: step 264, loss 0.443443, acc 0.8\n",
      "2018-04-17T05:44:36.095385: step 265, loss 0.419874, acc 0.83\n",
      "2018-04-17T05:44:36.115958: step 266, loss 0.39705, acc 0.88\n",
      "2018-04-17T05:44:36.136957: step 267, loss 0.347768, acc 0.88\n",
      "2018-04-17T05:44:36.158242: step 268, loss 0.373471, acc 0.85\n",
      "2018-04-17T05:44:36.179111: step 269, loss 0.462311, acc 0.84\n",
      "2018-04-17T05:44:36.196310: step 270, loss 0.536716, acc 0.726027\n",
      "2018-04-17T05:44:36.218627: step 271, loss 0.43872, acc 0.8\n",
      "2018-04-17T05:44:36.240534: step 272, loss 0.411129, acc 0.82\n",
      "2018-04-17T05:44:36.262629: step 273, loss 0.462001, acc 0.83\n",
      "2018-04-17T05:44:36.288372: step 274, loss 0.419173, acc 0.82\n",
      "2018-04-17T05:44:36.315161: step 275, loss 0.405774, acc 0.83\n",
      "2018-04-17T05:44:36.336196: step 276, loss 0.343536, acc 0.89\n",
      "2018-04-17T05:44:36.357563: step 277, loss 0.422248, acc 0.79\n",
      "2018-04-17T05:44:36.380417: step 278, loss 0.373524, acc 0.8\n",
      "2018-04-17T05:44:36.397829: step 279, loss 0.522898, acc 0.808219\n",
      "2018-04-17T05:44:36.418604: step 280, loss 0.458324, acc 0.78\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:36.528356: step 280, loss 0.517158, acc 0.793696, rec 0.741176, pre 0.818182, f1 0.777778\n",
      "\n",
      "2018-04-17T05:44:36.553759: step 281, loss 0.384023, acc 0.86\n",
      "2018-04-17T05:44:36.576373: step 282, loss 0.33926, acc 0.88\n",
      "2018-04-17T05:44:36.599612: step 283, loss 0.426271, acc 0.86\n",
      "2018-04-17T05:44:36.621838: step 284, loss 0.427958, acc 0.81\n",
      "2018-04-17T05:44:36.644625: step 285, loss 0.46405, acc 0.78\n",
      "2018-04-17T05:44:36.666998: step 286, loss 0.377155, acc 0.86\n",
      "2018-04-17T05:44:36.689431: step 287, loss 0.450481, acc 0.81\n",
      "2018-04-17T05:44:36.706139: step 288, loss 0.398075, acc 0.835616\n",
      "2018-04-17T05:44:36.733424: step 289, loss 0.334713, acc 0.89\n",
      "2018-04-17T05:44:36.754269: step 290, loss 0.457539, acc 0.75\n",
      "2018-04-17T05:44:36.775172: step 291, loss 0.473873, acc 0.78\n",
      "2018-04-17T05:44:36.796320: step 292, loss 0.409525, acc 0.81\n",
      "2018-04-17T05:44:36.817923: step 293, loss 0.42327, acc 0.8\n",
      "2018-04-17T05:44:36.840823: step 294, loss 0.360947, acc 0.87\n",
      "2018-04-17T05:44:36.861870: step 295, loss 0.452828, acc 0.78\n",
      "2018-04-17T05:44:36.883757: step 296, loss 0.373375, acc 0.86\n",
      "2018-04-17T05:44:36.900249: step 297, loss 0.301769, acc 0.917808\n",
      "2018-04-17T05:44:36.921722: step 298, loss 0.365556, acc 0.85\n",
      "2018-04-17T05:44:36.946139: step 299, loss 0.392081, acc 0.86\n",
      "2018-04-17T05:44:36.966383: step 300, loss 0.412448, acc 0.82\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:37.062503: step 300, loss 0.507948, acc 0.796562, rec 0.764706, pre 0.807453, f1 0.785498\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523943868/checkpoints/model-300\n",
      "\n",
      "2018-04-17T05:44:37.125316: step 301, loss 0.442791, acc 0.8\n",
      "2018-04-17T05:44:37.150464: step 302, loss 0.370017, acc 0.84\n",
      "2018-04-17T05:44:37.171539: step 303, loss 0.367151, acc 0.84\n",
      "2018-04-17T05:44:37.192282: step 304, loss 0.358752, acc 0.84\n",
      "2018-04-17T05:44:37.212826: step 305, loss 0.417794, acc 0.84\n",
      "2018-04-17T05:44:37.229671: step 306, loss 0.461112, acc 0.794521\n",
      "2018-04-17T05:44:37.251379: step 307, loss 0.397571, acc 0.82\n",
      "2018-04-17T05:44:37.273372: step 308, loss 0.334473, acc 0.88\n",
      "2018-04-17T05:44:37.294334: step 309, loss 0.364096, acc 0.83\n",
      "2018-04-17T05:44:37.316838: step 310, loss 0.404752, acc 0.85\n",
      "2018-04-17T05:44:37.337999: step 311, loss 0.511787, acc 0.76\n",
      "2018-04-17T05:44:37.362898: step 312, loss 0.387097, acc 0.84\n",
      "2018-04-17T05:44:37.385612: step 313, loss 0.417375, acc 0.82\n",
      "2018-04-17T05:44:37.406854: step 314, loss 0.373449, acc 0.82\n",
      "2018-04-17T05:44:37.424383: step 315, loss 0.517841, acc 0.739726\n",
      "2018-04-17T05:44:37.447022: step 316, loss 0.435409, acc 0.82\n",
      "2018-04-17T05:44:37.467428: step 317, loss 0.599294, acc 0.7\n",
      "2018-04-17T05:44:37.487983: step 318, loss 0.322301, acc 0.86\n",
      "2018-04-17T05:44:37.510099: step 319, loss 0.416076, acc 0.84\n",
      "2018-04-17T05:44:37.531857: step 320, loss 0.356679, acc 0.87\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:37.633005: step 320, loss 0.768592, acc 0.525501, rec 0.983529, pre 0.506667, f1 0.6688\n",
      "\n",
      "2018-04-17T05:44:37.656650: step 321, loss 0.389778, acc 0.8\n",
      "2018-04-17T05:44:37.677890: step 322, loss 0.409957, acc 0.83\n",
      "2018-04-17T05:44:37.699511: step 323, loss 0.397503, acc 0.82\n",
      "2018-04-17T05:44:37.716758: step 324, loss 0.344615, acc 0.876712\n",
      "2018-04-17T05:44:37.738056: step 325, loss 0.493909, acc 0.8\n",
      "2018-04-17T05:44:37.759404: step 326, loss 0.331816, acc 0.88\n",
      "2018-04-17T05:44:37.780725: step 327, loss 0.389819, acc 0.83\n",
      "2018-04-17T05:44:37.801974: step 328, loss 0.492224, acc 0.77\n",
      "2018-04-17T05:44:37.823175: step 329, loss 0.4129, acc 0.8\n",
      "2018-04-17T05:44:37.847058: step 330, loss 0.731943, acc 0.67\n",
      "2018-04-17T05:44:37.868132: step 331, loss 0.629201, acc 0.75\n",
      "2018-04-17T05:44:37.889978: step 332, loss 0.358246, acc 0.85\n",
      "2018-04-17T05:44:37.907057: step 333, loss 0.328817, acc 0.863014\n",
      "2018-04-17T05:44:37.929444: step 334, loss 0.370517, acc 0.83\n",
      "2018-04-17T05:44:37.950343: step 335, loss 0.401985, acc 0.8\n",
      "2018-04-17T05:44:37.972143: step 336, loss 0.396528, acc 0.83\n",
      "2018-04-17T05:44:37.992809: step 337, loss 0.419789, acc 0.82\n",
      "2018-04-17T05:44:38.013892: step 338, loss 0.342439, acc 0.84\n",
      "2018-04-17T05:44:38.036036: step 339, loss 0.305398, acc 0.9\n",
      "2018-04-17T05:44:38.060751: step 340, loss 0.383536, acc 0.83\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:38.156805: step 340, loss 0.523034, acc 0.739828, rec 0.528235, pre 0.894422, f1 0.664201\n",
      "\n",
      "2018-04-17T05:44:38.178792: step 341, loss 0.55466, acc 0.76\n",
      "2018-04-17T05:44:38.195919: step 342, loss 0.468638, acc 0.753425\n",
      "2018-04-17T05:44:38.217567: step 343, loss 0.442923, acc 0.84\n",
      "2018-04-17T05:44:38.237878: step 344, loss 0.410714, acc 0.85\n",
      "2018-04-17T05:44:38.258026: step 345, loss 0.36974, acc 0.88\n",
      "2018-04-17T05:44:38.283303: step 346, loss 0.407266, acc 0.8\n",
      "2018-04-17T05:44:38.305458: step 347, loss 0.332448, acc 0.89\n",
      "2018-04-17T05:44:38.325993: step 348, loss 0.350993, acc 0.82\n",
      "2018-04-17T05:44:38.346849: step 349, loss 0.368906, acc 0.85\n",
      "2018-04-17T05:44:38.368328: step 350, loss 0.361845, acc 0.81\n",
      "2018-04-17T05:44:38.386354: step 351, loss 0.370189, acc 0.821918\n",
      "2018-04-17T05:44:38.409857: step 352, loss 0.466629, acc 0.74\n",
      "2018-04-17T05:44:38.430541: step 353, loss 0.412789, acc 0.8\n",
      "2018-04-17T05:44:38.452981: step 354, loss 0.509074, acc 0.76\n",
      "2018-04-17T05:44:38.473731: step 355, loss 0.381392, acc 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17T05:44:38.500018: step 356, loss 0.409817, acc 0.79\n",
      "2018-04-17T05:44:38.520675: step 357, loss 0.429948, acc 0.84\n",
      "2018-04-17T05:44:38.542861: step 358, loss 0.334968, acc 0.88\n",
      "2018-04-17T05:44:38.563408: step 359, loss 0.313028, acc 0.93\n",
      "2018-04-17T05:44:38.579899: step 360, loss 0.367856, acc 0.863014\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:38.677417: step 360, loss 0.581095, acc 0.660745, rec 0.943529, pre 0.59584, f1 0.730419\n",
      "\n",
      "2018-04-17T05:44:38.704376: step 361, loss 0.356951, acc 0.85\n",
      "2018-04-17T05:44:38.729904: step 362, loss 0.344846, acc 0.88\n",
      "2018-04-17T05:44:38.750893: step 363, loss 0.472204, acc 0.81\n",
      "2018-04-17T05:44:38.771610: step 364, loss 0.358528, acc 0.82\n",
      "2018-04-17T05:44:38.793820: step 365, loss 0.430803, acc 0.79\n",
      "2018-04-17T05:44:38.815055: step 366, loss 0.395318, acc 0.82\n",
      "2018-04-17T05:44:38.838539: step 367, loss 0.309684, acc 0.87\n",
      "2018-04-17T05:44:38.859961: step 368, loss 0.35482, acc 0.86\n",
      "2018-04-17T05:44:38.884250: step 369, loss 0.298218, acc 0.890411\n",
      "2018-04-17T05:44:38.905314: step 370, loss 0.459691, acc 0.79\n",
      "2018-04-17T05:44:38.928429: step 371, loss 0.370713, acc 0.85\n",
      "2018-04-17T05:44:38.948876: step 372, loss 0.315225, acc 0.89\n",
      "2018-04-17T05:44:38.969025: step 373, loss 0.417109, acc 0.8\n",
      "2018-04-17T05:44:38.989354: step 374, loss 0.308363, acc 0.84\n",
      "2018-04-17T05:44:39.010034: step 375, loss 0.269075, acc 0.88\n",
      "2018-04-17T05:44:39.030740: step 376, loss 0.236546, acc 0.94\n",
      "2018-04-17T05:44:39.051883: step 377, loss 0.431569, acc 0.83\n",
      "2018-04-17T05:44:39.069050: step 378, loss 0.402349, acc 0.849315\n",
      "2018-04-17T05:44:39.090270: step 379, loss 0.274751, acc 0.93\n",
      "2018-04-17T05:44:39.110754: step 380, loss 0.347985, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:39.208645: step 380, loss 0.575989, acc 0.673926, rec 0.942353, pre 0.606359, f1 0.737909\n",
      "\n",
      "2018-04-17T05:44:39.230192: step 381, loss 0.3787, acc 0.85\n",
      "2018-04-17T05:44:39.250623: step 382, loss 0.351777, acc 0.89\n",
      "2018-04-17T05:44:39.270842: step 383, loss 0.483099, acc 0.77\n",
      "2018-04-17T05:44:39.291146: step 384, loss 0.594016, acc 0.7\n",
      "2018-04-17T05:44:39.313021: step 385, loss 0.346971, acc 0.86\n",
      "2018-04-17T05:44:39.333413: step 386, loss 0.313685, acc 0.87\n",
      "2018-04-17T05:44:39.349938: step 387, loss 0.411998, acc 0.794521\n",
      "2018-04-17T05:44:39.370845: step 388, loss 0.337379, acc 0.85\n",
      "2018-04-17T05:44:39.391697: step 389, loss 0.396195, acc 0.85\n",
      "2018-04-17T05:44:39.416802: step 390, loss 0.324391, acc 0.87\n",
      "2018-04-17T05:44:39.437060: step 391, loss 0.332523, acc 0.83\n",
      "2018-04-17T05:44:39.458136: step 392, loss 0.374585, acc 0.85\n",
      "2018-04-17T05:44:39.478995: step 393, loss 0.372447, acc 0.86\n",
      "2018-04-17T05:44:39.499586: step 394, loss 0.40275, acc 0.8\n",
      "2018-04-17T05:44:39.520394: step 395, loss 0.349349, acc 0.87\n",
      "2018-04-17T05:44:39.537366: step 396, loss 0.268388, acc 0.917808\n",
      "2018-04-17T05:44:39.575213: step 397, loss 0.38977, acc 0.84\n",
      "2018-04-17T05:44:39.612729: step 398, loss 0.289911, acc 0.87\n",
      "2018-04-17T05:44:39.653975: step 399, loss 0.340129, acc 0.85\n",
      "2018-04-17T05:44:39.690870: step 400, loss 0.306915, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:39.885433: step 400, loss 0.637909, acc 0.616046, rec 0.968235, pre 0.561392, f1 0.710708\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523943868/checkpoints/model-400\n",
      "\n",
      "2018-04-17T05:44:39.994175: step 401, loss 0.308957, acc 0.87\n",
      "2018-04-17T05:44:40.031270: step 402, loss 0.332127, acc 0.86\n",
      "2018-04-17T05:44:40.067958: step 403, loss 0.392887, acc 0.83\n",
      "2018-04-17T05:44:40.108843: step 404, loss 0.448565, acc 0.84\n",
      "2018-04-17T05:44:40.129944: step 405, loss 0.522131, acc 0.753425\n",
      "2018-04-17T05:44:40.151046: step 406, loss 0.458719, acc 0.75\n",
      "2018-04-17T05:44:40.172396: step 407, loss 0.453971, acc 0.82\n",
      "2018-04-17T05:44:40.196406: step 408, loss 0.389961, acc 0.9\n",
      "2018-04-17T05:44:40.219228: step 409, loss 0.454922, acc 0.81\n",
      "2018-04-17T05:44:40.242230: step 410, loss 0.380173, acc 0.84\n",
      "2018-04-17T05:44:40.264406: step 411, loss 0.44346, acc 0.79\n",
      "2018-04-17T05:44:40.289399: step 412, loss 0.545404, acc 0.75\n",
      "2018-04-17T05:44:40.315112: step 413, loss 0.475774, acc 0.81\n",
      "2018-04-17T05:44:40.332856: step 414, loss 0.284567, acc 0.90411\n",
      "2018-04-17T05:44:40.355232: step 415, loss 0.286272, acc 0.9\n",
      "2018-04-17T05:44:40.376429: step 416, loss 0.346744, acc 0.84\n",
      "2018-04-17T05:44:40.401280: step 417, loss 0.337759, acc 0.91\n",
      "2018-04-17T05:44:40.422104: step 418, loss 0.36723, acc 0.88\n",
      "2018-04-17T05:44:40.447498: step 419, loss 0.31415, acc 0.85\n",
      "2018-04-17T05:44:40.468654: step 420, loss 0.299956, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:40.569396: step 420, loss 0.796271, acc 0.530659, rec 0.984706, pre 0.509434, f1 0.67148\n",
      "\n",
      "2018-04-17T05:44:40.595781: step 421, loss 0.398422, acc 0.84\n",
      "2018-04-17T05:44:40.618073: step 422, loss 0.421126, acc 0.8\n",
      "2018-04-17T05:44:40.635577: step 423, loss 0.434344, acc 0.780822\n",
      "2018-04-17T05:44:40.656493: step 424, loss 0.395965, acc 0.84\n",
      "2018-04-17T05:44:40.676770: step 425, loss 0.418943, acc 0.85\n",
      "2018-04-17T05:44:40.701130: step 426, loss 0.413673, acc 0.85\n",
      "2018-04-17T05:44:40.722390: step 427, loss 0.30733, acc 0.86\n",
      "2018-04-17T05:44:40.743304: step 428, loss 0.315571, acc 0.87\n",
      "2018-04-17T05:44:40.764374: step 429, loss 0.386229, acc 0.83\n",
      "2018-04-17T05:44:40.787980: step 430, loss 0.405609, acc 0.85\n",
      "2018-04-17T05:44:40.808523: step 431, loss 0.333919, acc 0.85\n",
      "2018-04-17T05:44:40.825466: step 432, loss 0.307435, acc 0.876712\n",
      "2018-04-17T05:44:40.847846: step 433, loss 0.364797, acc 0.83\n",
      "2018-04-17T05:44:40.869440: step 434, loss 0.262707, acc 0.92\n",
      "2018-04-17T05:44:40.890758: step 435, loss 0.454611, acc 0.83\n",
      "2018-04-17T05:44:40.912486: step 436, loss 0.408243, acc 0.8\n",
      "2018-04-17T05:44:40.932775: step 437, loss 0.29647, acc 0.89\n",
      "2018-04-17T05:44:40.953364: step 438, loss 0.390529, acc 0.82\n",
      "2018-04-17T05:44:40.974344: step 439, loss 0.361662, acc 0.89\n",
      "2018-04-17T05:44:40.998826: step 440, loss 0.413705, acc 0.83\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:41.094047: step 440, loss 0.48474, acc 0.797135, rec 0.841176, pre 0.765525, f1 0.80157\n",
      "\n",
      "2018-04-17T05:44:41.111530: step 441, loss 0.303039, acc 0.849315\n",
      "2018-04-17T05:44:41.132595: step 442, loss 0.373917, acc 0.83\n",
      "2018-04-17T05:44:41.153173: step 443, loss 0.318445, acc 0.88\n",
      "2018-04-17T05:44:41.173839: step 444, loss 0.365068, acc 0.82\n",
      "2018-04-17T05:44:41.195815: step 445, loss 0.295786, acc 0.92\n",
      "2018-04-17T05:44:41.220405: step 446, loss 0.352547, acc 0.85\n",
      "2018-04-17T05:44:41.240685: step 447, loss 0.414589, acc 0.83\n",
      "2018-04-17T05:44:41.261994: step 448, loss 0.36855, acc 0.82\n",
      "2018-04-17T05:44:41.282878: step 449, loss 0.384815, acc 0.79\n",
      "2018-04-17T05:44:41.299492: step 450, loss 0.320007, acc 0.863014\n",
      "2018-04-17T05:44:41.320565: step 451, loss 0.240778, acc 0.9\n",
      "2018-04-17T05:44:41.341765: step 452, loss 0.327119, acc 0.86\n",
      "2018-04-17T05:44:41.362375: step 453, loss 0.404685, acc 0.85\n",
      "2018-04-17T05:44:41.383363: step 454, loss 0.497899, acc 0.76\n",
      "2018-04-17T05:44:41.405875: step 455, loss 0.352518, acc 0.85\n",
      "2018-04-17T05:44:41.429505: step 456, loss 0.313565, acc 0.86\n",
      "2018-04-17T05:44:41.450398: step 457, loss 0.325386, acc 0.89\n",
      "2018-04-17T05:44:41.471052: step 458, loss 0.280926, acc 0.92\n",
      "2018-04-17T05:44:41.487488: step 459, loss 0.367687, acc 0.863014\n",
      "2018-04-17T05:44:41.508353: step 460, loss 0.501594, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:41.604260: step 460, loss 0.492075, acc 0.770774, rec 0.595294, pre 0.900356, f1 0.716714\n",
      "\n",
      "2018-04-17T05:44:41.626346: step 461, loss 0.419279, acc 0.85\n",
      "2018-04-17T05:44:41.650656: step 462, loss 0.389288, acc 0.79\n",
      "2018-04-17T05:44:41.670889: step 463, loss 0.374005, acc 0.82\n",
      "2018-04-17T05:44:41.691435: step 464, loss 0.392281, acc 0.79\n",
      "2018-04-17T05:44:41.712154: step 465, loss 0.328087, acc 0.89\n",
      "2018-04-17T05:44:41.735727: step 466, loss 0.250964, acc 0.92\n",
      "2018-04-17T05:44:41.757281: step 467, loss 0.332939, acc 0.86\n",
      "2018-04-17T05:44:41.773306: step 468, loss 0.295859, acc 0.90411\n",
      "2018-04-17T05:44:41.793644: step 469, loss 0.3625, acc 0.86\n",
      "2018-04-17T05:44:41.813721: step 470, loss 0.338707, acc 0.89\n",
      "2018-04-17T05:44:41.833818: step 471, loss 0.285975, acc 0.89\n",
      "2018-04-17T05:44:41.858389: step 472, loss 0.263605, acc 0.9\n",
      "2018-04-17T05:44:41.879577: step 473, loss 0.507214, acc 0.82\n",
      "2018-04-17T05:44:41.900494: step 474, loss 0.263561, acc 0.92\n",
      "2018-04-17T05:44:41.920954: step 475, loss 0.396021, acc 0.83\n",
      "2018-04-17T05:44:41.941834: step 476, loss 0.299683, acc 0.89\n",
      "2018-04-17T05:44:41.958119: step 477, loss 0.323776, acc 0.835616\n",
      "2018-04-17T05:44:41.978392: step 478, loss 0.382725, acc 0.83\n",
      "2018-04-17T05:44:41.998493: step 479, loss 0.287073, acc 0.93\n",
      "2018-04-17T05:44:42.018757: step 480, loss 0.34295, acc 0.85\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17T05:44:42.115948: step 480, loss 0.809227, acc 0.529513, rec 0.989412, pre 0.508772, f1 0.671994\n",
      "\n",
      "2018-04-17T05:44:42.137808: step 481, loss 0.345881, acc 0.84\n",
      "2018-04-17T05:44:42.157957: step 482, loss 0.392612, acc 0.79\n",
      "2018-04-17T05:44:42.178183: step 483, loss 0.295618, acc 0.88\n",
      "2018-04-17T05:44:42.198497: step 484, loss 0.290149, acc 0.89\n",
      "2018-04-17T05:44:42.218972: step 485, loss 0.31511, acc 0.86\n",
      "2018-04-17T05:44:42.235273: step 486, loss 0.304912, acc 0.863014\n",
      "2018-04-17T05:44:42.255619: step 487, loss 0.343377, acc 0.86\n",
      "2018-04-17T05:44:42.276179: step 488, loss 0.353398, acc 0.84\n",
      "2018-04-17T05:44:42.296602: step 489, loss 0.299698, acc 0.86\n",
      "2018-04-17T05:44:42.323793: step 490, loss 0.339846, acc 0.87\n",
      "2018-04-17T05:44:42.344373: step 491, loss 0.370657, acc 0.89\n",
      "2018-04-17T05:44:42.365019: step 492, loss 0.325451, acc 0.88\n",
      "2018-04-17T05:44:42.385991: step 493, loss 0.219223, acc 0.93\n",
      "2018-04-17T05:44:42.410085: step 494, loss 0.323711, acc 0.86\n",
      "2018-04-17T05:44:42.429617: step 495, loss 0.318881, acc 0.849315\n",
      "2018-04-17T05:44:42.450369: step 496, loss 0.264633, acc 0.91\n",
      "2018-04-17T05:44:42.471010: step 497, loss 0.304162, acc 0.88\n",
      "2018-04-17T05:44:42.492978: step 498, loss 0.372664, acc 0.86\n",
      "2018-04-17T05:44:42.514023: step 499, loss 0.2345, acc 0.94\n",
      "2018-04-17T05:44:42.537427: step 500, loss 0.322249, acc 0.89\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:42.636802: step 500, loss 0.534773, acc 0.716905, rec 0.915294, pre 0.648333, f1 0.759024\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523943868/checkpoints/model-500\n",
      "\n",
      "2018-04-17T05:44:42.698930: step 501, loss 0.364795, acc 0.85\n",
      "2018-04-17T05:44:42.719849: step 502, loss 0.374719, acc 0.8\n",
      "2018-04-17T05:44:42.745154: step 503, loss 0.3802, acc 0.83\n",
      "2018-04-17T05:44:42.762697: step 504, loss 0.343214, acc 0.835616\n",
      "2018-04-17T05:44:42.784300: step 505, loss 0.216683, acc 0.93\n",
      "2018-04-17T05:44:42.805329: step 506, loss 0.438477, acc 0.88\n",
      "2018-04-17T05:44:42.826528: step 507, loss 0.290576, acc 0.87\n",
      "2018-04-17T05:44:42.849364: step 508, loss 0.326281, acc 0.87\n",
      "2018-04-17T05:44:42.870982: step 509, loss 0.32746, acc 0.87\n",
      "2018-04-17T05:44:42.892422: step 510, loss 0.381892, acc 0.81\n",
      "2018-04-17T05:44:42.913459: step 511, loss 0.445013, acc 0.82\n",
      "2018-04-17T05:44:42.934476: step 512, loss 0.314258, acc 0.88\n",
      "2018-04-17T05:44:42.954405: step 513, loss 0.310215, acc 0.835616\n",
      "2018-04-17T05:44:42.975267: step 514, loss 0.333015, acc 0.89\n",
      "2018-04-17T05:44:42.995417: step 515, loss 0.275831, acc 0.9\n",
      "2018-04-17T05:44:43.016195: step 516, loss 0.316153, acc 0.84\n",
      "2018-04-17T05:44:43.036597: step 517, loss 0.374357, acc 0.82\n",
      "2018-04-17T05:44:43.056922: step 518, loss 0.263429, acc 0.89\n",
      "2018-04-17T05:44:43.077009: step 519, loss 0.267278, acc 0.91\n",
      "2018-04-17T05:44:43.097871: step 520, loss 0.374438, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:43.192647: step 520, loss 0.481161, acc 0.779943, rec 0.882353, pre 0.725338, f1 0.796178\n",
      "\n",
      "2018-04-17T05:44:43.214250: step 521, loss 0.354168, acc 0.87\n",
      "2018-04-17T05:44:43.230440: step 522, loss 0.379009, acc 0.835616\n",
      "2018-04-17T05:44:43.250954: step 523, loss 0.380338, acc 0.87\n",
      "2018-04-17T05:44:43.271252: step 524, loss 0.276482, acc 0.91\n",
      "2018-04-17T05:44:43.291457: step 525, loss 0.310255, acc 0.88\n",
      "2018-04-17T05:44:43.311844: step 526, loss 0.334547, acc 0.84\n",
      "2018-04-17T05:44:43.332142: step 527, loss 0.359379, acc 0.85\n",
      "2018-04-17T05:44:43.352830: step 528, loss 0.385991, acc 0.84\n",
      "2018-04-17T05:44:43.373329: step 529, loss 0.360553, acc 0.86\n",
      "2018-04-17T05:44:43.397891: step 530, loss 0.29546, acc 0.87\n",
      "2018-04-17T05:44:43.414317: step 531, loss 0.289953, acc 0.90411\n",
      "2018-04-17T05:44:43.434824: step 532, loss 0.342073, acc 0.84\n",
      "2018-04-17T05:44:43.455482: step 533, loss 0.387442, acc 0.85\n",
      "2018-04-17T05:44:43.475638: step 534, loss 0.357881, acc 0.83\n",
      "2018-04-17T05:44:43.496697: step 535, loss 0.34257, acc 0.87\n",
      "2018-04-17T05:44:43.517462: step 536, loss 0.299822, acc 0.87\n",
      "2018-04-17T05:44:43.538197: step 537, loss 0.247545, acc 0.89\n",
      "2018-04-17T05:44:43.570094: step 538, loss 0.343866, acc 0.85\n",
      "2018-04-17T05:44:43.594618: step 539, loss 0.280937, acc 0.88\n",
      "2018-04-17T05:44:43.613477: step 540, loss 0.430365, acc 0.753425\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:43.707459: step 540, loss 0.489428, acc 0.770774, rec 0.9, pre 0.708333, f1 0.792746\n",
      "\n",
      "2018-04-17T05:44:43.729266: step 541, loss 0.289744, acc 0.86\n",
      "2018-04-17T05:44:43.750816: step 542, loss 0.418835, acc 0.87\n",
      "2018-04-17T05:44:43.771154: step 543, loss 0.348432, acc 0.84\n",
      "2018-04-17T05:44:43.791467: step 544, loss 0.430956, acc 0.79\n",
      "2018-04-17T05:44:43.811685: step 545, loss 0.323719, acc 0.84\n",
      "2018-04-17T05:44:43.835707: step 546, loss 0.265248, acc 0.88\n",
      "2018-04-17T05:44:43.856604: step 547, loss 0.285473, acc 0.87\n",
      "2018-04-17T05:44:43.877360: step 548, loss 0.258401, acc 0.92\n",
      "2018-04-17T05:44:43.893921: step 549, loss 0.266485, acc 0.876712\n",
      "2018-04-17T05:44:43.914711: step 550, loss 0.308568, acc 0.85\n",
      "2018-04-17T05:44:43.935309: step 551, loss 0.286668, acc 0.9\n",
      "2018-04-17T05:44:43.956270: step 552, loss 0.270323, acc 0.89\n",
      "2018-04-17T05:44:43.977610: step 553, loss 0.358607, acc 0.82\n",
      "2018-04-17T05:44:43.999959: step 554, loss 0.332342, acc 0.85\n",
      "2018-04-17T05:44:44.020915: step 555, loss 0.241596, acc 0.89\n",
      "2018-04-17T05:44:44.045485: step 556, loss 0.305846, acc 0.87\n",
      "2018-04-17T05:44:44.066403: step 557, loss 0.313287, acc 0.87\n",
      "2018-04-17T05:44:44.082977: step 558, loss 0.322668, acc 0.835616\n",
      "2018-04-17T05:44:44.103914: step 559, loss 0.363246, acc 0.85\n",
      "2018-04-17T05:44:44.124709: step 560, loss 0.411882, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:44.219927: step 560, loss 1.08704, acc 0.49914, rec 0.991765, pre 0.492982, f1 0.658594\n",
      "\n",
      "2018-04-17T05:44:44.241679: step 561, loss 0.333088, acc 0.84\n",
      "2018-04-17T05:44:44.265572: step 562, loss 0.274717, acc 0.9\n",
      "2018-04-17T05:44:44.286376: step 563, loss 0.248474, acc 0.93\n",
      "2018-04-17T05:44:44.307082: step 564, loss 0.3495, acc 0.84\n",
      "2018-04-17T05:44:44.327617: step 565, loss 0.241245, acc 0.91\n",
      "2018-04-17T05:44:44.348021: step 566, loss 0.288807, acc 0.86\n",
      "2018-04-17T05:44:44.364666: step 567, loss 0.289271, acc 0.90411\n",
      "2018-04-17T05:44:44.385392: step 568, loss 0.320563, acc 0.89\n",
      "2018-04-17T05:44:44.405758: step 569, loss 0.365966, acc 0.86\n",
      "2018-04-17T05:44:44.426237: step 570, loss 0.304732, acc 0.84\n",
      "2018-04-17T05:44:44.446480: step 571, loss 0.309971, acc 0.86\n",
      "2018-04-17T05:44:44.469254: step 572, loss 0.258253, acc 0.91\n",
      "2018-04-17T05:44:44.489455: step 573, loss 0.315854, acc 0.88\n",
      "2018-04-17T05:44:44.509807: step 574, loss 0.278861, acc 0.89\n",
      "2018-04-17T05:44:44.530086: step 575, loss 0.279213, acc 0.89\n",
      "2018-04-17T05:44:44.546421: step 576, loss 0.334886, acc 0.890411\n",
      "2018-04-17T05:44:44.566829: step 577, loss 0.319332, acc 0.89\n",
      "2018-04-17T05:44:44.587642: step 578, loss 0.318034, acc 0.89\n",
      "2018-04-17T05:44:44.608821: step 579, loss 0.352218, acc 0.9\n",
      "2018-04-17T05:44:44.629908: step 580, loss 0.338273, acc 0.87\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:44.728844: step 580, loss 0.466353, acc 0.794269, rec 0.870588, pre 0.748231, f1 0.804785\n",
      "\n",
      "2018-04-17T05:44:44.751834: step 581, loss 0.211427, acc 0.93\n",
      "2018-04-17T05:44:44.773537: step 582, loss 0.270984, acc 0.86\n",
      "2018-04-17T05:44:44.795245: step 583, loss 0.328443, acc 0.85\n",
      "2018-04-17T05:44:44.815820: step 584, loss 0.302807, acc 0.89\n",
      "2018-04-17T05:44:44.833148: step 585, loss 0.383006, acc 0.808219\n",
      "2018-04-17T05:44:44.854625: step 586, loss 0.328601, acc 0.88\n",
      "2018-04-17T05:44:44.875694: step 587, loss 0.26634, acc 0.88\n",
      "2018-04-17T05:44:44.896324: step 588, loss 0.263748, acc 0.92\n",
      "2018-04-17T05:44:44.918432: step 589, loss 0.266986, acc 0.89\n",
      "2018-04-17T05:44:44.943882: step 590, loss 0.341392, acc 0.87\n",
      "2018-04-17T05:44:44.964632: step 591, loss 0.39068, acc 0.84\n",
      "2018-04-17T05:44:44.986095: step 592, loss 0.252614, acc 0.91\n",
      "2018-04-17T05:44:45.006999: step 593, loss 0.270604, acc 0.89\n",
      "2018-04-17T05:44:45.023337: step 594, loss 0.410942, acc 0.821918\n",
      "2018-04-17T05:44:45.045313: step 595, loss 0.283168, acc 0.89\n",
      "2018-04-17T05:44:45.065874: step 596, loss 0.384892, acc 0.84\n",
      "2018-04-17T05:44:45.086394: step 597, loss 0.269255, acc 0.89\n",
      "2018-04-17T05:44:45.106922: step 598, loss 0.347259, acc 0.86\n",
      "2018-04-17T05:44:45.127748: step 599, loss 0.381928, acc 0.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17T05:44:45.150776: step 600, loss 0.516948, acc 0.79\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:45.246223: step 600, loss 0.4832, acc 0.768481, rec 0.589412, pre 0.901079, f1 0.71266\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523943868/checkpoints/model-600\n",
      "\n",
      "2018-04-17T05:44:45.312177: step 601, loss 0.321097, acc 0.86\n",
      "2018-04-17T05:44:45.333059: step 602, loss 0.34763, acc 0.81\n",
      "2018-04-17T05:44:45.349563: step 603, loss 0.233603, acc 0.931507\n",
      "2018-04-17T05:44:45.373684: step 604, loss 0.264111, acc 0.86\n",
      "2018-04-17T05:44:45.395311: step 605, loss 0.313541, acc 0.86\n",
      "2018-04-17T05:44:45.415952: step 606, loss 0.337978, acc 0.92\n",
      "2018-04-17T05:44:45.436988: step 607, loss 0.346709, acc 0.87\n",
      "2018-04-17T05:44:45.458107: step 608, loss 0.35488, acc 0.85\n",
      "2018-04-17T05:44:45.478494: step 609, loss 0.319072, acc 0.84\n",
      "2018-04-17T05:44:45.498655: step 610, loss 0.265014, acc 0.88\n",
      "2018-04-17T05:44:45.518662: step 611, loss 0.260612, acc 0.91\n",
      "2018-04-17T05:44:45.534872: step 612, loss 0.303591, acc 0.863014\n",
      "2018-04-17T05:44:45.556560: step 613, loss 0.177097, acc 0.93\n",
      "2018-04-17T05:44:45.580025: step 614, loss 0.355016, acc 0.87\n",
      "2018-04-17T05:44:45.601396: step 615, loss 0.277759, acc 0.89\n",
      "2018-04-17T05:44:45.621901: step 616, loss 0.256252, acc 0.86\n",
      "2018-04-17T05:44:45.642079: step 617, loss 0.304961, acc 0.89\n",
      "2018-04-17T05:44:45.662601: step 618, loss 0.386881, acc 0.84\n",
      "2018-04-17T05:44:45.683156: step 619, loss 0.329871, acc 0.83\n",
      "2018-04-17T05:44:45.704003: step 620, loss 0.294802, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:45.799172: step 620, loss 0.799541, acc 0.558166, rec 0.984706, pre 0.524765, f1 0.684663\n",
      "\n",
      "2018-04-17T05:44:45.816228: step 621, loss 0.366657, acc 0.821918\n",
      "2018-04-17T05:44:45.837210: step 622, loss 0.370712, acc 0.84\n",
      "2018-04-17T05:44:45.858709: step 623, loss 0.261724, acc 0.88\n",
      "2018-04-17T05:44:45.879524: step 624, loss 0.37863, acc 0.85\n",
      "2018-04-17T05:44:45.900731: step 625, loss 0.258938, acc 0.91\n",
      "2018-04-17T05:44:45.921418: step 626, loss 0.33739, acc 0.84\n",
      "2018-04-17T05:44:45.942030: step 627, loss 0.268571, acc 0.89\n",
      "2018-04-17T05:44:45.962284: step 628, loss 0.266266, acc 0.87\n",
      "2018-04-17T05:44:45.982863: step 629, loss 0.314363, acc 0.85\n",
      "2018-04-17T05:44:45.999518: step 630, loss 0.201285, acc 0.972603\n",
      "2018-04-17T05:44:46.023206: step 631, loss 0.25736, acc 0.91\n",
      "2018-04-17T05:44:46.044631: step 632, loss 0.326078, acc 0.88\n",
      "2018-04-17T05:44:46.065182: step 633, loss 0.367029, acc 0.82\n",
      "2018-04-17T05:44:46.085915: step 634, loss 0.285152, acc 0.91\n",
      "2018-04-17T05:44:46.106169: step 635, loss 0.235325, acc 0.89\n",
      "2018-04-17T05:44:46.126605: step 636, loss 0.32096, acc 0.89\n",
      "2018-04-17T05:44:46.146780: step 637, loss 0.312461, acc 0.87\n",
      "2018-04-17T05:44:46.166798: step 638, loss 0.226934, acc 0.92\n",
      "2018-04-17T05:44:46.183820: step 639, loss 0.324042, acc 0.849315\n",
      "2018-04-17T05:44:46.204880: step 640, loss 0.295138, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:46.302825: step 640, loss 0.516004, acc 0.744413, rec 0.938824, pre 0.669463, f1 0.781587\n",
      "\n",
      "2018-04-17T05:44:46.324780: step 641, loss 0.237307, acc 0.92\n",
      "2018-04-17T05:44:46.346394: step 642, loss 0.316954, acc 0.86\n",
      "2018-04-17T05:44:46.367637: step 643, loss 0.338062, acc 0.84\n",
      "2018-04-17T05:44:46.388505: step 644, loss 0.313194, acc 0.87\n",
      "2018-04-17T05:44:46.408951: step 645, loss 0.29857, acc 0.85\n",
      "2018-04-17T05:44:46.429312: step 646, loss 0.260627, acc 0.93\n",
      "2018-04-17T05:44:46.450038: step 647, loss 0.322691, acc 0.89\n",
      "2018-04-17T05:44:46.466573: step 648, loss 0.237314, acc 0.90411\n",
      "2018-04-17T05:44:46.487673: step 649, loss 0.298371, acc 0.86\n",
      "2018-04-17T05:44:46.511330: step 650, loss 0.274165, acc 0.89\n",
      "2018-04-17T05:44:46.531993: step 651, loss 0.281996, acc 0.9\n",
      "2018-04-17T05:44:46.553016: step 652, loss 0.326258, acc 0.84\n",
      "2018-04-17T05:44:46.574258: step 653, loss 0.273527, acc 0.86\n",
      "2018-04-17T05:44:46.596598: step 654, loss 0.341489, acc 0.86\n",
      "2018-04-17T05:44:46.617468: step 655, loss 0.314682, acc 0.9\n",
      "2018-04-17T05:44:46.638498: step 656, loss 0.266322, acc 0.9\n",
      "2018-04-17T05:44:46.655248: step 657, loss 0.346577, acc 0.808219\n",
      "2018-04-17T05:44:46.676827: step 658, loss 0.257798, acc 0.9\n",
      "2018-04-17T05:44:46.697698: step 659, loss 0.260704, acc 0.9\n",
      "2018-04-17T05:44:46.721702: step 660, loss 0.284481, acc 0.89\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:46.822854: step 660, loss 0.532699, acc 0.732951, rec 0.935294, pre 0.659204, f1 0.773346\n",
      "\n",
      "2018-04-17T05:44:46.844597: step 661, loss 0.23793, acc 0.93\n",
      "2018-04-17T05:44:46.864788: step 662, loss 0.269757, acc 0.88\n",
      "2018-04-17T05:44:46.885286: step 663, loss 0.301714, acc 0.87\n",
      "2018-04-17T05:44:46.906808: step 664, loss 0.299295, acc 0.87\n",
      "2018-04-17T05:44:46.932367: step 665, loss 0.349885, acc 0.86\n",
      "2018-04-17T05:44:46.949700: step 666, loss 0.377013, acc 0.821918\n",
      "2018-04-17T05:44:46.972177: step 667, loss 0.445443, acc 0.77\n",
      "2018-04-17T05:44:46.992460: step 668, loss 0.48584, acc 0.81\n",
      "2018-04-17T05:44:47.013767: step 669, loss 0.328057, acc 0.88\n",
      "2018-04-17T05:44:47.041305: step 670, loss 0.252313, acc 0.91\n",
      "2018-04-17T05:44:47.061748: step 671, loss 0.305984, acc 0.86\n",
      "2018-04-17T05:44:47.082519: step 672, loss 0.254816, acc 0.89\n",
      "2018-04-17T05:44:47.104167: step 673, loss 0.282181, acc 0.87\n",
      "2018-04-17T05:44:47.124305: step 674, loss 0.268987, acc 0.87\n",
      "2018-04-17T05:44:47.143633: step 675, loss 0.244048, acc 0.90411\n",
      "2018-04-17T05:44:47.164885: step 676, loss 0.319643, acc 0.86\n",
      "2018-04-17T05:44:47.185277: step 677, loss 0.30103, acc 0.88\n",
      "2018-04-17T05:44:47.207543: step 678, loss 0.216192, acc 0.92\n",
      "2018-04-17T05:44:47.227470: step 679, loss 0.264886, acc 0.88\n",
      "2018-04-17T05:44:47.247859: step 680, loss 0.312488, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:47.347566: step 680, loss 0.443896, acc 0.814327, rec 0.774118, pre 0.832911, f1 0.802439\n",
      "\n",
      "2018-04-17T05:44:47.370010: step 681, loss 0.374831, acc 0.86\n",
      "2018-04-17T05:44:47.391653: step 682, loss 0.231781, acc 0.91\n",
      "2018-04-17T05:44:47.412822: step 683, loss 0.233533, acc 0.9\n",
      "2018-04-17T05:44:47.430728: step 684, loss 0.384032, acc 0.821918\n",
      "2018-04-17T05:44:47.452438: step 685, loss 0.240968, acc 0.9\n",
      "2018-04-17T05:44:47.473909: step 686, loss 0.329063, acc 0.87\n",
      "2018-04-17T05:44:47.495118: step 687, loss 0.280227, acc 0.89\n",
      "2018-04-17T05:44:47.516369: step 688, loss 0.269018, acc 0.9\n",
      "2018-04-17T05:44:47.537344: step 689, loss 0.326921, acc 0.93\n",
      "2018-04-17T05:44:47.561189: step 690, loss 0.214224, acc 0.93\n",
      "2018-04-17T05:44:47.584491: step 691, loss 0.344271, acc 0.83\n",
      "2018-04-17T05:44:47.605508: step 692, loss 0.232929, acc 0.88\n",
      "2018-04-17T05:44:47.623652: step 693, loss 0.307511, acc 0.890411\n",
      "2018-04-17T05:44:47.645951: step 694, loss 0.333397, acc 0.87\n",
      "2018-04-17T05:44:47.667898: step 695, loss 0.210969, acc 0.94\n",
      "2018-04-17T05:44:47.689871: step 696, loss 0.267979, acc 0.88\n",
      "2018-04-17T05:44:47.710644: step 697, loss 0.257802, acc 0.9\n",
      "2018-04-17T05:44:47.731410: step 698, loss 0.231328, acc 0.88\n",
      "2018-04-17T05:44:47.752187: step 699, loss 0.265297, acc 0.9\n",
      "2018-04-17T05:44:47.776597: step 700, loss 0.273864, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:47.871358: step 700, loss 0.630788, acc 0.653295, rec 0.964706, pre 0.587814, f1 0.730512\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523943868/checkpoints/model-700\n",
      "\n",
      "2018-04-17T05:44:47.935961: step 701, loss 0.279702, acc 0.92\n",
      "2018-04-17T05:44:47.952602: step 702, loss 0.363164, acc 0.808219\n",
      "2018-04-17T05:44:47.973530: step 703, loss 0.241125, acc 0.92\n",
      "2018-04-17T05:44:47.997050: step 704, loss 0.352828, acc 0.85\n",
      "2018-04-17T05:44:48.017722: step 705, loss 0.306446, acc 0.9\n",
      "2018-04-17T05:44:48.038260: step 706, loss 0.242352, acc 0.9\n",
      "2018-04-17T05:44:48.058990: step 707, loss 0.214497, acc 0.91\n",
      "2018-04-17T05:44:48.079887: step 708, loss 0.2901, acc 0.88\n",
      "2018-04-17T05:44:48.100521: step 709, loss 0.196293, acc 0.95\n",
      "2018-04-17T05:44:48.121147: step 710, loss 0.235202, acc 0.91\n",
      "2018-04-17T05:44:48.138349: step 711, loss 0.330966, acc 0.821918\n",
      "2018-04-17T05:44:48.160084: step 712, loss 0.364259, acc 0.89\n",
      "2018-04-17T05:44:48.181219: step 713, loss 0.279266, acc 0.87\n",
      "2018-04-17T05:44:48.205888: step 714, loss 0.23323, acc 0.9\n",
      "2018-04-17T05:44:48.226768: step 715, loss 0.277531, acc 0.88\n",
      "2018-04-17T05:44:48.247430: step 716, loss 0.195048, acc 0.92\n",
      "2018-04-17T05:44:48.268764: step 717, loss 0.289153, acc 0.87\n",
      "2018-04-17T05:44:48.290747: step 718, loss 0.207801, acc 0.92\n",
      "2018-04-17T05:44:48.311024: step 719, loss 0.32794, acc 0.87\n",
      "2018-04-17T05:44:48.327352: step 720, loss 0.322069, acc 0.890411\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17T05:44:48.421945: step 720, loss 0.613746, acc 0.668768, rec 0.967059, pre 0.599125, f1 0.739874\n",
      "\n",
      "2018-04-17T05:44:48.444447: step 721, loss 0.312428, acc 0.88\n",
      "2018-04-17T05:44:48.465702: step 722, loss 0.221939, acc 0.91\n",
      "2018-04-17T05:44:48.486232: step 723, loss 0.284197, acc 0.87\n",
      "2018-04-17T05:44:48.506621: step 724, loss 0.288732, acc 0.89\n",
      "2018-04-17T05:44:48.527083: step 725, loss 0.338742, acc 0.86\n",
      "2018-04-17T05:44:48.548236: step 726, loss 0.21265, acc 0.9\n",
      "2018-04-17T05:44:48.568633: step 727, loss 0.186796, acc 0.96\n",
      "2018-04-17T05:44:48.589396: step 728, loss 0.409622, acc 0.8\n",
      "2018-04-17T05:44:48.605885: step 729, loss 0.252968, acc 0.90411\n",
      "2018-04-17T05:44:48.630421: step 730, loss 0.311581, acc 0.86\n",
      "2018-04-17T05:44:48.650806: step 731, loss 0.232219, acc 0.9\n",
      "2018-04-17T05:44:48.671136: step 732, loss 0.197253, acc 0.93\n",
      "2018-04-17T05:44:48.692272: step 733, loss 0.280776, acc 0.88\n",
      "2018-04-17T05:44:48.713409: step 734, loss 0.299755, acc 0.87\n",
      "2018-04-17T05:44:48.734107: step 735, loss 0.373135, acc 0.84\n",
      "2018-04-17T05:44:48.757292: step 736, loss 0.184673, acc 0.94\n",
      "2018-04-17T05:44:48.778341: step 737, loss 0.299819, acc 0.88\n",
      "2018-04-17T05:44:48.795177: step 738, loss 0.27739, acc 0.890411\n",
      "2018-04-17T05:44:48.816512: step 739, loss 0.252496, acc 0.89\n",
      "2018-04-17T05:44:48.840620: step 740, loss 0.296626, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:48.936112: step 740, loss 0.673647, acc 0.638395, rec 0.983529, pre 0.575361, f1 0.72601\n",
      "\n",
      "2018-04-17T05:44:48.958428: step 741, loss 0.200404, acc 0.92\n",
      "2018-04-17T05:44:48.980924: step 742, loss 0.281805, acc 0.85\n",
      "2018-04-17T05:44:49.001469: step 743, loss 0.255533, acc 0.91\n",
      "2018-04-17T05:44:49.022426: step 744, loss 0.333431, acc 0.89\n",
      "2018-04-17T05:44:49.046170: step 745, loss 0.375448, acc 0.85\n",
      "2018-04-17T05:44:49.067075: step 746, loss 0.213276, acc 0.91\n",
      "2018-04-17T05:44:49.083279: step 747, loss 0.272616, acc 0.90411\n",
      "2018-04-17T05:44:49.103834: step 748, loss 0.18805, acc 0.94\n",
      "2018-04-17T05:44:49.123663: step 749, loss 0.248922, acc 0.89\n",
      "2018-04-17T05:44:49.143713: step 750, loss 0.313081, acc 0.83\n",
      "2018-04-17T05:44:49.164277: step 751, loss 0.322269, acc 0.85\n",
      "2018-04-17T05:44:49.184658: step 752, loss 0.355113, acc 0.84\n",
      "2018-04-17T05:44:49.205250: step 753, loss 0.417285, acc 0.81\n",
      "2018-04-17T05:44:49.225512: step 754, loss 0.4664, acc 0.83\n",
      "2018-04-17T05:44:49.245661: step 755, loss 0.276859, acc 0.88\n",
      "2018-04-17T05:44:49.266050: step 756, loss 0.223258, acc 0.890411\n",
      "2018-04-17T05:44:49.291279: step 757, loss 0.319012, acc 0.88\n",
      "2018-04-17T05:44:49.313593: step 758, loss 0.414956, acc 0.82\n",
      "2018-04-17T05:44:49.334130: step 759, loss 0.445789, acc 0.81\n",
      "2018-04-17T05:44:49.354280: step 760, loss 0.462899, acc 0.77\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:49.448597: step 760, loss 0.461925, acc 0.779943, rec 0.621176, pre 0.894915, f1 0.733333\n",
      "\n",
      "2018-04-17T05:44:49.472885: step 761, loss 0.326026, acc 0.88\n",
      "2018-04-17T05:44:49.493125: step 762, loss 0.295113, acc 0.88\n",
      "2018-04-17T05:44:49.513580: step 763, loss 0.21913, acc 0.91\n",
      "2018-04-17T05:44:49.534389: step 764, loss 0.165539, acc 0.96\n",
      "2018-04-17T05:44:49.552966: step 765, loss 0.384475, acc 0.835616\n",
      "2018-04-17T05:44:49.573910: step 766, loss 0.253929, acc 0.89\n",
      "2018-04-17T05:44:49.595290: step 767, loss 0.250007, acc 0.89\n",
      "2018-04-17T05:44:49.615546: step 768, loss 0.3541, acc 0.83\n",
      "2018-04-17T05:44:49.635666: step 769, loss 0.223399, acc 0.91\n",
      "2018-04-17T05:44:49.656191: step 770, loss 0.257896, acc 0.89\n",
      "2018-04-17T05:44:49.679497: step 771, loss 0.244905, acc 0.91\n",
      "2018-04-17T05:44:49.700438: step 772, loss 0.186865, acc 0.93\n",
      "2018-04-17T05:44:49.720711: step 773, loss 0.259635, acc 0.94\n",
      "2018-04-17T05:44:49.736715: step 774, loss 0.358683, acc 0.876712\n",
      "2018-04-17T05:44:49.757159: step 775, loss 0.278653, acc 0.9\n",
      "2018-04-17T05:44:49.777488: step 776, loss 0.280953, acc 0.86\n",
      "2018-04-17T05:44:49.797778: step 777, loss 0.257061, acc 0.9\n",
      "2018-04-17T05:44:49.817937: step 778, loss 0.29434, acc 0.87\n",
      "2018-04-17T05:44:49.837977: step 779, loss 0.200059, acc 0.94\n",
      "2018-04-17T05:44:49.858497: step 780, loss 0.28862, acc 0.87\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:49.956346: step 780, loss 0.857229, acc 0.555874, rec 0.987059, pre 0.523394, f1 0.68406\n",
      "\n",
      "2018-04-17T05:44:49.978105: step 781, loss 0.291945, acc 0.87\n",
      "2018-04-17T05:44:49.998517: step 782, loss 0.257428, acc 0.88\n",
      "2018-04-17T05:44:50.015134: step 783, loss 0.304174, acc 0.876712\n",
      "2018-04-17T05:44:50.036465: step 784, loss 0.240129, acc 0.89\n",
      "2018-04-17T05:44:50.057708: step 785, loss 0.178765, acc 0.95\n",
      "2018-04-17T05:44:50.078410: step 786, loss 0.200306, acc 0.91\n",
      "2018-04-17T05:44:50.099145: step 787, loss 0.206854, acc 0.91\n",
      "2018-04-17T05:44:50.119543: step 788, loss 0.281506, acc 0.87\n",
      "2018-04-17T05:44:50.139999: step 789, loss 0.266193, acc 0.9\n",
      "2018-04-17T05:44:50.163998: step 790, loss 0.32932, acc 0.87\n",
      "2018-04-17T05:44:50.184516: step 791, loss 0.272587, acc 0.91\n",
      "2018-04-17T05:44:50.201404: step 792, loss 0.382696, acc 0.794521\n",
      "2018-04-17T05:44:50.222857: step 793, loss 0.368251, acc 0.85\n",
      "2018-04-17T05:44:50.243729: step 794, loss 0.457876, acc 0.78\n",
      "2018-04-17T05:44:50.264432: step 795, loss 0.282212, acc 0.87\n",
      "2018-04-17T05:44:50.284980: step 796, loss 0.226284, acc 0.9\n",
      "2018-04-17T05:44:50.305148: step 797, loss 0.301037, acc 0.87\n",
      "2018-04-17T05:44:50.325875: step 798, loss 0.247055, acc 0.9\n",
      "2018-04-17T05:44:50.346305: step 799, loss 0.342152, acc 0.89\n",
      "2018-04-17T05:44:50.370110: step 800, loss 0.26323, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:50.463819: step 800, loss 0.549983, acc 0.717479, rec 0.932941, pre 0.64524, f1 0.762867\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523943868/checkpoints/model-800\n",
      "\n",
      "2018-04-17T05:44:50.523227: step 801, loss 0.25414, acc 0.890411\n",
      "2018-04-17T05:44:50.544054: step 802, loss 0.234309, acc 0.88\n",
      "2018-04-17T05:44:50.564294: step 803, loss 0.22963, acc 0.88\n",
      "2018-04-17T05:44:50.587905: step 804, loss 0.259715, acc 0.89\n",
      "2018-04-17T05:44:50.608207: step 805, loss 0.379817, acc 0.83\n",
      "2018-04-17T05:44:50.628476: step 806, loss 0.295777, acc 0.87\n",
      "2018-04-17T05:44:50.648728: step 807, loss 0.272454, acc 0.9\n",
      "2018-04-17T05:44:50.668776: step 808, loss 0.215127, acc 0.92\n",
      "2018-04-17T05:44:50.689013: step 809, loss 0.261915, acc 0.87\n",
      "2018-04-17T05:44:50.705362: step 810, loss 0.147001, acc 0.972603\n",
      "2018-04-17T05:44:50.726215: step 811, loss 0.357918, acc 0.89\n",
      "2018-04-17T05:44:50.749058: step 812, loss 0.213618, acc 0.93\n",
      "2018-04-17T05:44:50.769631: step 813, loss 0.199182, acc 0.9\n",
      "2018-04-17T05:44:50.792670: step 814, loss 0.20134, acc 0.94\n",
      "2018-04-17T05:44:50.813063: step 815, loss 0.279855, acc 0.9\n",
      "2018-04-17T05:44:50.833777: step 816, loss 0.25918, acc 0.9\n",
      "2018-04-17T05:44:50.854314: step 817, loss 0.327167, acc 0.88\n",
      "2018-04-17T05:44:50.874926: step 818, loss 0.269301, acc 0.88\n",
      "2018-04-17T05:44:50.892626: step 819, loss 0.310746, acc 0.890411\n",
      "2018-04-17T05:44:50.913366: step 820, loss 0.282828, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:51.009414: step 820, loss 0.60694, acc 0.684814, rec 0.961176, pre 0.612444, f1 0.748168\n",
      "\n",
      "2018-04-17T05:44:51.031580: step 821, loss 0.23177, acc 0.91\n",
      "2018-04-17T05:44:51.051864: step 822, loss 0.285134, acc 0.85\n",
      "2018-04-17T05:44:51.072203: step 823, loss 0.315984, acc 0.88\n",
      "2018-04-17T05:44:51.093462: step 824, loss 0.217172, acc 0.92\n",
      "2018-04-17T05:44:51.113848: step 825, loss 0.243123, acc 0.92\n",
      "2018-04-17T05:44:51.133998: step 826, loss 0.183902, acc 0.91\n",
      "2018-04-17T05:44:51.154938: step 827, loss 0.235373, acc 0.9\n",
      "2018-04-17T05:44:51.171188: step 828, loss 0.201835, acc 0.931507\n",
      "2018-04-17T05:44:51.191887: step 829, loss 0.324123, acc 0.86\n",
      "2018-04-17T05:44:51.218446: step 830, loss 0.245353, acc 0.9\n",
      "2018-04-17T05:44:51.239429: step 831, loss 0.325913, acc 0.88\n",
      "2018-04-17T05:44:51.260608: step 832, loss 0.34568, acc 0.86\n",
      "2018-04-17T05:44:51.282569: step 833, loss 0.19963, acc 0.91\n",
      "2018-04-17T05:44:51.305755: step 834, loss 0.257212, acc 0.9\n",
      "2018-04-17T05:44:51.327213: step 835, loss 0.158625, acc 0.95\n",
      "2018-04-17T05:44:51.348205: step 836, loss 0.213102, acc 0.91\n",
      "2018-04-17T05:44:51.365066: step 837, loss 0.270579, acc 0.876712\n",
      "2018-04-17T05:44:51.386122: step 838, loss 0.166842, acc 0.93\n",
      "2018-04-17T05:44:51.407453: step 839, loss 0.224815, acc 0.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17T05:44:51.432201: step 840, loss 0.26608, acc 0.89\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:51.528759: step 840, loss 0.9307, acc 0.558166, rec 0.977647, pre 0.524953, f1 0.683107\n",
      "\n",
      "2018-04-17T05:44:51.552454: step 841, loss 0.246982, acc 0.9\n",
      "2018-04-17T05:44:51.573716: step 842, loss 0.284419, acc 0.89\n",
      "2018-04-17T05:44:51.595346: step 843, loss 0.285982, acc 0.85\n",
      "2018-04-17T05:44:51.616248: step 844, loss 0.325203, acc 0.89\n",
      "2018-04-17T05:44:51.640748: step 845, loss 0.198668, acc 0.92\n",
      "2018-04-17T05:44:51.657472: step 846, loss 0.222954, acc 0.890411\n",
      "2018-04-17T05:44:51.678569: step 847, loss 0.206, acc 0.92\n",
      "2018-04-17T05:44:51.699682: step 848, loss 0.255468, acc 0.91\n",
      "2018-04-17T05:44:51.720951: step 849, loss 0.210741, acc 0.9\n",
      "2018-04-17T05:44:51.742211: step 850, loss 0.324878, acc 0.85\n",
      "2018-04-17T05:44:51.763227: step 851, loss 0.349327, acc 0.83\n",
      "2018-04-17T05:44:51.783676: step 852, loss 0.340765, acc 0.83\n",
      "2018-04-17T05:44:51.804584: step 853, loss 0.195249, acc 0.92\n",
      "2018-04-17T05:44:51.825496: step 854, loss 0.300641, acc 0.87\n",
      "2018-04-17T05:44:51.845159: step 855, loss 0.258746, acc 0.890411\n",
      "2018-04-17T05:44:51.866296: step 856, loss 0.323145, acc 0.86\n",
      "2018-04-17T05:44:51.886802: step 857, loss 0.286978, acc 0.85\n",
      "2018-04-17T05:44:51.907126: step 858, loss 0.243223, acc 0.91\n",
      "2018-04-17T05:44:51.927536: step 859, loss 0.237802, acc 0.9\n",
      "2018-04-17T05:44:51.947972: step 860, loss 0.234587, acc 0.89\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:52.042301: step 860, loss 0.44059, acc 0.809742, rec 0.862353, pre 0.773207, f1 0.81535\n",
      "\n",
      "2018-04-17T05:44:52.067237: step 861, loss 0.217691, acc 0.93\n",
      "2018-04-17T05:44:52.089444: step 862, loss 0.1588, acc 0.93\n",
      "2018-04-17T05:44:52.110085: step 863, loss 0.173794, acc 0.92\n",
      "2018-04-17T05:44:52.126399: step 864, loss 0.292991, acc 0.849315\n",
      "2018-04-17T05:44:52.146725: step 865, loss 0.268239, acc 0.86\n",
      "2018-04-17T05:44:52.166573: step 866, loss 0.271203, acc 0.89\n",
      "2018-04-17T05:44:52.187006: step 867, loss 0.20795, acc 0.9\n",
      "2018-04-17T05:44:52.207595: step 868, loss 0.243983, acc 0.89\n",
      "2018-04-17T05:44:52.228391: step 869, loss 0.25168, acc 0.85\n",
      "2018-04-17T05:44:52.249056: step 870, loss 0.252811, acc 0.86\n",
      "2018-04-17T05:44:52.272030: step 871, loss 0.207398, acc 0.93\n",
      "2018-04-17T05:44:52.292383: step 872, loss 0.188203, acc 0.97\n",
      "2018-04-17T05:44:52.308803: step 873, loss 0.364012, acc 0.863014\n",
      "2018-04-17T05:44:52.329640: step 874, loss 0.246745, acc 0.89\n",
      "2018-04-17T05:44:52.350015: step 875, loss 0.136538, acc 0.95\n",
      "2018-04-17T05:44:52.370185: step 876, loss 0.150429, acc 0.96\n",
      "2018-04-17T05:44:52.390777: step 877, loss 0.245682, acc 0.9\n",
      "2018-04-17T05:44:52.411102: step 878, loss 0.374159, acc 0.85\n",
      "2018-04-17T05:44:52.431877: step 879, loss 0.230773, acc 0.9\n",
      "2018-04-17T05:44:52.452672: step 880, loss 0.234771, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:52.550136: step 880, loss 1.03921, acc 0.532378, rec 0.985882, pre 0.510353, f1 0.672552\n",
      "\n",
      "2018-04-17T05:44:52.572605: step 881, loss 0.273611, acc 0.86\n",
      "2018-04-17T05:44:52.589013: step 882, loss 0.258148, acc 0.917808\n",
      "2018-04-17T05:44:52.609361: step 883, loss 0.254829, acc 0.89\n",
      "2018-04-17T05:44:52.630928: step 884, loss 0.339736, acc 0.85\n",
      "2018-04-17T05:44:52.651526: step 885, loss 0.26801, acc 0.92\n",
      "2018-04-17T05:44:52.672672: step 886, loss 0.250561, acc 0.91\n",
      "2018-04-17T05:44:52.693187: step 887, loss 0.284099, acc 0.87\n",
      "2018-04-17T05:44:52.713617: step 888, loss 0.214368, acc 0.92\n",
      "2018-04-17T05:44:52.734246: step 889, loss 0.211993, acc 0.9\n",
      "2018-04-17T05:44:52.760269: step 890, loss 0.236197, acc 0.89\n",
      "2018-04-17T05:44:52.776721: step 891, loss 0.274367, acc 0.876712\n",
      "2018-04-17T05:44:52.797380: step 892, loss 0.244618, acc 0.88\n",
      "2018-04-17T05:44:52.817905: step 893, loss 0.197833, acc 0.91\n",
      "2018-04-17T05:44:52.837932: step 894, loss 0.235382, acc 0.91\n",
      "2018-04-17T05:44:52.859191: step 895, loss 0.226022, acc 0.91\n",
      "2018-04-17T05:44:52.879852: step 896, loss 0.192847, acc 0.94\n",
      "2018-04-17T05:44:52.901143: step 897, loss 0.252682, acc 0.89\n",
      "2018-04-17T05:44:52.922000: step 898, loss 0.213687, acc 0.93\n",
      "2018-04-17T05:44:52.943004: step 899, loss 0.169, acc 0.94\n",
      "2018-04-17T05:44:52.959680: step 900, loss 0.415746, acc 0.849315\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:44:53.059926: step 900, loss 0.43314, acc 0.806877, rec 0.705882, pre 0.873362, f1 0.780742\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523943868/checkpoints/model-900\n",
      "\n",
      "\n",
      "Test Set:\n",
      "2018-04-17T05:44:53.147880: step 900, loss 0.418632, acc 0.813287, rec 0.735426, pre 0.879357, f1 0.800977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", 0.75, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "\n",
    "# embedding of 60 is best so far\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\",50, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"1\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 50, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.4, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 100\n",
    "tf.flags.DEFINE_integer(\"batch_size\", batch_size, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 20, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=train_s.shape[1],\n",
    "            num_classes=2,\n",
    "            vocab_size=vocabulary_size,\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "        \n",
    "         # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        #vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch1, x_batch2, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch1, x_batch2, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            #r \n",
    "            step, summaries, loss, accuracy, recall, precision, f1 = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy,cnn.recall, cnn.precision, cnn.f1_score],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, rec {:g}, pre {:g}, f1 {:g}\".format(time_str, step, loss, accuracy, recall, precision, f1))# recall, precision, f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(train_s, train_c,  expanded_train_labels)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch1, x_batch2, y_batch = zip(*batch)\n",
    "            train_step(x_batch1, x_batch1, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(validation_s, validation_c, expanded_validation_labels, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "print(\"\\nTest Set:\")\n",
    "dev_step(test_s, test_c, expanded_test_labels, writer=dev_summary_writer)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
