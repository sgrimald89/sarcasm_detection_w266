{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saulgrimaldo1/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from w266_common import utils, vocabulary\n",
    "import time\n",
    "import datetime\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "tokenizer = TweetTokenizer()\n",
    "x_data = []\n",
    "x_contexts = []\n",
    "labels = []\n",
    "sentences  = []\n",
    "contexts = []\n",
    "with open('dta/merged_data_v2.csv', 'r') as csvfile:\n",
    "    linereader = csv.reader(csvfile, delimiter = '|')\n",
    "    for i, row in enumerate(linereader):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        sentence, context, sarcasm = row\n",
    "        sentences.append(sentence)\n",
    "        contexts.append(context)\n",
    "        x_tokens = utils.canonicalize_words(tokenizer.tokenize(sentence))\n",
    "        context_tokens = utils.canonicalize_words(tokenizer.tokenize(context))\n",
    "        x_data.append(x_tokens)\n",
    "        x_contexts.append(context_tokens)\n",
    "        labels.append(int(sarcasm))\n",
    "\n",
    "\n",
    "#rng = np.random.RandomState(5)\n",
    "#rng.shuffle(x_data)  # in-place\n",
    "#train_split_idx = int(0.7 * len(labels))\n",
    "#test_split_idx  = int(0.9 * len(labels))\n",
    "\n",
    "shuffle_indices = np.random.permutation(np.arange(len(labels)))\n",
    "train_split_idx = int(0.7 * len(labels))\n",
    "test_split_idx  = int(0.9 * len(labels))\n",
    "\n",
    "train_indices = shuffle_indices[:train_split_idx]\n",
    "validation_indices = shuffle_indices[train_split_idx:test_split_idx]\n",
    "test_indices = shuffle_indices[test_split_idx:]\n",
    "\n",
    "\n",
    "train_sentences = np.array(x_data)[train_indices]\n",
    "train_contexts = np.array(x_contexts)[train_indices]\n",
    "train_labels= np.array(labels)[train_indices] \n",
    "validation_sentences = np.array(x_data)[validation_indices]\n",
    "validation_labels = np.array(labels)[validation_indices]\n",
    "validation_contexts = train_contexts = np.array(x_contexts)[validation_indices]\n",
    "test_sentences = np.array(x_data)[test_indices]  \n",
    "test_contexts = train_contexts = np.array(x_contexts)[test_indices]\n",
    "test_labels = np.array(labels)[test_indices]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 6, 2]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [2]*4\n",
    "a[2] = 6\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_labels(raw_label_set, size):\n",
    "    label_set = []\n",
    "    for label in raw_label_set:\n",
    "        labels = [0] * size\n",
    "        labels[label] = 1\n",
    "        label_set.append(labels)\n",
    "    return np.array(label_set)\n",
    "\n",
    "expanded_train_labels = transform_labels(train_labels, 2)\n",
    "expanded_validation_labels = transform_labels(validation_labels,2)\n",
    "expanded_test_labels = transform_labels(test_labels,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4, 5, 6, 7, 8, 8, 1, 5, 6, 7]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,3,4,5,6,7,8,8,1,5,6,7]\n",
    "a + [\"<PADDING>\"]*0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'angry',\n",
       " 'man',\n",
       " 'is',\n",
       " 'angry',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PaddingAndTruncating:\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def pad_or_truncate(self, sentence):\n",
    "        sen_len = len(sentence)\n",
    "        paddings = self.max_len - sen_len\n",
    "        if paddings >=0:\n",
    "            return sentence + [\"<PADDING>\"] * paddings\n",
    "        return sentence[0:paddings]\n",
    "        \n",
    "PadAndTrunc = PaddingAndTruncating(10)\n",
    "        \n",
    "        \n",
    "PadAndTrunc.pad_or_truncate([\"the\",\"angry\",\"man\",\"is\",\"angry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  10,    4,    7, ...,    3,    3,    3],\n",
       "       [  10,    4,    7, ...,   14,    3,    3],\n",
       "       [2387,  246,   22, ..., 1691, 2391,    5],\n",
       "       ..., \n",
       "       [  10,    4,    7, ...,    3,    3,    3],\n",
       "       [   4,  130,    9, ...,  436,  849,  183],\n",
       "       [   2,    2,   15, ...,    3,    3,    3]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.contrib import learn\n",
    "\n",
    "#max_len = max([len(sent) for sent  in train_sentences])\n",
    "PadAndTrunc =PaddingAndTruncating(30)\n",
    "train_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, train_sentences))\n",
    "train_context_padded = list(map(PadAndTrunc.pad_or_truncate, train_contexts))\n",
    "validation_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, validation_sentences))\n",
    "validation_context_padded = list(map(PadAndTrunc.pad_or_truncate, validation_contexts))\n",
    "test_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, test_sentences))\n",
    "test_context_padded = list(map(PadAndTrunc.pad_or_truncate, test_contexts))\n",
    "\n",
    "vocab = vocabulary.Vocabulary(utils.flatten(list(train_sentences_padded) + list(train_context_padded)), 2500)\n",
    "train_s = np.array(list(map(vocab.words_to_ids, train_sentences_padded)))\n",
    "train_c = np.array(list(map(vocab.words_to_ids, train_context_padded)))\n",
    "validation_s = np.array(list(map(vocab.words_to_ids, validation_sentences_padded)))\n",
    "validation_c = np.array(list(map(vocab.words_to_ids, validation_context_padded)))\n",
    "test_s = np.array(list(map(vocab.words_to_ids, test_sentences_padded)))\n",
    "test_c = np.array(list(map(vocab.words_to_ids, test_context_padded)))\n",
    "train_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv-maxpool-3'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 3\n",
    "(\"conv-maxpool-%s\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size, \n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x1 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x1\")\n",
    "        self.input_x2 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x2\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars1 = tf.nn.embedding_lookup(self.W, self.input_x1)\n",
    "            self.embedded_chars_expanded1 = tf.expand_dims(self.embedded_chars1, -1)\n",
    "            self.embedded_chars2 = tf.nn.embedding_lookup(self.W, self.input_x2)\n",
    "            self.embedded_chars_expanded2 = tf.expand_dims(self.embedded_chars2, -1)\n",
    "\n",
    "        # Create a convolution + avgpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.variable_scope(\"conv-avgpool-%s\" % i) as scope:\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "              \n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded1,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h1 = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"tanh1\")\n",
    "               \n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.avg_pool(\n",
    "                    h1,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                \n",
    "                pooled_outputs.append(pooled)\n",
    "                #pooled_outputs.append(pooled)\n",
    "                scope.reuse_variables()\n",
    "                 \n",
    "                \n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded2,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h2 = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"tanh2\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled2 = tf.nn.avg_pool(\n",
    "                    h2,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled2)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "\n",
    "        num_filters_total = num_filters * len(filter_sizes) * 2\n",
    "\n",
    "        self.h_pool = tf.concat(pooled_outputs, 1)\n",
    "\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "        \n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "           \n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.labels = tf.argmax(self.input_y, 1)\n",
    "            TP = tf.count_nonzero(self.predictions * self.labels)\n",
    "            TN = tf.count_nonzero((self.predictions - 1) * (self.labels - 1))\n",
    "            FP = tf.count_nonzero(self.predictions * (self.labels - 1))\n",
    "            FN = tf.count_nonzero((self.predictions - 1) * self.labels)\n",
    "            correct_predictions = tf.equal(self.predictions, self.labels)\n",
    "            self.precision = TP / (TP + FP)\n",
    "            self.recall = TP / (TP + FN)\n",
    "            self.f1_score = 2 * self.precision * self.recall / (self.precision + self.recall)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=100\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.5\n",
      "DROPOUT_KEEP_PROB=0.25\n",
      "EMBEDDING_DIM=100\n",
      "EVALUATE_EVERY=20\n",
      "FILTER_SIZES=1\n",
      "L2_REG_LAMBDA=0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.neg\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=500\n",
      "NUM_FILTERS=100\n",
      "POSITIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.pos\n",
      "\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/W:0/grad/hist is illegal; using conv-avgpool-0/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/W:0/grad/sparsity is illegal; using conv-avgpool-0/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/b:0/grad/hist is illegal; using conv-avgpool-0/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/b:0/grad/sparsity is illegal; using conv-avgpool-0/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523838561\n",
      "\n",
      "2018-04-16T00:29:22.069208: step 1, loss 0.679766, acc 0.61\n",
      "2018-04-16T00:29:22.130510: step 2, loss 0.871675, acc 0.46\n",
      "2018-04-16T00:29:22.188336: step 3, loss 1.79831, acc 0.49\n",
      "2018-04-16T00:29:22.237308: step 4, loss 0.858018, acc 0.602564\n",
      "2018-04-16T00:29:22.295791: step 5, loss 0.728885, acc 0.61\n",
      "2018-04-16T00:29:22.353665: step 6, loss 0.65439, acc 0.65\n",
      "2018-04-16T00:29:22.410955: step 7, loss 0.680986, acc 0.57\n",
      "2018-04-16T00:29:22.456771: step 8, loss 0.699143, acc 0.589744\n",
      "2018-04-16T00:29:22.514029: step 9, loss 0.615773, acc 0.69\n",
      "2018-04-16T00:29:22.571983: step 10, loss 0.693056, acc 0.57\n",
      "2018-04-16T00:29:22.629073: step 11, loss 0.750639, acc 0.58\n",
      "2018-04-16T00:29:22.673286: step 12, loss 0.814681, acc 0.512821\n",
      "2018-04-16T00:29:22.729928: step 13, loss 0.908722, acc 0.59\n",
      "2018-04-16T00:29:22.784930: step 14, loss 0.620051, acc 0.65\n",
      "2018-04-16T00:29:22.839878: step 15, loss 0.700634, acc 0.59\n",
      "2018-04-16T00:29:22.884760: step 16, loss 0.658872, acc 0.602564\n",
      "2018-04-16T00:29:22.940489: step 17, loss 0.570003, acc 0.66\n",
      "2018-04-16T00:29:22.996848: step 18, loss 0.668303, acc 0.61\n",
      "2018-04-16T00:29:23.052522: step 19, loss 0.606425, acc 0.66\n",
      "2018-04-16T00:29:23.095528: step 20, loss 0.586015, acc 0.679487\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:23.230619: step 20, loss 0.757126, acc 0.472149, rec 0.878378, pre 0.479351, f1 0.620229\n",
      "\n",
      "2018-04-16T00:29:23.281548: step 21, loss 0.632098, acc 0.63\n",
      "2018-04-16T00:29:23.333326: step 22, loss 0.609915, acc 0.64\n",
      "2018-04-16T00:29:23.384291: step 23, loss 0.839922, acc 0.52\n",
      "2018-04-16T00:29:23.426415: step 24, loss 0.800314, acc 0.551282\n",
      "2018-04-16T00:29:23.481485: step 25, loss 0.634905, acc 0.67\n",
      "2018-04-16T00:29:23.533114: step 26, loss 0.717835, acc 0.54\n",
      "2018-04-16T00:29:23.584549: step 27, loss 0.584109, acc 0.69\n",
      "2018-04-16T00:29:23.626817: step 28, loss 0.554336, acc 0.717949\n",
      "2018-04-16T00:29:23.677966: step 29, loss 0.592205, acc 0.72\n",
      "2018-04-16T00:29:23.732866: step 30, loss 0.598121, acc 0.6\n",
      "2018-04-16T00:29:23.786919: step 31, loss 0.572739, acc 0.71\n",
      "2018-04-16T00:29:23.830616: step 32, loss 0.691654, acc 0.628205\n",
      "2018-04-16T00:29:23.883367: step 33, loss 0.6289, acc 0.64\n",
      "2018-04-16T00:29:23.939593: step 34, loss 0.626941, acc 0.65\n",
      "2018-04-16T00:29:23.992943: step 35, loss 0.639896, acc 0.67\n",
      "2018-04-16T00:29:24.035852: step 36, loss 0.69051, acc 0.615385\n",
      "2018-04-16T00:29:24.087952: step 37, loss 0.649221, acc 0.6\n",
      "2018-04-16T00:29:24.145200: step 38, loss 0.749384, acc 0.57\n",
      "2018-04-16T00:29:24.196273: step 39, loss 0.562504, acc 0.65\n",
      "2018-04-16T00:29:24.240680: step 40, loss 0.564588, acc 0.717949\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:24.343472: step 40, loss 0.760977, acc 0.484085, rec 0.897297, pre 0.486091, f1 0.630579\n",
      "\n",
      "2018-04-16T00:29:24.398006: step 41, loss 0.499541, acc 0.77\n",
      "2018-04-16T00:29:24.449951: step 42, loss 0.597383, acc 0.66\n",
      "2018-04-16T00:29:24.501657: step 43, loss 0.550621, acc 0.69\n",
      "2018-04-16T00:29:24.544907: step 44, loss 0.547416, acc 0.769231\n",
      "2018-04-16T00:29:24.598267: step 45, loss 0.567125, acc 0.73\n",
      "2018-04-16T00:29:24.654913: step 46, loss 0.515375, acc 0.75\n",
      "2018-04-16T00:29:24.707602: step 47, loss 0.575055, acc 0.68\n",
      "2018-04-16T00:29:24.751846: step 48, loss 0.594175, acc 0.679487\n",
      "2018-04-16T00:29:24.826303: step 49, loss 0.533114, acc 0.7\n",
      "2018-04-16T00:29:24.906740: step 50, loss 0.538383, acc 0.7\n",
      "2018-04-16T00:29:24.982091: step 51, loss 0.557592, acc 0.66\n",
      "2018-04-16T00:29:25.044979: step 52, loss 0.498921, acc 0.74359\n",
      "2018-04-16T00:29:25.125304: step 53, loss 0.460074, acc 0.78\n",
      "2018-04-16T00:29:25.202161: step 54, loss 0.452537, acc 0.8\n",
      "2018-04-16T00:29:25.277418: step 55, loss 0.621985, acc 0.65\n",
      "2018-04-16T00:29:25.345996: step 56, loss 0.812748, acc 0.551282\n",
      "2018-04-16T00:29:25.421610: step 57, loss 0.823766, acc 0.64\n",
      "2018-04-16T00:29:25.496881: step 58, loss 0.556791, acc 0.71\n",
      "2018-04-16T00:29:25.580103: step 59, loss 0.567721, acc 0.74\n",
      "2018-04-16T00:29:25.641618: step 60, loss 0.468151, acc 0.858974\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:25.761319: step 60, loss 0.634174, acc 0.637931, rec 0.3, pre 0.888, f1 0.448485\n",
      "\n",
      "2018-04-16T00:29:25.814920: step 61, loss 0.550195, acc 0.72\n",
      "2018-04-16T00:29:25.865870: step 62, loss 0.621955, acc 0.7\n",
      "2018-04-16T00:29:25.916336: step 63, loss 0.618004, acc 0.68\n",
      "2018-04-16T00:29:25.957872: step 64, loss 0.560376, acc 0.730769\n",
      "2018-04-16T00:29:26.008611: step 65, loss 0.545218, acc 0.67\n",
      "2018-04-16T00:29:26.062753: step 66, loss 0.667538, acc 0.62\n",
      "2018-04-16T00:29:26.114769: step 67, loss 0.521643, acc 0.74\n",
      "2018-04-16T00:29:26.156943: step 68, loss 0.501532, acc 0.769231\n",
      "2018-04-16T00:29:26.208000: step 69, loss 0.537309, acc 0.7\n",
      "2018-04-16T00:29:26.259017: step 70, loss 0.476905, acc 0.78\n",
      "2018-04-16T00:29:26.314008: step 71, loss 0.478194, acc 0.79\n",
      "2018-04-16T00:29:26.356245: step 72, loss 0.406717, acc 0.846154\n",
      "2018-04-16T00:29:26.407162: step 73, loss 0.47122, acc 0.76\n",
      "2018-04-16T00:29:26.459513: step 74, loss 0.605291, acc 0.67\n",
      "2018-04-16T00:29:26.512873: step 75, loss 0.623007, acc 0.66\n",
      "2018-04-16T00:29:26.558203: step 76, loss 0.588354, acc 0.679487\n",
      "2018-04-16T00:29:26.609662: step 77, loss 0.505581, acc 0.75\n",
      "2018-04-16T00:29:26.661297: step 78, loss 0.698365, acc 0.59\n",
      "2018-04-16T00:29:26.713370: step 79, loss 0.520016, acc 0.75\n",
      "2018-04-16T00:29:26.756842: step 80, loss 0.620948, acc 0.666667\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:26.861264: step 80, loss 0.754878, acc 0.55305, rec 0.0918919, pre 0.971429, f1 0.167901\n",
      "\n",
      "2018-04-16T00:29:26.912583: step 81, loss 0.687188, acc 0.59\n",
      "2018-04-16T00:29:26.963915: step 82, loss 0.553841, acc 0.71\n",
      "2018-04-16T00:29:27.016653: step 83, loss 0.501231, acc 0.74\n",
      "2018-04-16T00:29:27.059443: step 84, loss 0.451584, acc 0.782051\n",
      "2018-04-16T00:29:27.114892: step 85, loss 0.464554, acc 0.79\n",
      "2018-04-16T00:29:27.166909: step 86, loss 0.439505, acc 0.8\n",
      "2018-04-16T00:29:27.218648: step 87, loss 0.418286, acc 0.84\n",
      "2018-04-16T00:29:27.261116: step 88, loss 0.567517, acc 0.692308\n",
      "2018-04-16T00:29:27.313530: step 89, loss 0.590067, acc 0.68\n",
      "2018-04-16T00:29:27.368503: step 90, loss 0.458197, acc 0.8\n",
      "2018-04-16T00:29:27.420395: step 91, loss 0.449863, acc 0.81\n",
      "2018-04-16T00:29:27.466849: step 92, loss 0.490069, acc 0.717949\n",
      "2018-04-16T00:29:27.519585: step 93, loss 0.422778, acc 0.83\n",
      "2018-04-16T00:29:27.577701: step 94, loss 0.420576, acc 0.82\n",
      "2018-04-16T00:29:27.628721: step 95, loss 0.499715, acc 0.75\n",
      "2018-04-16T00:29:27.670301: step 96, loss 0.570178, acc 0.705128\n",
      "2018-04-16T00:29:27.723294: step 97, loss 0.626837, acc 0.71\n",
      "2018-04-16T00:29:27.773887: step 98, loss 0.502307, acc 0.76\n",
      "2018-04-16T00:29:27.829089: step 99, loss 0.45144, acc 0.78\n",
      "2018-04-16T00:29:27.871791: step 100, loss 0.448525, acc 0.75641\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:27.970162: step 100, loss 0.710375, acc 0.535809, rec 0.918919, pre 0.515152, f1 0.660194\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523838561/checkpoints/model-100\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:29:28.081324: step 101, loss 0.395969, acc 0.79\n",
      "2018-04-16T00:29:28.132633: step 102, loss 0.478528, acc 0.83\n",
      "2018-04-16T00:29:28.184826: step 103, loss 0.682247, acc 0.68\n",
      "2018-04-16T00:29:28.227021: step 104, loss 0.745632, acc 0.628205\n",
      "2018-04-16T00:29:28.278786: step 105, loss 0.465558, acc 0.76\n",
      "2018-04-16T00:29:28.334356: step 106, loss 0.471547, acc 0.79\n",
      "2018-04-16T00:29:28.386162: step 107, loss 0.424685, acc 0.78\n",
      "2018-04-16T00:29:28.428725: step 108, loss 0.409942, acc 0.820513\n",
      "2018-04-16T00:29:28.479528: step 109, loss 0.366337, acc 0.85\n",
      "2018-04-16T00:29:28.531568: step 110, loss 0.410731, acc 0.85\n",
      "2018-04-16T00:29:28.588669: step 111, loss 0.448227, acc 0.83\n",
      "2018-04-16T00:29:28.631631: step 112, loss 0.434256, acc 0.769231\n",
      "2018-04-16T00:29:28.682648: step 113, loss 0.44486, acc 0.78\n",
      "2018-04-16T00:29:28.734115: step 114, loss 0.553351, acc 0.73\n",
      "2018-04-16T00:29:28.785181: step 115, loss 0.603414, acc 0.67\n",
      "2018-04-16T00:29:28.830488: step 116, loss 0.634491, acc 0.730769\n",
      "2018-04-16T00:29:28.882757: step 117, loss 0.709773, acc 0.65\n",
      "2018-04-16T00:29:28.934081: step 118, loss 0.566135, acc 0.73\n",
      "2018-04-16T00:29:28.985209: step 119, loss 0.449918, acc 0.74\n",
      "2018-04-16T00:29:29.027521: step 120, loss 0.45657, acc 0.75641\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:29.130414: step 120, loss 1.13742, acc 0.480106, rec 0.951351, pre 0.484848, f1 0.642336\n",
      "\n",
      "2018-04-16T00:29:29.181469: step 121, loss 0.432011, acc 0.82\n",
      "2018-04-16T00:29:29.232290: step 122, loss 0.434856, acc 0.81\n",
      "2018-04-16T00:29:29.283277: step 123, loss 0.384881, acc 0.82\n",
      "2018-04-16T00:29:29.325447: step 124, loss 0.400897, acc 0.794872\n",
      "2018-04-16T00:29:29.381861: step 125, loss 0.395177, acc 0.8\n",
      "2018-04-16T00:29:29.433535: step 126, loss 0.399176, acc 0.83\n",
      "2018-04-16T00:29:29.484371: step 127, loss 0.426477, acc 0.77\n",
      "2018-04-16T00:29:29.526856: step 128, loss 0.330889, acc 0.923077\n",
      "2018-04-16T00:29:29.579645: step 129, loss 0.357837, acc 0.81\n",
      "2018-04-16T00:29:29.633948: step 130, loss 0.34857, acc 0.85\n",
      "2018-04-16T00:29:29.684690: step 131, loss 0.43971, acc 0.81\n",
      "2018-04-16T00:29:29.728154: step 132, loss 0.56977, acc 0.75641\n",
      "2018-04-16T00:29:29.778810: step 133, loss 0.878226, acc 0.66\n",
      "2018-04-16T00:29:29.829604: step 134, loss 0.758493, acc 0.63\n",
      "2018-04-16T00:29:29.883654: step 135, loss 0.414802, acc 0.82\n",
      "2018-04-16T00:29:29.925848: step 136, loss 0.3706, acc 0.846154\n",
      "2018-04-16T00:29:29.976547: step 137, loss 0.370978, acc 0.83\n",
      "2018-04-16T00:29:30.027615: step 138, loss 0.410886, acc 0.81\n",
      "2018-04-16T00:29:30.078995: step 139, loss 0.415898, acc 0.83\n",
      "2018-04-16T00:29:30.126103: step 140, loss 0.430911, acc 0.794872\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:30.224465: step 140, loss 0.575423, acc 0.713528, rec 0.827027, pre 0.668122, f1 0.73913\n",
      "\n",
      "2018-04-16T00:29:30.276348: step 141, loss 0.368294, acc 0.85\n",
      "2018-04-16T00:29:30.331193: step 142, loss 0.414206, acc 0.8\n",
      "2018-04-16T00:29:30.383232: step 143, loss 0.496011, acc 0.8\n",
      "2018-04-16T00:29:30.425727: step 144, loss 0.534497, acc 0.74359\n",
      "2018-04-16T00:29:30.477264: step 145, loss 0.601108, acc 0.71\n",
      "2018-04-16T00:29:30.529023: step 146, loss 0.455308, acc 0.81\n",
      "2018-04-16T00:29:30.584272: step 147, loss 0.527298, acc 0.74\n",
      "2018-04-16T00:29:30.626865: step 148, loss 0.41023, acc 0.782051\n",
      "2018-04-16T00:29:30.678271: step 149, loss 0.465982, acc 0.82\n",
      "2018-04-16T00:29:30.730072: step 150, loss 0.38745, acc 0.85\n",
      "2018-04-16T00:29:30.781282: step 151, loss 0.36865, acc 0.87\n",
      "2018-04-16T00:29:30.826438: step 152, loss 0.388709, acc 0.858974\n",
      "2018-04-16T00:29:30.877732: step 153, loss 0.369682, acc 0.8\n",
      "2018-04-16T00:29:30.928641: step 154, loss 0.39249, acc 0.82\n",
      "2018-04-16T00:29:30.980072: step 155, loss 0.337511, acc 0.85\n",
      "2018-04-16T00:29:31.023582: step 156, loss 0.351113, acc 0.820513\n",
      "2018-04-16T00:29:31.079052: step 157, loss 0.316162, acc 0.9\n",
      "2018-04-16T00:29:31.130739: step 158, loss 0.411349, acc 0.81\n",
      "2018-04-16T00:29:31.182214: step 159, loss 0.336506, acc 0.86\n",
      "2018-04-16T00:29:31.224251: step 160, loss 0.35456, acc 0.782051\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:31.326671: step 160, loss 0.566674, acc 0.714854, rec 0.862162, pre 0.660455, f1 0.747948\n",
      "\n",
      "2018-04-16T00:29:31.377495: step 161, loss 0.292785, acc 0.9\n",
      "2018-04-16T00:29:31.428516: step 162, loss 0.405513, acc 0.83\n",
      "2018-04-16T00:29:31.480404: step 163, loss 0.504409, acc 0.77\n",
      "2018-04-16T00:29:31.525579: step 164, loss 0.672678, acc 0.717949\n",
      "2018-04-16T00:29:31.582524: step 165, loss 0.47384, acc 0.79\n",
      "2018-04-16T00:29:31.634128: step 166, loss 0.796597, acc 0.71\n",
      "2018-04-16T00:29:31.685508: step 167, loss 0.427697, acc 0.79\n",
      "2018-04-16T00:29:31.729813: step 168, loss 0.363953, acc 0.846154\n",
      "2018-04-16T00:29:31.780766: step 169, loss 0.435594, acc 0.84\n",
      "2018-04-16T00:29:31.835996: step 170, loss 0.329291, acc 0.87\n",
      "2018-04-16T00:29:31.886617: step 171, loss 0.456593, acc 0.74\n",
      "2018-04-16T00:29:31.928750: step 172, loss 0.415867, acc 0.794872\n",
      "2018-04-16T00:29:31.981379: step 173, loss 0.454998, acc 0.83\n",
      "2018-04-16T00:29:32.033014: step 174, loss 0.351335, acc 0.86\n",
      "2018-04-16T00:29:32.088744: step 175, loss 0.359894, acc 0.86\n",
      "2018-04-16T00:29:32.131048: step 176, loss 0.38259, acc 0.833333\n",
      "2018-04-16T00:29:32.182248: step 177, loss 0.424334, acc 0.83\n",
      "2018-04-16T00:29:32.233429: step 178, loss 0.271479, acc 0.87\n",
      "2018-04-16T00:29:32.284136: step 179, loss 0.389392, acc 0.84\n",
      "2018-04-16T00:29:32.331450: step 180, loss 0.329237, acc 0.846154\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:32.431504: step 180, loss 0.597783, acc 0.653846, rec 0.881081, pre 0.600368, f1 0.714129\n",
      "\n",
      "2018-04-16T00:29:32.483270: step 181, loss 0.381818, acc 0.85\n",
      "2018-04-16T00:29:32.540744: step 182, loss 0.361985, acc 0.86\n",
      "2018-04-16T00:29:32.593877: step 183, loss 0.317312, acc 0.89\n",
      "2018-04-16T00:29:32.637015: step 184, loss 0.481086, acc 0.75641\n",
      "2018-04-16T00:29:32.690106: step 185, loss 0.45928, acc 0.76\n",
      "2018-04-16T00:29:32.747237: step 186, loss 0.562696, acc 0.72\n",
      "2018-04-16T00:29:32.801622: step 187, loss 0.327843, acc 0.86\n",
      "2018-04-16T00:29:32.844205: step 188, loss 0.274404, acc 0.897436\n",
      "2018-04-16T00:29:32.895477: step 189, loss 0.362456, acc 0.83\n",
      "2018-04-16T00:29:32.947444: step 190, loss 0.363256, acc 0.81\n",
      "2018-04-16T00:29:33.002738: step 191, loss 0.388502, acc 0.88\n",
      "2018-04-16T00:29:33.045349: step 192, loss 0.390366, acc 0.846154\n",
      "2018-04-16T00:29:33.097376: step 193, loss 0.550629, acc 0.72\n",
      "2018-04-16T00:29:33.148090: step 194, loss 0.718115, acc 0.63\n",
      "2018-04-16T00:29:33.199630: step 195, loss 0.792149, acc 0.69\n",
      "2018-04-16T00:29:33.245957: step 196, loss 0.329572, acc 0.871795\n",
      "2018-04-16T00:29:33.297317: step 197, loss 0.425717, acc 0.79\n",
      "2018-04-16T00:29:33.349474: step 198, loss 0.368425, acc 0.82\n",
      "2018-04-16T00:29:33.399925: step 199, loss 0.304608, acc 0.85\n",
      "2018-04-16T00:29:33.442232: step 200, loss 0.374736, acc 0.807692\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:33.545500: step 200, loss 1.1422, acc 0.496021, rec 0.962162, pre 0.493075, f1 0.652015\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523838561/checkpoints/model-200\n",
      "\n",
      "2018-04-16T00:29:33.644592: step 201, loss 0.323989, acc 0.89\n",
      "2018-04-16T00:29:33.695578: step 202, loss 0.335483, acc 0.86\n",
      "2018-04-16T00:29:33.753117: step 203, loss 0.326522, acc 0.89\n",
      "2018-04-16T00:29:33.795275: step 204, loss 0.535438, acc 0.717949\n",
      "2018-04-16T00:29:33.846957: step 205, loss 0.491171, acc 0.76\n",
      "2018-04-16T00:29:33.898549: step 206, loss 0.586353, acc 0.73\n",
      "2018-04-16T00:29:33.950817: step 207, loss 0.382568, acc 0.86\n",
      "2018-04-16T00:29:33.997237: step 208, loss 0.342564, acc 0.910256\n",
      "2018-04-16T00:29:34.048364: step 209, loss 0.306879, acc 0.89\n",
      "2018-04-16T00:29:34.099978: step 210, loss 0.330028, acc 0.91\n",
      "2018-04-16T00:29:34.152578: step 211, loss 0.358392, acc 0.85\n",
      "2018-04-16T00:29:34.202268: step 212, loss 0.32221, acc 0.782051\n",
      "2018-04-16T00:29:34.255445: step 213, loss 0.398484, acc 0.83\n",
      "2018-04-16T00:29:34.308164: step 214, loss 0.385368, acc 0.82\n",
      "2018-04-16T00:29:34.382247: step 215, loss 0.407554, acc 0.83\n",
      "2018-04-16T00:29:34.453552: step 216, loss 0.24599, acc 0.923077\n",
      "2018-04-16T00:29:34.533098: step 217, loss 0.291162, acc 0.87\n",
      "2018-04-16T00:29:34.611593: step 218, loss 0.294076, acc 0.92\n",
      "2018-04-16T00:29:34.696511: step 219, loss 0.297119, acc 0.85\n",
      "2018-04-16T00:29:34.761564: step 220, loss 0.402972, acc 0.858974\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:29:34.926466: step 220, loss 0.52624, acc 0.766578, rec 0.678378, pre 0.814935, f1 0.740413\n",
      "\n",
      "2018-04-16T00:29:35.005056: step 221, loss 0.289064, acc 0.87\n",
      "2018-04-16T00:29:35.083312: step 222, loss 0.293436, acc 0.88\n",
      "2018-04-16T00:29:35.163831: step 223, loss 0.333935, acc 0.9\n",
      "2018-04-16T00:29:35.224012: step 224, loss 0.371988, acc 0.858974\n",
      "2018-04-16T00:29:35.296728: step 225, loss 0.696238, acc 0.7\n",
      "2018-04-16T00:29:35.364954: step 226, loss 0.659648, acc 0.76\n",
      "2018-04-16T00:29:35.443581: step 227, loss 0.298231, acc 0.87\n",
      "2018-04-16T00:29:35.506747: step 228, loss 0.318098, acc 0.884615\n",
      "2018-04-16T00:29:35.568711: step 229, loss 0.326668, acc 0.87\n",
      "2018-04-16T00:29:35.619943: step 230, loss 0.314196, acc 0.89\n",
      "2018-04-16T00:29:35.675189: step 231, loss 0.336198, acc 0.86\n",
      "2018-04-16T00:29:35.719272: step 232, loss 0.311228, acc 0.833333\n",
      "2018-04-16T00:29:35.770284: step 233, loss 0.330532, acc 0.84\n",
      "2018-04-16T00:29:35.820878: step 234, loss 0.36611, acc 0.84\n",
      "2018-04-16T00:29:35.871822: step 235, loss 0.346803, acc 0.86\n",
      "2018-04-16T00:29:35.916730: step 236, loss 0.42611, acc 0.820513\n",
      "2018-04-16T00:29:35.967926: step 237, loss 0.813111, acc 0.72\n",
      "2018-04-16T00:29:36.020443: step 238, loss 0.292822, acc 0.87\n",
      "2018-04-16T00:29:36.072611: step 239, loss 0.340319, acc 0.89\n",
      "2018-04-16T00:29:36.115049: step 240, loss 0.244852, acc 0.897436\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:36.217584: step 240, loss 0.848661, acc 0.53183, rec 0.932432, pre 0.51263, f1 0.661553\n",
      "\n",
      "2018-04-16T00:29:36.269705: step 241, loss 0.396781, acc 0.84\n",
      "2018-04-16T00:29:36.321718: step 242, loss 0.262495, acc 0.91\n",
      "2018-04-16T00:29:36.373704: step 243, loss 0.246096, acc 0.9\n",
      "2018-04-16T00:29:36.416590: step 244, loss 0.309857, acc 0.871795\n",
      "2018-04-16T00:29:36.472964: step 245, loss 0.460695, acc 0.81\n",
      "2018-04-16T00:29:36.525000: step 246, loss 0.247197, acc 0.92\n",
      "2018-04-16T00:29:36.578250: step 247, loss 0.249231, acc 0.9\n",
      "2018-04-16T00:29:36.621516: step 248, loss 0.223925, acc 0.935897\n",
      "2018-04-16T00:29:36.673625: step 249, loss 0.23528, acc 0.92\n",
      "2018-04-16T00:29:36.729468: step 250, loss 0.252791, acc 0.93\n",
      "2018-04-16T00:29:36.780964: step 251, loss 0.308184, acc 0.87\n",
      "2018-04-16T00:29:36.823305: step 252, loss 0.237545, acc 0.897436\n",
      "2018-04-16T00:29:36.874661: step 253, loss 0.23013, acc 0.93\n",
      "2018-04-16T00:29:36.927048: step 254, loss 0.338923, acc 0.85\n",
      "2018-04-16T00:29:36.981340: step 255, loss 0.151329, acc 0.97\n",
      "2018-04-16T00:29:37.023902: step 256, loss 0.365381, acc 0.782051\n",
      "2018-04-16T00:29:37.075624: step 257, loss 0.274466, acc 0.89\n",
      "2018-04-16T00:29:37.127696: step 258, loss 0.276287, acc 0.88\n",
      "2018-04-16T00:29:37.186117: step 259, loss 0.253048, acc 0.9\n",
      "2018-04-16T00:29:37.228909: step 260, loss 0.335113, acc 0.884615\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:37.328120: step 260, loss 0.713816, acc 0.640584, rec 0.297297, pre 0.909091, f1 0.448065\n",
      "\n",
      "2018-04-16T00:29:37.379754: step 261, loss 0.45714, acc 0.78\n",
      "2018-04-16T00:29:37.435921: step 262, loss 1.17767, acc 0.68\n",
      "2018-04-16T00:29:37.487223: step 263, loss 0.389086, acc 0.85\n",
      "2018-04-16T00:29:37.529042: step 264, loss 0.360786, acc 0.884615\n",
      "2018-04-16T00:29:37.582164: step 265, loss 0.282253, acc 0.9\n",
      "2018-04-16T00:29:37.634676: step 266, loss 0.310503, acc 0.87\n",
      "2018-04-16T00:29:37.690081: step 267, loss 0.310913, acc 0.87\n",
      "2018-04-16T00:29:37.734041: step 268, loss 0.245709, acc 0.910256\n",
      "2018-04-16T00:29:37.785986: step 269, loss 0.278586, acc 0.9\n",
      "2018-04-16T00:29:37.842177: step 270, loss 0.327055, acc 0.87\n",
      "2018-04-16T00:29:37.897590: step 271, loss 0.349831, acc 0.86\n",
      "2018-04-16T00:29:37.940026: step 272, loss 0.315087, acc 0.897436\n",
      "2018-04-16T00:29:37.991839: step 273, loss 0.190423, acc 0.93\n",
      "2018-04-16T00:29:38.043800: step 274, loss 0.312122, acc 0.89\n",
      "2018-04-16T00:29:38.095496: step 275, loss 0.432228, acc 0.79\n",
      "2018-04-16T00:29:38.141614: step 276, loss 0.319536, acc 0.807692\n",
      "2018-04-16T00:29:38.194088: step 277, loss 0.206002, acc 0.93\n",
      "2018-04-16T00:29:38.245565: step 278, loss 0.282771, acc 0.89\n",
      "2018-04-16T00:29:38.297346: step 279, loss 0.232665, acc 0.92\n",
      "2018-04-16T00:29:38.341760: step 280, loss 0.544081, acc 0.782051\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:38.447372: step 280, loss 1.59285, acc 0.543767, rec 0.072973, pre 0.964286, f1 0.135678\n",
      "\n",
      "2018-04-16T00:29:38.499450: step 281, loss 0.823763, acc 0.67\n",
      "2018-04-16T00:29:38.556086: step 282, loss 0.480149, acc 0.83\n",
      "2018-04-16T00:29:38.608156: step 283, loss 0.264057, acc 0.86\n",
      "2018-04-16T00:29:38.651683: step 284, loss 0.371836, acc 0.820513\n",
      "2018-04-16T00:29:38.704869: step 285, loss 0.315245, acc 0.88\n",
      "2018-04-16T00:29:38.761542: step 286, loss 0.196416, acc 0.94\n",
      "2018-04-16T00:29:38.813833: step 287, loss 0.236625, acc 0.91\n",
      "2018-04-16T00:29:38.856787: step 288, loss 0.267274, acc 0.897436\n",
      "2018-04-16T00:29:38.909471: step 289, loss 0.227173, acc 0.9\n",
      "2018-04-16T00:29:38.960592: step 290, loss 0.224997, acc 0.92\n",
      "2018-04-16T00:29:39.016843: step 291, loss 0.321835, acc 0.86\n",
      "2018-04-16T00:29:39.059374: step 292, loss 0.272179, acc 0.910256\n",
      "2018-04-16T00:29:39.111635: step 293, loss 0.27542, acc 0.87\n",
      "2018-04-16T00:29:39.165931: step 294, loss 0.373184, acc 0.87\n",
      "2018-04-16T00:29:39.222070: step 295, loss 0.36653, acc 0.88\n",
      "2018-04-16T00:29:39.264510: step 296, loss 0.271678, acc 0.897436\n",
      "2018-04-16T00:29:39.316326: step 297, loss 0.278139, acc 0.9\n",
      "2018-04-16T00:29:39.368171: step 298, loss 0.375784, acc 0.87\n",
      "2018-04-16T00:29:39.422088: step 299, loss 0.273354, acc 0.9\n",
      "2018-04-16T00:29:39.468043: step 300, loss 0.298584, acc 0.897436\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:39.568549: step 300, loss 0.531949, acc 0.754642, rec 0.621622, pre 0.836364, f1 0.713178\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523838561/checkpoints/model-300\n",
      "\n",
      "2018-04-16T00:29:39.665531: step 301, loss 0.328074, acc 0.86\n",
      "2018-04-16T00:29:39.721482: step 302, loss 0.380315, acc 0.88\n",
      "2018-04-16T00:29:39.771273: step 303, loss 0.284946, acc 0.87\n",
      "2018-04-16T00:29:39.812194: step 304, loss 0.211, acc 0.910256\n",
      "2018-04-16T00:29:39.861980: step 305, loss 0.149267, acc 0.96\n",
      "2018-04-16T00:29:39.912033: step 306, loss 0.258161, acc 0.93\n",
      "2018-04-16T00:29:39.967116: step 307, loss 0.361286, acc 0.86\n",
      "2018-04-16T00:29:40.010130: step 308, loss 0.426566, acc 0.782051\n",
      "2018-04-16T00:29:40.062612: step 309, loss 0.4398, acc 0.86\n",
      "2018-04-16T00:29:40.114479: step 310, loss 0.195138, acc 0.94\n",
      "2018-04-16T00:29:40.166477: step 311, loss 0.328978, acc 0.86\n",
      "2018-04-16T00:29:40.212887: step 312, loss 0.252029, acc 0.897436\n",
      "2018-04-16T00:29:40.264249: step 313, loss 0.163014, acc 0.95\n",
      "2018-04-16T00:29:40.316618: step 314, loss 0.242437, acc 0.86\n",
      "2018-04-16T00:29:40.368615: step 315, loss 0.235041, acc 0.91\n",
      "2018-04-16T00:29:40.411129: step 316, loss 0.204736, acc 0.923077\n",
      "2018-04-16T00:29:40.465536: step 317, loss 0.203087, acc 0.92\n",
      "2018-04-16T00:29:40.517287: step 318, loss 0.159674, acc 0.94\n",
      "2018-04-16T00:29:40.569508: step 319, loss 0.235507, acc 0.91\n",
      "2018-04-16T00:29:40.612474: step 320, loss 0.27527, acc 0.884615\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:40.716646: step 320, loss 1.63236, acc 0.5, rec 0.983784, pre 0.495238, f1 0.658824\n",
      "\n",
      "2018-04-16T00:29:40.768415: step 321, loss 0.262728, acc 0.89\n",
      "2018-04-16T00:29:40.820881: step 322, loss 0.20596, acc 0.93\n",
      "2018-04-16T00:29:40.872811: step 323, loss 0.221549, acc 0.91\n",
      "2018-04-16T00:29:40.915064: step 324, loss 0.248039, acc 0.897436\n",
      "2018-04-16T00:29:40.969841: step 325, loss 0.402415, acc 0.81\n",
      "2018-04-16T00:29:41.021631: step 326, loss 0.805392, acc 0.67\n",
      "2018-04-16T00:29:41.073352: step 327, loss 0.620054, acc 0.78\n",
      "2018-04-16T00:29:41.115645: step 328, loss 0.165919, acc 0.961538\n",
      "2018-04-16T00:29:41.167082: step 329, loss 0.211835, acc 0.92\n",
      "2018-04-16T00:29:41.223256: step 330, loss 0.207948, acc 0.95\n",
      "2018-04-16T00:29:41.274795: step 331, loss 0.194424, acc 0.91\n",
      "2018-04-16T00:29:41.317413: step 332, loss 0.199344, acc 0.897436\n",
      "2018-04-16T00:29:41.369136: step 333, loss 0.232585, acc 0.92\n",
      "2018-04-16T00:29:41.420410: step 334, loss 0.169978, acc 0.96\n",
      "2018-04-16T00:29:41.475749: step 335, loss 0.140075, acc 0.97\n",
      "2018-04-16T00:29:41.518632: step 336, loss 0.296759, acc 0.858974\n",
      "2018-04-16T00:29:41.571736: step 337, loss 0.250338, acc 0.9\n",
      "2018-04-16T00:29:41.625973: step 338, loss 0.193127, acc 0.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:29:41.681531: step 339, loss 0.181098, acc 0.93\n",
      "2018-04-16T00:29:41.725815: step 340, loss 0.267112, acc 0.897436\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:41.826624: step 340, loss 0.847049, acc 0.582228, rec 0.932432, pre 0.543307, f1 0.686567\n",
      "\n",
      "2018-04-16T00:29:41.877930: step 341, loss 0.210307, acc 0.92\n",
      "2018-04-16T00:29:41.933063: step 342, loss 0.245581, acc 0.94\n",
      "2018-04-16T00:29:41.984019: step 343, loss 0.162007, acc 0.94\n",
      "2018-04-16T00:29:42.026934: step 344, loss 0.132163, acc 0.974359\n",
      "2018-04-16T00:29:42.079612: step 345, loss 0.171047, acc 0.94\n",
      "2018-04-16T00:29:42.131136: step 346, loss 0.200236, acc 0.94\n",
      "2018-04-16T00:29:42.187900: step 347, loss 0.320558, acc 0.82\n",
      "2018-04-16T00:29:42.230473: step 348, loss 0.492259, acc 0.807692\n",
      "2018-04-16T00:29:42.283191: step 349, loss 0.309111, acc 0.84\n",
      "2018-04-16T00:29:42.335131: step 350, loss 0.491845, acc 0.79\n",
      "2018-04-16T00:29:42.387461: step 351, loss 0.271168, acc 0.87\n",
      "2018-04-16T00:29:42.434433: step 352, loss 0.207394, acc 0.910256\n",
      "2018-04-16T00:29:42.486831: step 353, loss 0.180299, acc 0.95\n",
      "2018-04-16T00:29:42.540244: step 354, loss 0.181803, acc 0.96\n",
      "2018-04-16T00:29:42.591285: step 355, loss 0.216379, acc 0.91\n",
      "2018-04-16T00:29:42.633607: step 356, loss 0.204527, acc 0.884615\n",
      "2018-04-16T00:29:42.689551: step 357, loss 0.136137, acc 0.95\n",
      "2018-04-16T00:29:42.741499: step 358, loss 0.140353, acc 0.96\n",
      "2018-04-16T00:29:42.793697: step 359, loss 0.357358, acc 0.84\n",
      "2018-04-16T00:29:42.836101: step 360, loss 0.330601, acc 0.846154\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:42.938827: step 360, loss 2.89591, acc 0.490716, rec 0.989189, pre 0.490617, f1 0.655914\n",
      "\n",
      "2018-04-16T00:29:42.989623: step 361, loss 0.433552, acc 0.89\n",
      "2018-04-16T00:29:43.041334: step 362, loss 0.206957, acc 0.91\n",
      "2018-04-16T00:29:43.093116: step 363, loss 0.184157, acc 0.94\n",
      "2018-04-16T00:29:43.136092: step 364, loss 0.210923, acc 0.910256\n",
      "2018-04-16T00:29:43.191563: step 365, loss 0.12629, acc 0.97\n",
      "2018-04-16T00:29:43.243572: step 366, loss 0.204864, acc 0.94\n",
      "2018-04-16T00:29:43.295324: step 367, loss 0.170595, acc 0.93\n",
      "2018-04-16T00:29:43.338718: step 368, loss 0.194708, acc 0.910256\n",
      "2018-04-16T00:29:43.390453: step 369, loss 0.272348, acc 0.9\n",
      "2018-04-16T00:29:43.445927: step 370, loss 0.35836, acc 0.83\n",
      "2018-04-16T00:29:43.496921: step 371, loss 0.690524, acc 0.79\n",
      "2018-04-16T00:29:43.539264: step 372, loss 0.333666, acc 0.833333\n",
      "2018-04-16T00:29:43.590769: step 373, loss 0.41963, acc 0.85\n",
      "2018-04-16T00:29:43.642319: step 374, loss 0.409795, acc 0.84\n",
      "2018-04-16T00:29:43.697663: step 375, loss 0.38944, acc 0.88\n",
      "2018-04-16T00:29:43.741403: step 376, loss 0.131445, acc 0.948718\n",
      "2018-04-16T00:29:43.793343: step 377, loss 0.226695, acc 0.89\n",
      "2018-04-16T00:29:43.845063: step 378, loss 0.218275, acc 0.92\n",
      "2018-04-16T00:29:43.896896: step 379, loss 0.26918, acc 0.9\n",
      "2018-04-16T00:29:43.943459: step 380, loss 0.23563, acc 0.897436\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:44.042893: step 380, loss 0.776489, acc 0.619363, rec 0.92973, pre 0.568595, f1 0.705641\n",
      "\n",
      "2018-04-16T00:29:44.095115: step 381, loss 0.213572, acc 0.88\n",
      "2018-04-16T00:29:44.151885: step 382, loss 0.149352, acc 0.97\n",
      "2018-04-16T00:29:44.205210: step 383, loss 0.249431, acc 0.9\n",
      "2018-04-16T00:29:44.247210: step 384, loss 0.234727, acc 0.923077\n",
      "2018-04-16T00:29:44.300463: step 385, loss 0.132464, acc 0.94\n",
      "2018-04-16T00:29:44.356883: step 386, loss 0.170014, acc 0.91\n",
      "2018-04-16T00:29:44.408039: step 387, loss 0.191621, acc 0.93\n",
      "2018-04-16T00:29:44.450677: step 388, loss 0.132695, acc 0.961538\n",
      "2018-04-16T00:29:44.502050: step 389, loss 0.155695, acc 0.95\n",
      "2018-04-16T00:29:44.554815: step 390, loss 0.1882, acc 0.93\n",
      "2018-04-16T00:29:44.609050: step 391, loss 0.253591, acc 0.89\n",
      "2018-04-16T00:29:44.651250: step 392, loss 0.239379, acc 0.897436\n",
      "2018-04-16T00:29:44.702631: step 393, loss 0.170762, acc 0.96\n",
      "2018-04-16T00:29:44.755567: step 394, loss 0.18507, acc 0.92\n",
      "2018-04-16T00:29:44.807751: step 395, loss 0.121624, acc 0.98\n",
      "2018-04-16T00:29:44.853169: step 396, loss 0.135308, acc 0.948718\n",
      "2018-04-16T00:29:44.904888: step 397, loss 0.228941, acc 0.93\n",
      "2018-04-16T00:29:44.956127: step 398, loss 0.13365, acc 0.95\n",
      "2018-04-16T00:29:45.007245: step 399, loss 0.113981, acc 0.98\n",
      "2018-04-16T00:29:45.049420: step 400, loss 0.14247, acc 0.974359\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:45.151443: step 400, loss 0.695968, acc 0.660477, rec 0.924324, pre 0.6, f1 0.72766\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523838561/checkpoints/model-400\n",
      "\n",
      "2018-04-16T00:29:45.250729: step 401, loss 0.162717, acc 0.93\n",
      "2018-04-16T00:29:45.303077: step 402, loss 0.135081, acc 0.95\n",
      "2018-04-16T00:29:45.358306: step 403, loss 0.113846, acc 0.98\n",
      "2018-04-16T00:29:45.401062: step 404, loss 0.1464, acc 0.961538\n",
      "2018-04-16T00:29:45.452817: step 405, loss 0.179132, acc 0.94\n",
      "2018-04-16T00:29:45.504404: step 406, loss 0.148312, acc 0.94\n",
      "2018-04-16T00:29:45.555676: step 407, loss 0.171347, acc 0.92\n",
      "2018-04-16T00:29:45.601115: step 408, loss 0.203358, acc 0.948718\n",
      "2018-04-16T00:29:45.652287: step 409, loss 0.29876, acc 0.9\n",
      "2018-04-16T00:29:45.705250: step 410, loss 0.390095, acc 0.83\n",
      "2018-04-16T00:29:45.756116: step 411, loss 0.473143, acc 0.84\n",
      "2018-04-16T00:29:45.798372: step 412, loss 0.127899, acc 0.961538\n",
      "2018-04-16T00:29:45.853033: step 413, loss 0.169811, acc 0.91\n",
      "2018-04-16T00:29:45.904337: step 414, loss 0.0783271, acc 0.99\n",
      "2018-04-16T00:29:45.960718: step 415, loss 0.17469, acc 0.94\n",
      "2018-04-16T00:29:46.003184: step 416, loss 0.0997405, acc 0.974359\n",
      "2018-04-16T00:29:46.058993: step 417, loss 0.174438, acc 0.95\n",
      "2018-04-16T00:29:46.110452: step 418, loss 0.119436, acc 0.97\n",
      "2018-04-16T00:29:46.162562: step 419, loss 0.0963293, acc 0.98\n",
      "2018-04-16T00:29:46.205964: step 420, loss 0.097751, acc 0.974359\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:46.311821: step 420, loss 0.79059, acc 0.627321, rec 0.927027, pre 0.574539, f1 0.709411\n",
      "\n",
      "2018-04-16T00:29:46.365405: step 421, loss 0.171637, acc 0.92\n",
      "2018-04-16T00:29:46.417068: step 422, loss 0.161316, acc 0.93\n",
      "2018-04-16T00:29:46.468526: step 423, loss 0.145714, acc 0.94\n",
      "2018-04-16T00:29:46.517267: step 424, loss 0.0987577, acc 0.974359\n",
      "2018-04-16T00:29:46.572904: step 425, loss 0.128549, acc 0.97\n",
      "2018-04-16T00:29:46.627188: step 426, loss 0.111419, acc 0.96\n",
      "2018-04-16T00:29:46.679074: step 427, loss 0.0982031, acc 0.99\n",
      "2018-04-16T00:29:46.725702: step 428, loss 0.181485, acc 0.935897\n",
      "2018-04-16T00:29:46.778040: step 429, loss 0.118088, acc 0.95\n",
      "2018-04-16T00:29:46.828672: step 430, loss 0.101373, acc 0.96\n",
      "2018-04-16T00:29:46.879885: step 431, loss 0.253577, acc 0.88\n",
      "2018-04-16T00:29:46.921973: step 432, loss 0.357091, acc 0.858974\n",
      "2018-04-16T00:29:46.979174: step 433, loss 1.06332, acc 0.73\n",
      "2018-04-16T00:29:47.031523: step 434, loss 1.02883, acc 0.75\n",
      "2018-04-16T00:29:47.087190: step 435, loss 0.355755, acc 0.88\n",
      "2018-04-16T00:29:47.130258: step 436, loss 0.117778, acc 0.935897\n",
      "2018-04-16T00:29:47.186710: step 437, loss 0.146673, acc 0.95\n",
      "2018-04-16T00:29:47.238374: step 438, loss 0.125015, acc 0.95\n",
      "2018-04-16T00:29:47.289698: step 439, loss 0.140806, acc 0.96\n",
      "2018-04-16T00:29:47.332586: step 440, loss 0.170383, acc 0.935897\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:47.435366: step 440, loss 0.761306, acc 0.637931, rec 0.927027, pre 0.582343, f1 0.715328\n",
      "\n",
      "2018-04-16T00:29:47.487558: step 441, loss 0.131019, acc 0.97\n",
      "2018-04-16T00:29:47.539139: step 442, loss 0.166443, acc 0.93\n",
      "2018-04-16T00:29:47.597901: step 443, loss 0.158201, acc 0.95\n",
      "2018-04-16T00:29:47.643953: step 444, loss 0.123299, acc 0.961538\n",
      "2018-04-16T00:29:47.695323: step 445, loss 0.148434, acc 0.96\n",
      "2018-04-16T00:29:47.747761: step 446, loss 0.148762, acc 0.95\n",
      "2018-04-16T00:29:47.798950: step 447, loss 0.135808, acc 0.96\n",
      "2018-04-16T00:29:47.840914: step 448, loss 0.13987, acc 0.948718\n",
      "2018-04-16T00:29:47.895623: step 449, loss 0.134097, acc 0.97\n",
      "2018-04-16T00:29:47.946970: step 450, loss 0.1524, acc 0.92\n",
      "2018-04-16T00:29:47.998825: step 451, loss 0.122714, acc 0.96\n",
      "2018-04-16T00:29:48.040648: step 452, loss 0.178175, acc 0.935897\n",
      "2018-04-16T00:29:48.092494: step 453, loss 0.200399, acc 0.93\n",
      "2018-04-16T00:29:48.147450: step 454, loss 0.113623, acc 0.96\n",
      "2018-04-16T00:29:48.199016: step 455, loss 0.110863, acc 0.97\n",
      "2018-04-16T00:29:48.241233: step 456, loss 0.200517, acc 0.935897\n",
      "2018-04-16T00:29:48.293538: step 457, loss 0.289633, acc 0.87\n",
      "2018-04-16T00:29:48.345511: step 458, loss 0.521373, acc 0.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:29:48.401496: step 459, loss 0.213252, acc 0.92\n",
      "2018-04-16T00:29:48.444032: step 460, loss 0.188731, acc 0.948718\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:48.545366: step 460, loss 0.550382, acc 0.751989, rec 0.854054, pre 0.703786, f1 0.771673\n",
      "\n",
      "2018-04-16T00:29:48.600263: step 461, loss 0.133802, acc 0.97\n",
      "2018-04-16T00:29:48.654434: step 462, loss 0.151451, acc 0.93\n",
      "2018-04-16T00:29:48.706679: step 463, loss 0.151445, acc 0.95\n",
      "2018-04-16T00:29:48.749342: step 464, loss 0.209987, acc 0.923077\n",
      "2018-04-16T00:29:48.800755: step 465, loss 0.112172, acc 0.96\n",
      "2018-04-16T00:29:48.852558: step 466, loss 0.174064, acc 0.95\n",
      "2018-04-16T00:29:48.908064: step 467, loss 0.134817, acc 0.96\n",
      "2018-04-16T00:29:48.950863: step 468, loss 0.158545, acc 0.935897\n",
      "2018-04-16T00:29:49.002919: step 469, loss 0.121735, acc 0.97\n",
      "2018-04-16T00:29:49.054274: step 470, loss 0.108759, acc 0.97\n",
      "2018-04-16T00:29:49.105830: step 471, loss 0.0977494, acc 0.97\n",
      "2018-04-16T00:29:49.151588: step 472, loss 0.137123, acc 0.935897\n",
      "2018-04-16T00:29:49.203766: step 473, loss 0.119164, acc 0.98\n",
      "2018-04-16T00:29:49.254646: step 474, loss 0.135785, acc 0.97\n",
      "2018-04-16T00:29:49.305739: step 475, loss 0.121349, acc 0.96\n",
      "2018-04-16T00:29:49.347558: step 476, loss 0.123455, acc 0.948718\n",
      "2018-04-16T00:29:49.402315: step 477, loss 0.0897719, acc 0.99\n",
      "2018-04-16T00:29:49.453249: step 478, loss 0.0837802, acc 0.98\n",
      "2018-04-16T00:29:49.504471: step 479, loss 0.160895, acc 0.96\n",
      "2018-04-16T00:29:49.546733: step 480, loss 0.0981886, acc 0.974359\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:49.650141: step 480, loss 0.787243, acc 0.65252, rec 0.940541, pre 0.591837, f1 0.726514\n",
      "\n",
      "2018-04-16T00:29:49.702850: step 481, loss 0.137496, acc 0.95\n",
      "2018-04-16T00:29:49.754709: step 482, loss 0.131523, acc 0.95\n",
      "2018-04-16T00:29:49.808880: step 483, loss 0.168601, acc 0.96\n",
      "2018-04-16T00:29:49.874756: step 484, loss 0.132596, acc 0.948718\n",
      "2018-04-16T00:29:49.928450: step 485, loss 0.154701, acc 0.93\n",
      "2018-04-16T00:29:49.982041: step 486, loss 0.195557, acc 0.89\n",
      "2018-04-16T00:29:50.034852: step 487, loss 0.211601, acc 0.91\n",
      "2018-04-16T00:29:50.080709: step 488, loss 0.114853, acc 0.961538\n",
      "2018-04-16T00:29:50.133055: step 489, loss 0.0692558, acc 0.99\n",
      "2018-04-16T00:29:50.185306: step 490, loss 0.120859, acc 0.96\n",
      "2018-04-16T00:29:50.237996: step 491, loss 0.129815, acc 0.96\n",
      "2018-04-16T00:29:50.287520: step 492, loss 0.0711238, acc 0.974359\n",
      "2018-04-16T00:29:50.339957: step 493, loss 0.103784, acc 0.96\n",
      "2018-04-16T00:29:50.391866: step 494, loss 0.0506269, acc 1\n",
      "2018-04-16T00:29:50.444733: step 495, loss 0.0991977, acc 0.95\n",
      "2018-04-16T00:29:50.487417: step 496, loss 0.112991, acc 0.948718\n",
      "2018-04-16T00:29:50.547682: step 497, loss 0.0696665, acc 0.98\n",
      "2018-04-16T00:29:50.600129: step 498, loss 0.0970431, acc 0.98\n",
      "2018-04-16T00:29:50.654484: step 499, loss 0.062182, acc 0.99\n",
      "2018-04-16T00:29:50.698156: step 500, loss 0.0897899, acc 0.974359\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:50.803595: step 500, loss 0.511813, acc 0.781167, rec 0.783784, pre 0.773333, f1 0.778523\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523838561/checkpoints/model-500\n",
      "\n",
      "2018-04-16T00:29:50.907643: step 501, loss 0.105067, acc 0.97\n",
      "2018-04-16T00:29:50.962151: step 502, loss 0.0750846, acc 0.99\n",
      "2018-04-16T00:29:51.019142: step 503, loss 0.0734316, acc 0.99\n",
      "2018-04-16T00:29:51.062815: step 504, loss 0.126234, acc 0.961538\n",
      "2018-04-16T00:29:51.115594: step 505, loss 0.116926, acc 0.94\n",
      "2018-04-16T00:29:51.168824: step 506, loss 0.160528, acc 0.92\n",
      "2018-04-16T00:29:51.224773: step 507, loss 0.0856975, acc 0.97\n",
      "2018-04-16T00:29:51.269980: step 508, loss 0.0766945, acc 0.987179\n",
      "2018-04-16T00:29:51.322100: step 509, loss 0.0549042, acc 1\n",
      "2018-04-16T00:29:51.373187: step 510, loss 0.091225, acc 0.95\n",
      "2018-04-16T00:29:51.424733: step 511, loss 0.09031, acc 0.99\n",
      "2018-04-16T00:29:51.469470: step 512, loss 0.110201, acc 0.974359\n",
      "2018-04-16T00:29:51.521026: step 513, loss 0.0804968, acc 0.97\n",
      "2018-04-16T00:29:51.572038: step 514, loss 0.0526866, acc 0.99\n",
      "2018-04-16T00:29:51.626657: step 515, loss 0.174033, acc 0.94\n",
      "2018-04-16T00:29:51.669757: step 516, loss 0.118883, acc 0.961538\n",
      "2018-04-16T00:29:51.727311: step 517, loss 0.0951348, acc 0.96\n",
      "2018-04-16T00:29:51.780614: step 518, loss 0.141603, acc 0.94\n",
      "2018-04-16T00:29:51.833642: step 519, loss 0.0971324, acc 0.96\n",
      "2018-04-16T00:29:51.876964: step 520, loss 0.0704654, acc 0.974359\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:51.980756: step 520, loss 0.993938, acc 0.614058, rec 0.932432, pre 0.564648, f1 0.703364\n",
      "\n",
      "2018-04-16T00:29:52.032941: step 521, loss 0.071548, acc 0.98\n",
      "2018-04-16T00:29:52.084011: step 522, loss 0.0779357, acc 0.99\n",
      "2018-04-16T00:29:52.136286: step 523, loss 0.186884, acc 0.91\n",
      "2018-04-16T00:29:52.179210: step 524, loss 0.326609, acc 0.871795\n",
      "2018-04-16T00:29:52.234114: step 525, loss 0.443201, acc 0.87\n",
      "2018-04-16T00:29:52.286537: step 526, loss 0.214773, acc 0.89\n",
      "2018-04-16T00:29:52.337870: step 527, loss 0.224418, acc 0.89\n",
      "2018-04-16T00:29:52.380515: step 528, loss 0.200328, acc 0.871795\n",
      "2018-04-16T00:29:52.432007: step 529, loss 0.125495, acc 0.95\n",
      "2018-04-16T00:29:52.486552: step 530, loss 0.0562416, acc 0.98\n",
      "2018-04-16T00:29:52.539539: step 531, loss 0.0635455, acc 0.98\n",
      "2018-04-16T00:29:52.582326: step 532, loss 0.186117, acc 0.923077\n",
      "2018-04-16T00:29:52.634258: step 533, loss 0.0651519, acc 0.98\n",
      "2018-04-16T00:29:52.685748: step 534, loss 0.078367, acc 0.98\n",
      "2018-04-16T00:29:52.742121: step 535, loss 0.103984, acc 0.99\n",
      "2018-04-16T00:29:52.784354: step 536, loss 0.0738297, acc 0.987179\n",
      "2018-04-16T00:29:52.835261: step 537, loss 0.149602, acc 0.94\n",
      "2018-04-16T00:29:52.889032: step 538, loss 0.307057, acc 0.86\n",
      "2018-04-16T00:29:52.941410: step 539, loss 0.192313, acc 0.9\n",
      "2018-04-16T00:29:52.988986: step 540, loss 0.141566, acc 0.948718\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:53.090799: step 540, loss 0.59585, acc 0.749337, rec 0.581081, pre 0.863454, f1 0.694669\n",
      "\n",
      "2018-04-16T00:29:53.142961: step 541, loss 0.215382, acc 0.91\n",
      "2018-04-16T00:29:53.201879: step 542, loss 0.46991, acc 0.89\n",
      "2018-04-16T00:29:53.254181: step 543, loss 0.099635, acc 0.96\n",
      "2018-04-16T00:29:53.297684: step 544, loss 0.0631389, acc 0.987179\n",
      "2018-04-16T00:29:53.350190: step 545, loss 0.0856157, acc 0.97\n",
      "2018-04-16T00:29:53.406614: step 546, loss 0.144936, acc 0.95\n",
      "2018-04-16T00:29:53.459237: step 547, loss 0.0467875, acc 0.99\n",
      "2018-04-16T00:29:53.501571: step 548, loss 0.125933, acc 0.974359\n",
      "2018-04-16T00:29:53.554038: step 549, loss 0.0846751, acc 0.99\n",
      "2018-04-16T00:29:53.605315: step 550, loss 0.0817109, acc 0.99\n",
      "2018-04-16T00:29:53.660928: step 551, loss 0.093485, acc 0.96\n",
      "2018-04-16T00:29:53.704062: step 552, loss 0.0738224, acc 0.961538\n",
      "2018-04-16T00:29:53.755445: step 553, loss 0.0992672, acc 0.96\n",
      "2018-04-16T00:29:53.807619: step 554, loss 0.0568988, acc 0.99\n",
      "2018-04-16T00:29:53.861891: step 555, loss 0.0996786, acc 0.97\n",
      "2018-04-16T00:29:53.908920: step 556, loss 0.0600214, acc 0.987179\n",
      "2018-04-16T00:29:53.962157: step 557, loss 0.0680351, acc 0.99\n",
      "2018-04-16T00:29:54.015507: step 558, loss 0.0784307, acc 0.97\n",
      "2018-04-16T00:29:54.067201: step 559, loss 0.0744982, acc 0.98\n",
      "2018-04-16T00:29:54.113402: step 560, loss 0.0684263, acc 0.974359\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:54.213308: step 560, loss 0.522752, acc 0.793103, rec 0.762162, pre 0.805714, f1 0.783333\n",
      "\n",
      "2018-04-16T00:29:54.265173: step 561, loss 0.0957897, acc 0.97\n",
      "2018-04-16T00:29:54.320633: step 562, loss 0.139333, acc 0.95\n",
      "2018-04-16T00:29:54.372902: step 563, loss 0.156602, acc 0.92\n",
      "2018-04-16T00:29:54.415650: step 564, loss 0.419414, acc 0.923077\n",
      "2018-04-16T00:29:54.468843: step 565, loss 0.279117, acc 0.89\n",
      "2018-04-16T00:29:54.525202: step 566, loss 0.420353, acc 0.86\n",
      "2018-04-16T00:29:54.578620: step 567, loss 0.195442, acc 0.92\n",
      "2018-04-16T00:29:54.622117: step 568, loss 0.191555, acc 0.935897\n",
      "2018-04-16T00:29:54.676568: step 569, loss 0.0713033, acc 0.98\n",
      "2018-04-16T00:29:54.732506: step 570, loss 0.0508618, acc 0.99\n",
      "2018-04-16T00:29:54.785178: step 571, loss 0.071021, acc 0.99\n",
      "2018-04-16T00:29:54.828710: step 572, loss 0.136335, acc 0.923077\n",
      "2018-04-16T00:29:54.881780: step 573, loss 0.192928, acc 0.93\n",
      "2018-04-16T00:29:54.939747: step 574, loss 0.121808, acc 0.93\n",
      "2018-04-16T00:29:54.992079: step 575, loss 0.0751203, acc 0.98\n",
      "2018-04-16T00:29:55.034583: step 576, loss 0.085076, acc 0.961538\n",
      "2018-04-16T00:29:55.086118: step 577, loss 0.0672817, acc 0.99\n",
      "2018-04-16T00:29:55.137268: step 578, loss 0.0606079, acc 0.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:29:55.193291: step 579, loss 0.0320262, acc 1\n",
      "2018-04-16T00:29:55.235802: step 580, loss 0.0233853, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:55.334771: step 580, loss 0.814933, acc 0.672414, rec 0.940541, pre 0.60733, f1 0.73807\n",
      "\n",
      "2018-04-16T00:29:55.387053: step 581, loss 0.0920377, acc 0.97\n",
      "2018-04-16T00:29:55.450264: step 582, loss 0.0844786, acc 0.98\n",
      "2018-04-16T00:29:55.504013: step 583, loss 0.0467828, acc 1\n",
      "2018-04-16T00:29:55.547301: step 584, loss 0.0420094, acc 1\n",
      "2018-04-16T00:29:55.599945: step 585, loss 0.0791015, acc 0.99\n",
      "2018-04-16T00:29:55.656977: step 586, loss 0.0971629, acc 0.97\n",
      "2018-04-16T00:29:55.709425: step 587, loss 0.0828781, acc 0.98\n",
      "2018-04-16T00:29:55.752238: step 588, loss 0.0387537, acc 1\n",
      "2018-04-16T00:29:55.805292: step 589, loss 0.0451118, acc 0.99\n",
      "2018-04-16T00:29:55.857392: step 590, loss 0.0409253, acc 1\n",
      "2018-04-16T00:29:55.912328: step 591, loss 0.0697863, acc 0.97\n",
      "2018-04-16T00:29:55.955146: step 592, loss 0.119656, acc 0.948718\n",
      "2018-04-16T00:29:56.007177: step 593, loss 0.100627, acc 0.96\n",
      "2018-04-16T00:29:56.059524: step 594, loss 0.122345, acc 0.95\n",
      "2018-04-16T00:29:56.111259: step 595, loss 0.0744403, acc 0.98\n",
      "2018-04-16T00:29:56.156942: step 596, loss 0.0536627, acc 0.987179\n",
      "2018-04-16T00:29:56.208423: step 597, loss 0.0369685, acc 1\n",
      "2018-04-16T00:29:56.259932: step 598, loss 0.0568485, acc 0.99\n",
      "2018-04-16T00:29:56.312890: step 599, loss 0.0692595, acc 0.98\n",
      "2018-04-16T00:29:56.356182: step 600, loss 0.0493673, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:56.466666: step 600, loss 0.828136, acc 0.671088, rec 0.924324, pre 0.608541, f1 0.733906\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523838561/checkpoints/model-600\n",
      "\n",
      "2018-04-16T00:29:56.577764: step 601, loss 0.0883731, acc 0.97\n",
      "2018-04-16T00:29:56.630473: step 602, loss 0.0422973, acc 1\n",
      "2018-04-16T00:29:56.681471: step 603, loss 0.0418918, acc 0.99\n",
      "2018-04-16T00:29:56.723672: step 604, loss 0.0522952, acc 0.987179\n",
      "2018-04-16T00:29:56.775476: step 605, loss 0.0695826, acc 0.98\n",
      "2018-04-16T00:29:56.831417: step 606, loss 0.0720938, acc 0.97\n",
      "2018-04-16T00:29:56.883601: step 607, loss 0.0530354, acc 0.99\n",
      "2018-04-16T00:29:56.926139: step 608, loss 0.0673713, acc 0.974359\n",
      "2018-04-16T00:29:56.977724: step 609, loss 0.088218, acc 0.97\n",
      "2018-04-16T00:29:57.029935: step 610, loss 0.0410736, acc 1\n",
      "2018-04-16T00:29:57.084717: step 611, loss 0.0730807, acc 0.97\n",
      "2018-04-16T00:29:57.127771: step 612, loss 0.0318298, acc 1\n",
      "2018-04-16T00:29:57.180608: step 613, loss 0.0626799, acc 0.98\n",
      "2018-04-16T00:29:57.231849: step 614, loss 0.0912816, acc 0.98\n",
      "2018-04-16T00:29:57.290836: step 615, loss 0.0477097, acc 0.98\n",
      "2018-04-16T00:29:57.333424: step 616, loss 0.0324241, acc 1\n",
      "2018-04-16T00:29:57.385160: step 617, loss 0.0599735, acc 0.98\n",
      "2018-04-16T00:29:57.436072: step 618, loss 0.0274927, acc 1\n",
      "2018-04-16T00:29:57.487604: step 619, loss 0.0391986, acc 1\n",
      "2018-04-16T00:29:57.538433: step 620, loss 0.0507986, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:57.642579: step 620, loss 0.930651, acc 0.649867, rec 0.943243, pre 0.589527, f1 0.725572\n",
      "\n",
      "2018-04-16T00:29:57.695955: step 621, loss 0.0397533, acc 0.98\n",
      "2018-04-16T00:29:57.752494: step 622, loss 0.0561873, acc 0.99\n",
      "2018-04-16T00:29:57.804102: step 623, loss 0.090975, acc 0.96\n",
      "2018-04-16T00:29:57.846407: step 624, loss 0.187914, acc 0.910256\n",
      "2018-04-16T00:29:57.897654: step 625, loss 0.356014, acc 0.88\n",
      "2018-04-16T00:29:57.948818: step 626, loss 0.305754, acc 0.88\n",
      "2018-04-16T00:29:58.003819: step 627, loss 0.41166, acc 0.88\n",
      "2018-04-16T00:29:58.046422: step 628, loss 0.63136, acc 0.794872\n",
      "2018-04-16T00:29:58.097620: step 629, loss 1.49345, acc 0.74\n",
      "2018-04-16T00:29:58.148811: step 630, loss 0.136593, acc 0.92\n",
      "2018-04-16T00:29:58.201272: step 631, loss 0.064582, acc 0.97\n",
      "2018-04-16T00:29:58.247641: step 632, loss 0.0807674, acc 0.974359\n",
      "2018-04-16T00:29:58.299106: step 633, loss 0.102103, acc 0.98\n",
      "2018-04-16T00:29:58.350538: step 634, loss 0.0376373, acc 1\n",
      "2018-04-16T00:29:58.406772: step 635, loss 0.0507846, acc 0.99\n",
      "2018-04-16T00:29:58.453446: step 636, loss 0.0527923, acc 0.987179\n",
      "2018-04-16T00:29:58.505140: step 637, loss 0.0508235, acc 0.99\n",
      "2018-04-16T00:29:58.557972: step 638, loss 0.0534458, acc 0.99\n",
      "2018-04-16T00:29:58.609161: step 639, loss 0.0496272, acc 0.99\n",
      "2018-04-16T00:29:58.651654: step 640, loss 0.0484915, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:58.758947: step 640, loss 1.01179, acc 0.625995, rec 0.937838, pre 0.572607, f1 0.711066\n",
      "\n",
      "2018-04-16T00:29:58.811671: step 641, loss 0.101967, acc 0.98\n",
      "2018-04-16T00:29:58.863173: step 642, loss 0.0566885, acc 0.99\n",
      "2018-04-16T00:29:58.920392: step 643, loss 0.0440929, acc 1\n",
      "2018-04-16T00:29:58.965912: step 644, loss 0.0446678, acc 0.987179\n",
      "2018-04-16T00:29:59.017661: step 645, loss 0.0523662, acc 0.98\n",
      "2018-04-16T00:29:59.069316: step 646, loss 0.0520245, acc 0.98\n",
      "2018-04-16T00:29:59.121255: step 647, loss 0.0433735, acc 1\n",
      "2018-04-16T00:29:59.163481: step 648, loss 0.0256872, acc 1\n",
      "2018-04-16T00:29:59.219019: step 649, loss 0.084522, acc 0.98\n",
      "2018-04-16T00:29:59.270429: step 650, loss 0.0298705, acc 1\n",
      "2018-04-16T00:29:59.322125: step 651, loss 0.0322266, acc 1\n",
      "2018-04-16T00:29:59.367152: step 652, loss 0.0225938, acc 1\n",
      "2018-04-16T00:29:59.424658: step 653, loss 0.0318643, acc 0.99\n",
      "2018-04-16T00:29:59.476849: step 654, loss 0.0558451, acc 0.99\n",
      "2018-04-16T00:29:59.528986: step 655, loss 0.0217958, acc 1\n",
      "2018-04-16T00:29:59.573349: step 656, loss 0.0400951, acc 1\n",
      "2018-04-16T00:29:59.640668: step 657, loss 0.041835, acc 1\n",
      "2018-04-16T00:29:59.696993: step 658, loss 0.104294, acc 0.98\n",
      "2018-04-16T00:29:59.753270: step 659, loss 0.03388, acc 1\n",
      "2018-04-16T00:29:59.797512: step 660, loss 0.0579612, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:29:59.901730: step 660, loss 0.574261, acc 0.758621, rec 0.808108, pre 0.729268, f1 0.766667\n",
      "\n",
      "2018-04-16T00:29:59.951712: step 661, loss 0.0499044, acc 0.98\n",
      "2018-04-16T00:30:00.002705: step 662, loss 0.0359446, acc 0.99\n",
      "2018-04-16T00:30:00.053772: step 663, loss 0.0821597, acc 0.99\n",
      "2018-04-16T00:30:00.095189: step 664, loss 0.0354334, acc 1\n",
      "2018-04-16T00:30:00.149658: step 665, loss 0.0325008, acc 1\n",
      "2018-04-16T00:30:00.201632: step 666, loss 0.034426, acc 1\n",
      "2018-04-16T00:30:00.252484: step 667, loss 0.0237796, acc 1\n",
      "2018-04-16T00:30:00.295412: step 668, loss 0.0703919, acc 0.974359\n",
      "2018-04-16T00:30:00.346879: step 669, loss 0.0376943, acc 0.99\n",
      "2018-04-16T00:30:00.402377: step 670, loss 0.0945655, acc 0.97\n",
      "2018-04-16T00:30:00.454709: step 671, loss 0.0493584, acc 1\n",
      "2018-04-16T00:30:00.497264: step 672, loss 0.0557276, acc 0.974359\n",
      "2018-04-16T00:30:00.549919: step 673, loss 0.0376209, acc 0.99\n",
      "2018-04-16T00:30:00.607487: step 674, loss 0.0397338, acc 1\n",
      "2018-04-16T00:30:00.662148: step 675, loss 0.0563234, acc 0.99\n",
      "2018-04-16T00:30:00.706713: step 676, loss 0.0277296, acc 1\n",
      "2018-04-16T00:30:00.761286: step 677, loss 0.0360055, acc 1\n",
      "2018-04-16T00:30:00.831963: step 678, loss 0.0293038, acc 0.99\n",
      "2018-04-16T00:30:00.894916: step 679, loss 0.0310194, acc 1\n",
      "2018-04-16T00:30:00.937621: step 680, loss 0.0419408, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:01.039842: step 680, loss 0.634385, acc 0.742706, rec 0.862162, pre 0.690476, f1 0.766827\n",
      "\n",
      "2018-04-16T00:30:01.091587: step 681, loss 0.0467456, acc 0.99\n",
      "2018-04-16T00:30:01.143695: step 682, loss 0.0600823, acc 0.97\n",
      "2018-04-16T00:30:01.196576: step 683, loss 0.0453252, acc 1\n",
      "2018-04-16T00:30:01.238940: step 684, loss 0.0272753, acc 1\n",
      "2018-04-16T00:30:01.294614: step 685, loss 0.0253093, acc 1\n",
      "2018-04-16T00:30:01.348015: step 686, loss 0.0251976, acc 1\n",
      "2018-04-16T00:30:01.400527: step 687, loss 0.0279369, acc 1\n",
      "2018-04-16T00:30:01.455476: step 688, loss 0.0225732, acc 1\n",
      "2018-04-16T00:30:01.515904: step 689, loss 0.0240908, acc 1\n",
      "2018-04-16T00:30:01.569842: step 690, loss 0.0353601, acc 0.99\n",
      "2018-04-16T00:30:01.621989: step 691, loss 0.0379882, acc 1\n",
      "2018-04-16T00:30:01.668626: step 692, loss 0.0385559, acc 1\n",
      "2018-04-16T00:30:01.728187: step 693, loss 0.0298401, acc 1\n",
      "2018-04-16T00:30:01.781820: step 694, loss 0.038949, acc 0.99\n",
      "2018-04-16T00:30:01.833239: step 695, loss 0.0559436, acc 0.99\n",
      "2018-04-16T00:30:01.874948: step 696, loss 0.0435354, acc 0.987179\n",
      "2018-04-16T00:30:01.926818: step 697, loss 0.0152164, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:30:01.981658: step 698, loss 0.0600437, acc 0.99\n",
      "2018-04-16T00:30:02.033224: step 699, loss 0.0512024, acc 0.99\n",
      "2018-04-16T00:30:02.075608: step 700, loss 0.0342376, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:02.178117: step 700, loss 0.818037, acc 0.688329, rec 0.924324, pre 0.622951, f1 0.744287\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523838561/checkpoints/model-700\n",
      "\n",
      "2018-04-16T00:30:02.292590: step 701, loss 0.0182211, acc 1\n",
      "2018-04-16T00:30:02.345150: step 702, loss 0.0532038, acc 0.98\n",
      "2018-04-16T00:30:02.397776: step 703, loss 0.0497453, acc 0.99\n",
      "2018-04-16T00:30:02.443579: step 704, loss 0.015108, acc 1\n",
      "2018-04-16T00:30:02.495250: step 705, loss 0.0327728, acc 1\n",
      "2018-04-16T00:30:02.548077: step 706, loss 0.0331893, acc 1\n",
      "2018-04-16T00:30:02.609261: step 707, loss 0.0306036, acc 1\n",
      "2018-04-16T00:30:02.666534: step 708, loss 0.0420312, acc 0.987179\n",
      "2018-04-16T00:30:02.718580: step 709, loss 0.0463355, acc 0.99\n",
      "2018-04-16T00:30:02.771801: step 710, loss 0.0226362, acc 1\n",
      "2018-04-16T00:30:02.823737: step 711, loss 0.0326794, acc 1\n",
      "2018-04-16T00:30:02.872826: step 712, loss 0.0414338, acc 0.987179\n",
      "2018-04-16T00:30:02.925325: step 713, loss 0.033147, acc 0.99\n",
      "2018-04-16T00:30:02.978038: step 714, loss 0.0293922, acc 0.99\n",
      "2018-04-16T00:30:03.030642: step 715, loss 0.0238832, acc 1\n",
      "2018-04-16T00:30:03.073648: step 716, loss 0.0365031, acc 0.987179\n",
      "2018-04-16T00:30:03.129689: step 717, loss 0.0215776, acc 1\n",
      "2018-04-16T00:30:03.182760: step 718, loss 0.0580108, acc 0.98\n",
      "2018-04-16T00:30:03.235049: step 719, loss 0.0747556, acc 0.97\n",
      "2018-04-16T00:30:03.279004: step 720, loss 0.0742422, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:03.382041: step 720, loss 0.840973, acc 0.688329, rec 0.913514, pre 0.624769, f1 0.742042\n",
      "\n",
      "2018-04-16T00:30:03.434050: step 721, loss 0.026522, acc 0.99\n",
      "2018-04-16T00:30:03.487216: step 722, loss 0.0388482, acc 1\n",
      "2018-04-16T00:30:03.539760: step 723, loss 0.0247532, acc 1\n",
      "2018-04-16T00:30:03.593958: step 724, loss 0.0248205, acc 1\n",
      "2018-04-16T00:30:03.671628: step 725, loss 0.0412725, acc 0.98\n",
      "2018-04-16T00:30:03.747838: step 726, loss 0.0352716, acc 0.99\n",
      "2018-04-16T00:30:03.833036: step 727, loss 0.0204212, acc 1\n",
      "2018-04-16T00:30:03.884862: step 728, loss 0.0166302, acc 1\n",
      "2018-04-16T00:30:03.938273: step 729, loss 0.0294178, acc 1\n",
      "2018-04-16T00:30:03.991176: step 730, loss 0.0205388, acc 1\n",
      "2018-04-16T00:30:04.049486: step 731, loss 0.0385729, acc 0.98\n",
      "2018-04-16T00:30:04.092049: step 732, loss 0.0269124, acc 1\n",
      "2018-04-16T00:30:04.144309: step 733, loss 0.0141202, acc 1\n",
      "2018-04-16T00:30:04.200407: step 734, loss 0.0253715, acc 1\n",
      "2018-04-16T00:30:04.261520: step 735, loss 0.0267387, acc 1\n",
      "2018-04-16T00:30:04.306288: step 736, loss 0.0254213, acc 1\n",
      "2018-04-16T00:30:04.359121: step 737, loss 0.0498983, acc 0.98\n",
      "2018-04-16T00:30:04.412298: step 738, loss 0.0254216, acc 1\n",
      "2018-04-16T00:30:04.468062: step 739, loss 0.0197197, acc 1\n",
      "2018-04-16T00:30:04.511631: step 740, loss 0.0283335, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:04.613687: step 740, loss 0.715837, acc 0.729443, rec 0.897297, pre 0.666667, f1 0.764977\n",
      "\n",
      "2018-04-16T00:30:04.665461: step 741, loss 0.023694, acc 1\n",
      "2018-04-16T00:30:04.723329: step 742, loss 0.0208523, acc 1\n",
      "2018-04-16T00:30:04.776245: step 743, loss 0.0192559, acc 1\n",
      "2018-04-16T00:30:04.819773: step 744, loss 0.0412865, acc 0.987179\n",
      "2018-04-16T00:30:04.873469: step 745, loss 0.0323598, acc 0.98\n",
      "2018-04-16T00:30:04.931018: step 746, loss 0.01992, acc 1\n",
      "2018-04-16T00:30:04.983873: step 747, loss 0.0356759, acc 1\n",
      "2018-04-16T00:30:05.027385: step 748, loss 0.0360175, acc 0.987179\n",
      "2018-04-16T00:30:05.081016: step 749, loss 0.0314048, acc 1\n",
      "2018-04-16T00:30:05.139628: step 750, loss 0.0241108, acc 0.99\n",
      "2018-04-16T00:30:05.195358: step 751, loss 0.0156645, acc 1\n",
      "2018-04-16T00:30:05.239435: step 752, loss 0.0539773, acc 0.987179\n",
      "2018-04-16T00:30:05.295881: step 753, loss 0.0248204, acc 1\n",
      "2018-04-16T00:30:05.355925: step 754, loss 0.0170221, acc 1\n",
      "2018-04-16T00:30:05.409351: step 755, loss 0.0265864, acc 1\n",
      "2018-04-16T00:30:05.453201: step 756, loss 0.0520919, acc 0.987179\n",
      "2018-04-16T00:30:05.506183: step 757, loss 0.0453344, acc 0.99\n",
      "2018-04-16T00:30:05.562952: step 758, loss 0.0228807, acc 1\n",
      "2018-04-16T00:30:05.616565: step 759, loss 0.0392816, acc 0.99\n",
      "2018-04-16T00:30:05.659763: step 760, loss 0.0217961, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:05.763102: step 760, loss 0.661261, acc 0.758621, rec 0.87027, pre 0.70614, f1 0.779661\n",
      "\n",
      "2018-04-16T00:30:05.819874: step 761, loss 0.0321829, acc 0.99\n",
      "2018-04-16T00:30:05.876525: step 762, loss 0.023053, acc 1\n",
      "2018-04-16T00:30:05.929070: step 763, loss 0.0145966, acc 1\n",
      "2018-04-16T00:30:05.973344: step 764, loss 0.0410127, acc 0.987179\n",
      "2018-04-16T00:30:06.031792: step 765, loss 0.0515072, acc 0.99\n",
      "2018-04-16T00:30:06.085134: step 766, loss 0.0337223, acc 0.99\n",
      "2018-04-16T00:30:06.139580: step 767, loss 0.0130573, acc 1\n",
      "2018-04-16T00:30:06.181963: step 768, loss 0.0332323, acc 1\n",
      "2018-04-16T00:30:06.236947: step 769, loss 0.0494761, acc 0.99\n",
      "2018-04-16T00:30:06.290762: step 770, loss 0.0504581, acc 1\n",
      "2018-04-16T00:30:06.345321: step 771, loss 0.0353751, acc 0.98\n",
      "2018-04-16T00:30:06.391861: step 772, loss 0.0176851, acc 1\n",
      "2018-04-16T00:30:06.450843: step 773, loss 0.0154168, acc 1\n",
      "2018-04-16T00:30:06.504513: step 774, loss 0.0131679, acc 1\n",
      "2018-04-16T00:30:06.557832: step 775, loss 0.0187007, acc 1\n",
      "2018-04-16T00:30:06.605209: step 776, loss 0.0479675, acc 0.987179\n",
      "2018-04-16T00:30:06.664971: step 777, loss 0.0360287, acc 0.99\n",
      "2018-04-16T00:30:06.727141: step 778, loss 0.0206297, acc 1\n",
      "2018-04-16T00:30:06.782764: step 779, loss 0.0516946, acc 0.99\n",
      "2018-04-16T00:30:06.833690: step 780, loss 0.0676264, acc 0.974359\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:06.966968: step 780, loss 1.72069, acc 0.564987, rec 0.959459, pre 0.531437, f1 0.684008\n",
      "\n",
      "2018-04-16T00:30:07.026047: step 781, loss 0.111775, acc 0.95\n",
      "2018-04-16T00:30:07.084124: step 782, loss 0.201237, acc 0.89\n",
      "2018-04-16T00:30:07.142060: step 783, loss 0.682279, acc 0.84\n",
      "2018-04-16T00:30:07.196518: step 784, loss 0.401887, acc 0.833333\n",
      "2018-04-16T00:30:07.254554: step 785, loss 0.666525, acc 0.88\n",
      "2018-04-16T00:30:07.309161: step 786, loss 0.225413, acc 0.89\n",
      "2018-04-16T00:30:07.363931: step 787, loss 0.189151, acc 0.91\n",
      "2018-04-16T00:30:07.412569: step 788, loss 0.0209247, acc 1\n",
      "2018-04-16T00:30:07.468607: step 789, loss 0.0652388, acc 0.99\n",
      "2018-04-16T00:30:07.522610: step 790, loss 0.0696238, acc 0.97\n",
      "2018-04-16T00:30:07.578222: step 791, loss 0.0112201, acc 1\n",
      "2018-04-16T00:30:07.627993: step 792, loss 0.016279, acc 1\n",
      "2018-04-16T00:30:07.684481: step 793, loss 0.0197307, acc 1\n",
      "2018-04-16T00:30:07.738816: step 794, loss 0.0188743, acc 0.99\n",
      "2018-04-16T00:30:07.794918: step 795, loss 0.0557633, acc 0.99\n",
      "2018-04-16T00:30:07.843305: step 796, loss 0.118819, acc 0.987179\n",
      "2018-04-16T00:30:07.895581: step 797, loss 0.0479299, acc 0.99\n",
      "2018-04-16T00:30:07.948726: step 798, loss 0.0312547, acc 0.99\n",
      "2018-04-16T00:30:08.004671: step 799, loss 0.0212907, acc 1\n",
      "2018-04-16T00:30:08.054838: step 800, loss 0.0344878, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:08.157933: step 800, loss 0.658086, acc 0.758621, rec 0.867568, pre 0.707048, f1 0.779126\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523838561/checkpoints/model-800\n",
      "\n",
      "2018-04-16T00:30:08.272676: step 801, loss 0.0164916, acc 1\n",
      "2018-04-16T00:30:08.327613: step 802, loss 0.0260964, acc 0.99\n",
      "2018-04-16T00:30:08.382307: step 803, loss 0.0223815, acc 1\n",
      "2018-04-16T00:30:08.427550: step 804, loss 0.0735347, acc 0.987179\n",
      "2018-04-16T00:30:08.487846: step 805, loss 0.0465693, acc 0.98\n",
      "2018-04-16T00:30:08.541732: step 806, loss 0.0391949, acc 0.99\n",
      "2018-04-16T00:30:08.597143: step 807, loss 0.0161117, acc 1\n",
      "2018-04-16T00:30:08.640447: step 808, loss 0.0147399, acc 1\n",
      "2018-04-16T00:30:08.697124: step 809, loss 0.0397454, acc 0.98\n",
      "2018-04-16T00:30:08.752308: step 810, loss 0.021587, acc 1\n",
      "2018-04-16T00:30:08.818639: step 811, loss 0.0190576, acc 1\n",
      "2018-04-16T00:30:08.863791: step 812, loss 0.115748, acc 0.987179\n",
      "2018-04-16T00:30:08.921795: step 813, loss 0.0449891, acc 0.98\n",
      "2018-04-16T00:30:08.975928: step 814, loss 0.0135641, acc 1\n",
      "2018-04-16T00:30:09.029476: step 815, loss 0.0262663, acc 0.99\n",
      "2018-04-16T00:30:09.073798: step 816, loss 0.0116515, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:30:09.129891: step 817, loss 0.00944699, acc 1\n",
      "2018-04-16T00:30:09.184909: step 818, loss 0.0240922, acc 1\n",
      "2018-04-16T00:30:09.237744: step 819, loss 0.0283919, acc 0.99\n",
      "2018-04-16T00:30:09.287136: step 820, loss 0.0266178, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:09.402572: step 820, loss 0.711854, acc 0.738727, rec 0.891892, pre 0.677618, f1 0.770128\n",
      "\n",
      "2018-04-16T00:30:09.463301: step 821, loss 0.0187418, acc 1\n",
      "2018-04-16T00:30:09.519528: step 822, loss 0.0216506, acc 1\n",
      "2018-04-16T00:30:09.574519: step 823, loss 0.0192687, acc 1\n",
      "2018-04-16T00:30:09.621422: step 824, loss 0.030773, acc 0.987179\n",
      "2018-04-16T00:30:09.674887: step 825, loss 0.0317018, acc 0.99\n",
      "2018-04-16T00:30:09.729259: step 826, loss 0.0242411, acc 1\n",
      "2018-04-16T00:30:09.785297: step 827, loss 0.0389972, acc 0.99\n",
      "2018-04-16T00:30:09.842183: step 828, loss 0.0268529, acc 1\n",
      "2018-04-16T00:30:09.913617: step 829, loss 0.0186404, acc 1\n",
      "2018-04-16T00:30:09.967409: step 830, loss 0.0520829, acc 0.98\n",
      "2018-04-16T00:30:10.025268: step 831, loss 0.0597802, acc 0.97\n",
      "2018-04-16T00:30:10.075481: step 832, loss 0.022243, acc 1\n",
      "2018-04-16T00:30:10.131123: step 833, loss 0.0179468, acc 1\n",
      "2018-04-16T00:30:10.188920: step 834, loss 0.0186722, acc 1\n",
      "2018-04-16T00:30:10.244793: step 835, loss 0.0263606, acc 0.99\n",
      "2018-04-16T00:30:10.294932: step 836, loss 0.0306601, acc 0.987179\n",
      "2018-04-16T00:30:10.353854: step 837, loss 0.0234255, acc 1\n",
      "2018-04-16T00:30:10.408979: step 838, loss 0.0174156, acc 1\n",
      "2018-04-16T00:30:10.462391: step 839, loss 0.0244615, acc 1\n",
      "2018-04-16T00:30:10.511748: step 840, loss 0.0141306, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:10.615309: step 840, loss 0.832499, acc 0.710875, rec 0.916216, pre 0.644487, f1 0.756696\n",
      "\n",
      "2018-04-16T00:30:10.671626: step 841, loss 0.0212398, acc 0.99\n",
      "2018-04-16T00:30:10.730480: step 842, loss 0.023942, acc 0.99\n",
      "2018-04-16T00:30:10.790389: step 843, loss 0.0187871, acc 1\n",
      "2018-04-16T00:30:10.844878: step 844, loss 0.0470663, acc 0.987179\n",
      "2018-04-16T00:30:10.916674: step 845, loss 0.0562519, acc 0.99\n",
      "2018-04-16T00:30:10.975230: step 846, loss 0.0647038, acc 0.99\n",
      "2018-04-16T00:30:11.029304: step 847, loss 0.0192001, acc 1\n",
      "2018-04-16T00:30:11.074439: step 848, loss 0.0132166, acc 1\n",
      "2018-04-16T00:30:11.129166: step 849, loss 0.0144582, acc 1\n",
      "2018-04-16T00:30:11.186161: step 850, loss 0.0140669, acc 1\n",
      "2018-04-16T00:30:11.239091: step 851, loss 0.0325142, acc 0.99\n",
      "2018-04-16T00:30:11.283063: step 852, loss 0.0345394, acc 0.987179\n",
      "2018-04-16T00:30:11.337867: step 853, loss 0.0258677, acc 0.99\n",
      "2018-04-16T00:30:11.394768: step 854, loss 0.0192611, acc 1\n",
      "2018-04-16T00:30:11.448619: step 855, loss 0.0550989, acc 0.99\n",
      "2018-04-16T00:30:11.492606: step 856, loss 0.039429, acc 0.987179\n",
      "2018-04-16T00:30:11.545947: step 857, loss 0.0232859, acc 1\n",
      "2018-04-16T00:30:11.602919: step 858, loss 0.0273629, acc 0.99\n",
      "2018-04-16T00:30:11.661049: step 859, loss 0.0046809, acc 1\n",
      "2018-04-16T00:30:11.707162: step 860, loss 0.0338006, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:11.830083: step 860, loss 0.662886, acc 0.769231, rec 0.872973, pre 0.717778, f1 0.787805\n",
      "\n",
      "2018-04-16T00:30:11.883818: step 861, loss 0.0338417, acc 0.98\n",
      "2018-04-16T00:30:11.937269: step 862, loss 0.0237666, acc 1\n",
      "2018-04-16T00:30:11.990267: step 863, loss 0.0176297, acc 1\n",
      "2018-04-16T00:30:12.039904: step 864, loss 0.0132358, acc 1\n",
      "2018-04-16T00:30:12.096105: step 865, loss 0.0241163, acc 0.99\n",
      "2018-04-16T00:30:12.151890: step 866, loss 0.0181508, acc 0.99\n",
      "2018-04-16T00:30:12.205207: step 867, loss 0.0222318, acc 1\n",
      "2018-04-16T00:30:12.254579: step 868, loss 0.017528, acc 1\n",
      "2018-04-16T00:30:12.317566: step 869, loss 0.0289477, acc 0.99\n",
      "2018-04-16T00:30:12.383960: step 870, loss 0.0346507, acc 0.98\n",
      "2018-04-16T00:30:12.437023: step 871, loss 0.0137479, acc 1\n",
      "2018-04-16T00:30:12.484975: step 872, loss 0.0200951, acc 1\n",
      "2018-04-16T00:30:12.540383: step 873, loss 0.0125877, acc 1\n",
      "2018-04-16T00:30:12.611809: step 874, loss 0.00679193, acc 1\n",
      "2018-04-16T00:30:12.674085: step 875, loss 0.0267521, acc 0.99\n",
      "2018-04-16T00:30:12.725480: step 876, loss 0.021778, acc 1\n",
      "2018-04-16T00:30:12.781770: step 877, loss 0.00850598, acc 1\n",
      "2018-04-16T00:30:12.842221: step 878, loss 0.0161252, acc 1\n",
      "2018-04-16T00:30:12.908801: step 879, loss 0.0175155, acc 1\n",
      "2018-04-16T00:30:12.956409: step 880, loss 0.0185093, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:13.055977: step 880, loss 0.826122, acc 0.713528, rec 0.913514, pre 0.64751, f1 0.757848\n",
      "\n",
      "2018-04-16T00:30:13.107792: step 881, loss 0.00936033, acc 1\n",
      "2018-04-16T00:30:13.168492: step 882, loss 0.0160435, acc 1\n",
      "2018-04-16T00:30:13.226091: step 883, loss 0.0203665, acc 0.99\n",
      "2018-04-16T00:30:13.270127: step 884, loss 0.0228342, acc 1\n",
      "2018-04-16T00:30:13.323684: step 885, loss 0.0155193, acc 1\n",
      "2018-04-16T00:30:13.381797: step 886, loss 0.0280797, acc 0.99\n",
      "2018-04-16T00:30:13.435075: step 887, loss 0.0230537, acc 1\n",
      "2018-04-16T00:30:13.478633: step 888, loss 0.0117742, acc 1\n",
      "2018-04-16T00:30:13.534139: step 889, loss 0.0108036, acc 1\n",
      "2018-04-16T00:30:13.595290: step 890, loss 0.0364954, acc 0.99\n",
      "2018-04-16T00:30:13.652444: step 891, loss 0.0106807, acc 1\n",
      "2018-04-16T00:30:13.698482: step 892, loss 0.0327608, acc 0.987179\n",
      "2018-04-16T00:30:13.752617: step 893, loss 0.0393857, acc 0.99\n",
      "2018-04-16T00:30:13.810023: step 894, loss 0.0194365, acc 1\n",
      "2018-04-16T00:30:13.862516: step 895, loss 0.0110681, acc 1\n",
      "2018-04-16T00:30:13.905927: step 896, loss 0.0149115, acc 1\n",
      "2018-04-16T00:30:13.960020: step 897, loss 0.0150268, acc 1\n",
      "2018-04-16T00:30:14.017469: step 898, loss 0.0220589, acc 0.99\n",
      "2018-04-16T00:30:14.072692: step 899, loss 0.0248081, acc 1\n",
      "2018-04-16T00:30:14.116630: step 900, loss 0.00795715, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:14.223144: step 900, loss 0.998303, acc 0.675066, rec 0.937838, pre 0.609842, f1 0.739084\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523838561/checkpoints/model-900\n",
      "\n",
      "2018-04-16T00:30:14.330697: step 901, loss 0.0146282, acc 1\n",
      "2018-04-16T00:30:14.383813: step 902, loss 0.0168287, acc 1\n",
      "2018-04-16T00:30:14.440392: step 903, loss 0.00927093, acc 1\n",
      "2018-04-16T00:30:14.483760: step 904, loss 0.0672434, acc 0.987179\n",
      "2018-04-16T00:30:14.538271: step 905, loss 0.0270499, acc 1\n",
      "2018-04-16T00:30:14.598658: step 906, loss 0.0513552, acc 0.99\n",
      "2018-04-16T00:30:14.654780: step 907, loss 0.0505946, acc 0.98\n",
      "2018-04-16T00:30:14.697323: step 908, loss 0.0371718, acc 1\n",
      "2018-04-16T00:30:14.748968: step 909, loss 0.0149258, acc 1\n",
      "2018-04-16T00:30:14.800659: step 910, loss 0.0135164, acc 1\n",
      "2018-04-16T00:30:14.851899: step 911, loss 0.00874642, acc 1\n",
      "2018-04-16T00:30:14.898353: step 912, loss 0.014331, acc 0.987179\n",
      "2018-04-16T00:30:14.950374: step 913, loss 0.0132371, acc 1\n",
      "2018-04-16T00:30:15.002511: step 914, loss 0.0120066, acc 1\n",
      "2018-04-16T00:30:15.055872: step 915, loss 0.00674096, acc 1\n",
      "2018-04-16T00:30:15.103301: step 916, loss 0.0106861, acc 1\n",
      "2018-04-16T00:30:15.158850: step 917, loss 0.0104407, acc 1\n",
      "2018-04-16T00:30:15.213214: step 918, loss 0.0137548, acc 1\n",
      "2018-04-16T00:30:15.269915: step 919, loss 0.0122447, acc 1\n",
      "2018-04-16T00:30:15.318730: step 920, loss 0.00810544, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:15.418456: step 920, loss 0.700038, acc 0.758621, rec 0.851351, pre 0.71267, f1 0.775862\n",
      "\n",
      "2018-04-16T00:30:15.470800: step 921, loss 0.0100609, acc 1\n",
      "2018-04-16T00:30:15.527554: step 922, loss 0.0138361, acc 1\n",
      "2018-04-16T00:30:15.582251: step 923, loss 0.0111387, acc 1\n",
      "2018-04-16T00:30:15.625405: step 924, loss 0.0191543, acc 0.987179\n",
      "2018-04-16T00:30:15.678031: step 925, loss 0.0164768, acc 1\n",
      "2018-04-16T00:30:15.735653: step 926, loss 0.0170098, acc 1\n",
      "2018-04-16T00:30:15.787750: step 927, loss 0.013985, acc 1\n",
      "2018-04-16T00:30:15.831550: step 928, loss 0.00706094, acc 1\n",
      "2018-04-16T00:30:15.887326: step 929, loss 0.0205983, acc 0.98\n",
      "2018-04-16T00:30:15.943883: step 930, loss 0.025302, acc 0.99\n",
      "2018-04-16T00:30:15.997922: step 931, loss 0.00746296, acc 1\n",
      "2018-04-16T00:30:16.043249: step 932, loss 0.00603431, acc 1\n",
      "2018-04-16T00:30:16.095518: step 933, loss 0.0205945, acc 0.99\n",
      "2018-04-16T00:30:16.151396: step 934, loss 0.0140012, acc 1\n",
      "2018-04-16T00:30:16.203873: step 935, loss 0.00930595, acc 1\n",
      "2018-04-16T00:30:16.246308: step 936, loss 0.00773643, acc 1\n",
      "2018-04-16T00:30:16.298690: step 937, loss 0.0100384, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:30:16.359464: step 938, loss 0.0282539, acc 0.99\n",
      "2018-04-16T00:30:16.415444: step 939, loss 0.0231085, acc 1\n",
      "2018-04-16T00:30:16.465932: step 940, loss 0.035716, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:16.579467: step 940, loss 0.763451, acc 0.746684, rec 0.854054, pre 0.697572, f1 0.767922\n",
      "\n",
      "2018-04-16T00:30:16.631456: step 941, loss 0.0121935, acc 1\n",
      "2018-04-16T00:30:16.683990: step 942, loss 0.0141931, acc 1\n",
      "2018-04-16T00:30:16.738563: step 943, loss 0.0135156, acc 1\n",
      "2018-04-16T00:30:16.788430: step 944, loss 0.00890977, acc 1\n",
      "2018-04-16T00:30:16.842484: step 945, loss 0.0170502, acc 1\n",
      "2018-04-16T00:30:16.896263: step 946, loss 0.0121734, acc 1\n",
      "2018-04-16T00:30:16.950139: step 947, loss 0.0145027, acc 1\n",
      "2018-04-16T00:30:16.996977: step 948, loss 0.00542722, acc 1\n",
      "2018-04-16T00:30:17.051459: step 949, loss 0.0223652, acc 0.99\n",
      "2018-04-16T00:30:17.105192: step 950, loss 0.0123186, acc 1\n",
      "2018-04-16T00:30:17.159836: step 951, loss 0.0114958, acc 1\n",
      "2018-04-16T00:30:17.207149: step 952, loss 0.0133016, acc 1\n",
      "2018-04-16T00:30:17.259837: step 953, loss 0.00955881, acc 1\n",
      "2018-04-16T00:30:17.316471: step 954, loss 0.0217164, acc 0.99\n",
      "2018-04-16T00:30:17.373270: step 955, loss 0.0178534, acc 1\n",
      "2018-04-16T00:30:17.420667: step 956, loss 0.0122759, acc 1\n",
      "2018-04-16T00:30:17.474684: step 957, loss 0.0112138, acc 1\n",
      "2018-04-16T00:30:17.530064: step 958, loss 0.00766746, acc 1\n",
      "2018-04-16T00:30:17.583884: step 959, loss 0.0274537, acc 0.99\n",
      "2018-04-16T00:30:17.632138: step 960, loss 0.0430218, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:17.735871: step 960, loss 1.21657, acc 0.647215, rec 0.951351, pre 0.586667, f1 0.725773\n",
      "\n",
      "2018-04-16T00:30:17.788436: step 961, loss 0.0172181, acc 1\n",
      "2018-04-16T00:30:17.845738: step 962, loss 0.0078084, acc 1\n",
      "2018-04-16T00:30:17.899095: step 963, loss 0.0155148, acc 1\n",
      "2018-04-16T00:30:17.942009: step 964, loss 0.0148981, acc 1\n",
      "2018-04-16T00:30:17.995221: step 965, loss 0.0130435, acc 1\n",
      "2018-04-16T00:30:18.053819: step 966, loss 0.0143139, acc 1\n",
      "2018-04-16T00:30:18.107054: step 967, loss 0.0127805, acc 1\n",
      "2018-04-16T00:30:18.149868: step 968, loss 0.0109227, acc 1\n",
      "2018-04-16T00:30:18.202149: step 969, loss 0.00977547, acc 1\n",
      "2018-04-16T00:30:18.253799: step 970, loss 0.0120036, acc 1\n",
      "2018-04-16T00:30:18.309974: step 971, loss 0.00534533, acc 1\n",
      "2018-04-16T00:30:18.352718: step 972, loss 0.0118539, acc 1\n",
      "2018-04-16T00:30:18.404818: step 973, loss 0.00836338, acc 1\n",
      "2018-04-16T00:30:18.457888: step 974, loss 0.00747206, acc 1\n",
      "2018-04-16T00:30:18.510072: step 975, loss 0.00944895, acc 1\n",
      "2018-04-16T00:30:18.557135: step 976, loss 0.0246733, acc 0.987179\n",
      "2018-04-16T00:30:18.609830: step 977, loss 0.00760697, acc 1\n",
      "2018-04-16T00:30:18.662343: step 978, loss 0.00787845, acc 1\n",
      "2018-04-16T00:30:18.716131: step 979, loss 0.0151014, acc 1\n",
      "2018-04-16T00:30:18.764321: step 980, loss 0.0114805, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:18.865681: step 980, loss 0.731852, acc 0.765252, rec 0.878378, pre 0.71116, f1 0.785973\n",
      "\n",
      "2018-04-16T00:30:18.917505: step 981, loss 0.00709024, acc 1\n",
      "2018-04-16T00:30:18.973482: step 982, loss 0.011648, acc 1\n",
      "2018-04-16T00:30:19.026271: step 983, loss 0.00936681, acc 1\n",
      "2018-04-16T00:30:19.069074: step 984, loss 0.00690933, acc 1\n",
      "2018-04-16T00:30:19.121165: step 985, loss 0.00655508, acc 1\n",
      "2018-04-16T00:30:19.174439: step 986, loss 0.00673412, acc 1\n",
      "2018-04-16T00:30:19.229990: step 987, loss 0.00841744, acc 1\n",
      "2018-04-16T00:30:19.273158: step 988, loss 0.0146902, acc 1\n",
      "2018-04-16T00:30:19.325074: step 989, loss 0.0220566, acc 0.99\n",
      "2018-04-16T00:30:19.377358: step 990, loss 0.0171902, acc 1\n",
      "2018-04-16T00:30:19.430227: step 991, loss 0.0108009, acc 1\n",
      "2018-04-16T00:30:19.476880: step 992, loss 0.00493668, acc 1\n",
      "2018-04-16T00:30:19.529608: step 993, loss 0.00722893, acc 1\n",
      "2018-04-16T00:30:19.581530: step 994, loss 0.013477, acc 1\n",
      "2018-04-16T00:30:19.634601: step 995, loss 0.0144795, acc 0.99\n",
      "2018-04-16T00:30:19.682482: step 996, loss 0.00688978, acc 1\n",
      "2018-04-16T00:30:19.738566: step 997, loss 0.0120747, acc 1\n",
      "2018-04-16T00:30:19.790896: step 998, loss 0.00816404, acc 1\n",
      "2018-04-16T00:30:19.842801: step 999, loss 0.00682575, acc 1\n",
      "2018-04-16T00:30:19.893380: step 1000, loss 0.00947743, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:19.993425: step 1000, loss 0.846002, acc 0.72679, rec 0.910811, pre 0.660784, f1 0.765909\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523838561/checkpoints/model-1000\n",
      "\n",
      "2018-04-16T00:30:20.104115: step 1001, loss 0.00762994, acc 1\n",
      "2018-04-16T00:30:20.156369: step 1002, loss 0.00624408, acc 1\n",
      "2018-04-16T00:30:20.208489: step 1003, loss 0.00741161, acc 1\n",
      "2018-04-16T00:30:20.251021: step 1004, loss 0.00743625, acc 1\n",
      "2018-04-16T00:30:20.304006: step 1005, loss 0.0141684, acc 1\n",
      "2018-04-16T00:30:20.359805: step 1006, loss 0.0220696, acc 0.99\n",
      "2018-04-16T00:30:20.411873: step 1007, loss 0.0122177, acc 1\n",
      "2018-04-16T00:30:20.454520: step 1008, loss 0.00662844, acc 1\n",
      "2018-04-16T00:30:20.508204: step 1009, loss 0.00981855, acc 1\n",
      "2018-04-16T00:30:20.566373: step 1010, loss 0.00852574, acc 1\n",
      "2018-04-16T00:30:20.619747: step 1011, loss 0.0123502, acc 1\n",
      "2018-04-16T00:30:20.663940: step 1012, loss 0.00341813, acc 1\n",
      "2018-04-16T00:30:20.717667: step 1013, loss 0.0224914, acc 0.99\n",
      "2018-04-16T00:30:20.778781: step 1014, loss 0.0325572, acc 0.99\n",
      "2018-04-16T00:30:20.833497: step 1015, loss 0.0166841, acc 1\n",
      "2018-04-16T00:30:20.876974: step 1016, loss 0.00724401, acc 1\n",
      "2018-04-16T00:30:20.930444: step 1017, loss 0.00707075, acc 1\n",
      "2018-04-16T00:30:20.986909: step 1018, loss 0.028225, acc 0.99\n",
      "2018-04-16T00:30:21.040523: step 1019, loss 0.0100607, acc 1\n",
      "2018-04-16T00:30:21.083902: step 1020, loss 0.0177527, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:21.185143: step 1020, loss 0.946035, acc 0.697613, rec 0.927027, pre 0.630515, f1 0.750547\n",
      "\n",
      "2018-04-16T00:30:21.243241: step 1021, loss 0.0170064, acc 1\n",
      "2018-04-16T00:30:21.297345: step 1022, loss 0.0140303, acc 1\n",
      "2018-04-16T00:30:21.350484: step 1023, loss 0.0105913, acc 1\n",
      "2018-04-16T00:30:21.395642: step 1024, loss 0.00759124, acc 1\n",
      "2018-04-16T00:30:21.452659: step 1025, loss 0.00790489, acc 1\n",
      "2018-04-16T00:30:21.504794: step 1026, loss 0.00708567, acc 1\n",
      "2018-04-16T00:30:21.556902: step 1027, loss 0.00904945, acc 1\n",
      "2018-04-16T00:30:21.600389: step 1028, loss 0.0288104, acc 0.987179\n",
      "2018-04-16T00:30:21.657691: step 1029, loss 0.0352979, acc 1\n",
      "2018-04-16T00:30:21.711775: step 1030, loss 0.0593991, acc 0.99\n",
      "2018-04-16T00:30:21.763386: step 1031, loss 0.0211114, acc 1\n",
      "2018-04-16T00:30:21.807144: step 1032, loss 0.00783115, acc 1\n",
      "2018-04-16T00:30:21.863141: step 1033, loss 0.0102074, acc 1\n",
      "2018-04-16T00:30:21.914703: step 1034, loss 0.014361, acc 1\n",
      "2018-04-16T00:30:21.966493: step 1035, loss 0.0119261, acc 1\n",
      "2018-04-16T00:30:22.008707: step 1036, loss 0.00835404, acc 1\n",
      "2018-04-16T00:30:22.060599: step 1037, loss 0.0168248, acc 0.99\n",
      "2018-04-16T00:30:22.116139: step 1038, loss 0.0159036, acc 1\n",
      "2018-04-16T00:30:22.168771: step 1039, loss 0.011593, acc 1\n",
      "2018-04-16T00:30:22.211884: step 1040, loss 0.0127955, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:22.311569: step 1040, loss 1.05126, acc 0.676393, rec 0.92973, pre 0.6121, f1 0.738197\n",
      "\n",
      "2018-04-16T00:30:22.367818: step 1041, loss 0.00854229, acc 1\n",
      "2018-04-16T00:30:22.419847: step 1042, loss 0.0260622, acc 0.99\n",
      "2018-04-16T00:30:22.471519: step 1043, loss 0.00805489, acc 1\n",
      "2018-04-16T00:30:22.513651: step 1044, loss 0.0152755, acc 1\n",
      "2018-04-16T00:30:22.575607: step 1045, loss 0.0228277, acc 0.99\n",
      "2018-04-16T00:30:22.629898: step 1046, loss 0.00689329, acc 1\n",
      "2018-04-16T00:30:22.682422: step 1047, loss 0.0107406, acc 1\n",
      "2018-04-16T00:30:22.726666: step 1048, loss 0.00985346, acc 1\n",
      "2018-04-16T00:30:22.783843: step 1049, loss 0.0067874, acc 1\n",
      "2018-04-16T00:30:22.836294: step 1050, loss 0.00581872, acc 1\n",
      "2018-04-16T00:30:22.889576: step 1051, loss 0.00568494, acc 1\n",
      "2018-04-16T00:30:22.932823: step 1052, loss 0.0178192, acc 0.987179\n",
      "2018-04-16T00:30:22.989103: step 1053, loss 0.0154221, acc 0.99\n",
      "2018-04-16T00:30:23.044149: step 1054, loss 0.00585892, acc 1\n",
      "2018-04-16T00:30:23.098187: step 1055, loss 0.00326299, acc 1\n",
      "2018-04-16T00:30:23.141999: step 1056, loss 0.0106765, acc 1\n",
      "2018-04-16T00:30:23.198843: step 1057, loss 0.0298214, acc 0.99\n",
      "2018-04-16T00:30:23.251861: step 1058, loss 0.0151535, acc 1\n",
      "2018-04-16T00:30:23.305378: step 1059, loss 0.00860105, acc 1\n",
      "2018-04-16T00:30:23.348899: step 1060, loss 0.00542236, acc 1\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:30:23.453247: step 1060, loss 0.803785, acc 0.737401, rec 0.902703, pre 0.673387, f1 0.771363\n",
      "\n",
      "2018-04-16T00:30:23.505345: step 1061, loss 0.0160979, acc 0.99\n",
      "2018-04-16T00:30:23.559089: step 1062, loss 0.012058, acc 1\n",
      "2018-04-16T00:30:23.612398: step 1063, loss 0.00904307, acc 1\n",
      "2018-04-16T00:30:23.659995: step 1064, loss 0.00891257, acc 1\n",
      "2018-04-16T00:30:23.715503: step 1065, loss 0.00671332, acc 1\n",
      "2018-04-16T00:30:23.769793: step 1066, loss 0.00646786, acc 1\n",
      "2018-04-16T00:30:23.822392: step 1067, loss 0.0074278, acc 1\n",
      "2018-04-16T00:30:23.869176: step 1068, loss 0.00894126, acc 1\n",
      "2018-04-16T00:30:23.921042: step 1069, loss 0.00753201, acc 1\n",
      "2018-04-16T00:30:23.972706: step 1070, loss 0.0105025, acc 1\n",
      "2018-04-16T00:30:24.024580: step 1071, loss 0.00428432, acc 1\n",
      "2018-04-16T00:30:24.066939: step 1072, loss 0.00382606, acc 1\n",
      "2018-04-16T00:30:24.122334: step 1073, loss 0.00489033, acc 1\n",
      "2018-04-16T00:30:24.173938: step 1074, loss 0.00793795, acc 1\n",
      "2018-04-16T00:30:24.224908: step 1075, loss 0.00583586, acc 1\n",
      "2018-04-16T00:30:24.267463: step 1076, loss 0.0176736, acc 0.987179\n",
      "2018-04-16T00:30:24.319164: step 1077, loss 0.017273, acc 1\n",
      "2018-04-16T00:30:24.374178: step 1078, loss 0.00952455, acc 1\n",
      "2018-04-16T00:30:24.426245: step 1079, loss 0.00619036, acc 1\n",
      "2018-04-16T00:30:24.468418: step 1080, loss 0.00580448, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:24.567243: step 1080, loss 0.792863, acc 0.742706, rec 0.891892, pre 0.681818, f1 0.772834\n",
      "\n",
      "2018-04-16T00:30:24.622310: step 1081, loss 0.00702675, acc 1\n",
      "2018-04-16T00:30:24.674042: step 1082, loss 0.00789031, acc 1\n",
      "2018-04-16T00:30:24.727922: step 1083, loss 0.0230264, acc 0.99\n",
      "2018-04-16T00:30:24.778371: step 1084, loss 0.052322, acc 0.974359\n",
      "2018-04-16T00:30:24.844964: step 1085, loss 0.133235, acc 0.95\n",
      "2018-04-16T00:30:24.906402: step 1086, loss 0.102121, acc 0.98\n",
      "2018-04-16T00:30:24.969979: step 1087, loss 0.0295203, acc 0.99\n",
      "2018-04-16T00:30:25.017086: step 1088, loss 0.017473, acc 1\n",
      "2018-04-16T00:30:25.071696: step 1089, loss 0.00489001, acc 1\n",
      "2018-04-16T00:30:25.123211: step 1090, loss 0.0069166, acc 1\n",
      "2018-04-16T00:30:25.175323: step 1091, loss 0.00613589, acc 1\n",
      "2018-04-16T00:30:25.217332: step 1092, loss 0.0170833, acc 0.987179\n",
      "2018-04-16T00:30:25.268661: step 1093, loss 0.0124554, acc 1\n",
      "2018-04-16T00:30:25.323462: step 1094, loss 0.0107918, acc 1\n",
      "2018-04-16T00:30:25.374993: step 1095, loss 0.00454342, acc 1\n",
      "2018-04-16T00:30:25.417446: step 1096, loss 0.0380761, acc 0.987179\n",
      "2018-04-16T00:30:25.469599: step 1097, loss 0.0304757, acc 0.99\n",
      "2018-04-16T00:30:25.522099: step 1098, loss 0.00412605, acc 1\n",
      "2018-04-16T00:30:25.578441: step 1099, loss 0.00864975, acc 1\n",
      "2018-04-16T00:30:25.621449: step 1100, loss 0.00690715, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:25.721397: step 1100, loss 0.825927, acc 0.730769, rec 0.889189, pre 0.670061, f1 0.764228\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523838561/checkpoints/model-1100\n",
      "\n",
      "2018-04-16T00:30:25.829367: step 1101, loss 0.00750698, acc 1\n",
      "2018-04-16T00:30:25.881147: step 1102, loss 0.00801945, acc 1\n",
      "2018-04-16T00:30:25.933196: step 1103, loss 0.0186762, acc 0.99\n",
      "2018-04-16T00:30:25.976322: step 1104, loss 0.00714978, acc 1\n",
      "2018-04-16T00:30:26.028255: step 1105, loss 0.0210984, acc 0.99\n",
      "2018-04-16T00:30:26.083140: step 1106, loss 0.0127005, acc 1\n",
      "2018-04-16T00:30:26.135938: step 1107, loss 0.00378963, acc 1\n",
      "2018-04-16T00:30:26.178853: step 1108, loss 0.00483113, acc 1\n",
      "2018-04-16T00:30:26.230996: step 1109, loss 0.0059558, acc 1\n",
      "2018-04-16T00:30:26.282825: step 1110, loss 0.00493345, acc 1\n",
      "2018-04-16T00:30:26.338908: step 1111, loss 0.00799473, acc 1\n",
      "2018-04-16T00:30:26.382051: step 1112, loss 0.00876089, acc 1\n",
      "2018-04-16T00:30:26.434043: step 1113, loss 0.0080226, acc 1\n",
      "2018-04-16T00:30:26.486752: step 1114, loss 0.00555145, acc 1\n",
      "2018-04-16T00:30:26.538533: step 1115, loss 0.00516155, acc 1\n",
      "2018-04-16T00:30:26.585858: step 1116, loss 0.0243394, acc 0.987179\n",
      "2018-04-16T00:30:26.638525: step 1117, loss 0.014114, acc 1\n",
      "2018-04-16T00:30:26.690368: step 1118, loss 0.00830841, acc 1\n",
      "2018-04-16T00:30:26.743383: step 1119, loss 0.00706909, acc 1\n",
      "2018-04-16T00:30:26.786277: step 1120, loss 0.00797967, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:26.889726: step 1120, loss 0.815864, acc 0.738727, rec 0.902703, pre 0.674747, f1 0.772254\n",
      "\n",
      "2018-04-16T00:30:26.940973: step 1121, loss 0.00654947, acc 1\n",
      "2018-04-16T00:30:26.996010: step 1122, loss 0.0324105, acc 0.99\n",
      "2018-04-16T00:30:27.047801: step 1123, loss 0.0284686, acc 0.99\n",
      "2018-04-16T00:30:27.090733: step 1124, loss 0.0111378, acc 1\n",
      "2018-04-16T00:30:27.142609: step 1125, loss 0.00545361, acc 1\n",
      "2018-04-16T00:30:27.194308: step 1126, loss 0.0085803, acc 1\n",
      "2018-04-16T00:30:27.249528: step 1127, loss 0.004548, acc 1\n",
      "2018-04-16T00:30:27.293330: step 1128, loss 0.00495228, acc 1\n",
      "2018-04-16T00:30:27.344874: step 1129, loss 0.00989845, acc 1\n",
      "2018-04-16T00:30:27.396103: step 1130, loss 0.0115756, acc 1\n",
      "2018-04-16T00:30:27.447876: step 1131, loss 0.00930586, acc 1\n",
      "2018-04-16T00:30:27.494486: step 1132, loss 0.00505605, acc 1\n",
      "2018-04-16T00:30:27.547770: step 1133, loss 0.00897181, acc 1\n",
      "2018-04-16T00:30:27.600407: step 1134, loss 0.0054268, acc 1\n",
      "2018-04-16T00:30:27.652356: step 1135, loss 0.0272663, acc 0.99\n",
      "2018-04-16T00:30:27.695240: step 1136, loss 0.0239971, acc 1\n",
      "2018-04-16T00:30:27.751944: step 1137, loss 0.0061373, acc 1\n",
      "2018-04-16T00:30:27.803590: step 1138, loss 0.00526153, acc 1\n",
      "2018-04-16T00:30:27.854821: step 1139, loss 0.00755231, acc 1\n",
      "2018-04-16T00:30:27.896939: step 1140, loss 0.00536229, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:27.999453: step 1140, loss 0.835163, acc 0.733422, rec 0.891892, pre 0.672098, f1 0.766551\n",
      "\n",
      "2018-04-16T00:30:28.050776: step 1141, loss 0.00500996, acc 1\n",
      "2018-04-16T00:30:28.101949: step 1142, loss 0.0202293, acc 0.99\n",
      "2018-04-16T00:30:28.153486: step 1143, loss 0.0243967, acc 0.99\n",
      "2018-04-16T00:30:28.196117: step 1144, loss 0.00686777, acc 1\n",
      "2018-04-16T00:30:28.251889: step 1145, loss 0.0378603, acc 0.99\n",
      "2018-04-16T00:30:28.303472: step 1146, loss 0.0100948, acc 1\n",
      "2018-04-16T00:30:28.356518: step 1147, loss 0.00741172, acc 1\n",
      "2018-04-16T00:30:28.401239: step 1148, loss 0.00427402, acc 1\n",
      "2018-04-16T00:30:28.457011: step 1149, loss 0.0173803, acc 0.99\n",
      "2018-04-16T00:30:28.508728: step 1150, loss 0.00724777, acc 1\n",
      "2018-04-16T00:30:28.560660: step 1151, loss 0.00598947, acc 1\n",
      "2018-04-16T00:30:28.603812: step 1152, loss 0.00605761, acc 1\n",
      "2018-04-16T00:30:28.656826: step 1153, loss 0.00414863, acc 1\n",
      "2018-04-16T00:30:28.713682: step 1154, loss 0.00602925, acc 1\n",
      "2018-04-16T00:30:28.766513: step 1155, loss 0.00355955, acc 1\n",
      "2018-04-16T00:30:28.809768: step 1156, loss 0.00852363, acc 1\n",
      "2018-04-16T00:30:28.862597: step 1157, loss 0.00648181, acc 1\n",
      "2018-04-16T00:30:28.919157: step 1158, loss 0.0165836, acc 0.99\n",
      "2018-04-16T00:30:28.972257: step 1159, loss 0.00922698, acc 1\n",
      "2018-04-16T00:30:29.017493: step 1160, loss 0.00537399, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:29.128405: step 1160, loss 0.738516, acc 0.77321, rec 0.872973, pre 0.722595, f1 0.790698\n",
      "\n",
      "2018-04-16T00:30:29.180516: step 1161, loss 0.00500522, acc 1\n",
      "2018-04-16T00:30:29.231537: step 1162, loss 0.00344328, acc 1\n",
      "2018-04-16T00:30:29.282939: step 1163, loss 0.0134127, acc 1\n",
      "2018-04-16T00:30:29.325476: step 1164, loss 0.0129827, acc 1\n",
      "2018-04-16T00:30:29.381036: step 1165, loss 0.00757803, acc 1\n",
      "2018-04-16T00:30:29.432792: step 1166, loss 0.00802342, acc 1\n",
      "2018-04-16T00:30:29.484376: step 1167, loss 0.00374716, acc 1\n",
      "2018-04-16T00:30:29.526617: step 1168, loss 0.00815085, acc 1\n",
      "2018-04-16T00:30:29.579956: step 1169, loss 0.00324867, acc 1\n",
      "2018-04-16T00:30:29.634886: step 1170, loss 0.00672274, acc 1\n",
      "2018-04-16T00:30:29.686619: step 1171, loss 0.0131761, acc 1\n",
      "2018-04-16T00:30:29.731775: step 1172, loss 0.0105673, acc 1\n",
      "2018-04-16T00:30:29.784476: step 1173, loss 0.0119179, acc 1\n",
      "2018-04-16T00:30:29.840797: step 1174, loss 0.00368248, acc 1\n",
      "2018-04-16T00:30:29.893281: step 1175, loss 0.00689579, acc 1\n",
      "2018-04-16T00:30:29.936072: step 1176, loss 0.00511718, acc 1\n",
      "2018-04-16T00:30:29.988304: step 1177, loss 0.00317693, acc 1\n",
      "2018-04-16T00:30:30.040515: step 1178, loss 0.00455133, acc 1\n",
      "2018-04-16T00:30:30.095505: step 1179, loss 0.00681978, acc 1\n",
      "2018-04-16T00:30:30.138148: step 1180, loss 0.00897886, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:30.236985: step 1180, loss 0.659249, acc 0.789125, rec 0.797297, pre 0.778364, f1 0.787717\n",
      "\n",
      "2018-04-16T00:30:30.289065: step 1181, loss 0.0159257, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:30:30.344689: step 1182, loss 0.00447393, acc 1\n",
      "2018-04-16T00:30:30.396577: step 1183, loss 0.00819642, acc 1\n",
      "2018-04-16T00:30:30.438636: step 1184, loss 0.00810885, acc 1\n",
      "2018-04-16T00:30:30.490112: step 1185, loss 0.00244277, acc 1\n",
      "2018-04-16T00:30:30.550557: step 1186, loss 0.00611829, acc 1\n",
      "2018-04-16T00:30:30.602423: step 1187, loss 0.00374318, acc 1\n",
      "2018-04-16T00:30:30.644582: step 1188, loss 0.0168628, acc 0.987179\n",
      "2018-04-16T00:30:30.696439: step 1189, loss 0.0132444, acc 1\n",
      "2018-04-16T00:30:30.749140: step 1190, loss 0.00387964, acc 1\n",
      "2018-04-16T00:30:30.803834: step 1191, loss 0.00812067, acc 1\n",
      "2018-04-16T00:30:30.847379: step 1192, loss 0.0131655, acc 1\n",
      "2018-04-16T00:30:30.901454: step 1193, loss 0.00562015, acc 1\n",
      "2018-04-16T00:30:30.955079: step 1194, loss 0.003575, acc 1\n",
      "2018-04-16T00:30:31.012530: step 1195, loss 0.00546154, acc 1\n",
      "2018-04-16T00:30:31.055337: step 1196, loss 0.0112764, acc 1\n",
      "2018-04-16T00:30:31.106958: step 1197, loss 0.0102437, acc 1\n",
      "2018-04-16T00:30:31.158301: step 1198, loss 0.00785515, acc 1\n",
      "2018-04-16T00:30:31.210286: step 1199, loss 0.00484333, acc 1\n",
      "2018-04-16T00:30:31.255939: step 1200, loss 0.00437903, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:31.355000: step 1200, loss 0.786269, acc 0.763926, rec 0.878378, pre 0.709607, f1 0.785024\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523838561/checkpoints/model-1200\n",
      "\n",
      "2018-04-16T00:30:31.462107: step 1201, loss 0.00649655, acc 1\n",
      "2018-04-16T00:30:31.513628: step 1202, loss 0.0122161, acc 1\n",
      "2018-04-16T00:30:31.564715: step 1203, loss 0.00536377, acc 1\n",
      "2018-04-16T00:30:31.607267: step 1204, loss 0.00364696, acc 1\n",
      "2018-04-16T00:30:31.660001: step 1205, loss 0.0040288, acc 1\n",
      "2018-04-16T00:30:31.720321: step 1206, loss 0.00520679, acc 1\n",
      "2018-04-16T00:30:31.775352: step 1207, loss 0.00876235, acc 1\n",
      "2018-04-16T00:30:31.819876: step 1208, loss 0.00696363, acc 1\n",
      "2018-04-16T00:30:31.878850: step 1209, loss 0.00717348, acc 1\n",
      "2018-04-16T00:30:31.936893: step 1210, loss 0.0176762, acc 0.99\n",
      "2018-04-16T00:30:31.992910: step 1211, loss 0.00530736, acc 1\n",
      "2018-04-16T00:30:32.036244: step 1212, loss 0.00407993, acc 1\n",
      "2018-04-16T00:30:32.091137: step 1213, loss 0.00736268, acc 1\n",
      "2018-04-16T00:30:32.149787: step 1214, loss 0.00802154, acc 1\n",
      "2018-04-16T00:30:32.205408: step 1215, loss 0.00895801, acc 1\n",
      "2018-04-16T00:30:32.250508: step 1216, loss 0.00397758, acc 1\n",
      "2018-04-16T00:30:32.305861: step 1217, loss 0.00837967, acc 1\n",
      "2018-04-16T00:30:32.364021: step 1218, loss 0.00234501, acc 1\n",
      "2018-04-16T00:30:32.417029: step 1219, loss 0.00388489, acc 1\n",
      "2018-04-16T00:30:32.460338: step 1220, loss 0.00433718, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:32.560778: step 1220, loss 0.790075, acc 0.763926, rec 0.878378, pre 0.709607, f1 0.785024\n",
      "\n",
      "2018-04-16T00:30:32.619000: step 1221, loss 0.00509093, acc 1\n",
      "2018-04-16T00:30:32.672017: step 1222, loss 0.00840189, acc 1\n",
      "2018-04-16T00:30:32.726984: step 1223, loss 0.00337949, acc 1\n",
      "2018-04-16T00:30:32.771265: step 1224, loss 0.00318293, acc 1\n",
      "2018-04-16T00:30:32.827923: step 1225, loss 0.003919, acc 1\n",
      "2018-04-16T00:30:32.881212: step 1226, loss 0.0121335, acc 0.99\n",
      "2018-04-16T00:30:32.934384: step 1227, loss 0.00980058, acc 1\n",
      "2018-04-16T00:30:32.978473: step 1228, loss 0.0110516, acc 1\n",
      "2018-04-16T00:30:33.035679: step 1229, loss 0.00812559, acc 1\n",
      "2018-04-16T00:30:33.087720: step 1230, loss 0.00546118, acc 1\n",
      "2018-04-16T00:30:33.139422: step 1231, loss 0.00690222, acc 1\n",
      "2018-04-16T00:30:33.183254: step 1232, loss 0.00679939, acc 1\n",
      "2018-04-16T00:30:33.235990: step 1233, loss 0.0103401, acc 1\n",
      "2018-04-16T00:30:33.295523: step 1234, loss 0.00602661, acc 1\n",
      "2018-04-16T00:30:33.349591: step 1235, loss 0.00442762, acc 1\n",
      "2018-04-16T00:30:33.393733: step 1236, loss 0.00455716, acc 1\n",
      "2018-04-16T00:30:33.446824: step 1237, loss 0.00689714, acc 1\n",
      "2018-04-16T00:30:33.504550: step 1238, loss 0.00482606, acc 1\n",
      "2018-04-16T00:30:33.560536: step 1239, loss 0.00385997, acc 1\n",
      "2018-04-16T00:30:33.602925: step 1240, loss 0.00933419, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:33.702002: step 1240, loss 0.711533, acc 0.754642, rec 0.818919, pre 0.719715, f1 0.766119\n",
      "\n",
      "2018-04-16T00:30:33.757969: step 1241, loss 0.0048047, acc 1\n",
      "2018-04-16T00:30:33.809817: step 1242, loss 0.00611649, acc 1\n",
      "2018-04-16T00:30:33.861530: step 1243, loss 0.00735736, acc 1\n",
      "2018-04-16T00:30:33.904448: step 1244, loss 0.00930872, acc 1\n",
      "2018-04-16T00:30:33.956289: step 1245, loss 0.00383689, acc 1\n",
      "2018-04-16T00:30:34.012363: step 1246, loss 0.00498831, acc 1\n",
      "2018-04-16T00:30:34.065410: step 1247, loss 0.00398726, acc 1\n",
      "2018-04-16T00:30:34.108128: step 1248, loss 0.00729586, acc 1\n",
      "2018-04-16T00:30:34.160663: step 1249, loss 0.00356536, acc 1\n",
      "2018-04-16T00:30:34.213182: step 1250, loss 0.00277048, acc 1\n",
      "2018-04-16T00:30:34.270715: step 1251, loss 0.00680104, acc 1\n",
      "2018-04-16T00:30:34.314000: step 1252, loss 0.00703186, acc 1\n",
      "2018-04-16T00:30:34.366623: step 1253, loss 0.00325588, acc 1\n",
      "2018-04-16T00:30:34.418552: step 1254, loss 0.00998319, acc 1\n",
      "2018-04-16T00:30:34.475950: step 1255, loss 0.00366104, acc 1\n",
      "2018-04-16T00:30:34.519844: step 1256, loss 0.00493348, acc 1\n",
      "2018-04-16T00:30:34.574241: step 1257, loss 0.00579044, acc 1\n",
      "2018-04-16T00:30:34.627574: step 1258, loss 0.0035116, acc 1\n",
      "2018-04-16T00:30:34.684041: step 1259, loss 0.011814, acc 0.99\n",
      "2018-04-16T00:30:34.730126: step 1260, loss 0.016474, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:34.838477: step 1260, loss 0.868893, acc 0.732095, rec 0.902703, pre 0.668, f1 0.767816\n",
      "\n",
      "2018-04-16T00:30:34.896660: step 1261, loss 0.00787082, acc 1\n",
      "2018-04-16T00:30:34.948451: step 1262, loss 0.0044204, acc 1\n",
      "2018-04-16T00:30:35.000985: step 1263, loss 0.00649729, acc 1\n",
      "2018-04-16T00:30:35.043914: step 1264, loss 0.00615233, acc 1\n",
      "2018-04-16T00:30:35.095827: step 1265, loss 0.00329673, acc 1\n",
      "2018-04-16T00:30:35.153647: step 1266, loss 0.00626385, acc 1\n",
      "2018-04-16T00:30:35.205506: step 1267, loss 0.00615058, acc 1\n",
      "2018-04-16T00:30:35.248242: step 1268, loss 0.00375534, acc 1\n",
      "2018-04-16T00:30:35.301405: step 1269, loss 0.00563916, acc 1\n",
      "2018-04-16T00:30:35.358793: step 1270, loss 0.00384556, acc 1\n",
      "2018-04-16T00:30:35.412499: step 1271, loss 0.00371993, acc 1\n",
      "2018-04-16T00:30:35.455337: step 1272, loss 0.00534444, acc 1\n",
      "2018-04-16T00:30:35.507507: step 1273, loss 0.00874384, acc 1\n",
      "2018-04-16T00:30:35.568439: step 1274, loss 0.00496623, acc 1\n",
      "2018-04-16T00:30:35.624941: step 1275, loss 0.00384288, acc 1\n",
      "2018-04-16T00:30:35.671826: step 1276, loss 0.00252639, acc 1\n",
      "2018-04-16T00:30:35.726797: step 1277, loss 0.00720279, acc 1\n",
      "2018-04-16T00:30:35.785769: step 1278, loss 0.006247, acc 1\n",
      "2018-04-16T00:30:35.839978: step 1279, loss 0.00312607, acc 1\n",
      "2018-04-16T00:30:35.884563: step 1280, loss 0.0193775, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:35.994943: step 1280, loss 0.830851, acc 0.759947, rec 0.851351, pre 0.714286, f1 0.776819\n",
      "\n",
      "2018-04-16T00:30:36.048125: step 1281, loss 0.0122493, acc 1\n",
      "2018-04-16T00:30:36.100919: step 1282, loss 0.00351221, acc 1\n",
      "2018-04-16T00:30:36.154639: step 1283, loss 0.00406211, acc 1\n",
      "2018-04-16T00:30:36.201722: step 1284, loss 0.00854742, acc 1\n",
      "2018-04-16T00:30:36.254042: step 1285, loss 0.0172812, acc 0.99\n",
      "2018-04-16T00:30:36.307592: step 1286, loss 0.00916097, acc 1\n",
      "2018-04-16T00:30:36.361441: step 1287, loss 0.00465986, acc 1\n",
      "2018-04-16T00:30:36.415224: step 1288, loss 0.00415456, acc 1\n",
      "2018-04-16T00:30:36.478010: step 1289, loss 0.00462586, acc 1\n",
      "2018-04-16T00:30:36.529920: step 1290, loss 0.00663545, acc 1\n",
      "2018-04-16T00:30:36.582912: step 1291, loss 0.00448823, acc 1\n",
      "2018-04-16T00:30:36.630930: step 1292, loss 0.00517861, acc 1\n",
      "2018-04-16T00:30:36.683116: step 1293, loss 0.00351585, acc 1\n",
      "2018-04-16T00:30:36.736949: step 1294, loss 0.00414443, acc 1\n",
      "2018-04-16T00:30:36.789891: step 1295, loss 0.00348285, acc 1\n",
      "2018-04-16T00:30:36.837532: step 1296, loss 0.00889085, acc 1\n",
      "2018-04-16T00:30:36.894555: step 1297, loss 0.0048453, acc 1\n",
      "2018-04-16T00:30:36.947582: step 1298, loss 0.00633136, acc 1\n",
      "2018-04-16T00:30:36.999321: step 1299, loss 0.00599797, acc 1\n",
      "2018-04-16T00:30:37.045147: step 1300, loss 0.00562315, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:37.144765: step 1300, loss 0.891303, acc 0.733422, rec 0.9, pre 0.67002, f1 0.768166\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523838561/checkpoints/model-1300\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:30:37.255329: step 1301, loss 0.0068382, acc 1\n",
      "2018-04-16T00:30:37.309425: step 1302, loss 0.00518091, acc 1\n",
      "2018-04-16T00:30:37.362848: step 1303, loss 0.00583422, acc 1\n",
      "2018-04-16T00:30:37.407485: step 1304, loss 0.0053116, acc 1\n",
      "2018-04-16T00:30:37.463546: step 1305, loss 0.0109421, acc 0.99\n",
      "2018-04-16T00:30:37.515725: step 1306, loss 0.00750154, acc 1\n",
      "2018-04-16T00:30:37.567296: step 1307, loss 0.00395264, acc 1\n",
      "2018-04-16T00:30:37.610751: step 1308, loss 0.00357933, acc 1\n",
      "2018-04-16T00:30:37.662001: step 1309, loss 0.00638384, acc 1\n",
      "2018-04-16T00:30:37.718012: step 1310, loss 0.0030945, acc 1\n",
      "2018-04-16T00:30:37.772556: step 1311, loss 0.0111649, acc 1\n",
      "2018-04-16T00:30:37.815115: step 1312, loss 0.00299688, acc 1\n",
      "2018-04-16T00:30:37.866736: step 1313, loss 0.00840263, acc 1\n",
      "2018-04-16T00:30:37.916995: step 1314, loss 0.0171905, acc 0.99\n",
      "2018-04-16T00:30:37.971273: step 1315, loss 0.00716268, acc 1\n",
      "2018-04-16T00:30:38.013123: step 1316, loss 0.00514323, acc 1\n",
      "2018-04-16T00:30:38.064462: step 1317, loss 0.00436488, acc 1\n",
      "2018-04-16T00:30:38.115776: step 1318, loss 0.00404102, acc 1\n",
      "2018-04-16T00:30:38.166976: step 1319, loss 0.0088424, acc 1\n",
      "2018-04-16T00:30:38.214001: step 1320, loss 0.0164738, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:38.316908: step 1320, loss 0.943436, acc 0.708223, rec 0.902703, pre 0.644788, f1 0.752252\n",
      "\n",
      "2018-04-16T00:30:38.370380: step 1321, loss 0.0210643, acc 0.99\n",
      "2018-04-16T00:30:38.431292: step 1322, loss 0.01257, acc 1\n",
      "2018-04-16T00:30:38.487078: step 1323, loss 0.00313594, acc 1\n",
      "2018-04-16T00:30:38.531264: step 1324, loss 0.00325097, acc 1\n",
      "2018-04-16T00:30:38.584355: step 1325, loss 0.00735631, acc 1\n",
      "2018-04-16T00:30:38.643197: step 1326, loss 0.00675564, acc 1\n",
      "2018-04-16T00:30:38.695826: step 1327, loss 0.00352517, acc 1\n",
      "2018-04-16T00:30:38.742837: step 1328, loss 0.00418611, acc 1\n",
      "2018-04-16T00:30:38.817509: step 1329, loss 0.00348101, acc 1\n",
      "2018-04-16T00:30:38.897220: step 1330, loss 0.00762342, acc 1\n",
      "2018-04-16T00:30:38.949083: step 1331, loss 0.00954457, acc 1\n",
      "2018-04-16T00:30:38.992126: step 1332, loss 0.00410946, acc 1\n",
      "2018-04-16T00:30:39.043573: step 1333, loss 0.00190361, acc 1\n",
      "2018-04-16T00:30:39.095003: step 1334, loss 0.00264146, acc 1\n",
      "2018-04-16T00:30:39.150214: step 1335, loss 0.00458963, acc 1\n",
      "2018-04-16T00:30:39.193477: step 1336, loss 0.00483184, acc 1\n",
      "2018-04-16T00:30:39.246016: step 1337, loss 0.00320103, acc 1\n",
      "2018-04-16T00:30:39.298739: step 1338, loss 0.00239619, acc 1\n",
      "2018-04-16T00:30:39.350734: step 1339, loss 0.0054063, acc 1\n",
      "2018-04-16T00:30:39.396625: step 1340, loss 0.00438368, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:39.496489: step 1340, loss 0.859901, acc 0.740053, rec 0.883784, pre 0.68125, f1 0.769412\n",
      "\n",
      "2018-04-16T00:30:39.547845: step 1341, loss 0.00560805, acc 1\n",
      "2018-04-16T00:30:39.604082: step 1342, loss 0.00498111, acc 1\n",
      "2018-04-16T00:30:39.657565: step 1343, loss 0.00403297, acc 1\n",
      "2018-04-16T00:30:39.702194: step 1344, loss 0.00341763, acc 1\n",
      "2018-04-16T00:30:39.754167: step 1345, loss 0.00346091, acc 1\n",
      "2018-04-16T00:30:39.810411: step 1346, loss 0.00337921, acc 1\n",
      "2018-04-16T00:30:39.862157: step 1347, loss 0.00351136, acc 1\n",
      "2018-04-16T00:30:39.906104: step 1348, loss 0.0103732, acc 1\n",
      "2018-04-16T00:30:39.960072: step 1349, loss 0.0110612, acc 1\n",
      "2018-04-16T00:30:40.014845: step 1350, loss 0.00804281, acc 1\n",
      "2018-04-16T00:30:40.066561: step 1351, loss 0.00843828, acc 1\n",
      "2018-04-16T00:30:40.108956: step 1352, loss 0.00359859, acc 1\n",
      "2018-04-16T00:30:40.160244: step 1353, loss 0.0119349, acc 1\n",
      "2018-04-16T00:30:40.212339: step 1354, loss 0.0219169, acc 0.99\n",
      "2018-04-16T00:30:40.267753: step 1355, loss 0.00781257, acc 1\n",
      "2018-04-16T00:30:40.310306: step 1356, loss 0.00317076, acc 1\n",
      "2018-04-16T00:30:40.362696: step 1357, loss 0.00444688, acc 1\n",
      "2018-04-16T00:30:40.415352: step 1358, loss 0.0127575, acc 1\n",
      "2018-04-16T00:30:40.467733: step 1359, loss 0.00292258, acc 1\n",
      "2018-04-16T00:30:40.513382: step 1360, loss 0.00283169, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:40.613245: step 1360, loss 0.786645, acc 0.769231, rec 0.872973, pre 0.717778, f1 0.787805\n",
      "\n",
      "2018-04-16T00:30:40.664479: step 1361, loss 0.00347326, acc 1\n",
      "2018-04-16T00:30:40.721070: step 1362, loss 0.00425824, acc 1\n",
      "2018-04-16T00:30:40.774323: step 1363, loss 0.00802362, acc 1\n",
      "2018-04-16T00:30:40.817378: step 1364, loss 0.00552746, acc 1\n",
      "2018-04-16T00:30:40.869674: step 1365, loss 0.00385739, acc 1\n",
      "2018-04-16T00:30:40.921001: step 1366, loss 0.00731381, acc 1\n",
      "2018-04-16T00:30:40.976164: step 1367, loss 0.00332007, acc 1\n",
      "2018-04-16T00:30:41.019069: step 1368, loss 0.00387764, acc 1\n",
      "2018-04-16T00:30:41.070798: step 1369, loss 0.00455566, acc 1\n",
      "2018-04-16T00:30:41.122138: step 1370, loss 0.00321149, acc 1\n",
      "2018-04-16T00:30:41.174253: step 1371, loss 0.00434849, acc 1\n",
      "2018-04-16T00:30:41.220815: step 1372, loss 0.00349255, acc 1\n",
      "2018-04-16T00:30:41.272486: step 1373, loss 0.0055049, acc 1\n",
      "2018-04-16T00:30:41.323676: step 1374, loss 0.00174848, acc 1\n",
      "2018-04-16T00:30:41.375796: step 1375, loss 0.00534316, acc 1\n",
      "2018-04-16T00:30:41.418565: step 1376, loss 0.00480167, acc 1\n",
      "2018-04-16T00:30:41.473917: step 1377, loss 0.0033817, acc 1\n",
      "2018-04-16T00:30:41.525835: step 1378, loss 0.00296709, acc 1\n",
      "2018-04-16T00:30:41.577981: step 1379, loss 0.00463124, acc 1\n",
      "2018-04-16T00:30:41.620403: step 1380, loss 0.0108034, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:41.724704: step 1380, loss 0.693498, acc 0.77321, rec 0.805405, pre 0.75063, f1 0.777053\n",
      "\n",
      "2018-04-16T00:30:41.776760: step 1381, loss 0.00526246, acc 1\n",
      "2018-04-16T00:30:41.829327: step 1382, loss 0.00408196, acc 1\n",
      "2018-04-16T00:30:41.881497: step 1383, loss 0.00494529, acc 1\n",
      "2018-04-16T00:30:41.931235: step 1384, loss 0.00698994, acc 1\n",
      "2018-04-16T00:30:41.986205: step 1385, loss 0.00484682, acc 1\n",
      "2018-04-16T00:30:42.038983: step 1386, loss 0.00361183, acc 1\n",
      "2018-04-16T00:30:42.090278: step 1387, loss 0.00255325, acc 1\n",
      "2018-04-16T00:30:42.136121: step 1388, loss 0.0147391, acc 0.987179\n",
      "2018-04-16T00:30:42.188131: step 1389, loss 0.0109606, acc 1\n",
      "2018-04-16T00:30:42.241247: step 1390, loss 0.00309583, acc 1\n",
      "2018-04-16T00:30:42.293402: step 1391, loss 0.00354632, acc 1\n",
      "2018-04-16T00:30:42.335528: step 1392, loss 0.00225617, acc 1\n",
      "2018-04-16T00:30:42.391819: step 1393, loss 0.00899474, acc 1\n",
      "2018-04-16T00:30:42.444062: step 1394, loss 0.00375163, acc 1\n",
      "2018-04-16T00:30:42.495079: step 1395, loss 0.00265185, acc 1\n",
      "2018-04-16T00:30:42.536984: step 1396, loss 0.0144373, acc 0.987179\n",
      "2018-04-16T00:30:42.590606: step 1397, loss 0.0135348, acc 0.99\n",
      "2018-04-16T00:30:42.646175: step 1398, loss 0.00270751, acc 1\n",
      "2018-04-16T00:30:42.698789: step 1399, loss 0.00266516, acc 1\n",
      "2018-04-16T00:30:42.741879: step 1400, loss 0.00671138, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:42.842135: step 1400, loss 0.774495, acc 0.770557, rec 0.864865, pre 0.722348, f1 0.787208\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523838561/checkpoints/model-1400\n",
      "\n",
      "2018-04-16T00:30:42.950200: step 1401, loss 0.00412731, acc 1\n",
      "2018-04-16T00:30:43.001246: step 1402, loss 0.00283517, acc 1\n",
      "2018-04-16T00:30:43.052147: step 1403, loss 0.0067421, acc 1\n",
      "2018-04-16T00:30:43.099730: step 1404, loss 0.00355513, acc 1\n",
      "2018-04-16T00:30:43.151839: step 1405, loss 0.00123731, acc 1\n",
      "2018-04-16T00:30:43.203780: step 1406, loss 0.00417665, acc 1\n",
      "2018-04-16T00:30:43.255207: step 1407, loss 0.010725, acc 0.99\n",
      "2018-04-16T00:30:43.297262: step 1408, loss 0.0097024, acc 1\n",
      "2018-04-16T00:30:43.353290: step 1409, loss 0.00321483, acc 1\n",
      "2018-04-16T00:30:43.405031: step 1410, loss 0.00287475, acc 1\n",
      "2018-04-16T00:30:43.456663: step 1411, loss 0.0107044, acc 1\n",
      "2018-04-16T00:30:43.498561: step 1412, loss 0.00562559, acc 1\n",
      "2018-04-16T00:30:43.549620: step 1413, loss 0.005503, acc 1\n",
      "2018-04-16T00:30:43.605872: step 1414, loss 0.00277279, acc 1\n",
      "2018-04-16T00:30:43.657425: step 1415, loss 0.00715518, acc 1\n",
      "2018-04-16T00:30:43.699596: step 1416, loss 0.00429563, acc 1\n",
      "2018-04-16T00:30:43.752068: step 1417, loss 0.00442302, acc 1\n",
      "2018-04-16T00:30:43.803580: step 1418, loss 0.00252651, acc 1\n",
      "2018-04-16T00:30:43.858884: step 1419, loss 0.00494484, acc 1\n",
      "2018-04-16T00:30:43.901056: step 1420, loss 0.00285229, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:43.999146: step 1420, loss 0.772644, acc 0.771883, rec 0.872973, pre 0.720982, f1 0.789731\n",
      "\n",
      "2018-04-16T00:30:44.050502: step 1421, loss 0.00450116, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:30:44.107792: step 1422, loss 0.00338191, acc 1\n",
      "2018-04-16T00:30:44.161959: step 1423, loss 0.00393432, acc 1\n",
      "2018-04-16T00:30:44.204091: step 1424, loss 0.0309795, acc 0.987179\n",
      "2018-04-16T00:30:44.255836: step 1425, loss 0.034418, acc 0.99\n",
      "2018-04-16T00:30:44.307122: step 1426, loss 0.00299861, acc 1\n",
      "2018-04-16T00:30:44.362828: step 1427, loss 0.0056579, acc 1\n",
      "2018-04-16T00:30:44.406520: step 1428, loss 0.0155989, acc 0.987179\n",
      "2018-04-16T00:30:44.459054: step 1429, loss 0.00489312, acc 1\n",
      "2018-04-16T00:30:44.511034: step 1430, loss 0.00550287, acc 1\n",
      "2018-04-16T00:30:44.568521: step 1431, loss 0.00226709, acc 1\n",
      "2018-04-16T00:30:44.611812: step 1432, loss 0.00237711, acc 1\n",
      "2018-04-16T00:30:44.665745: step 1433, loss 0.00188631, acc 1\n",
      "2018-04-16T00:30:44.718883: step 1434, loss 0.00267572, acc 1\n",
      "2018-04-16T00:30:44.775273: step 1435, loss 0.0104046, acc 1\n",
      "2018-04-16T00:30:44.818386: step 1436, loss 0.00476522, acc 1\n",
      "2018-04-16T00:30:44.871246: step 1437, loss 0.00428417, acc 1\n",
      "2018-04-16T00:30:44.922607: step 1438, loss 0.0108454, acc 0.99\n",
      "2018-04-16T00:30:44.973858: step 1439, loss 0.0035493, acc 1\n",
      "2018-04-16T00:30:45.020010: step 1440, loss 0.00465347, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:45.118784: step 1440, loss 0.768487, acc 0.767905, rec 0.862162, pre 0.72009, f1 0.784748\n",
      "\n",
      "2018-04-16T00:30:45.170284: step 1441, loss 0.00226662, acc 1\n",
      "2018-04-16T00:30:45.226393: step 1442, loss 0.0127634, acc 0.99\n",
      "2018-04-16T00:30:45.279708: step 1443, loss 0.00977758, acc 1\n",
      "2018-04-16T00:30:45.322042: step 1444, loss 0.00233172, acc 1\n",
      "2018-04-16T00:30:45.373997: step 1445, loss 0.0059928, acc 1\n",
      "2018-04-16T00:30:45.426273: step 1446, loss 0.00424478, acc 1\n",
      "2018-04-16T00:30:45.482289: step 1447, loss 0.00356867, acc 1\n",
      "2018-04-16T00:30:45.525559: step 1448, loss 0.00315703, acc 1\n",
      "2018-04-16T00:30:45.578369: step 1449, loss 0.00463417, acc 1\n",
      "2018-04-16T00:30:45.630098: step 1450, loss 0.00311724, acc 1\n",
      "2018-04-16T00:30:45.681837: step 1451, loss 0.00343342, acc 1\n",
      "2018-04-16T00:30:45.730380: step 1452, loss 0.0041209, acc 1\n",
      "2018-04-16T00:30:45.782783: step 1453, loss 0.00394199, acc 1\n",
      "2018-04-16T00:30:45.834522: step 1454, loss 0.00735079, acc 1\n",
      "2018-04-16T00:30:45.887908: step 1455, loss 0.00379504, acc 1\n",
      "2018-04-16T00:30:45.930774: step 1456, loss 0.00331851, acc 1\n",
      "2018-04-16T00:30:45.986387: step 1457, loss 0.00238599, acc 1\n",
      "2018-04-16T00:30:46.038972: step 1458, loss 0.00729198, acc 1\n",
      "2018-04-16T00:30:46.091294: step 1459, loss 0.00538261, acc 1\n",
      "2018-04-16T00:30:46.134203: step 1460, loss 0.00197745, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:46.243289: step 1460, loss 0.799974, acc 0.766578, rec 0.862162, pre 0.718468, f1 0.783784\n",
      "\n",
      "2018-04-16T00:30:46.295036: step 1461, loss 0.00299021, acc 1\n",
      "2018-04-16T00:30:46.346300: step 1462, loss 0.00325304, acc 1\n",
      "2018-04-16T00:30:46.398000: step 1463, loss 0.00163837, acc 1\n",
      "2018-04-16T00:30:46.441030: step 1464, loss 0.00471075, acc 1\n",
      "2018-04-16T00:30:46.498141: step 1465, loss 0.00268162, acc 1\n",
      "2018-04-16T00:30:46.550780: step 1466, loss 0.00578176, acc 1\n",
      "2018-04-16T00:30:46.603099: step 1467, loss 0.00192746, acc 1\n",
      "2018-04-16T00:30:46.645790: step 1468, loss 0.00193199, acc 1\n",
      "2018-04-16T00:30:46.698094: step 1469, loss 0.00482113, acc 1\n",
      "2018-04-16T00:30:46.753402: step 1470, loss 0.00797807, acc 1\n",
      "2018-04-16T00:30:46.807334: step 1471, loss 0.00290754, acc 1\n",
      "2018-04-16T00:30:46.855402: step 1472, loss 0.00178709, acc 1\n",
      "2018-04-16T00:30:46.907574: step 1473, loss 0.00259542, acc 1\n",
      "2018-04-16T00:30:46.965479: step 1474, loss 0.00388983, acc 1\n",
      "2018-04-16T00:30:47.017078: step 1475, loss 0.00336621, acc 1\n",
      "2018-04-16T00:30:47.059844: step 1476, loss 0.00561305, acc 1\n",
      "2018-04-16T00:30:47.112460: step 1477, loss 0.00377316, acc 1\n",
      "2018-04-16T00:30:47.165680: step 1478, loss 0.00364045, acc 1\n",
      "2018-04-16T00:30:47.222561: step 1479, loss 0.00292194, acc 1\n",
      "2018-04-16T00:30:47.265058: step 1480, loss 0.00644895, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:47.364476: step 1480, loss 0.726196, acc 0.765252, rec 0.813514, pre 0.735941, f1 0.772786\n",
      "\n",
      "2018-04-16T00:30:47.415945: step 1481, loss 0.00424793, acc 1\n",
      "2018-04-16T00:30:47.471829: step 1482, loss 0.0037398, acc 1\n",
      "2018-04-16T00:30:47.524042: step 1483, loss 0.00252438, acc 1\n",
      "2018-04-16T00:30:47.568656: step 1484, loss 0.00314996, acc 1\n",
      "2018-04-16T00:30:47.620653: step 1485, loss 0.00629125, acc 1\n",
      "2018-04-16T00:30:47.677958: step 1486, loss 0.00514586, acc 1\n",
      "2018-04-16T00:30:47.734970: step 1487, loss 0.00362163, acc 1\n",
      "2018-04-16T00:30:47.779795: step 1488, loss 0.00330465, acc 1\n",
      "2018-04-16T00:30:47.833572: step 1489, loss 0.00230433, acc 1\n",
      "2018-04-16T00:30:47.890627: step 1490, loss 0.002126, acc 1\n",
      "2018-04-16T00:30:47.942729: step 1491, loss 0.00222234, acc 1\n",
      "2018-04-16T00:30:47.984860: step 1492, loss 0.00984942, acc 1\n",
      "2018-04-16T00:30:48.036495: step 1493, loss 0.00457069, acc 1\n",
      "2018-04-16T00:30:48.088806: step 1494, loss 0.00341946, acc 1\n",
      "2018-04-16T00:30:48.144430: step 1495, loss 0.00463641, acc 1\n",
      "2018-04-16T00:30:48.188595: step 1496, loss 0.00263249, acc 1\n",
      "2018-04-16T00:30:48.240471: step 1497, loss 0.00306585, acc 1\n",
      "2018-04-16T00:30:48.293040: step 1498, loss 0.00468416, acc 1\n",
      "2018-04-16T00:30:48.344518: step 1499, loss 0.00377672, acc 1\n",
      "2018-04-16T00:30:48.390280: step 1500, loss 0.00346645, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:48.489927: step 1500, loss 0.837763, acc 0.765252, rec 0.886486, pre 0.708423, f1 0.787515\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523838561/checkpoints/model-1500\n",
      "\n",
      "2018-04-16T00:30:48.600083: step 1501, loss 0.00166095, acc 1\n",
      "2018-04-16T00:30:48.651943: step 1502, loss 0.00243596, acc 1\n",
      "2018-04-16T00:30:48.705386: step 1503, loss 0.00233784, acc 1\n",
      "2018-04-16T00:30:48.747736: step 1504, loss 0.00398318, acc 1\n",
      "2018-04-16T00:30:48.799825: step 1505, loss 0.00365864, acc 1\n",
      "2018-04-16T00:30:48.856156: step 1506, loss 0.00233339, acc 1\n",
      "2018-04-16T00:30:48.907476: step 1507, loss 0.00488042, acc 1\n",
      "2018-04-16T00:30:48.949575: step 1508, loss 0.00137663, acc 1\n",
      "2018-04-16T00:30:49.001558: step 1509, loss 0.00218012, acc 1\n",
      "2018-04-16T00:30:49.055112: step 1510, loss 0.00405508, acc 1\n",
      "2018-04-16T00:30:49.110257: step 1511, loss 0.00212439, acc 1\n",
      "2018-04-16T00:30:49.153159: step 1512, loss 0.00362021, acc 1\n",
      "2018-04-16T00:30:49.208266: step 1513, loss 0.00195043, acc 1\n",
      "2018-04-16T00:30:49.260587: step 1514, loss 0.00406048, acc 1\n",
      "2018-04-16T00:30:49.317029: step 1515, loss 0.00349961, acc 1\n",
      "2018-04-16T00:30:49.359328: step 1516, loss 0.00870866, acc 1\n",
      "2018-04-16T00:30:49.412143: step 1517, loss 0.00214521, acc 1\n",
      "2018-04-16T00:30:49.464405: step 1518, loss 0.00341137, acc 1\n",
      "2018-04-16T00:30:49.516604: step 1519, loss 0.0019594, acc 1\n",
      "2018-04-16T00:30:49.565790: step 1520, loss 0.0095353, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:49.665434: step 1520, loss 0.719379, acc 0.781167, rec 0.783784, pre 0.773333, f1 0.778523\n",
      "\n",
      "2018-04-16T00:30:49.718457: step 1521, loss 0.0041023, acc 1\n",
      "2018-04-16T00:30:49.776035: step 1522, loss 0.00318815, acc 1\n",
      "2018-04-16T00:30:49.835059: step 1523, loss 0.00329649, acc 1\n",
      "2018-04-16T00:30:49.882444: step 1524, loss 0.00225663, acc 1\n",
      "2018-04-16T00:30:49.933612: step 1525, loss 0.00395773, acc 1\n",
      "2018-04-16T00:30:49.989000: step 1526, loss 0.00265439, acc 1\n",
      "2018-04-16T00:30:50.040798: step 1527, loss 0.00193379, acc 1\n",
      "2018-04-16T00:30:50.083502: step 1528, loss 0.00643965, acc 1\n",
      "2018-04-16T00:30:50.135354: step 1529, loss 0.00484018, acc 1\n",
      "2018-04-16T00:30:50.186953: step 1530, loss 0.0025065, acc 1\n",
      "2018-04-16T00:30:50.243015: step 1531, loss 0.00149607, acc 1\n",
      "2018-04-16T00:30:50.285638: step 1532, loss 0.00178445, acc 1\n",
      "2018-04-16T00:30:50.338259: step 1533, loss 0.00964428, acc 0.99\n",
      "2018-04-16T00:30:50.391099: step 1534, loss 0.0115093, acc 1\n",
      "2018-04-16T00:30:50.442921: step 1535, loss 0.00202117, acc 1\n",
      "2018-04-16T00:30:50.488692: step 1536, loss 0.00194515, acc 1\n",
      "2018-04-16T00:30:50.542137: step 1537, loss 0.00444674, acc 1\n",
      "2018-04-16T00:30:50.594269: step 1538, loss 0.00231345, acc 1\n",
      "2018-04-16T00:30:50.646850: step 1539, loss 0.00967276, acc 0.99\n",
      "2018-04-16T00:30:50.694686: step 1540, loss 0.00729211, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:50.796889: step 1540, loss 0.934859, acc 0.730769, rec 0.897297, pre 0.668008, f1 0.765859\n",
      "\n",
      "2018-04-16T00:30:50.850918: step 1541, loss 0.00289184, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:30:50.909430: step 1542, loss 0.00345432, acc 1\n",
      "2018-04-16T00:30:50.964849: step 1543, loss 0.00339374, acc 1\n",
      "2018-04-16T00:30:51.007609: step 1544, loss 0.0162208, acc 0.987179\n",
      "2018-04-16T00:30:51.059851: step 1545, loss 0.00886624, acc 1\n",
      "2018-04-16T00:30:51.116474: step 1546, loss 0.0035078, acc 1\n",
      "2018-04-16T00:30:51.168867: step 1547, loss 0.00292148, acc 1\n",
      "2018-04-16T00:30:51.212354: step 1548, loss 0.00518706, acc 1\n",
      "2018-04-16T00:30:51.352281: step 1549, loss 0.00105927, acc 1\n",
      "2018-04-16T00:30:51.405031: step 1550, loss 0.00346291, acc 1\n",
      "2018-04-16T00:30:51.457865: step 1551, loss 0.0103977, acc 0.99\n",
      "2018-04-16T00:30:51.501076: step 1552, loss 0.0074642, acc 1\n",
      "2018-04-16T00:30:51.557575: step 1553, loss 0.00320567, acc 1\n",
      "2018-04-16T00:30:51.608764: step 1554, loss 0.00193645, acc 1\n",
      "2018-04-16T00:30:51.660135: step 1555, loss 0.00225444, acc 1\n",
      "2018-04-16T00:30:51.702765: step 1556, loss 0.00214543, acc 1\n",
      "2018-04-16T00:30:51.756082: step 1557, loss 0.00589442, acc 1\n",
      "2018-04-16T00:30:51.810887: step 1558, loss 0.0022697, acc 1\n",
      "2018-04-16T00:30:51.862721: step 1559, loss 0.00288881, acc 1\n",
      "2018-04-16T00:30:51.904938: step 1560, loss 0.0027081, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:52.004466: step 1560, loss 0.843287, acc 0.766578, rec 0.881081, pre 0.71179, f1 0.78744\n",
      "\n",
      "2018-04-16T00:30:52.060163: step 1561, loss 0.00470988, acc 1\n",
      "2018-04-16T00:30:52.113534: step 1562, loss 0.00507398, acc 1\n",
      "2018-04-16T00:30:52.165085: step 1563, loss 0.00187874, acc 1\n",
      "2018-04-16T00:30:52.207819: step 1564, loss 0.00311619, acc 1\n",
      "2018-04-16T00:30:52.259590: step 1565, loss 0.00300674, acc 1\n",
      "2018-04-16T00:30:52.315585: step 1566, loss 0.00182083, acc 1\n",
      "2018-04-16T00:30:52.367806: step 1567, loss 0.00227686, acc 1\n",
      "2018-04-16T00:30:52.410434: step 1568, loss 0.0096504, acc 1\n",
      "2018-04-16T00:30:52.462594: step 1569, loss 0.00522394, acc 1\n",
      "2018-04-16T00:30:52.513955: step 1570, loss 0.00263259, acc 1\n",
      "2018-04-16T00:30:52.569128: step 1571, loss 0.00288395, acc 1\n",
      "2018-04-16T00:30:52.612181: step 1572, loss 0.00272252, acc 1\n",
      "2018-04-16T00:30:52.665533: step 1573, loss 0.00658921, acc 1\n",
      "2018-04-16T00:30:52.717702: step 1574, loss 0.0033399, acc 1\n",
      "2018-04-16T00:30:52.774129: step 1575, loss 0.00276768, acc 1\n",
      "2018-04-16T00:30:52.816719: step 1576, loss 0.00111157, acc 1\n",
      "2018-04-16T00:30:52.868597: step 1577, loss 0.00381071, acc 1\n",
      "2018-04-16T00:30:52.920645: step 1578, loss 0.00388679, acc 1\n",
      "2018-04-16T00:30:52.971876: step 1579, loss 0.00310451, acc 1\n",
      "2018-04-16T00:30:53.017324: step 1580, loss 0.00385192, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:53.115836: step 1580, loss 0.767907, acc 0.765252, rec 0.845946, pre 0.722864, f1 0.779577\n",
      "\n",
      "2018-04-16T00:30:53.166457: step 1581, loss 0.00202531, acc 1\n",
      "2018-04-16T00:30:53.217414: step 1582, loss 0.00365208, acc 1\n",
      "2018-04-16T00:30:53.272420: step 1583, loss 0.00471759, acc 1\n",
      "2018-04-16T00:30:53.314687: step 1584, loss 0.00253658, acc 1\n",
      "2018-04-16T00:30:53.366652: step 1585, loss 0.00320742, acc 1\n",
      "2018-04-16T00:30:53.418916: step 1586, loss 0.00394403, acc 1\n",
      "2018-04-16T00:30:53.470546: step 1587, loss 0.00577305, acc 1\n",
      "2018-04-16T00:30:53.517640: step 1588, loss 0.00255946, acc 1\n",
      "2018-04-16T00:30:53.570711: step 1589, loss 0.00279037, acc 1\n",
      "2018-04-16T00:30:53.622319: step 1590, loss 0.00339061, acc 1\n",
      "2018-04-16T00:30:53.673664: step 1591, loss 0.00266832, acc 1\n",
      "2018-04-16T00:30:53.718318: step 1592, loss 0.00107428, acc 1\n",
      "2018-04-16T00:30:53.772872: step 1593, loss 0.001609, acc 1\n",
      "2018-04-16T00:30:53.823960: step 1594, loss 0.00201443, acc 1\n",
      "2018-04-16T00:30:53.875543: step 1595, loss 0.00177142, acc 1\n",
      "2018-04-16T00:30:53.917663: step 1596, loss 0.00823101, acc 1\n",
      "2018-04-16T00:30:53.969545: step 1597, loss 0.00146238, acc 1\n",
      "2018-04-16T00:30:54.024199: step 1598, loss 0.00276992, acc 1\n",
      "2018-04-16T00:30:54.075679: step 1599, loss 0.00237891, acc 1\n",
      "2018-04-16T00:30:54.118138: step 1600, loss 0.00605989, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:54.216739: step 1600, loss 0.792248, acc 0.767905, rec 0.862162, pre 0.72009, f1 0.784748\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523838561/checkpoints/model-1600\n",
      "\n",
      "2018-04-16T00:30:54.330678: step 1601, loss 0.00200623, acc 1\n",
      "2018-04-16T00:30:54.384399: step 1602, loss 0.00428616, acc 1\n",
      "2018-04-16T00:30:54.437931: step 1603, loss 0.00199967, acc 1\n",
      "2018-04-16T00:30:54.487530: step 1604, loss 0.0045011, acc 1\n",
      "2018-04-16T00:30:54.544259: step 1605, loss 0.00331587, acc 1\n",
      "2018-04-16T00:30:54.595488: step 1606, loss 0.0029318, acc 1\n",
      "2018-04-16T00:30:54.646468: step 1607, loss 0.00470148, acc 1\n",
      "2018-04-16T00:30:54.692175: step 1608, loss 0.00192461, acc 1\n",
      "2018-04-16T00:30:54.743729: step 1609, loss 0.00227787, acc 1\n",
      "2018-04-16T00:30:54.795662: step 1610, loss 0.00177011, acc 1\n",
      "2018-04-16T00:30:54.847249: step 1611, loss 0.00589274, acc 1\n",
      "2018-04-16T00:30:54.889403: step 1612, loss 0.00857747, acc 1\n",
      "2018-04-16T00:30:54.945004: step 1613, loss 0.014685, acc 0.99\n",
      "2018-04-16T00:30:54.997558: step 1614, loss 0.0195859, acc 0.99\n",
      "2018-04-16T00:30:55.049494: step 1615, loss 0.00312145, acc 1\n",
      "2018-04-16T00:30:55.092513: step 1616, loss 0.00253978, acc 1\n",
      "2018-04-16T00:30:55.144181: step 1617, loss 0.00309029, acc 1\n",
      "2018-04-16T00:30:55.199917: step 1618, loss 0.00903924, acc 1\n",
      "2018-04-16T00:30:55.251246: step 1619, loss 0.00347748, acc 1\n",
      "2018-04-16T00:30:55.293627: step 1620, loss 0.00397565, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:55.397264: step 1620, loss 0.786586, acc 0.763926, rec 0.856757, pre 0.717195, f1 0.780788\n",
      "\n",
      "2018-04-16T00:30:55.456608: step 1621, loss 0.00209735, acc 1\n",
      "2018-04-16T00:30:55.509748: step 1622, loss 0.00313523, acc 1\n",
      "2018-04-16T00:30:55.563277: step 1623, loss 0.0036063, acc 1\n",
      "2018-04-16T00:30:55.604953: step 1624, loss 0.00229711, acc 1\n",
      "2018-04-16T00:30:55.655840: step 1625, loss 0.00397794, acc 1\n",
      "2018-04-16T00:30:55.712597: step 1626, loss 0.0031991, acc 1\n",
      "2018-04-16T00:30:55.764064: step 1627, loss 0.00349466, acc 1\n",
      "2018-04-16T00:30:55.808003: step 1628, loss 0.00272876, acc 1\n",
      "2018-04-16T00:30:55.863168: step 1629, loss 0.00483203, acc 1\n",
      "2018-04-16T00:30:55.918521: step 1630, loss 0.00159978, acc 1\n",
      "2018-04-16T00:30:55.969400: step 1631, loss 0.00331739, acc 1\n",
      "2018-04-16T00:30:56.011878: step 1632, loss 0.00305642, acc 1\n",
      "2018-04-16T00:30:56.063389: step 1633, loss 0.00824373, acc 1\n",
      "2018-04-16T00:30:56.114164: step 1634, loss 0.00464476, acc 1\n",
      "2018-04-16T00:30:56.170011: step 1635, loss 0.00163085, acc 1\n",
      "2018-04-16T00:30:56.212941: step 1636, loss 0.00187057, acc 1\n",
      "2018-04-16T00:30:56.265615: step 1637, loss 0.00249139, acc 1\n",
      "2018-04-16T00:30:56.316869: step 1638, loss 0.00261157, acc 1\n",
      "2018-04-16T00:30:56.369323: step 1639, loss 0.0186194, acc 0.99\n",
      "2018-04-16T00:30:56.416656: step 1640, loss 0.0191692, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:56.514888: step 1640, loss 0.960465, acc 0.714854, rec 0.902703, pre 0.651072, f1 0.756512\n",
      "\n",
      "2018-04-16T00:30:56.565761: step 1641, loss 0.00186225, acc 1\n",
      "2018-04-16T00:30:56.616812: step 1642, loss 0.0026246, acc 1\n",
      "2018-04-16T00:30:56.670950: step 1643, loss 0.00226583, acc 1\n",
      "2018-04-16T00:30:56.714141: step 1644, loss 0.0090554, acc 1\n",
      "2018-04-16T00:30:56.766170: step 1645, loss 0.00462378, acc 1\n",
      "2018-04-16T00:30:56.817854: step 1646, loss 0.00292669, acc 1\n",
      "2018-04-16T00:30:56.868844: step 1647, loss 0.00346062, acc 1\n",
      "2018-04-16T00:30:56.914479: step 1648, loss 0.00165441, acc 1\n",
      "2018-04-16T00:30:56.966193: step 1649, loss 0.00248587, acc 1\n",
      "2018-04-16T00:30:57.016862: step 1650, loss 0.00136922, acc 1\n",
      "2018-04-16T00:30:57.067980: step 1651, loss 0.00653296, acc 1\n",
      "2018-04-16T00:30:57.125291: step 1652, loss 0.00300521, acc 1\n",
      "2018-04-16T00:30:57.185998: step 1653, loss 0.00261231, acc 1\n",
      "2018-04-16T00:30:57.248612: step 1654, loss 0.0015301, acc 1\n",
      "2018-04-16T00:30:57.311519: step 1655, loss 0.00317193, acc 1\n",
      "2018-04-16T00:30:57.369905: step 1656, loss 0.00257773, acc 1\n",
      "2018-04-16T00:30:57.431697: step 1657, loss 0.00228224, acc 1\n",
      "2018-04-16T00:30:57.493474: step 1658, loss 0.00307212, acc 1\n",
      "2018-04-16T00:30:57.554289: step 1659, loss 0.00266678, acc 1\n",
      "2018-04-16T00:30:57.601287: step 1660, loss 0.00103154, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:57.700656: step 1660, loss 0.848638, acc 0.761273, rec 0.875676, pre 0.707424, f1 0.782609\n",
      "\n",
      "2018-04-16T00:30:57.752677: step 1661, loss 0.00220635, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:30:57.808434: step 1662, loss 0.00176495, acc 1\n",
      "2018-04-16T00:30:57.859766: step 1663, loss 0.00316674, acc 1\n",
      "2018-04-16T00:30:57.902402: step 1664, loss 0.00365181, acc 1\n",
      "2018-04-16T00:30:57.954554: step 1665, loss 0.00184882, acc 1\n",
      "2018-04-16T00:30:58.006848: step 1666, loss 0.00574128, acc 1\n",
      "2018-04-16T00:30:58.062941: step 1667, loss 0.00485967, acc 1\n",
      "2018-04-16T00:30:58.106197: step 1668, loss 0.00392346, acc 1\n",
      "2018-04-16T00:30:58.159037: step 1669, loss 0.00366793, acc 1\n",
      "2018-04-16T00:30:58.211756: step 1670, loss 0.00371983, acc 1\n",
      "2018-04-16T00:30:58.263503: step 1671, loss 0.00394917, acc 1\n",
      "2018-04-16T00:30:58.309460: step 1672, loss 0.00113689, acc 1\n",
      "2018-04-16T00:30:58.361876: step 1673, loss 0.00170519, acc 1\n",
      "2018-04-16T00:30:58.414106: step 1674, loss 0.00220214, acc 1\n",
      "2018-04-16T00:30:58.465885: step 1675, loss 0.00199975, acc 1\n",
      "2018-04-16T00:30:58.507962: step 1676, loss 0.0076415, acc 1\n",
      "2018-04-16T00:30:58.563609: step 1677, loss 0.00251109, acc 1\n",
      "2018-04-16T00:30:58.615696: step 1678, loss 0.00170883, acc 1\n",
      "2018-04-16T00:30:58.669567: step 1679, loss 0.0024398, acc 1\n",
      "2018-04-16T00:30:58.713192: step 1680, loss 0.00348294, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:58.818180: step 1680, loss 0.870635, acc 0.762599, rec 0.891892, pre 0.703625, f1 0.786651\n",
      "\n",
      "2018-04-16T00:30:58.871541: step 1681, loss 0.00311889, acc 1\n",
      "2018-04-16T00:30:58.923529: step 1682, loss 0.00205339, acc 1\n",
      "2018-04-16T00:30:58.975041: step 1683, loss 0.00259194, acc 1\n",
      "2018-04-16T00:30:59.017334: step 1684, loss 0.00297804, acc 1\n",
      "2018-04-16T00:30:59.072497: step 1685, loss 0.00349396, acc 1\n",
      "2018-04-16T00:30:59.128654: step 1686, loss 0.00143909, acc 1\n",
      "2018-04-16T00:30:59.179870: step 1687, loss 0.00236064, acc 1\n",
      "2018-04-16T00:30:59.222434: step 1688, loss 0.00315274, acc 1\n",
      "2018-04-16T00:30:59.273548: step 1689, loss 0.00165634, acc 1\n",
      "2018-04-16T00:30:59.329123: step 1690, loss 0.00199819, acc 1\n",
      "2018-04-16T00:30:59.380632: step 1691, loss 0.00147324, acc 1\n",
      "2018-04-16T00:30:59.422696: step 1692, loss 0.00144966, acc 1\n",
      "2018-04-16T00:30:59.474526: step 1693, loss 0.00152574, acc 1\n",
      "2018-04-16T00:30:59.525617: step 1694, loss 0.0014133, acc 1\n",
      "2018-04-16T00:30:59.582938: step 1695, loss 0.00118165, acc 1\n",
      "2018-04-16T00:30:59.625165: step 1696, loss 0.00763703, acc 1\n",
      "2018-04-16T00:30:59.676550: step 1697, loss 0.00504204, acc 1\n",
      "2018-04-16T00:30:59.728652: step 1698, loss 0.00294094, acc 1\n",
      "2018-04-16T00:30:59.780556: step 1699, loss 0.00206549, acc 1\n",
      "2018-04-16T00:30:59.826107: step 1700, loss 0.00326274, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:30:59.924577: step 1700, loss 0.823139, acc 0.767905, rec 0.867568, pre 0.718121, f1 0.785802\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523838561/checkpoints/model-1700\n",
      "\n",
      "2018-04-16T00:31:00.032716: step 1701, loss 0.00465625, acc 1\n",
      "2018-04-16T00:31:00.083976: step 1702, loss 0.00249225, acc 1\n",
      "2018-04-16T00:31:00.135604: step 1703, loss 0.00292334, acc 1\n",
      "2018-04-16T00:31:00.177673: step 1704, loss 0.00161593, acc 1\n",
      "2018-04-16T00:31:00.228753: step 1705, loss 0.00209691, acc 1\n",
      "2018-04-16T00:31:00.285026: step 1706, loss 0.00217164, acc 1\n",
      "2018-04-16T00:31:00.336344: step 1707, loss 0.00135458, acc 1\n",
      "2018-04-16T00:31:00.378816: step 1708, loss 0.00756588, acc 1\n",
      "2018-04-16T00:31:00.430352: step 1709, loss 0.00448691, acc 1\n",
      "2018-04-16T00:31:00.481813: step 1710, loss 0.00315595, acc 1\n",
      "2018-04-16T00:31:00.536571: step 1711, loss 0.00258703, acc 1\n",
      "2018-04-16T00:31:00.578986: step 1712, loss 0.00195662, acc 1\n",
      "2018-04-16T00:31:00.630641: step 1713, loss 0.00208244, acc 1\n",
      "2018-04-16T00:31:00.681972: step 1714, loss 0.00288171, acc 1\n",
      "2018-04-16T00:31:00.733820: step 1715, loss 0.00186398, acc 1\n",
      "2018-04-16T00:31:00.780294: step 1716, loss 0.00232153, acc 1\n",
      "2018-04-16T00:31:00.835632: step 1717, loss 0.00412736, acc 1\n",
      "2018-04-16T00:31:00.893276: step 1718, loss 0.00300874, acc 1\n",
      "2018-04-16T00:31:00.949644: step 1719, loss 0.00193177, acc 1\n",
      "2018-04-16T00:31:01.020605: step 1720, loss 0.00240196, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:31:01.165556: step 1720, loss 0.861239, acc 0.765252, rec 0.881081, pre 0.71024, f1 0.78649\n",
      "\n",
      "2018-04-16T00:31:01.218054: step 1721, loss 0.00193501, acc 1\n",
      "2018-04-16T00:31:01.273376: step 1722, loss 0.00256235, acc 1\n",
      "2018-04-16T00:31:01.324555: step 1723, loss 0.00116863, acc 1\n",
      "2018-04-16T00:31:01.366588: step 1724, loss 0.00244938, acc 1\n",
      "2018-04-16T00:31:01.418027: step 1725, loss 0.00217585, acc 1\n",
      "2018-04-16T00:31:01.469429: step 1726, loss 0.00416402, acc 1\n",
      "2018-04-16T00:31:01.525982: step 1727, loss 0.00178321, acc 1\n",
      "2018-04-16T00:31:01.569932: step 1728, loss 0.00180129, acc 1\n",
      "2018-04-16T00:31:01.622161: step 1729, loss 0.00317264, acc 1\n",
      "2018-04-16T00:31:01.674895: step 1730, loss 0.00182721, acc 1\n",
      "2018-04-16T00:31:01.732384: step 1731, loss 0.00211533, acc 1\n",
      "2018-04-16T00:31:01.778288: step 1732, loss 0.00243195, acc 1\n",
      "2018-04-16T00:31:01.830082: step 1733, loss 0.00195386, acc 1\n",
      "2018-04-16T00:31:01.881778: step 1734, loss 0.00177219, acc 1\n",
      "2018-04-16T00:31:01.938828: step 1735, loss 0.0012856, acc 1\n",
      "2018-04-16T00:31:01.982882: step 1736, loss 0.00434753, acc 1\n",
      "2018-04-16T00:31:02.040973: step 1737, loss 0.00285515, acc 1\n",
      "2018-04-16T00:31:02.104444: step 1738, loss 0.00152145, acc 1\n",
      "2018-04-16T00:31:02.163761: step 1739, loss 0.0132908, acc 0.99\n",
      "2018-04-16T00:31:02.208459: step 1740, loss 0.00713478, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:31:02.313303: step 1740, loss 0.735847, acc 0.775862, rec 0.816216, pre 0.74938, f1 0.781371\n",
      "\n",
      "2018-04-16T00:31:02.371250: step 1741, loss 0.00244663, acc 1\n",
      "2018-04-16T00:31:02.425948: step 1742, loss 0.00283753, acc 1\n",
      "2018-04-16T00:31:02.483309: step 1743, loss 0.00280992, acc 1\n",
      "2018-04-16T00:31:02.527503: step 1744, loss 0.00253895, acc 1\n",
      "2018-04-16T00:31:02.586191: step 1745, loss 0.00214675, acc 1\n",
      "2018-04-16T00:31:02.640948: step 1746, loss 0.00165247, acc 1\n",
      "2018-04-16T00:31:02.696038: step 1747, loss 0.00175759, acc 1\n",
      "2018-04-16T00:31:02.741216: step 1748, loss 0.00480556, acc 1\n",
      "2018-04-16T00:31:02.799846: step 1749, loss 0.00340677, acc 1\n",
      "2018-04-16T00:31:02.854250: step 1750, loss 0.00233476, acc 1\n",
      "2018-04-16T00:31:02.905363: step 1751, loss 0.00250118, acc 1\n",
      "2018-04-16T00:31:02.950118: step 1752, loss 0.00198137, acc 1\n",
      "2018-04-16T00:31:03.006090: step 1753, loss 0.00261182, acc 1\n",
      "2018-04-16T00:31:03.061217: step 1754, loss 0.00230348, acc 1\n",
      "2018-04-16T00:31:03.112429: step 1755, loss 0.00444162, acc 1\n",
      "2018-04-16T00:31:03.155806: step 1756, loss 0.00186985, acc 1\n",
      "2018-04-16T00:31:03.217005: step 1757, loss 0.00235073, acc 1\n",
      "2018-04-16T00:31:03.272894: step 1758, loss 0.00153044, acc 1\n",
      "2018-04-16T00:31:03.327596: step 1759, loss 0.0021252, acc 1\n",
      "2018-04-16T00:31:03.370816: step 1760, loss 0.0042624, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:31:03.484424: step 1760, loss 0.758589, acc 0.769231, rec 0.832432, pre 0.733333, f1 0.779747\n",
      "\n",
      "2018-04-16T00:31:03.538906: step 1761, loss 0.00274288, acc 1\n",
      "2018-04-16T00:31:03.589668: step 1762, loss 0.00476229, acc 1\n",
      "2018-04-16T00:31:03.641205: step 1763, loss 0.0019429, acc 1\n",
      "2018-04-16T00:31:03.683068: step 1764, loss 0.00192095, acc 1\n",
      "2018-04-16T00:31:03.740541: step 1765, loss 0.00210872, acc 1\n",
      "2018-04-16T00:31:03.794688: step 1766, loss 0.00334843, acc 1\n",
      "2018-04-16T00:31:03.846052: step 1767, loss 0.001381, acc 1\n",
      "2018-04-16T00:31:03.889199: step 1768, loss 0.00125417, acc 1\n",
      "2018-04-16T00:31:03.941176: step 1769, loss 0.00259226, acc 1\n",
      "2018-04-16T00:31:03.996151: step 1770, loss 0.00126191, acc 1\n",
      "2018-04-16T00:31:04.047430: step 1771, loss 0.00178459, acc 1\n",
      "2018-04-16T00:31:04.089396: step 1772, loss 0.00457589, acc 1\n",
      "2018-04-16T00:31:04.141477: step 1773, loss 0.00251009, acc 1\n",
      "2018-04-16T00:31:04.193216: step 1774, loss 0.00332765, acc 1\n",
      "2018-04-16T00:31:04.249196: step 1775, loss 0.00212993, acc 1\n",
      "2018-04-16T00:31:04.291939: step 1776, loss 0.00108193, acc 1\n",
      "2018-04-16T00:31:04.343962: step 1777, loss 0.00163639, acc 1\n",
      "2018-04-16T00:31:04.395780: step 1778, loss 0.001455, acc 1\n",
      "2018-04-16T00:31:04.448847: step 1779, loss 0.00326796, acc 1\n",
      "2018-04-16T00:31:04.495801: step 1780, loss 0.00422893, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:31:04.594500: step 1780, loss 0.880943, acc 0.761273, rec 0.886486, pre 0.703863, f1 0.784689\n",
      "\n",
      "2018-04-16T00:31:04.646038: step 1781, loss 0.00232608, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:31:04.701647: step 1782, loss 0.00271606, acc 1\n",
      "2018-04-16T00:31:04.753795: step 1783, loss 0.00337688, acc 1\n",
      "2018-04-16T00:31:04.796618: step 1784, loss 0.00214017, acc 1\n",
      "2018-04-16T00:31:04.854240: step 1785, loss 0.00263886, acc 1\n",
      "2018-04-16T00:31:04.909130: step 1786, loss 0.00145514, acc 1\n",
      "2018-04-16T00:31:04.960840: step 1787, loss 0.00157672, acc 1\n",
      "2018-04-16T00:31:05.002839: step 1788, loss 0.00178652, acc 1\n",
      "2018-04-16T00:31:05.053952: step 1789, loss 0.00195738, acc 1\n",
      "2018-04-16T00:31:05.105338: step 1790, loss 0.00114672, acc 1\n",
      "2018-04-16T00:31:05.160428: step 1791, loss 0.00629129, acc 1\n",
      "2018-04-16T00:31:05.203391: step 1792, loss 0.00167803, acc 1\n",
      "2018-04-16T00:31:05.255179: step 1793, loss 0.00379324, acc 1\n",
      "2018-04-16T00:31:05.307245: step 1794, loss 0.00237796, acc 1\n",
      "2018-04-16T00:31:05.359324: step 1795, loss 0.00138722, acc 1\n",
      "2018-04-16T00:31:05.406491: step 1796, loss 0.00221096, acc 1\n",
      "2018-04-16T00:31:05.458106: step 1797, loss 0.0024289, acc 1\n",
      "2018-04-16T00:31:05.509535: step 1798, loss 0.00167307, acc 1\n",
      "2018-04-16T00:31:05.562531: step 1799, loss 0.00602407, acc 1\n",
      "2018-04-16T00:31:05.605604: step 1800, loss 0.00353264, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:31:05.710640: step 1800, loss 0.772814, acc 0.765252, rec 0.837838, pre 0.725995, f1 0.777917\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523838561/checkpoints/model-1800\n",
      "\n",
      "2018-04-16T00:31:05.816627: step 1801, loss 0.00204185, acc 1\n",
      "2018-04-16T00:31:05.868117: step 1802, loss 0.00284905, acc 1\n",
      "2018-04-16T00:31:05.919541: step 1803, loss 0.00462495, acc 1\n",
      "2018-04-16T00:31:05.961226: step 1804, loss 0.0011047, acc 1\n",
      "2018-04-16T00:31:06.012094: step 1805, loss 0.0021993, acc 1\n",
      "2018-04-16T00:31:06.066923: step 1806, loss 0.00219758, acc 1\n",
      "2018-04-16T00:31:06.120287: step 1807, loss 0.0018039, acc 1\n",
      "2018-04-16T00:31:06.163495: step 1808, loss 0.00392924, acc 1\n",
      "2018-04-16T00:31:06.214571: step 1809, loss 0.00379561, acc 1\n",
      "2018-04-16T00:31:06.266605: step 1810, loss 0.00348131, acc 1\n",
      "2018-04-16T00:31:06.321468: step 1811, loss 0.00109596, acc 1\n",
      "2018-04-16T00:31:06.363815: step 1812, loss 0.0016787, acc 1\n",
      "2018-04-16T00:31:06.414942: step 1813, loss 0.00176616, acc 1\n",
      "2018-04-16T00:31:06.466348: step 1814, loss 0.00263394, acc 1\n",
      "2018-04-16T00:31:06.517592: step 1815, loss 0.0022525, acc 1\n",
      "2018-04-16T00:31:06.565171: step 1816, loss 0.00126728, acc 1\n",
      "2018-04-16T00:31:06.617008: step 1817, loss 0.00379623, acc 1\n",
      "2018-04-16T00:31:06.668442: step 1818, loss 0.00169848, acc 1\n",
      "2018-04-16T00:31:06.720075: step 1819, loss 0.0021837, acc 1\n",
      "2018-04-16T00:31:06.762550: step 1820, loss 0.0030854, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:31:06.865959: step 1820, loss 0.851211, acc 0.769231, rec 0.875676, pre 0.716814, f1 0.788321\n",
      "\n",
      "2018-04-16T00:31:06.917092: step 1821, loss 0.00190506, acc 1\n",
      "2018-04-16T00:31:06.968474: step 1822, loss 0.00154707, acc 1\n",
      "2018-04-16T00:31:07.021005: step 1823, loss 0.00311669, acc 1\n",
      "2018-04-16T00:31:07.063099: step 1824, loss 0.00199631, acc 1\n",
      "2018-04-16T00:31:07.118691: step 1825, loss 0.00378223, acc 1\n",
      "2018-04-16T00:31:07.171088: step 1826, loss 0.0030218, acc 1\n",
      "2018-04-16T00:31:07.222876: step 1827, loss 0.00223268, acc 1\n",
      "2018-04-16T00:31:07.265211: step 1828, loss 0.00213595, acc 1\n",
      "2018-04-16T00:31:07.316942: step 1829, loss 0.00786453, acc 1\n",
      "2018-04-16T00:31:07.374065: step 1830, loss 0.0036956, acc 1\n",
      "2018-04-16T00:31:07.426647: step 1831, loss 0.00391838, acc 1\n",
      "2018-04-16T00:31:07.470589: step 1832, loss 0.00198602, acc 1\n",
      "2018-04-16T00:31:07.523223: step 1833, loss 0.00250131, acc 1\n",
      "2018-04-16T00:31:07.579575: step 1834, loss 0.00420779, acc 1\n",
      "2018-04-16T00:31:07.631465: step 1835, loss 0.00274432, acc 1\n",
      "2018-04-16T00:31:07.675916: step 1836, loss 0.00393553, acc 1\n",
      "2018-04-16T00:31:07.731547: step 1837, loss 0.00409562, acc 1\n",
      "2018-04-16T00:31:07.787141: step 1838, loss 0.00193599, acc 1\n",
      "2018-04-16T00:31:07.838597: step 1839, loss 0.00213982, acc 1\n",
      "2018-04-16T00:31:07.881036: step 1840, loss 0.00167176, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:31:07.980504: step 1840, loss 0.826391, acc 0.771883, rec 0.872973, pre 0.720982, f1 0.789731\n",
      "\n",
      "2018-04-16T00:31:08.036725: step 1841, loss 0.00145279, acc 1\n",
      "2018-04-16T00:31:08.088147: step 1842, loss 0.00157198, acc 1\n",
      "2018-04-16T00:31:08.141472: step 1843, loss 0.00406302, acc 1\n",
      "2018-04-16T00:31:08.185301: step 1844, loss 0.00179031, acc 1\n",
      "2018-04-16T00:31:08.241404: step 1845, loss 0.00264369, acc 1\n",
      "2018-04-16T00:31:08.293139: step 1846, loss 0.00357694, acc 1\n",
      "2018-04-16T00:31:08.344297: step 1847, loss 0.00157625, acc 1\n",
      "2018-04-16T00:31:08.387486: step 1848, loss 0.000976535, acc 1\n",
      "2018-04-16T00:31:08.440736: step 1849, loss 0.00168243, acc 1\n",
      "2018-04-16T00:31:08.497095: step 1850, loss 0.00156271, acc 1\n",
      "2018-04-16T00:31:08.549049: step 1851, loss 0.00284346, acc 1\n",
      "2018-04-16T00:31:08.591425: step 1852, loss 0.00122832, acc 1\n",
      "2018-04-16T00:31:08.643492: step 1853, loss 0.00177905, acc 1\n",
      "2018-04-16T00:31:08.696593: step 1854, loss 0.0016608, acc 1\n",
      "2018-04-16T00:31:08.753468: step 1855, loss 0.00158838, acc 1\n",
      "2018-04-16T00:31:08.797317: step 1856, loss 0.00354718, acc 1\n",
      "2018-04-16T00:31:08.850400: step 1857, loss 0.00219354, acc 1\n",
      "2018-04-16T00:31:08.903683: step 1858, loss 0.00200265, acc 1\n",
      "2018-04-16T00:31:08.962345: step 1859, loss 0.00276975, acc 1\n",
      "2018-04-16T00:31:09.006391: step 1860, loss 0.00103858, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:31:09.104811: step 1860, loss 0.833473, acc 0.770557, rec 0.872973, pre 0.719376, f1 0.788767\n",
      "\n",
      "2018-04-16T00:31:09.156202: step 1861, loss 0.00259288, acc 1\n",
      "2018-04-16T00:31:09.210903: step 1862, loss 0.0024495, acc 1\n",
      "2018-04-16T00:31:09.263709: step 1863, loss 0.00122085, acc 1\n",
      "2018-04-16T00:31:09.306132: step 1864, loss 0.00200738, acc 1\n",
      "2018-04-16T00:31:09.357954: step 1865, loss 0.00142826, acc 1\n",
      "2018-04-16T00:31:09.409579: step 1866, loss 0.00171293, acc 1\n",
      "2018-04-16T00:31:09.464791: step 1867, loss 0.00335941, acc 1\n",
      "2018-04-16T00:31:09.507013: step 1868, loss 0.00136312, acc 1\n",
      "2018-04-16T00:31:09.569775: step 1869, loss 0.00198469, acc 1\n",
      "2018-04-16T00:31:09.646067: step 1870, loss 0.00108543, acc 1\n",
      "2018-04-16T00:31:09.718089: step 1871, loss 0.00316332, acc 1\n",
      "2018-04-16T00:31:09.761265: step 1872, loss 0.00229238, acc 1\n",
      "2018-04-16T00:31:09.814262: step 1873, loss 0.0028157, acc 1\n",
      "2018-04-16T00:31:09.866832: step 1874, loss 0.000852162, acc 1\n",
      "2018-04-16T00:31:09.925382: step 1875, loss 0.00290692, acc 1\n",
      "2018-04-16T00:31:09.969121: step 1876, loss 0.00176956, acc 1\n",
      "2018-04-16T00:31:10.021928: step 1877, loss 0.00161625, acc 1\n",
      "2018-04-16T00:31:10.075553: step 1878, loss 0.00239419, acc 1\n",
      "2018-04-16T00:31:10.133089: step 1879, loss 0.00947955, acc 0.99\n",
      "2018-04-16T00:31:10.178473: step 1880, loss 0.00198097, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:31:10.280020: step 1880, loss 0.725088, acc 0.787798, rec 0.794595, pre 0.777778, f1 0.786096\n",
      "\n",
      "2018-04-16T00:31:10.333118: step 1881, loss 0.00254651, acc 1\n",
      "2018-04-16T00:31:10.391133: step 1882, loss 0.00461418, acc 1\n",
      "2018-04-16T00:31:10.446474: step 1883, loss 0.00256721, acc 1\n",
      "2018-04-16T00:31:10.491703: step 1884, loss 0.00141911, acc 1\n",
      "2018-04-16T00:31:10.546849: step 1885, loss 0.00279552, acc 1\n",
      "2018-04-16T00:31:10.603140: step 1886, loss 0.00162193, acc 1\n",
      "2018-04-16T00:31:10.656714: step 1887, loss 0.00159273, acc 1\n",
      "2018-04-16T00:31:10.701056: step 1888, loss 0.00118039, acc 1\n",
      "2018-04-16T00:31:10.753310: step 1889, loss 0.00127441, acc 1\n",
      "2018-04-16T00:31:10.811087: step 1890, loss 0.00189437, acc 1\n",
      "2018-04-16T00:31:10.864592: step 1891, loss 0.00156169, acc 1\n",
      "2018-04-16T00:31:10.907784: step 1892, loss 0.00189942, acc 1\n",
      "2018-04-16T00:31:10.960875: step 1893, loss 0.00121909, acc 1\n",
      "2018-04-16T00:31:11.018628: step 1894, loss 0.00111409, acc 1\n",
      "2018-04-16T00:31:11.075520: step 1895, loss 0.00494203, acc 1\n",
      "2018-04-16T00:31:11.122626: step 1896, loss 0.00149784, acc 1\n",
      "2018-04-16T00:31:11.175706: step 1897, loss 0.00289121, acc 1\n",
      "2018-04-16T00:31:11.232260: step 1898, loss 0.00109551, acc 1\n",
      "2018-04-16T00:31:11.284129: step 1899, loss 0.00314226, acc 1\n",
      "2018-04-16T00:31:11.327149: step 1900, loss 0.00188606, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:31:11.427136: step 1900, loss 0.850324, acc 0.770557, rec 0.875676, pre 0.718404, f1 0.789281\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523838561/checkpoints/model-1900\n",
      "\n",
      "2018-04-16T00:31:11.542394: step 1901, loss 0.00147693, acc 1\n",
      "2018-04-16T00:31:11.594229: step 1902, loss 0.00256313, acc 1\n",
      "2018-04-16T00:31:11.650573: step 1903, loss 0.00290862, acc 1\n",
      "2018-04-16T00:31:11.696927: step 1904, loss 0.00155127, acc 1\n",
      "2018-04-16T00:31:11.749674: step 1905, loss 0.00152615, acc 1\n",
      "2018-04-16T00:31:11.802906: step 1906, loss 0.00204615, acc 1\n",
      "2018-04-16T00:31:11.855989: step 1907, loss 0.00231597, acc 1\n",
      "2018-04-16T00:31:11.905412: step 1908, loss 0.00130831, acc 1\n",
      "2018-04-16T00:31:11.957751: step 1909, loss 0.00192467, acc 1\n",
      "2018-04-16T00:31:12.009072: step 1910, loss 0.00129459, acc 1\n",
      "2018-04-16T00:31:12.060943: step 1911, loss 0.00110215, acc 1\n",
      "2018-04-16T00:31:12.102880: step 1912, loss 0.00273609, acc 1\n",
      "2018-04-16T00:31:12.158682: step 1913, loss 0.000872197, acc 1\n",
      "2018-04-16T00:31:12.210786: step 1914, loss 0.00158883, acc 1\n",
      "2018-04-16T00:31:12.264062: step 1915, loss 0.0015638, acc 1\n",
      "2018-04-16T00:31:12.306814: step 1916, loss 0.00407545, acc 1\n",
      "2018-04-16T00:31:12.364900: step 1917, loss 0.00244932, acc 1\n",
      "2018-04-16T00:31:12.417840: step 1918, loss 0.00339746, acc 1\n",
      "2018-04-16T00:31:12.469737: step 1919, loss 0.000995627, acc 1\n",
      "2018-04-16T00:31:12.512531: step 1920, loss 0.00520344, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:31:12.623168: step 1920, loss 0.747795, acc 0.77321, rec 0.808108, pre 0.749373, f1 0.777633\n",
      "\n",
      "2018-04-16T00:31:12.676623: step 1921, loss 0.00163238, acc 1\n",
      "2018-04-16T00:31:12.746981: step 1922, loss 0.00219907, acc 1\n",
      "2018-04-16T00:31:12.832890: step 1923, loss 0.00152425, acc 1\n",
      "2018-04-16T00:31:12.900504: step 1924, loss 0.00223567, acc 1\n",
      "2018-04-16T00:31:12.981813: step 1925, loss 0.00182184, acc 1\n",
      "2018-04-16T00:31:13.068124: step 1926, loss 0.00177055, acc 1\n",
      "2018-04-16T00:31:13.150523: step 1927, loss 0.00552922, acc 1\n",
      "2018-04-16T00:31:13.218487: step 1928, loss 0.00102235, acc 1\n",
      "2018-04-16T00:31:13.306792: step 1929, loss 0.00156716, acc 1\n",
      "2018-04-16T00:31:13.386994: step 1930, loss 0.00170941, acc 1\n",
      "2018-04-16T00:31:13.467428: step 1931, loss 0.00750009, acc 1\n",
      "2018-04-16T00:31:13.534238: step 1932, loss 0.00656414, acc 1\n",
      "2018-04-16T00:31:13.614926: step 1933, loss 0.00248723, acc 1\n",
      "2018-04-16T00:31:13.694184: step 1934, loss 0.00174153, acc 1\n",
      "2018-04-16T00:31:13.758626: step 1935, loss 0.00173167, acc 1\n",
      "2018-04-16T00:31:13.801029: step 1936, loss 0.0016108, acc 1\n",
      "2018-04-16T00:31:13.852598: step 1937, loss 0.0038275, acc 1\n",
      "2018-04-16T00:31:13.905950: step 1938, loss 0.00192627, acc 1\n",
      "2018-04-16T00:31:13.958112: step 1939, loss 0.00101186, acc 1\n",
      "2018-04-16T00:31:14.004859: step 1940, loss 0.000850004, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:31:14.105673: step 1940, loss 0.867343, acc 0.767905, rec 0.878378, pre 0.714286, f1 0.787879\n",
      "\n",
      "2018-04-16T00:31:14.158237: step 1941, loss 0.00215518, acc 1\n",
      "2018-04-16T00:31:14.214832: step 1942, loss 0.00208993, acc 1\n",
      "2018-04-16T00:31:14.267608: step 1943, loss 0.00372546, acc 1\n",
      "2018-04-16T00:31:14.309848: step 1944, loss 0.00173791, acc 1\n",
      "2018-04-16T00:31:14.362247: step 1945, loss 0.00255488, acc 1\n",
      "2018-04-16T00:31:14.413584: step 1946, loss 0.00333461, acc 1\n",
      "2018-04-16T00:31:14.468623: step 1947, loss 0.00493921, acc 1\n",
      "2018-04-16T00:31:14.511400: step 1948, loss 0.0024444, acc 1\n",
      "2018-04-16T00:31:14.565407: step 1949, loss 0.00342996, acc 1\n",
      "2018-04-16T00:31:14.617657: step 1950, loss 0.00229005, acc 1\n",
      "2018-04-16T00:31:14.669233: step 1951, loss 0.00111647, acc 1\n",
      "2018-04-16T00:31:14.714949: step 1952, loss 0.00275023, acc 1\n",
      "2018-04-16T00:31:14.768014: step 1953, loss 0.00142338, acc 1\n",
      "2018-04-16T00:31:14.820369: step 1954, loss 0.0012219, acc 1\n",
      "2018-04-16T00:31:14.871932: step 1955, loss 0.00129866, acc 1\n",
      "2018-04-16T00:31:14.919993: step 1956, loss 0.00331159, acc 1\n",
      "2018-04-16T00:31:14.971836: step 1957, loss 0.00168221, acc 1\n",
      "2018-04-16T00:31:15.023822: step 1958, loss 0.00195384, acc 1\n",
      "2018-04-16T00:31:15.076292: step 1959, loss 0.00243101, acc 1\n",
      "2018-04-16T00:31:15.119826: step 1960, loss 0.00216207, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:31:15.223021: step 1960, loss 0.829583, acc 0.767905, rec 0.864865, pre 0.719101, f1 0.785276\n",
      "\n",
      "2018-04-16T00:31:15.275179: step 1961, loss 0.00179687, acc 1\n",
      "2018-04-16T00:31:15.330552: step 1962, loss 0.00186696, acc 1\n",
      "2018-04-16T00:31:15.384130: step 1963, loss 0.00133472, acc 1\n",
      "2018-04-16T00:31:15.427403: step 1964, loss 0.00129768, acc 1\n",
      "2018-04-16T00:31:15.479843: step 1965, loss 0.00210499, acc 1\n",
      "2018-04-16T00:31:15.535489: step 1966, loss 0.00383305, acc 1\n",
      "2018-04-16T00:31:15.587107: step 1967, loss 0.00100718, acc 1\n",
      "2018-04-16T00:31:15.629422: step 1968, loss 0.00287275, acc 1\n",
      "2018-04-16T00:31:15.681801: step 1969, loss 0.00181689, acc 1\n",
      "2018-04-16T00:31:15.735219: step 1970, loss 0.00153655, acc 1\n",
      "2018-04-16T00:31:15.791308: step 1971, loss 0.0021958, acc 1\n",
      "2018-04-16T00:31:15.833403: step 1972, loss 0.00218391, acc 1\n",
      "2018-04-16T00:31:15.884938: step 1973, loss 0.0021265, acc 1\n",
      "2018-04-16T00:31:15.936698: step 1974, loss 0.00133007, acc 1\n",
      "2018-04-16T00:31:15.988096: step 1975, loss 0.00170285, acc 1\n",
      "2018-04-16T00:31:16.035246: step 1976, loss 0.00135613, acc 1\n",
      "2018-04-16T00:31:16.089049: step 1977, loss 0.00312852, acc 1\n",
      "2018-04-16T00:31:16.143832: step 1978, loss 0.00159822, acc 1\n",
      "2018-04-16T00:31:16.197324: step 1979, loss 0.00235089, acc 1\n",
      "2018-04-16T00:31:16.244783: step 1980, loss 0.00208435, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:31:16.344024: step 1980, loss 0.855963, acc 0.766578, rec 0.867568, pre 0.716518, f1 0.784841\n",
      "\n",
      "2018-04-16T00:31:16.395335: step 1981, loss 0.00308079, acc 1\n",
      "2018-04-16T00:31:16.459430: step 1982, loss 0.00161529, acc 1\n",
      "2018-04-16T00:31:16.513247: step 1983, loss 0.00120393, acc 1\n",
      "2018-04-16T00:31:16.558810: step 1984, loss 0.00189212, acc 1\n",
      "2018-04-16T00:31:16.610108: step 1985, loss 0.0016333, acc 1\n",
      "2018-04-16T00:31:16.664850: step 1986, loss 0.00108528, acc 1\n",
      "2018-04-16T00:31:16.716180: step 1987, loss 0.000868793, acc 1\n",
      "2018-04-16T00:31:16.759074: step 1988, loss 0.00451447, acc 1\n",
      "2018-04-16T00:31:16.810771: step 1989, loss 0.00225058, acc 1\n",
      "2018-04-16T00:31:16.862241: step 1990, loss 0.00150256, acc 1\n",
      "2018-04-16T00:31:16.917222: step 1991, loss 0.00189981, acc 1\n",
      "2018-04-16T00:31:16.959937: step 1992, loss 0.00348285, acc 1\n",
      "2018-04-16T00:31:17.012169: step 1993, loss 0.00191059, acc 1\n",
      "2018-04-16T00:31:17.063975: step 1994, loss 0.00153131, acc 1\n",
      "2018-04-16T00:31:17.116156: step 1995, loss 0.00255786, acc 1\n",
      "2018-04-16T00:31:17.161872: step 1996, loss 0.00225233, acc 1\n",
      "2018-04-16T00:31:17.214612: step 1997, loss 0.00231365, acc 1\n",
      "2018-04-16T00:31:17.267019: step 1998, loss 0.00270422, acc 1\n",
      "2018-04-16T00:31:17.318838: step 1999, loss 0.0013958, acc 1\n",
      "2018-04-16T00:31:17.361618: step 2000, loss 0.00255539, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:31:17.464927: step 2000, loss 0.816825, acc 0.762599, rec 0.851351, pre 0.71754, f1 0.778739\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523838561/checkpoints/model-2000\n",
      "\n",
      "\n",
      "Test Set:\n",
      "2018-04-16T00:31:17.575224: step 2000, loss 0.741119, acc 0.793651, rec 0.872093, pre 0.728155, f1 0.793651\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.2\n",
    "\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", 0.5, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 100, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"1\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 100, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.25, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 100\n",
    "tf.flags.DEFINE_integer(\"batch_size\", batch_size, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 500, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 20, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=train_s.shape[1],\n",
    "            num_classes=2,\n",
    "            vocab_size=2500,\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "        \n",
    "         # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        #vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch1, x_batch2, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch1, x_batch2, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            #r \n",
    "            step, summaries, loss, accuracy, recall, precision, f1 = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy,cnn.recall, cnn.precision, cnn.f1_score],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, rec {:g}, pre {:g}, f1 {:g}\".format(time_str, step, loss, accuracy, recall, precision, f1))# recall, precision, f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(train_s, train_c,  expanded_train_labels)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch1, x_batch2, y_batch = zip(*batch)\n",
    "            train_step(x_batch1, x_batch1, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(validation_s, validation_c, expanded_validation_labels, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "print(\"\\nTest Set:\")\n",
    "dev_step(test_s, test_c, expanded_test_labels, writer=dev_summary_writer)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
