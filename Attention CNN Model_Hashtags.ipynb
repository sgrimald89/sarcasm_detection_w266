{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saulgrimaldo1/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from w266_common import utils, vocabulary\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "np.random.seed(266)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "tokenizer = TweetTokenizer()\n",
    "x_data = []\n",
    "x_contexts = []\n",
    "labels = []\n",
    "sentences  = []\n",
    "contexts = []\n",
    "with open('merged_data_v4.csv', 'r') as csvfile:\n",
    "    linereader = csv.reader(csvfile, delimiter = '|')\n",
    "    for i, row in enumerate(linereader):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        sentence, context, sarcasm = row\n",
    "        sentence = re.sub(\"RT @[^\\s]+: \", \"retweet\", sentence)\n",
    "        sentences.append(sentence)\n",
    "        contexts.append(context)\n",
    "        x_tokens = utils.canonicalize_words(tokenizer.tokenize(sentence))\n",
    "        context_tokens = utils.canonicalize_words(tokenizer.tokenize(context))\n",
    "        x_data.append(x_tokens)\n",
    "        x_contexts.append(context_tokens)\n",
    "        labels.append(int(sarcasm))\n",
    "\n",
    "\n",
    "#rng = np.random.RandomState(5)\n",
    "#rng.shuffle(x_data)  # in-place\n",
    "#train_split_idx = int(0.7 * len(labels))\n",
    "#test_split_idx  = int(0.9 * len(labels))\n",
    "\n",
    "shuffle_indices = np.random.permutation(np.arange(len(labels)))\n",
    "train_split_idx = int(0.7 * len(labels))\n",
    "test_split_idx  = int(0.9 * len(labels))\n",
    "\n",
    "train_indices = shuffle_indices[:train_split_idx]\n",
    "validation_indices = shuffle_indices[train_split_idx:test_split_idx]\n",
    "test_indices = shuffle_indices[test_split_idx:]\n",
    "\n",
    "\n",
    "train_sentences = np.array(x_data)[train_indices]\n",
    "train_contexts = np.array(x_contexts)[train_indices]\n",
    "train_labels= np.array(labels)[train_indices] \n",
    "validation_sentences = np.array(x_data)[validation_indices]\n",
    "validation_labels = np.array(labels)[validation_indices]\n",
    "validation_contexts = train_contexts = np.array(x_contexts)[validation_indices]\n",
    "test_sentences = np.array(x_data)[test_indices]  \n",
    "test_contexts = train_contexts = np.array(x_contexts)[test_indices]\n",
    "test_labels = np.array(labels)[test_indices]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9447,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_labels(raw_label_set, size):\n",
    "    label_set = []\n",
    "    for label in raw_label_set:\n",
    "        labels = [0] * size\n",
    "        labels[label] = 1\n",
    "        label_set.append(labels)\n",
    "    return np.array(label_set)\n",
    "\n",
    "expanded_train_labels = transform_labels(train_labels, 2)\n",
    "expanded_validation_labels = transform_labels(validation_labels,2)\n",
    "expanded_test_labels = transform_labels(test_labels,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4, 5, 6, 7, 8, 8, 1, 5, 6, 7]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,3,4,5,6,7,8,8,1,5,6,7]\n",
    "a + [\"<PADDING>\"]*0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'angry',\n",
       " 'man',\n",
       " 'is',\n",
       " 'angry',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PaddingAndTruncating:\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def pad_or_truncate(self, sentence):\n",
    "        sen_len = len(sentence)\n",
    "        paddings = self.max_len - sen_len\n",
    "        if paddings >=0:\n",
    "            return list(sentence) + [\"<PADDING>\"] * paddings\n",
    "        return sentence[0:paddings]\n",
    "        \n",
    "PadAndTrunc = PaddingAndTruncating(10)\n",
    "        \n",
    "        \n",
    "PadAndTrunc.pad_or_truncate([\"the\",\"angry\",\"man\",\"is\",\"angry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  12,    5,    9, ...,    3,    3,    3],\n",
       "       [  12,    5,    9, ...,    3,    3,    3],\n",
       "       [  92,  632, 7612, ...,    3,    3,    3],\n",
       "       ..., \n",
       "       [  12,    5,    9, ...,    3,    3,    3],\n",
       "       [7150, 2713, 3957, ...,    3,    3,    3],\n",
       "       [  12,    5,    9, ...,    3,    3,    3]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.contrib import learn\n",
    "vocab_size = 8000\n",
    "#max_len = max([len(sent) for sent  in train_sentences])\n",
    "PadAndTrunc =PaddingAndTruncating(75)\n",
    "train_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, train_sentences))\n",
    "train_context_padded = list(map(PadAndTrunc.pad_or_truncate, train_contexts))\n",
    "validation_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, validation_sentences))\n",
    "validation_context_padded = list(map(PadAndTrunc.pad_or_truncate, validation_contexts))\n",
    "test_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, test_sentences))\n",
    "test_context_padded = list(map(PadAndTrunc.pad_or_truncate, test_contexts))\n",
    "\n",
    "\n",
    "vocab = vocabulary.Vocabulary(utils.flatten(list(train_sentences_padded) + list(train_context_padded)), vocab_size)\n",
    "train_s = np.array(list(map(vocab.words_to_ids, train_sentences_padded)))\n",
    "train_c = np.array(list(map(vocab.words_to_ids, train_context_padded)))\n",
    "validation_s = np.array(list(map(vocab.words_to_ids, validation_sentences_padded)))\n",
    "validation_c = np.array(list(map(vocab.words_to_ids, validation_context_padded)))\n",
    "test_s = np.array(list(map(vocab.words_to_ids, test_sentences_padded)))\n",
    "test_c = np.array(list(map(vocab.words_to_ids, test_context_padded)))\n",
    "train_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv-maxpool-3'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 3\n",
    "(\"conv-maxpool-%s\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size, \n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x1 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x1\")\n",
    "        self.input_x2 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x2\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars1 = tf.nn.embedding_lookup(self.W, self.input_x1)\n",
    "            self.embedded_chars_expanded1 = tf.expand_dims(self.embedded_chars1, -1)\n",
    "            self.embedded_chars2 = tf.nn.embedding_lookup(self.W, self.input_x2)\n",
    "            self.embedded_chars_expanded2 = tf.expand_dims(self.embedded_chars2, -1)\n",
    "\n",
    "        # Create a convolution + avgpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.variable_scope(\"conv-avgpool-%s\" % i) as scope:\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "              \n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded1,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h1 = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"tanh1\")\n",
    "               \n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.avg_pool(\n",
    "                    h1,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                \n",
    "                pooled_outputs.append(pooled)\n",
    "                #pooled_outputs.append(pooled)\n",
    "                scope.reuse_variables()\n",
    "                 \n",
    "                \n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded2,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h2 = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"tanh2\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled2 = tf.nn.avg_pool(\n",
    "                    h2,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled2)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "\n",
    "        num_filters_total = num_filters * len(filter_sizes) * 2\n",
    "\n",
    "        self.h_pool = tf.concat(pooled_outputs, 1)\n",
    "\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "        \n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "           \n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.labels = tf.argmax(self.input_y, 1)\n",
    "            TP = tf.count_nonzero(self.predictions * self.labels)\n",
    "            TN = tf.count_nonzero((self.predictions - 1) * (self.labels - 1))\n",
    "            FP = tf.count_nonzero(self.predictions * (self.labels - 1))\n",
    "            FN = tf.count_nonzero((self.predictions - 1) * self.labels)\n",
    "            correct_predictions = tf.equal(self.predictions, self.labels)\n",
    "            self.precision = TP / (TP + FP)\n",
    "            self.recall = TP / (TP + FN)\n",
    "            self.f1_score = 2 * self.precision * self.recall / (self.precision + self.recall)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=250\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.75\n",
      "DROPOUT_KEEP_PROB=0.4\n",
      "EMBEDDING_DIM=50\n",
      "EVALUATE_EVERY=20\n",
      "FILTER_SIZES=1\n",
      "L2_REG_LAMBDA=0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.neg\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=50\n",
      "NUM_FILTERS=50\n",
      "POSITIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.pos\n",
      "\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/W:0/grad/hist is illegal; using conv-avgpool-0/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/W:0/grad/sparsity is illegal; using conv-avgpool-0/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/b:0/grad/hist is illegal; using conv-avgpool-0/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/b:0/grad/sparsity is illegal; using conv-avgpool-0/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524366155\n",
      "\n",
      "2018-04-22T03:02:36.420553: step 1, loss 0.725467, acc 0.524\n",
      "2018-04-22T03:02:36.684538: step 2, loss 0.966389, acc 0.428\n",
      "2018-04-22T03:02:36.936542: step 3, loss 1.24438, acc 0.496\n",
      "2018-04-22T03:02:37.200518: step 4, loss 0.7403, acc 0.512\n",
      "2018-04-22T03:02:37.454600: step 5, loss 0.70805, acc 0.528\n",
      "2018-04-22T03:02:37.564537: step 6, loss 0.697419, acc 0.54\n",
      "2018-04-22T03:02:37.816527: step 7, loss 0.712625, acc 0.536\n",
      "2018-04-22T03:02:38.077922: step 8, loss 0.70276, acc 0.54\n",
      "2018-04-22T03:02:38.336517: step 9, loss 0.699791, acc 0.476\n",
      "2018-04-22T03:02:38.596524: step 10, loss 0.686516, acc 0.548\n",
      "2018-04-22T03:02:38.852512: step 11, loss 0.686115, acc 0.56\n",
      "2018-04-22T03:02:38.956499: step 12, loss 0.70485, acc 0.47\n",
      "2018-04-22T03:02:39.197830: step 13, loss 0.718756, acc 0.5\n",
      "2018-04-22T03:02:39.452611: step 14, loss 0.702087, acc 0.476\n",
      "2018-04-22T03:02:39.708471: step 15, loss 0.690021, acc 0.548\n",
      "2018-04-22T03:02:39.968687: step 16, loss 0.689622, acc 0.548\n",
      "2018-04-22T03:02:40.215374: step 17, loss 0.688563, acc 0.548\n",
      "2018-04-22T03:02:40.326522: step 18, loss 0.678055, acc 0.63\n",
      "2018-04-22T03:02:40.572699: step 19, loss 0.692352, acc 0.5\n",
      "2018-04-22T03:02:40.836021: step 20, loss 0.680703, acc 0.572\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T03:02:41.709682: step 20, loss 0.690112, acc 0.505926, rec 0.995601, pre 0.505585, f1 0.670617\n",
      "\n",
      "2018-04-22T03:02:41.964525: step 21, loss 0.671096, acc 0.58\n",
      "2018-04-22T03:02:42.216518: step 22, loss 0.677382, acc 0.592\n",
      "2018-04-22T03:02:42.476515: step 23, loss 0.677417, acc 0.592\n",
      "2018-04-22T03:02:42.586365: step 24, loss 0.680836, acc 0.57\n",
      "2018-04-22T03:02:42.819247: step 25, loss 0.682475, acc 0.56\n",
      "2018-04-22T03:02:43.076577: step 26, loss 0.68548, acc 0.572\n",
      "2018-04-22T03:02:43.327705: step 27, loss 0.675307, acc 0.604\n",
      "2018-04-22T03:02:43.586280: step 28, loss 0.673729, acc 0.624\n",
      "2018-04-22T03:02:43.814938: step 29, loss 0.683106, acc 0.552\n",
      "2018-04-22T03:02:43.920931: step 30, loss 0.673337, acc 0.6\n",
      "2018-04-22T03:02:44.176537: step 31, loss 0.672979, acc 0.576\n",
      "2018-04-22T03:02:44.430847: step 32, loss 0.700612, acc 0.472\n",
      "2018-04-22T03:02:44.690989: step 33, loss 0.678665, acc 0.556\n",
      "2018-04-22T03:02:44.952503: step 34, loss 0.677309, acc 0.6\n",
      "2018-04-22T03:02:45.200519: step 35, loss 0.664436, acc 0.628\n",
      "2018-04-22T03:02:45.308492: step 36, loss 0.66366, acc 0.61\n",
      "2018-04-22T03:02:45.536537: step 37, loss 0.666633, acc 0.596\n",
      "2018-04-22T03:02:45.788512: step 38, loss 0.678178, acc 0.564\n",
      "2018-04-22T03:02:46.040679: step 39, loss 0.6721, acc 0.568\n",
      "2018-04-22T03:02:46.294246: step 40, loss 0.672962, acc 0.576\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T03:02:47.148522: step 40, loss 0.682941, acc 0.501481, rec 0.0139296, pre 0.95, f1 0.0274566\n",
      "\n",
      "2018-04-22T03:02:47.395217: step 41, loss 0.672225, acc 0.592\n",
      "2018-04-22T03:02:47.505303: step 42, loss 0.667603, acc 0.55\n",
      "2018-04-22T03:02:47.747157: step 43, loss 0.667738, acc 0.6\n",
      "2018-04-22T03:02:47.976559: step 44, loss 0.67945, acc 0.556\n",
      "2018-04-22T03:02:48.230740: step 45, loss 0.689222, acc 0.536\n",
      "2018-04-22T03:02:48.492515: step 46, loss 0.700535, acc 0.5\n",
      "2018-04-22T03:02:48.744759: step 47, loss 0.675473, acc 0.532\n",
      "2018-04-22T03:02:48.845675: step 48, loss 0.705561, acc 0.48\n",
      "2018-04-22T03:02:49.093784: step 49, loss 0.690036, acc 0.524\n",
      "2018-04-22T03:02:49.343912: step 50, loss 0.716989, acc 0.476\n",
      "2018-04-22T03:02:49.598844: step 51, loss 0.67492, acc 0.54\n",
      "2018-04-22T03:02:49.860506: step 52, loss 0.665942, acc 0.616\n",
      "2018-04-22T03:02:50.113218: step 53, loss 0.66667, acc 0.62\n",
      "2018-04-22T03:02:50.228539: step 54, loss 0.679274, acc 0.58\n",
      "2018-04-22T03:02:50.472529: step 55, loss 0.661632, acc 0.58\n",
      "2018-04-22T03:02:50.732727: step 56, loss 0.664518, acc 0.64\n",
      "2018-04-22T03:02:50.988505: step 57, loss 0.666183, acc 0.616\n",
      "2018-04-22T03:02:51.248520: step 58, loss 0.650217, acc 0.688\n",
      "2018-04-22T03:02:51.508739: step 59, loss 0.662946, acc 0.64\n",
      "2018-04-22T03:02:51.620539: step 60, loss 0.651723, acc 0.69\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T03:02:52.470966: step 60, loss 0.725279, acc 0.505556, rec 0.999267, pre 0.505376, f1 0.671263\n",
      "\n",
      "2018-04-22T03:02:52.715225: step 61, loss 0.70353, acc 0.5\n",
      "2018-04-22T03:02:52.976551: step 62, loss 0.708754, acc 0.532\n",
      "2018-04-22T03:02:53.228537: step 63, loss 0.647689, acc 0.6\n",
      "2018-04-22T03:02:53.484543: step 64, loss 0.663299, acc 0.596\n",
      "2018-04-22T03:02:53.741870: step 65, loss 0.705385, acc 0.504\n",
      "2018-04-22T03:02:53.852516: step 66, loss 0.654301, acc 0.59\n",
      "2018-04-22T03:02:54.096711: step 67, loss 0.64126, acc 0.652\n",
      "2018-04-22T03:02:54.349865: step 68, loss 0.663215, acc 0.588\n",
      "2018-04-22T03:02:54.580529: step 69, loss 0.680697, acc 0.576\n",
      "2018-04-22T03:02:54.836573: step 70, loss 0.685092, acc 0.512\n",
      "2018-04-22T03:02:55.096545: step 71, loss 0.671315, acc 0.548\n",
      "2018-04-22T03:02:55.208505: step 72, loss 0.645731, acc 0.68\n",
      "2018-04-22T03:02:55.450753: step 73, loss 0.656639, acc 0.64\n",
      "2018-04-22T03:02:55.703169: step 74, loss 0.682984, acc 0.524\n",
      "2018-04-22T03:02:55.959215: step 75, loss 0.684763, acc 0.524\n",
      "2018-04-22T03:02:56.204504: step 76, loss 0.656227, acc 0.584\n",
      "2018-04-22T03:02:56.457021: step 77, loss 0.657564, acc 0.6\n",
      "2018-04-22T03:02:56.562046: step 78, loss 0.671473, acc 0.57\n",
      "2018-04-22T03:02:56.808670: step 79, loss 0.694043, acc 0.504\n",
      "2018-04-22T03:02:57.063087: step 80, loss 0.671278, acc 0.56\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T03:02:57.915961: step 80, loss 0.704212, acc 0.506667, rec 0.994135, pre 0.50597, f1 0.670623\n",
      "\n",
      "2018-04-22T03:02:58.173906: step 81, loss 0.656461, acc 0.6\n",
      "2018-04-22T03:02:58.432532: step 82, loss 0.64721, acc 0.704\n",
      "2018-04-22T03:02:58.661951: step 83, loss 0.627795, acc 0.724\n",
      "2018-04-22T03:02:58.769916: step 84, loss 0.678475, acc 0.59\n",
      "2018-04-22T03:02:59.012793: step 85, loss 0.642139, acc 0.672\n",
      "2018-04-22T03:02:59.264486: step 86, loss 0.638651, acc 0.656\n",
      "2018-04-22T03:02:59.511806: step 87, loss 0.663308, acc 0.636\n",
      "2018-04-22T03:02:59.764494: step 88, loss 0.643443, acc 0.644\n",
      "2018-04-22T03:03:00.020520: step 89, loss 0.658611, acc 0.552\n",
      "2018-04-22T03:03:00.127538: step 90, loss 0.652109, acc 0.56\n",
      "2018-04-22T03:03:00.372592: step 91, loss 0.638684, acc 0.72\n",
      "2018-04-22T03:03:00.636459: step 92, loss 0.670539, acc 0.568\n",
      "2018-04-22T03:03:00.888469: step 93, loss 0.705037, acc 0.516\n",
      "2018-04-22T03:03:01.139486: step 94, loss 0.723415, acc 0.508\n",
      "2018-04-22T03:03:01.400530: step 95, loss 0.730127, acc 0.504\n",
      "2018-04-22T03:03:01.508695: step 96, loss 0.811094, acc 0.44\n",
      "2018-04-22T03:03:01.746952: step 97, loss 0.76439, acc 0.528\n",
      "2018-04-22T03:03:01.984467: step 98, loss 0.663829, acc 0.584\n",
      "2018-04-22T03:03:02.225028: step 99, loss 0.657524, acc 0.648\n",
      "2018-04-22T03:03:02.469564: step 100, loss 0.635433, acc 0.716\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T03:03:03.309551: step 100, loss 0.673995, acc 0.495185, rec 0.000733138, pre 1, f1 0.0014652\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524366155/checkpoints/model-100\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-22T03:03:03.657140: step 101, loss 0.638381, acc 0.684\n",
      "2018-04-22T03:03:03.772531: step 102, loss 0.676527, acc 0.55\n",
      "2018-04-22T03:03:04.013220: step 103, loss 0.653903, acc 0.668\n",
      "2018-04-22T03:03:04.269022: step 104, loss 0.663022, acc 0.588\n",
      "2018-04-22T03:03:04.522958: step 105, loss 0.656028, acc 0.632\n",
      "2018-04-22T03:03:04.784569: step 106, loss 0.661811, acc 0.572\n",
      "2018-04-22T03:03:05.032864: step 107, loss 0.653815, acc 0.584\n",
      "2018-04-22T03:03:05.148687: step 108, loss 0.675319, acc 0.55\n",
      "2018-04-22T03:03:05.392716: step 109, loss 0.712866, acc 0.484\n",
      "2018-04-22T03:03:05.649520: step 110, loss 0.660156, acc 0.548\n",
      "2018-04-22T03:03:05.881830: step 111, loss 0.630438, acc 0.704\n",
      "2018-04-22T03:03:06.128532: step 112, loss 0.642105, acc 0.728\n",
      "2018-04-22T03:03:06.384521: step 113, loss 0.628976, acc 0.716\n",
      "2018-04-22T03:03:06.488541: step 114, loss 0.657736, acc 0.68\n",
      "2018-04-22T03:03:06.740677: step 115, loss 0.655148, acc 0.568\n",
      "2018-04-22T03:03:06.992704: step 116, loss 0.668821, acc 0.596\n",
      "2018-04-22T03:03:07.238750: step 117, loss 0.682061, acc 0.54\n",
      "2018-04-22T03:03:07.474732: step 118, loss 0.650536, acc 0.568\n",
      "2018-04-22T03:03:07.715141: step 119, loss 0.634971, acc 0.596\n",
      "2018-04-22T03:03:07.828459: step 120, loss 0.70009, acc 0.55\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T03:03:08.652482: step 120, loss 0.699485, acc 0.509259, rec 0.990469, pre 0.507323, f1 0.670971\n",
      "\n",
      "2018-04-22T03:03:08.884481: step 121, loss 0.646169, acc 0.568\n",
      "2018-04-22T03:03:09.128472: step 122, loss 0.639383, acc 0.664\n",
      "2018-04-22T03:03:09.371388: step 123, loss 0.633995, acc 0.684\n",
      "2018-04-22T03:03:09.614686: step 124, loss 0.670064, acc 0.552\n",
      "2018-04-22T03:03:09.856583: step 125, loss 0.669813, acc 0.52\n",
      "2018-04-22T03:03:09.964461: step 126, loss 0.667891, acc 0.56\n",
      "2018-04-22T03:03:10.204562: step 127, loss 0.653284, acc 0.6\n",
      "2018-04-22T03:03:10.447331: step 128, loss 0.658149, acc 0.58\n",
      "2018-04-22T03:03:10.691527: step 129, loss 0.635744, acc 0.66\n",
      "2018-04-22T03:03:10.948478: step 130, loss 0.599257, acc 0.752\n",
      "2018-04-22T03:03:11.242554: step 131, loss 0.637756, acc 0.676\n",
      "2018-04-22T03:03:11.400522: step 132, loss 0.604486, acc 0.66\n",
      "2018-04-22T03:03:11.711332: step 133, loss 0.636226, acc 0.628\n",
      "2018-04-22T03:03:11.988460: step 134, loss 0.622441, acc 0.636\n",
      "2018-04-22T03:03:12.306139: step 135, loss 0.614827, acc 0.704\n",
      "2018-04-22T03:03:12.588512: step 136, loss 0.62358, acc 0.72\n",
      "2018-04-22T03:03:12.843149: step 137, loss 0.619336, acc 0.696\n",
      "2018-04-22T03:03:12.949458: step 138, loss 0.646988, acc 0.63\n",
      "2018-04-22T03:03:13.191250: step 139, loss 0.653426, acc 0.572\n",
      "2018-04-22T03:03:13.449750: step 140, loss 0.62209, acc 0.632\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T03:03:14.301701: step 140, loss 0.687937, acc 0.511852, rec 0.98827, pre 0.508679, f1 0.671649\n",
      "\n",
      "2018-04-22T03:03:14.557823: step 141, loss 0.640589, acc 0.612\n",
      "2018-04-22T03:03:14.808050: step 142, loss 0.614681, acc 0.66\n",
      "2018-04-22T03:03:15.060480: step 143, loss 0.617165, acc 0.704\n",
      "2018-04-22T03:03:15.168527: step 144, loss 0.630762, acc 0.68\n",
      "2018-04-22T03:03:15.412527: step 145, loss 0.728523, acc 0.496\n",
      "2018-04-22T03:03:15.664547: step 146, loss 0.817624, acc 0.496\n",
      "2018-04-22T03:03:15.920520: step 147, loss 0.761427, acc 0.544\n",
      "2018-04-22T03:03:16.176517: step 148, loss 0.674993, acc 0.524\n",
      "2018-04-22T03:03:16.421784: step 149, loss 0.64125, acc 0.572\n",
      "2018-04-22T03:03:16.529614: step 150, loss 0.672779, acc 0.58\n",
      "2018-04-22T03:03:16.772535: step 151, loss 0.693255, acc 0.504\n",
      "2018-04-22T03:03:17.020817: step 152, loss 0.651917, acc 0.556\n",
      "2018-04-22T03:03:17.272517: step 153, loss 0.6323, acc 0.62\n",
      "2018-04-22T03:03:17.524508: step 154, loss 0.620223, acc 0.732\n",
      "2018-04-22T03:03:17.773969: step 155, loss 0.613115, acc 0.708\n",
      "2018-04-22T03:03:17.880496: step 156, loss 0.630323, acc 0.75\n",
      "2018-04-22T03:03:18.112652: step 157, loss 0.646632, acc 0.58\n",
      "2018-04-22T03:03:18.360500: step 158, loss 0.61338, acc 0.688\n",
      "2018-04-22T03:03:18.614237: step 159, loss 0.667648, acc 0.556\n",
      "2018-04-22T03:03:18.868507: step 160, loss 0.712692, acc 0.52\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T03:03:19.316981: step 160, loss 0.708978, acc 0.508889, rec 0.990469, pre 0.507132, f1 0.670804\n",
      "\n",
      "2018-04-22T03:03:19.436802: step 161, loss 0.639287, acc 0.572\n",
      "2018-04-22T03:03:19.490094: step 162, loss 0.651731, acc 0.61\n",
      "2018-04-22T03:03:19.607755: step 163, loss 0.655821, acc 0.576\n",
      "2018-04-22T03:03:19.725622: step 164, loss 0.627581, acc 0.68\n",
      "2018-04-22T03:03:19.849412: step 165, loss 0.61098, acc 0.736\n",
      "2018-04-22T03:03:19.968275: step 166, loss 0.59788, acc 0.736\n",
      "2018-04-22T03:03:20.091497: step 167, loss 0.600955, acc 0.724\n",
      "2018-04-22T03:03:20.145063: step 168, loss 0.633944, acc 0.72\n",
      "2018-04-22T03:03:20.264655: step 169, loss 0.620771, acc 0.644\n",
      "2018-04-22T03:03:20.394040: step 170, loss 0.678159, acc 0.552\n",
      "2018-04-22T03:03:20.519252: step 171, loss 0.649562, acc 0.576\n",
      "2018-04-22T03:03:20.644880: step 172, loss 0.692848, acc 0.516\n",
      "2018-04-22T03:03:20.768398: step 173, loss 0.657411, acc 0.572\n",
      "2018-04-22T03:03:20.822482: step 174, loss 0.594941, acc 0.69\n",
      "2018-04-22T03:03:20.946640: step 175, loss 0.645447, acc 0.544\n",
      "2018-04-22T03:03:21.067737: step 176, loss 0.685171, acc 0.536\n",
      "2018-04-22T03:03:21.192595: step 177, loss 0.658773, acc 0.532\n",
      "2018-04-22T03:03:21.311037: step 178, loss 0.677754, acc 0.532\n",
      "2018-04-22T03:03:21.434998: step 179, loss 0.635371, acc 0.58\n",
      "2018-04-22T03:03:21.489055: step 180, loss 0.613116, acc 0.74\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T03:03:21.908599: step 180, loss 0.682162, acc 0.513333, rec 0.987537, pre 0.509455, f1 0.672156\n",
      "\n",
      "2018-04-22T03:03:22.022378: step 181, loss 0.634251, acc 0.6\n",
      "2018-04-22T03:03:22.147038: step 182, loss 0.622298, acc 0.728\n",
      "2018-04-22T03:03:22.266483: step 183, loss 0.607072, acc 0.696\n",
      "2018-04-22T03:03:22.390500: step 184, loss 0.609684, acc 0.676\n",
      "2018-04-22T03:03:22.511546: step 185, loss 0.609358, acc 0.652\n",
      "2018-04-22T03:03:22.564017: step 186, loss 0.60116, acc 0.76\n",
      "2018-04-22T03:03:22.681622: step 187, loss 0.614652, acc 0.728\n",
      "2018-04-22T03:03:22.801066: step 188, loss 0.594976, acc 0.764\n",
      "2018-04-22T03:03:22.924759: step 189, loss 0.612032, acc 0.744\n",
      "2018-04-22T03:03:23.044507: step 190, loss 0.583745, acc 0.764\n",
      "2018-04-22T03:03:23.169782: step 191, loss 0.617145, acc 0.696\n",
      "2018-04-22T03:03:23.223425: step 192, loss 0.718266, acc 0.52\n",
      "2018-04-22T03:03:23.337347: step 193, loss 0.759546, acc 0.548\n",
      "2018-04-22T03:03:23.462354: step 194, loss 0.644663, acc 0.568\n",
      "2018-04-22T03:03:23.582625: step 195, loss 0.65583, acc 0.572\n",
      "2018-04-22T03:03:23.709330: step 196, loss 0.727988, acc 0.484\n",
      "2018-04-22T03:03:23.825335: step 197, loss 0.693086, acc 0.516\n",
      "2018-04-22T03:03:23.875323: step 198, loss 0.648956, acc 0.6\n",
      "2018-04-22T03:03:23.990221: step 199, loss 0.623546, acc 0.712\n",
      "2018-04-22T03:03:24.104858: step 200, loss 0.602953, acc 0.764\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T03:03:24.507083: step 200, loss 0.645235, acc 0.679259, rec 0.923754, pre 0.623145, f1 0.744241\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524366155/checkpoints/model-200\n",
      "\n",
      "2018-04-22T03:03:24.665059: step 201, loss 0.601014, acc 0.772\n",
      "2018-04-22T03:03:24.786412: step 202, loss 0.609758, acc 0.72\n",
      "2018-04-22T03:03:24.907030: step 203, loss 0.633292, acc 0.604\n",
      "2018-04-22T03:03:24.958470: step 204, loss 0.575786, acc 0.7\n",
      "2018-04-22T03:03:25.079984: step 205, loss 0.58316, acc 0.792\n",
      "2018-04-22T03:03:25.199859: step 206, loss 0.603124, acc 0.692\n",
      "2018-04-22T03:03:25.322873: step 207, loss 0.587181, acc 0.728\n",
      "2018-04-22T03:03:25.442767: step 208, loss 0.590064, acc 0.716\n",
      "2018-04-22T03:03:25.568277: step 209, loss 0.6488, acc 0.612\n",
      "2018-04-22T03:03:25.621365: step 210, loss 0.727278, acc 0.51\n",
      "2018-04-22T03:03:25.738831: step 211, loss 0.654154, acc 0.552\n",
      "2018-04-22T03:03:25.863491: step 212, loss 0.624335, acc 0.6\n",
      "2018-04-22T03:03:25.983411: step 213, loss 0.579512, acc 0.736\n",
      "2018-04-22T03:03:26.109480: step 214, loss 0.59317, acc 0.72\n",
      "2018-04-22T03:03:26.228491: step 215, loss 0.613902, acc 0.772\n",
      "2018-04-22T03:03:26.282447: step 216, loss 0.570153, acc 0.67\n",
      "2018-04-22T03:03:26.400848: step 217, loss 0.610908, acc 0.72\n",
      "2018-04-22T03:03:26.523039: step 218, loss 0.655455, acc 0.576\n",
      "2018-04-22T03:03:26.645983: step 219, loss 0.695365, acc 0.544\n",
      "2018-04-22T03:03:26.769050: step 220, loss 0.646787, acc 0.584\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-22T03:03:27.185347: step 220, loss 0.665624, acc 0.513333, rec 0.0366569, pre 1, f1 0.0707214\n",
      "\n",
      "2018-04-22T03:03:27.302247: step 221, loss 0.56714, acc 0.664\n",
      "2018-04-22T03:03:27.354058: step 222, loss 0.604471, acc 0.64\n",
      "2018-04-22T03:03:27.473449: step 223, loss 0.572194, acc 0.828\n",
      "2018-04-22T03:03:27.593147: step 224, loss 0.588202, acc 0.72\n",
      "2018-04-22T03:03:27.725868: step 225, loss 0.57412, acc 0.668\n",
      "2018-04-22T03:03:27.846495: step 226, loss 0.596859, acc 0.76\n",
      "2018-04-22T03:03:27.970675: step 227, loss 0.610565, acc 0.656\n",
      "2018-04-22T03:03:28.023793: step 228, loss 0.557085, acc 0.82\n",
      "2018-04-22T03:03:28.138659: step 229, loss 0.58524, acc 0.808\n",
      "2018-04-22T03:03:28.262335: step 230, loss 0.554629, acc 0.8\n",
      "2018-04-22T03:03:28.382610: step 231, loss 0.576428, acc 0.756\n",
      "2018-04-22T03:03:28.508632: step 232, loss 0.593567, acc 0.748\n",
      "2018-04-22T03:03:28.627033: step 233, loss 0.557799, acc 0.816\n",
      "2018-04-22T03:03:28.679283: step 234, loss 0.598425, acc 0.68\n",
      "2018-04-22T03:03:28.795032: step 235, loss 0.607791, acc 0.64\n",
      "2018-04-22T03:03:28.910252: step 236, loss 0.694684, acc 0.536\n",
      "2018-04-22T03:03:29.028942: step 237, loss 0.844628, acc 0.524\n",
      "2018-04-22T03:03:29.143318: step 238, loss 0.840944, acc 0.516\n",
      "2018-04-22T03:03:29.264027: step 239, loss 0.727085, acc 0.496\n",
      "2018-04-22T03:03:29.315404: step 240, loss 0.706762, acc 0.5\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T03:03:29.720668: step 240, loss 0.676555, acc 0.501852, rec 0.0139296, pre 1, f1 0.0274765\n",
      "\n",
      "2018-04-22T03:03:29.834639: step 241, loss 0.64658, acc 0.604\n",
      "2018-04-22T03:03:29.953126: step 242, loss 0.652151, acc 0.568\n",
      "2018-04-22T03:03:30.068954: step 243, loss 0.610022, acc 0.656\n",
      "2018-04-22T03:03:30.187400: step 244, loss 0.591625, acc 0.688\n",
      "2018-04-22T03:03:30.308186: step 245, loss 0.557803, acc 0.76\n",
      "2018-04-22T03:03:30.357925: step 246, loss 0.600862, acc 0.68\n",
      "2018-04-22T03:03:30.472103: step 247, loss 0.608543, acc 0.756\n",
      "2018-04-22T03:03:30.601207: step 248, loss 0.579409, acc 0.728\n",
      "2018-04-22T03:03:30.722464: step 249, loss 0.570142, acc 0.816\n",
      "2018-04-22T03:03:30.838894: step 250, loss 0.576399, acc 0.76\n",
      "2018-04-22T03:03:30.958575: step 251, loss 0.573529, acc 0.784\n",
      "2018-04-22T03:03:31.008887: step 252, loss 0.594042, acc 0.69\n",
      "2018-04-22T03:03:31.119876: step 253, loss 0.577496, acc 0.784\n",
      "2018-04-22T03:03:31.237616: step 254, loss 0.574106, acc 0.768\n",
      "2018-04-22T03:03:31.353834: step 255, loss 0.559234, acc 0.78\n",
      "2018-04-22T03:03:31.472272: step 256, loss 0.573251, acc 0.804\n",
      "2018-04-22T03:03:31.590167: step 257, loss 0.578966, acc 0.792\n",
      "2018-04-22T03:03:31.641018: step 258, loss 0.592546, acc 0.7\n",
      "2018-04-22T03:03:31.756055: step 259, loss 0.614111, acc 0.64\n",
      "2018-04-22T03:03:31.875397: step 260, loss 0.578868, acc 0.612\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T03:03:32.279630: step 260, loss 0.672492, acc 0.507407, rec 0.0249267, pre 1, f1 0.0486409\n",
      "\n",
      "2018-04-22T03:03:32.395243: step 261, loss 0.589859, acc 0.672\n",
      "2018-04-22T03:03:32.504566: step 262, loss 0.568936, acc 0.688\n",
      "2018-04-22T03:03:32.622493: step 263, loss 0.571958, acc 0.824\n",
      "2018-04-22T03:03:32.673111: step 264, loss 0.559831, acc 0.77\n",
      "2018-04-22T03:03:32.789470: step 265, loss 0.570148, acc 0.776\n",
      "2018-04-22T03:03:32.906509: step 266, loss 0.560071, acc 0.656\n",
      "2018-04-22T03:03:33.028287: step 267, loss 0.535592, acc 0.812\n",
      "2018-04-22T03:03:33.145511: step 268, loss 0.574349, acc 0.704\n",
      "2018-04-22T03:03:33.267031: step 269, loss 0.732169, acc 0.536\n",
      "2018-04-22T03:03:33.318868: step 270, loss 0.905111, acc 0.51\n",
      "2018-04-22T03:03:33.430016: step 271, loss 0.897874, acc 0.48\n",
      "2018-04-22T03:03:33.549467: step 272, loss 0.710925, acc 0.56\n",
      "2018-04-22T03:03:33.667460: step 273, loss 0.600127, acc 0.688\n",
      "2018-04-22T03:03:33.786307: step 274, loss 0.616008, acc 0.608\n",
      "2018-04-22T03:03:33.903451: step 275, loss 0.603869, acc 0.62\n",
      "2018-04-22T03:03:33.954423: step 276, loss 0.571362, acc 0.73\n",
      "2018-04-22T03:03:34.070677: step 277, loss 0.603629, acc 0.724\n",
      "2018-04-22T03:03:34.187035: step 278, loss 0.536362, acc 0.748\n",
      "2018-04-22T03:03:34.305262: step 279, loss 0.586867, acc 0.704\n",
      "2018-04-22T03:03:34.423594: step 280, loss 0.556234, acc 0.752\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T03:03:34.830012: step 280, loss 0.63475, acc 0.57963, rec 0.969941, pre 0.547373, f1 0.699815\n",
      "\n",
      "2018-04-22T03:03:34.942545: step 281, loss 0.58303, acc 0.7\n",
      "2018-04-22T03:03:34.994654: step 282, loss 0.616087, acc 0.81\n",
      "2018-04-22T03:03:35.109169: step 283, loss 0.566564, acc 0.708\n",
      "2018-04-22T03:03:35.226202: step 284, loss 0.584153, acc 0.816\n",
      "2018-04-22T03:03:35.345303: step 285, loss 0.574817, acc 0.66\n",
      "2018-04-22T03:03:35.462479: step 286, loss 0.534002, acc 0.808\n",
      "2018-04-22T03:03:35.583250: step 287, loss 0.583464, acc 0.76\n",
      "2018-04-22T03:03:35.637519: step 288, loss 0.549062, acc 0.69\n",
      "2018-04-22T03:03:35.750369: step 289, loss 0.634325, acc 0.604\n",
      "2018-04-22T03:03:35.870748: step 290, loss 0.631837, acc 0.62\n",
      "2018-04-22T03:03:35.987445: step 291, loss 0.546022, acc 0.7\n",
      "2018-04-22T03:03:36.105715: step 292, loss 0.547636, acc 0.748\n",
      "2018-04-22T03:03:36.221013: step 293, loss 0.579745, acc 0.728\n",
      "2018-04-22T03:03:36.274697: step 294, loss 0.58007, acc 0.8\n",
      "2018-04-22T03:03:36.390598: step 295, loss 0.665087, acc 0.568\n",
      "2018-04-22T03:03:36.507690: step 296, loss 0.717628, acc 0.516\n",
      "2018-04-22T03:03:36.630439: step 297, loss 0.773502, acc 0.496\n",
      "2018-04-22T03:03:36.749329: step 298, loss 0.811468, acc 0.512\n",
      "2018-04-22T03:03:36.870603: step 299, loss 0.689499, acc 0.552\n",
      "2018-04-22T03:03:36.922637: step 300, loss 0.592653, acc 0.67\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T03:03:37.324756: step 300, loss 0.672781, acc 0.520741, rec 0.984604, pre 0.513379, f1 0.674874\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524366155/checkpoints/model-300\n",
      "\n",
      "\n",
      "Test Set:\n",
      "2018-04-22T03:03:37.564095: step 300, loss 0.668009, acc 0.525926, rec 0.989855, pre 0.518997, f1 0.680957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", 0.75, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "\n",
    "# embedding of 60 is best so far\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\",60, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"1\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 60, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.4, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 250\n",
    "tf.flags.DEFINE_integer(\"batch_size\", batch_size, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 50, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 20, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=train_s.shape[1],\n",
    "            num_classes=2,\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "        \n",
    "         # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        #vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch1, x_batch2, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch1, x_batch2, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            #r \n",
    "            step, summaries, loss, accuracy, recall, precision, f1 = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy,cnn.recall, cnn.precision, cnn.f1_score],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, rec {:g}, pre {:g}, f1 {:g}\".format(time_str, step, loss, accuracy, recall, precision, f1))# recall, precision, f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(train_s, train_c,  expanded_train_labels)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch1, x_batch2, y_batch = zip(*batch)\n",
    "            train_step(x_batch1, x_batch1, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(validation_s, validation_c, expanded_validation_labels, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "print(\"\\nTest Set:\")\n",
    "dev_step(test_s, test_c, expanded_test_labels, writer=dev_summary_writer)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
