{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saulgrimaldo1/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from w266_common import utils, vocabulary\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "np.random.seed(266)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "tokenizer = TweetTokenizer()\n",
    "x_data = []\n",
    "x_contexts = []\n",
    "labels = []\n",
    "sentences  = []\n",
    "contexts = []\n",
    "with open('merged_data_v4.csv', 'r') as csvfile:\n",
    "    linereader = csv.reader(csvfile, delimiter = '|')\n",
    "    for i, row in enumerate(linereader):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        sentence, context, sarcasm = row\n",
    "        sentence = re.sub(\"RT @[^\\s]+:\", \"retweet\", sentence)\n",
    "        sentences.append(sentence)\n",
    "        contexts.append(context)\n",
    "        x_tokens = utils.canonicalize_words(tokenizer.tokenize(sentence), hashtags = False)\n",
    "        context_tokens = utils.canonicalize_words(tokenizer.tokenize(context))\n",
    "        x_data.append(x_tokens)\n",
    "        x_contexts.append(context_tokens)\n",
    "        labels.append(int(sarcasm))\n",
    "\n",
    "\n",
    "#rng = np.random.RandomState(5)\n",
    "#rng.shuffle(x_data)  # in-place\n",
    "#train_split_idx = int(0.7 * len(labels))\n",
    "#test_split_idx  = int(0.9 * len(labels))\n",
    "\n",
    "shuffle_indices = np.random.permutation(np.arange(len(labels)))\n",
    "train_split_idx = int(0.7 * len(labels))\n",
    "test_split_idx  = int(0.9 * len(labels))\n",
    "\n",
    "train_indices = shuffle_indices[:train_split_idx]\n",
    "validation_indices = shuffle_indices[train_split_idx:test_split_idx]\n",
    "test_indices = shuffle_indices[test_split_idx:]\n",
    "\n",
    "\n",
    "train_sentences = np.array(x_data)[train_indices]\n",
    "train_contexts = np.array(x_contexts)[train_indices]\n",
    "train_labels= np.array(labels)[train_indices] \n",
    "validation_sentences = np.array(x_data)[validation_indices]\n",
    "validation_labels = np.array(labels)[validation_indices]\n",
    "validation_contexts = np.array(x_contexts)[validation_indices]\n",
    "test_sentences = np.array(x_data)[test_indices]  \n",
    "test_contexts = np.array(x_contexts)[test_indices]\n",
    "test_labels = np.array(labels)[test_indices]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9447, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_contexts)[train_indices].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9447,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_labels(raw_label_set, size):\n",
    "    label_set = []\n",
    "    for label in raw_label_set:\n",
    "        labels = [0] * size\n",
    "        labels[label] = 1\n",
    "        label_set.append(labels)\n",
    "    return np.array(label_set)\n",
    "\n",
    "expanded_train_labels = transform_labels(train_labels, 2)\n",
    "expanded_validation_labels = transform_labels(validation_labels,2)\n",
    "expanded_test_labels = transform_labels(test_labels,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4, 5, 6, 7, 8, 8, 1, 5, 6, 7]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,3,4,5,6,7,8,8,1,5,6,7]\n",
    "a + [\"<PADDING>\"]*0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'angry',\n",
       " 'man',\n",
       " 'is',\n",
       " 'angry',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PaddingAndTruncating:\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def pad_or_truncate(self, sentence):\n",
    "        sen_len = len(sentence)\n",
    "        paddings = self.max_len - sen_len\n",
    "        if paddings >=0:\n",
    "            return list(sentence) + [\"<PADDING>\"] * paddings\n",
    "        return sentence[0:paddings]\n",
    "        \n",
    "PadAndTrunc = PaddingAndTruncating(10)\n",
    "        \n",
    "        \n",
    "PadAndTrunc.pad_or_truncate([\"the\",\"angry\",\"man\",\"is\",\"angry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  11, 1020,   21, ...,    3,    3,    3],\n",
       "       [  11,   79,   40, ...,    3,    3,    3],\n",
       "       [  92,  706,    2, ...,    3,    3,    3],\n",
       "       ..., \n",
       "       [  11,    2,  192, ...,    3,    3,    3],\n",
       "       [   2, 3228, 4810, ...,    3,    3,    3],\n",
       "       [  11,   38, 1140, ...,    3,    3,    3]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.contrib import learn\n",
    "vocab_size = 5000\n",
    "#max_len = max([len(sent) for sent  in train_sentences])\n",
    "PadAndTrunc =PaddingAndTruncating(40)\n",
    "train_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, train_sentences))\n",
    "train_context_padded = list(map(PadAndTrunc.pad_or_truncate, train_contexts))\n",
    "validation_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, validation_sentences))\n",
    "validation_context_padded = list(map(PadAndTrunc.pad_or_truncate, validation_contexts))\n",
    "test_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, test_sentences))\n",
    "test_context_padded = list(map(PadAndTrunc.pad_or_truncate, test_contexts))\n",
    "\n",
    "\n",
    "vocab = vocabulary.Vocabulary(utils.flatten(list(train_sentences_padded) + list(train_context_padded)), vocab_size)\n",
    "train_s = np.array(list(map(vocab.words_to_ids, train_sentences_padded)))\n",
    "train_c = np.array(list(map(vocab.words_to_ids, train_context_padded)))\n",
    "validation_s = np.array(list(map(vocab.words_to_ids, validation_sentences_padded)))\n",
    "validation_c = np.array(list(map(vocab.words_to_ids, validation_context_padded)))\n",
    "test_s = np.array(list(map(vocab.words_to_ids, test_sentences_padded)))\n",
    "test_c = np.array(list(map(vocab.words_to_ids, test_context_padded)))\n",
    "train_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv-maxpool-3'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 3\n",
    "(\"conv-maxpool-%s\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size, \n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x1 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x1\")\n",
    "        self.input_x2 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x2\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars1 = tf.nn.embedding_lookup(self.W, self.input_x1)\n",
    "            self.embedded_chars_expanded1 = tf.expand_dims(self.embedded_chars1, -1)\n",
    "            self.embedded_chars2 = tf.nn.embedding_lookup(self.W, self.input_x2)\n",
    "            self.embedded_chars_expanded2 = tf.expand_dims(self.embedded_chars2, -1)\n",
    "\n",
    "        # Create a convolution + avgpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.variable_scope(\"conv-avgpool-%s\" % i) as scope:\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "              \n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded1,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h1 = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"tanh1\")\n",
    "               \n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.avg_pool(\n",
    "                    h1,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                \n",
    "                pooled_outputs.append(pooled)\n",
    "                #pooled_outputs.append(pooled)\n",
    "                scope.reuse_variables()\n",
    "                 \n",
    "                \n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded2,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h2 = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"tanh2\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled2 = tf.nn.avg_pool(\n",
    "                    h2,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled2)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "\n",
    "        num_filters_total = num_filters * len(filter_sizes) * 2\n",
    "\n",
    "        self.h_pool = tf.concat(pooled_outputs, 1)\n",
    "\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "        \n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "           \n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.labels = tf.argmax(self.input_y, 1)\n",
    "            TP = tf.count_nonzero(self.predictions * self.labels)\n",
    "            TN = tf.count_nonzero((self.predictions - 1) * (self.labels - 1))\n",
    "            FP = tf.count_nonzero(self.predictions * (self.labels - 1))\n",
    "            FN = tf.count_nonzero((self.predictions - 1) * self.labels)\n",
    "            self.correct_predictions = tf.equal(self.predictions, self.labels)\n",
    "            self.precision = TP / (TP + FP)\n",
    "            self.recall = TP / (TP + FN)\n",
    "            self.f1_score = 2 * self.precision * self.recall / (self.precision + self.recall)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=200\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.75\n",
      "DROPOUT_KEEP_PROB=0.4\n",
      "EMBEDDING_DIM=60\n",
      "EVALUATE_EVERY=20\n",
      "FILTER_SIZES=1\n",
      "L2_REG_LAMBDA=0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.neg\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=10\n",
      "NUM_FILTERS=60\n",
      "POSITIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.pos\n",
      "\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/W:0/grad/hist is illegal; using conv-avgpool-0/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/W:0/grad/sparsity is illegal; using conv-avgpool-0/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/b:0/grad/hist is illegal; using conv-avgpool-0/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/b:0/grad/sparsity is illegal; using conv-avgpool-0/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524427953\n",
      "\n",
      "2018-04-22T20:12:34.225737: step 1, loss 0.744678, acc 0.525\n",
      "2018-04-22T20:12:34.296808: step 2, loss 1.80209, acc 0.435\n",
      "2018-04-22T20:12:34.364294: step 3, loss 3.01201, acc 0.52\n",
      "2018-04-22T20:12:34.431482: step 4, loss 1.79479, acc 0.595\n",
      "2018-04-22T20:12:34.501855: step 5, loss 0.815878, acc 0.67\n",
      "2018-04-22T20:12:34.569503: step 6, loss 1.02289, acc 0.63\n",
      "2018-04-22T20:12:34.643366: step 7, loss 1.33182, acc 0.565\n",
      "2018-04-22T20:12:34.711420: step 8, loss 0.92479, acc 0.62\n",
      "2018-04-22T20:12:34.778404: step 9, loss 0.605943, acc 0.755\n",
      "2018-04-22T20:12:34.850006: step 10, loss 0.598559, acc 0.78\n",
      "2018-04-22T20:12:34.916733: step 11, loss 0.768729, acc 0.715\n",
      "2018-04-22T20:12:34.985446: step 12, loss 0.553116, acc 0.795\n",
      "2018-04-22T20:12:35.058594: step 13, loss 0.636761, acc 0.78\n",
      "2018-04-22T20:12:35.127136: step 14, loss 0.440928, acc 0.785\n",
      "2018-04-22T20:12:35.194096: step 15, loss 0.440901, acc 0.86\n",
      "2018-04-22T20:12:35.265656: step 16, loss 0.473495, acc 0.805\n",
      "2018-04-22T20:12:35.333027: step 17, loss 0.353656, acc 0.845\n",
      "2018-04-22T20:12:35.401465: step 18, loss 0.508627, acc 0.8\n",
      "2018-04-22T20:12:35.470575: step 19, loss 0.421463, acc 0.83\n",
      "2018-04-22T20:12:35.537336: step 20, loss 0.386829, acc 0.825\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:12:35.818651: step 20, loss 0.851053, acc 0.60963, rec 0.997067, pre 0.564315, f1 0.720721\n",
      "\n",
      "2018-04-22T20:12:35.878011: step 21, loss 0.421194, acc 0.86\n",
      "2018-04-22T20:12:35.939673: step 22, loss 0.333946, acc 0.855\n",
      "2018-04-22T20:12:35.999721: step 23, loss 0.664265, acc 0.795\n",
      "2018-04-22T20:12:36.064052: step 24, loss 0.505427, acc 0.835\n",
      "2018-04-22T20:12:36.124439: step 25, loss 0.409931, acc 0.83\n",
      "2018-04-22T20:12:36.183902: step 26, loss 0.462795, acc 0.81\n",
      "2018-04-22T20:12:36.243772: step 27, loss 0.494964, acc 0.82\n",
      "2018-04-22T20:12:36.307213: step 28, loss 0.315183, acc 0.835\n",
      "2018-04-22T20:12:36.368460: step 29, loss 0.321281, acc 0.885\n",
      "2018-04-22T20:12:36.427839: step 30, loss 0.308192, acc 0.875\n",
      "2018-04-22T20:12:36.487325: step 31, loss 0.267704, acc 0.885\n",
      "2018-04-22T20:12:36.550446: step 32, loss 0.327425, acc 0.88\n",
      "2018-04-22T20:12:36.610599: step 33, loss 0.324511, acc 0.855\n",
      "2018-04-22T20:12:36.671736: step 34, loss 0.357343, acc 0.86\n",
      "2018-04-22T20:12:36.732242: step 35, loss 0.385326, acc 0.85\n",
      "2018-04-22T20:12:36.795408: step 36, loss 0.328284, acc 0.87\n",
      "2018-04-22T20:12:36.858974: step 37, loss 0.351263, acc 0.85\n",
      "2018-04-22T20:12:36.920405: step 38, loss 0.423515, acc 0.835\n",
      "2018-04-22T20:12:36.981985: step 39, loss 0.300262, acc 0.86\n",
      "2018-04-22T20:12:37.047275: step 40, loss 0.408584, acc 0.82\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:12:37.264392: step 40, loss 0.358249, acc 0.895926, rec 0.937683, pre 0.867119, f1 0.901021\n",
      "\n",
      "2018-04-22T20:12:37.324174: step 41, loss 0.408438, acc 0.83\n",
      "2018-04-22T20:12:37.384201: step 42, loss 0.336331, acc 0.85\n",
      "2018-04-22T20:12:37.446097: step 43, loss 0.381309, acc 0.875\n",
      "2018-04-22T20:12:37.510408: step 44, loss 0.330712, acc 0.885\n",
      "2018-04-22T20:12:37.575719: step 45, loss 0.307839, acc 0.86\n",
      "2018-04-22T20:12:37.638238: step 46, loss 0.327284, acc 0.84\n",
      "2018-04-22T20:12:37.698928: step 47, loss 0.30728, acc 0.875\n",
      "2018-04-22T20:12:37.724880: step 48, loss 0.434049, acc 0.808511\n",
      "2018-04-22T20:12:37.789320: step 49, loss 0.324233, acc 0.875\n",
      "2018-04-22T20:12:37.850331: step 50, loss 0.274795, acc 0.88\n",
      "2018-04-22T20:12:37.912001: step 51, loss 0.224837, acc 0.915\n",
      "2018-04-22T20:12:37.978418: step 52, loss 0.254537, acc 0.89\n",
      "2018-04-22T20:12:38.040751: step 53, loss 0.293503, acc 0.9\n",
      "2018-04-22T20:12:38.100963: step 54, loss 0.280637, acc 0.88\n",
      "2018-04-22T20:12:38.159935: step 55, loss 0.353341, acc 0.87\n",
      "2018-04-22T20:12:38.224754: step 56, loss 0.306747, acc 0.855\n",
      "2018-04-22T20:12:38.286984: step 57, loss 0.24113, acc 0.915\n",
      "2018-04-22T20:12:38.348063: step 58, loss 0.33926, acc 0.89\n",
      "2018-04-22T20:12:38.408612: step 59, loss 0.336526, acc 0.86\n",
      "2018-04-22T20:12:38.472848: step 60, loss 0.209777, acc 0.91\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:12:38.693960: step 60, loss 0.35395, acc 0.844074, rec 0.963343, pre 0.779822, f1 0.861922\n",
      "\n",
      "2018-04-22T20:12:38.755543: step 61, loss 0.23566, acc 0.895\n",
      "2018-04-22T20:12:38.820627: step 62, loss 0.320726, acc 0.87\n",
      "2018-04-22T20:12:38.881928: step 63, loss 0.258348, acc 0.9\n",
      "2018-04-22T20:12:38.948843: step 64, loss 0.248562, acc 0.895\n",
      "2018-04-22T20:12:39.010723: step 65, loss 0.247691, acc 0.89\n",
      "2018-04-22T20:12:39.072447: step 66, loss 0.335389, acc 0.87\n",
      "2018-04-22T20:12:39.134299: step 67, loss 0.311984, acc 0.88\n",
      "2018-04-22T20:12:39.199829: step 68, loss 0.294819, acc 0.885\n",
      "2018-04-22T20:12:39.262037: step 69, loss 0.266591, acc 0.915\n",
      "2018-04-22T20:12:39.325407: step 70, loss 0.234801, acc 0.905\n",
      "2018-04-22T20:12:39.389656: step 71, loss 0.269788, acc 0.88\n",
      "2018-04-22T20:12:39.454893: step 72, loss 0.262917, acc 0.875\n",
      "2018-04-22T20:12:39.517048: step 73, loss 0.25648, acc 0.885\n",
      "2018-04-22T20:12:39.578856: step 74, loss 0.183471, acc 0.92\n",
      "2018-04-22T20:12:39.640812: step 75, loss 0.185778, acc 0.92\n",
      "2018-04-22T20:12:39.709202: step 76, loss 0.156869, acc 0.94\n",
      "2018-04-22T20:12:39.771583: step 77, loss 0.206318, acc 0.915\n",
      "2018-04-22T20:12:39.833659: step 78, loss 0.302837, acc 0.89\n",
      "2018-04-22T20:12:39.896262: step 79, loss 0.247449, acc 0.905\n",
      "2018-04-22T20:12:39.962180: step 80, loss 0.247618, acc 0.895\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:12:40.185030: step 80, loss 0.328928, acc 0.855926, rec 0.958944, pre 0.797075, f1 0.870549\n",
      "\n",
      "2018-04-22T20:12:40.247125: step 81, loss 0.29779, acc 0.895\n",
      "2018-04-22T20:12:40.308676: step 82, loss 0.309055, acc 0.865\n",
      "2018-04-22T20:12:40.370210: step 83, loss 0.28259, acc 0.9\n",
      "2018-04-22T20:12:40.437032: step 84, loss 0.264772, acc 0.88\n",
      "2018-04-22T20:12:40.499952: step 85, loss 0.306443, acc 0.885\n",
      "2018-04-22T20:12:40.562373: step 86, loss 0.272313, acc 0.905\n",
      "2018-04-22T20:12:40.624596: step 87, loss 0.290154, acc 0.89\n",
      "2018-04-22T20:12:40.689832: step 88, loss 0.214526, acc 0.925\n",
      "2018-04-22T20:12:40.752317: step 89, loss 0.290083, acc 0.9\n",
      "2018-04-22T20:12:40.813705: step 90, loss 0.33867, acc 0.865\n",
      "2018-04-22T20:12:40.874645: step 91, loss 0.283762, acc 0.88\n",
      "2018-04-22T20:12:40.939262: step 92, loss 0.302259, acc 0.885\n",
      "2018-04-22T20:12:41.000605: step 93, loss 0.327393, acc 0.885\n",
      "2018-04-22T20:12:41.061837: step 94, loss 0.140461, acc 0.955\n",
      "2018-04-22T20:12:41.130092: step 95, loss 0.291882, acc 0.885\n",
      "2018-04-22T20:12:41.154903: step 96, loss 0.152378, acc 0.957447\n",
      "2018-04-22T20:12:41.256562: step 97, loss 0.259821, acc 0.895\n",
      "2018-04-22T20:12:41.388559: step 98, loss 0.212966, acc 0.905\n",
      "2018-04-22T20:12:41.514201: step 99, loss 0.215251, acc 0.905\n",
      "2018-04-22T20:12:41.639362: step 100, loss 0.276664, acc 0.905\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:12:42.085189: step 100, loss 0.307284, acc 0.891481, rec 0.887097, pre 0.896961, f1 0.892001\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524427953/checkpoints/model-100\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-22T20:12:42.320805: step 101, loss 0.269843, acc 0.915\n",
      "2018-04-22T20:12:42.443968: step 102, loss 0.183998, acc 0.915\n",
      "2018-04-22T20:12:42.573308: step 103, loss 0.362776, acc 0.865\n",
      "2018-04-22T20:12:42.700507: step 104, loss 0.238299, acc 0.9\n",
      "2018-04-22T20:12:42.821557: step 105, loss 0.221549, acc 0.94\n",
      "2018-04-22T20:12:42.948498: step 106, loss 0.348556, acc 0.87\n",
      "2018-04-22T20:12:43.072503: step 107, loss 0.279797, acc 0.87\n",
      "2018-04-22T20:12:43.196368: step 108, loss 0.1846, acc 0.935\n",
      "2018-04-22T20:12:43.322832: step 109, loss 0.308791, acc 0.885\n",
      "2018-04-22T20:12:43.461069: step 110, loss 0.249862, acc 0.915\n",
      "2018-04-22T20:12:43.599355: step 111, loss 0.233687, acc 0.9\n",
      "2018-04-22T20:12:43.724487: step 112, loss 0.211152, acc 0.895\n",
      "2018-04-22T20:12:43.852477: step 113, loss 0.202156, acc 0.93\n",
      "2018-04-22T20:12:43.976758: step 114, loss 0.361554, acc 0.915\n",
      "2018-04-22T20:12:44.116485: step 115, loss 0.228218, acc 0.905\n",
      "2018-04-22T20:12:44.240515: step 116, loss 0.247208, acc 0.905\n",
      "2018-04-22T20:12:44.368511: step 117, loss 0.168608, acc 0.94\n",
      "2018-04-22T20:12:44.492498: step 118, loss 0.185924, acc 0.915\n",
      "2018-04-22T20:12:44.620486: step 119, loss 0.257373, acc 0.915\n",
      "2018-04-22T20:12:44.746752: step 120, loss 0.234029, acc 0.91\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:12:45.194122: step 120, loss 0.37526, acc 0.818148, rec 0.989736, pre 0.738916, f1 0.84613\n",
      "\n",
      "2018-04-22T20:12:45.320488: step 121, loss 0.193229, acc 0.93\n",
      "2018-04-22T20:12:45.424904: step 122, loss 0.302634, acc 0.865\n",
      "2018-04-22T20:12:45.484934: step 123, loss 0.23417, acc 0.905\n",
      "2018-04-22T20:12:45.546682: step 124, loss 0.206776, acc 0.93\n",
      "2018-04-22T20:12:45.608165: step 125, loss 0.268461, acc 0.89\n",
      "2018-04-22T20:12:45.681619: step 126, loss 0.27453, acc 0.885\n",
      "2018-04-22T20:12:45.751671: step 127, loss 0.236999, acc 0.9\n",
      "2018-04-22T20:12:45.831187: step 128, loss 0.24641, acc 0.925\n",
      "2018-04-22T20:12:45.970303: step 129, loss 0.279697, acc 0.925\n",
      "2018-04-22T20:12:46.096506: step 130, loss 0.198543, acc 0.925\n",
      "2018-04-22T20:12:46.224513: step 131, loss 0.224919, acc 0.9\n",
      "2018-04-22T20:12:46.348489: step 132, loss 0.204128, acc 0.92\n",
      "2018-04-22T20:12:46.480546: step 133, loss 0.225786, acc 0.91\n",
      "2018-04-22T20:12:46.604490: step 134, loss 0.283316, acc 0.915\n",
      "2018-04-22T20:12:46.732693: step 135, loss 0.294948, acc 0.89\n",
      "2018-04-22T20:12:46.855798: step 136, loss 0.225438, acc 0.905\n",
      "2018-04-22T20:12:46.984613: step 137, loss 0.309337, acc 0.86\n",
      "2018-04-22T20:12:47.108478: step 138, loss 0.211804, acc 0.93\n",
      "2018-04-22T20:12:47.240487: step 139, loss 0.189696, acc 0.935\n",
      "2018-04-22T20:12:47.364488: step 140, loss 0.315798, acc 0.89\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:12:47.805409: step 140, loss 0.473334, acc 0.747037, rec 0.991202, pre 0.668314, f1 0.798347\n",
      "\n",
      "2018-04-22T20:12:47.928791: step 141, loss 0.255788, acc 0.895\n",
      "2018-04-22T20:12:48.056755: step 142, loss 0.258154, acc 0.9\n",
      "2018-04-22T20:12:48.180495: step 143, loss 0.383142, acc 0.845\n",
      "2018-04-22T20:12:48.224327: step 144, loss 0.233105, acc 0.957447\n",
      "2018-04-22T20:12:48.360507: step 145, loss 0.194996, acc 0.93\n",
      "2018-04-22T20:12:48.485268: step 146, loss 0.246767, acc 0.89\n",
      "2018-04-22T20:12:48.611873: step 147, loss 0.299716, acc 0.875\n",
      "2018-04-22T20:12:48.744476: step 148, loss 0.187112, acc 0.92\n",
      "2018-04-22T20:12:48.872499: step 149, loss 0.201303, acc 0.935\n",
      "2018-04-22T20:12:48.996485: step 150, loss 0.300593, acc 0.875\n",
      "2018-04-22T20:12:49.128487: step 151, loss 0.241046, acc 0.915\n",
      "2018-04-22T20:12:49.252506: step 152, loss 0.221609, acc 0.905\n",
      "2018-04-22T20:12:49.380512: step 153, loss 0.285547, acc 0.885\n",
      "2018-04-22T20:12:49.506562: step 154, loss 0.190871, acc 0.93\n",
      "2018-04-22T20:12:49.634786: step 155, loss 0.370397, acc 0.86\n",
      "2018-04-22T20:12:49.764658: step 156, loss 0.268443, acc 0.895\n",
      "2018-04-22T20:12:49.893152: step 157, loss 0.231574, acc 0.9\n",
      "2018-04-22T20:12:50.020461: step 158, loss 0.230759, acc 0.915\n",
      "2018-04-22T20:12:50.142534: step 159, loss 0.229891, acc 0.925\n",
      "2018-04-22T20:12:50.265231: step 160, loss 0.196984, acc 0.93\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:12:50.704794: step 160, loss 0.320437, acc 0.858148, rec 0.969941, pre 0.794595, f1 0.873556\n",
      "\n",
      "2018-04-22T20:12:50.832640: step 161, loss 0.34742, acc 0.87\n",
      "2018-04-22T20:12:50.964486: step 162, loss 0.281582, acc 0.91\n",
      "2018-04-22T20:12:51.083874: step 163, loss 0.308429, acc 0.87\n",
      "2018-04-22T20:12:51.216497: step 164, loss 0.200297, acc 0.92\n",
      "2018-04-22T20:12:51.340495: step 165, loss 0.182152, acc 0.92\n",
      "2018-04-22T20:12:51.468482: step 166, loss 0.271766, acc 0.88\n",
      "2018-04-22T20:12:51.592886: step 167, loss 0.295642, acc 0.885\n",
      "2018-04-22T20:12:51.725874: step 168, loss 0.209394, acc 0.91\n",
      "2018-04-22T20:12:51.852479: step 169, loss 0.255542, acc 0.905\n",
      "2018-04-22T20:12:51.977315: step 170, loss 0.279622, acc 0.89\n",
      "2018-04-22T20:12:52.104484: step 171, loss 0.268919, acc 0.895\n",
      "2018-04-22T20:12:52.232491: step 172, loss 0.245239, acc 0.91\n",
      "2018-04-22T20:12:52.353500: step 173, loss 0.234374, acc 0.91\n",
      "2018-04-22T20:12:52.488812: step 174, loss 0.237215, acc 0.9\n",
      "2018-04-22T20:12:52.610409: step 175, loss 0.307072, acc 0.885\n",
      "2018-04-22T20:12:52.737203: step 176, loss 0.218955, acc 0.925\n",
      "2018-04-22T20:12:52.864654: step 177, loss 0.232985, acc 0.9\n",
      "2018-04-22T20:12:52.990726: step 178, loss 0.261538, acc 0.92\n",
      "2018-04-22T20:12:53.116473: step 179, loss 0.313903, acc 0.865\n",
      "2018-04-22T20:12:53.240737: step 180, loss 0.242638, acc 0.905\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:12:53.676494: step 180, loss 0.607336, acc 0.664074, rec 0.339443, pre 0.987207, f1 0.505183\n",
      "\n",
      "2018-04-22T20:12:53.804483: step 181, loss 0.31784, acc 0.89\n",
      "2018-04-22T20:12:53.932478: step 182, loss 0.23349, acc 0.905\n",
      "2018-04-22T20:12:54.052998: step 183, loss 0.2865, acc 0.9\n",
      "2018-04-22T20:12:54.180476: step 184, loss 0.374048, acc 0.88\n",
      "2018-04-22T20:12:54.303414: step 185, loss 0.431625, acc 0.86\n",
      "2018-04-22T20:12:54.433258: step 186, loss 0.210151, acc 0.925\n",
      "2018-04-22T20:12:54.560479: step 187, loss 0.238233, acc 0.905\n",
      "2018-04-22T20:12:54.688493: step 188, loss 0.335813, acc 0.865\n",
      "2018-04-22T20:12:54.816479: step 189, loss 0.35301, acc 0.885\n",
      "2018-04-22T20:12:54.944461: step 190, loss 0.191659, acc 0.92\n",
      "2018-04-22T20:12:55.065755: step 191, loss 0.297735, acc 0.9\n",
      "2018-04-22T20:12:55.113033: step 192, loss 0.241504, acc 0.93617\n",
      "2018-04-22T20:12:55.242049: step 193, loss 0.201991, acc 0.92\n",
      "2018-04-22T20:12:55.364639: step 194, loss 0.298416, acc 0.895\n",
      "2018-04-22T20:12:55.496471: step 195, loss 0.206818, acc 0.915\n",
      "2018-04-22T20:12:55.620479: step 196, loss 0.194691, acc 0.91\n",
      "2018-04-22T20:12:55.756521: step 197, loss 0.245381, acc 0.9\n",
      "2018-04-22T20:12:55.876854: step 198, loss 0.283668, acc 0.9\n",
      "2018-04-22T20:12:56.054355: step 199, loss 0.223168, acc 0.925\n",
      "2018-04-22T20:12:56.180494: step 200, loss 0.240097, acc 0.91\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:12:56.620945: step 200, loss 0.283486, acc 0.895185, rec 0.891496, pre 0.900074, f1 0.895764\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524427953/checkpoints/model-200\n",
      "\n",
      "2018-04-22T20:12:56.849322: step 201, loss 0.233395, acc 0.87\n",
      "2018-04-22T20:12:56.972500: step 202, loss 0.363438, acc 0.9\n",
      "2018-04-22T20:12:57.100498: step 203, loss 0.255359, acc 0.905\n",
      "2018-04-22T20:12:57.224500: step 204, loss 0.330085, acc 0.9\n",
      "2018-04-22T20:12:57.362677: step 205, loss 0.21583, acc 0.925\n",
      "2018-04-22T20:12:57.486381: step 206, loss 0.188542, acc 0.91\n",
      "2018-04-22T20:12:57.612474: step 207, loss 0.263177, acc 0.915\n",
      "2018-04-22T20:12:57.736701: step 208, loss 0.334734, acc 0.885\n",
      "2018-04-22T20:12:57.872495: step 209, loss 0.248309, acc 0.885\n",
      "2018-04-22T20:12:57.996511: step 210, loss 0.223451, acc 0.925\n",
      "2018-04-22T20:12:58.124821: step 211, loss 0.117661, acc 0.925\n",
      "2018-04-22T20:12:58.248478: step 212, loss 0.225477, acc 0.92\n",
      "2018-04-22T20:12:58.377736: step 213, loss 0.283607, acc 0.875\n",
      "2018-04-22T20:12:58.497068: step 214, loss 0.173751, acc 0.93\n",
      "2018-04-22T20:12:58.628504: step 215, loss 0.359891, acc 0.885\n",
      "2018-04-22T20:12:58.752508: step 216, loss 0.383015, acc 0.905\n",
      "2018-04-22T20:12:58.880488: step 217, loss 0.223504, acc 0.905\n",
      "2018-04-22T20:12:59.005884: step 218, loss 0.281326, acc 0.915\n",
      "2018-04-22T20:12:59.136504: step 219, loss 0.249447, acc 0.91\n",
      "2018-04-22T20:12:59.255034: step 220, loss 0.227067, acc 0.905\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-22T20:12:59.698103: step 220, loss 0.288151, acc 0.888519, rec 0.97654, pre 0.83198, f1 0.898482\n",
      "\n",
      "2018-04-22T20:12:59.816478: step 221, loss 0.206332, acc 0.92\n",
      "2018-04-22T20:12:59.940477: step 222, loss 0.204908, acc 0.92\n",
      "2018-04-22T20:13:00.062803: step 223, loss 0.231601, acc 0.905\n",
      "2018-04-22T20:13:00.192503: step 224, loss 0.329621, acc 0.89\n",
      "2018-04-22T20:13:00.312493: step 225, loss 0.298876, acc 0.9\n",
      "2018-04-22T20:13:00.457966: step 226, loss 0.329726, acc 0.885\n",
      "2018-04-22T20:13:00.584647: step 227, loss 0.311988, acc 0.875\n",
      "2018-04-22T20:13:00.713460: step 228, loss 0.239379, acc 0.905\n",
      "2018-04-22T20:13:00.840814: step 229, loss 0.247776, acc 0.895\n",
      "2018-04-22T20:13:00.972501: step 230, loss 0.363719, acc 0.88\n",
      "2018-04-22T20:13:01.100513: step 231, loss 0.257711, acc 0.905\n",
      "2018-04-22T20:13:01.228508: step 232, loss 0.293105, acc 0.89\n",
      "2018-04-22T20:13:01.353965: step 233, loss 0.159691, acc 0.93\n",
      "2018-04-22T20:13:01.488535: step 234, loss 0.469395, acc 0.85\n",
      "2018-04-22T20:13:01.620513: step 235, loss 0.346448, acc 0.875\n",
      "2018-04-22T20:13:01.760488: step 236, loss 0.260584, acc 0.895\n",
      "2018-04-22T20:13:01.884480: step 237, loss 0.412803, acc 0.88\n",
      "2018-04-22T20:13:02.009648: step 238, loss 0.283051, acc 0.92\n",
      "2018-04-22T20:13:02.136495: step 239, loss 0.299676, acc 0.905\n",
      "2018-04-22T20:13:02.180504: step 240, loss 0.177551, acc 0.957447\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:02.624501: step 240, loss 0.388741, acc 0.789259, rec 0.61437, pre 0.951192, f1 0.746548\n",
      "\n",
      "2018-04-22T20:13:02.754084: step 241, loss 0.34379, acc 0.88\n",
      "2018-04-22T20:13:02.874933: step 242, loss 0.376813, acc 0.905\n",
      "2018-04-22T20:13:03.004462: step 243, loss 0.153061, acc 0.935\n",
      "2018-04-22T20:13:03.127846: step 244, loss 0.370449, acc 0.905\n",
      "2018-04-22T20:13:03.254465: step 245, loss 0.239756, acc 0.92\n",
      "2018-04-22T20:13:03.380455: step 246, loss 0.336185, acc 0.915\n",
      "2018-04-22T20:13:03.504452: step 247, loss 0.311086, acc 0.895\n",
      "2018-04-22T20:13:03.652460: step 248, loss 0.189581, acc 0.92\n",
      "2018-04-22T20:13:03.780533: step 249, loss 0.369305, acc 0.895\n",
      "2018-04-22T20:13:03.908965: step 250, loss 0.236251, acc 0.9\n",
      "2018-04-22T20:13:04.039658: step 251, loss 0.406285, acc 0.895\n",
      "2018-04-22T20:13:04.166215: step 252, loss 0.372784, acc 0.87\n",
      "2018-04-22T20:13:04.293804: step 253, loss 0.235331, acc 0.94\n",
      "2018-04-22T20:13:04.428454: step 254, loss 0.392302, acc 0.835\n",
      "2018-04-22T20:13:04.560447: step 255, loss 0.406192, acc 0.885\n",
      "2018-04-22T20:13:04.689013: step 256, loss 0.138427, acc 0.95\n",
      "2018-04-22T20:13:04.824447: step 257, loss 0.190603, acc 0.91\n",
      "2018-04-22T20:13:04.952475: step 258, loss 0.353713, acc 0.87\n",
      "2018-04-22T20:13:05.080973: step 259, loss 0.371857, acc 0.905\n",
      "2018-04-22T20:13:05.217837: step 260, loss 0.276787, acc 0.89\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:05.672457: step 260, loss 0.297447, acc 0.887407, rec 0.818182, pre 0.952218, f1 0.880126\n",
      "\n",
      "2018-04-22T20:13:05.801617: step 261, loss 0.241501, acc 0.935\n",
      "2018-04-22T20:13:05.935011: step 262, loss 0.243695, acc 0.92\n",
      "2018-04-22T20:13:06.060448: step 263, loss 0.229273, acc 0.91\n",
      "2018-04-22T20:13:06.185276: step 264, loss 0.252568, acc 0.905\n",
      "2018-04-22T20:13:06.309540: step 265, loss 0.3798, acc 0.88\n",
      "2018-04-22T20:13:06.444455: step 266, loss 0.225119, acc 0.915\n",
      "2018-04-22T20:13:06.570116: step 267, loss 0.166601, acc 0.945\n",
      "2018-04-22T20:13:06.703356: step 268, loss 0.20056, acc 0.94\n",
      "2018-04-22T20:13:06.832752: step 269, loss 0.246901, acc 0.88\n",
      "2018-04-22T20:13:06.972665: step 270, loss 0.243459, acc 0.895\n",
      "2018-04-22T20:13:07.104523: step 271, loss 0.263781, acc 0.89\n",
      "2018-04-22T20:13:07.237433: step 272, loss 0.28956, acc 0.895\n",
      "2018-04-22T20:13:07.370268: step 273, loss 0.272002, acc 0.9\n",
      "2018-04-22T20:13:07.502452: step 274, loss 0.143772, acc 0.94\n",
      "2018-04-22T20:13:07.632500: step 275, loss 0.28326, acc 0.875\n",
      "2018-04-22T20:13:07.769371: step 276, loss 0.270267, acc 0.9\n",
      "2018-04-22T20:13:07.901109: step 277, loss 0.267325, acc 0.915\n",
      "2018-04-22T20:13:08.026481: step 278, loss 0.313141, acc 0.895\n",
      "2018-04-22T20:13:08.156547: step 279, loss 0.189818, acc 0.925\n",
      "2018-04-22T20:13:08.288518: step 280, loss 0.233841, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:08.760509: step 280, loss 0.33586, acc 0.851111, rec 0.978739, pre 0.781616, f1 0.869141\n",
      "\n",
      "2018-04-22T20:13:08.888521: step 281, loss 0.295617, acc 0.9\n",
      "2018-04-22T20:13:09.021851: step 282, loss 0.179891, acc 0.94\n",
      "2018-04-22T20:13:09.151839: step 283, loss 0.222414, acc 0.925\n",
      "2018-04-22T20:13:09.285691: step 284, loss 0.222823, acc 0.915\n",
      "2018-04-22T20:13:09.418028: step 285, loss 0.225373, acc 0.9\n",
      "2018-04-22T20:13:09.550666: step 286, loss 0.21036, acc 0.915\n",
      "2018-04-22T20:13:09.685089: step 287, loss 0.318004, acc 0.905\n",
      "2018-04-22T20:13:09.732689: step 288, loss 0.27673, acc 0.893617\n",
      "2018-04-22T20:13:09.864461: step 289, loss 0.261993, acc 0.9\n",
      "2018-04-22T20:13:09.988456: step 290, loss 0.152712, acc 0.945\n",
      "2018-04-22T20:13:10.112830: step 291, loss 0.330348, acc 0.9\n",
      "2018-04-22T20:13:10.240449: step 292, loss 0.350678, acc 0.875\n",
      "2018-04-22T20:13:10.367628: step 293, loss 0.32892, acc 0.905\n",
      "2018-04-22T20:13:10.499395: step 294, loss 0.244217, acc 0.9\n",
      "2018-04-22T20:13:10.657837: step 295, loss 0.255957, acc 0.895\n",
      "2018-04-22T20:13:10.792527: step 296, loss 0.256799, acc 0.9\n",
      "2018-04-22T20:13:10.920047: step 297, loss 0.269122, acc 0.89\n",
      "2018-04-22T20:13:11.051324: step 298, loss 0.218333, acc 0.915\n",
      "2018-04-22T20:13:11.188512: step 299, loss 0.348268, acc 0.865\n",
      "2018-04-22T20:13:11.324511: step 300, loss 0.200566, acc 0.925\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:11.784502: step 300, loss 0.378391, acc 0.818519, rec 0.986804, pre 0.740374, f1 0.846009\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524427953/checkpoints/model-300\n",
      "\n",
      "2018-04-22T20:13:12.008462: step 301, loss 0.235761, acc 0.92\n",
      "2018-04-22T20:13:12.136465: step 302, loss 0.167733, acc 0.935\n",
      "2018-04-22T20:13:12.262055: step 303, loss 0.241449, acc 0.89\n",
      "2018-04-22T20:13:12.389060: step 304, loss 0.19005, acc 0.94\n",
      "2018-04-22T20:13:12.520961: step 305, loss 0.329583, acc 0.875\n",
      "2018-04-22T20:13:12.640990: step 306, loss 0.24705, acc 0.91\n",
      "2018-04-22T20:13:12.793870: step 307, loss 0.274643, acc 0.91\n",
      "2018-04-22T20:13:12.984508: step 308, loss 0.268543, acc 0.89\n",
      "2018-04-22T20:13:13.184490: step 309, loss 0.148068, acc 0.95\n",
      "2018-04-22T20:13:13.396515: step 310, loss 0.180767, acc 0.94\n",
      "2018-04-22T20:13:13.616499: step 311, loss 0.375373, acc 0.865\n",
      "2018-04-22T20:13:13.821501: step 312, loss 0.257541, acc 0.915\n",
      "2018-04-22T20:13:14.040501: step 313, loss 0.222246, acc 0.915\n",
      "2018-04-22T20:13:14.248471: step 314, loss 0.286205, acc 0.9\n",
      "2018-04-22T20:13:14.453387: step 315, loss 0.23693, acc 0.93\n",
      "2018-04-22T20:13:14.649692: step 316, loss 0.275741, acc 0.885\n",
      "2018-04-22T20:13:14.783637: step 317, loss 0.175554, acc 0.925\n",
      "2018-04-22T20:13:14.908908: step 318, loss 0.425073, acc 0.87\n",
      "2018-04-22T20:13:15.044461: step 319, loss 0.250462, acc 0.895\n",
      "2018-04-22T20:13:15.169580: step 320, loss 0.328195, acc 0.895\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:15.624462: step 320, loss 0.606829, acc 0.724074, rec 0.992669, pre 0.648157, f1 0.784246\n",
      "\n",
      "2018-04-22T20:13:15.756497: step 321, loss 0.414665, acc 0.845\n",
      "2018-04-22T20:13:15.884993: step 322, loss 0.330245, acc 0.895\n",
      "2018-04-22T20:13:16.014606: step 323, loss 0.236011, acc 0.91\n",
      "2018-04-22T20:13:16.141636: step 324, loss 0.203352, acc 0.925\n",
      "2018-04-22T20:13:16.272455: step 325, loss 0.364199, acc 0.875\n",
      "2018-04-22T20:13:16.400458: step 326, loss 0.269736, acc 0.91\n",
      "2018-04-22T20:13:16.524457: step 327, loss 0.265771, acc 0.9\n",
      "2018-04-22T20:13:16.650669: step 328, loss 0.372367, acc 0.88\n",
      "2018-04-22T20:13:16.778493: step 329, loss 0.224384, acc 0.915\n",
      "2018-04-22T20:13:16.916477: step 330, loss 0.176037, acc 0.92\n",
      "2018-04-22T20:13:17.036445: step 331, loss 0.214081, acc 0.91\n",
      "2018-04-22T20:13:17.172473: step 332, loss 0.20388, acc 0.925\n",
      "2018-04-22T20:13:17.296604: step 333, loss 0.243024, acc 0.895\n",
      "2018-04-22T20:13:17.428452: step 334, loss 0.361773, acc 0.915\n",
      "2018-04-22T20:13:17.556461: step 335, loss 0.333181, acc 0.885\n",
      "2018-04-22T20:13:17.601703: step 336, loss 0.333217, acc 0.87234\n",
      "2018-04-22T20:13:17.736463: step 337, loss 0.326914, acc 0.905\n",
      "2018-04-22T20:13:17.864475: step 338, loss 0.405803, acc 0.88\n",
      "2018-04-22T20:13:17.988766: step 339, loss 0.243446, acc 0.92\n",
      "2018-04-22T20:13:18.120467: step 340, loss 0.220909, acc 0.915\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-22T20:13:18.557458: step 340, loss 1.2232, acc 0.614444, rec 0.996334, pre 0.567432, f1 0.723065\n",
      "\n",
      "2018-04-22T20:13:18.684472: step 341, loss 0.26657, acc 0.905\n",
      "2018-04-22T20:13:18.814457: step 342, loss 0.297013, acc 0.9\n",
      "2018-04-22T20:13:18.942544: step 343, loss 0.276054, acc 0.92\n",
      "2018-04-22T20:13:19.072492: step 344, loss 0.295549, acc 0.92\n",
      "2018-04-22T20:13:19.198668: step 345, loss 0.269766, acc 0.9\n",
      "2018-04-22T20:13:19.333086: step 346, loss 0.342929, acc 0.91\n",
      "2018-04-22T20:13:19.456318: step 347, loss 0.375748, acc 0.9\n",
      "2018-04-22T20:13:19.584814: step 348, loss 0.230102, acc 0.91\n",
      "2018-04-22T20:13:19.712457: step 349, loss 0.24475, acc 0.915\n",
      "2018-04-22T20:13:19.843805: step 350, loss 0.427957, acc 0.855\n",
      "2018-04-22T20:13:19.968081: step 351, loss 0.49029, acc 0.875\n",
      "2018-04-22T20:13:20.096288: step 352, loss 0.310965, acc 0.885\n",
      "2018-04-22T20:13:20.224460: step 353, loss 0.33519, acc 0.885\n",
      "2018-04-22T20:13:20.350302: step 354, loss 0.335802, acc 0.915\n",
      "2018-04-22T20:13:20.472458: step 355, loss 0.270104, acc 0.91\n",
      "2018-04-22T20:13:20.594405: step 356, loss 0.214186, acc 0.93\n",
      "2018-04-22T20:13:20.729525: step 357, loss 0.374079, acc 0.865\n",
      "2018-04-22T20:13:20.857746: step 358, loss 0.192383, acc 0.925\n",
      "2018-04-22T20:13:20.985553: step 359, loss 0.246809, acc 0.92\n",
      "2018-04-22T20:13:21.116455: step 360, loss 0.379077, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:21.572475: step 360, loss 0.292303, acc 0.891852, rec 0.968475, pre 0.841401, f1 0.900477\n",
      "\n",
      "2018-04-22T20:13:21.704478: step 361, loss 0.250623, acc 0.905\n",
      "2018-04-22T20:13:21.834575: step 362, loss 0.288184, acc 0.91\n",
      "2018-04-22T20:13:21.962183: step 363, loss 0.211809, acc 0.945\n",
      "2018-04-22T20:13:22.093041: step 364, loss 0.20175, acc 0.915\n",
      "2018-04-22T20:13:22.216454: step 365, loss 0.243619, acc 0.925\n",
      "2018-04-22T20:13:22.335751: step 366, loss 0.219713, acc 0.92\n",
      "2018-04-22T20:13:22.464459: step 367, loss 0.286573, acc 0.905\n",
      "2018-04-22T20:13:22.593625: step 368, loss 0.161053, acc 0.93\n",
      "2018-04-22T20:13:22.720506: step 369, loss 0.268432, acc 0.88\n",
      "2018-04-22T20:13:22.853084: step 370, loss 0.322752, acc 0.875\n",
      "2018-04-22T20:13:22.984533: step 371, loss 0.306912, acc 0.905\n",
      "2018-04-22T20:13:23.120514: step 372, loss 0.240782, acc 0.92\n",
      "2018-04-22T20:13:23.248129: step 373, loss 0.386809, acc 0.895\n",
      "2018-04-22T20:13:23.386791: step 374, loss 0.405233, acc 0.875\n",
      "2018-04-22T20:13:23.516500: step 375, loss 0.218755, acc 0.93\n",
      "2018-04-22T20:13:23.651445: step 376, loss 0.366636, acc 0.895\n",
      "2018-04-22T20:13:23.782319: step 377, loss 0.552592, acc 0.86\n",
      "2018-04-22T20:13:23.920483: step 378, loss 0.284502, acc 0.895\n",
      "2018-04-22T20:13:24.042637: step 379, loss 0.373137, acc 0.875\n",
      "2018-04-22T20:13:24.176544: step 380, loss 0.456976, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:24.640587: step 380, loss 1.23098, acc 0.586667, rec 0.997067, pre 0.550162, f1 0.709072\n",
      "\n",
      "2018-04-22T20:13:24.776510: step 381, loss 0.280128, acc 0.9\n",
      "2018-04-22T20:13:24.912516: step 382, loss 0.214882, acc 0.91\n",
      "2018-04-22T20:13:25.048515: step 383, loss 0.281409, acc 0.91\n",
      "2018-04-22T20:13:25.090167: step 384, loss 0.468986, acc 0.893617\n",
      "2018-04-22T20:13:25.233319: step 385, loss 0.260046, acc 0.92\n",
      "2018-04-22T20:13:25.368494: step 386, loss 0.213405, acc 0.88\n",
      "2018-04-22T20:13:25.504030: step 387, loss 0.362864, acc 0.89\n",
      "2018-04-22T20:13:25.632526: step 388, loss 0.291508, acc 0.905\n",
      "2018-04-22T20:13:25.767131: step 389, loss 0.216282, acc 0.93\n",
      "2018-04-22T20:13:25.893039: step 390, loss 0.278956, acc 0.91\n",
      "2018-04-22T20:13:26.032518: step 391, loss 0.361182, acc 0.885\n",
      "2018-04-22T20:13:26.156496: step 392, loss 0.307135, acc 0.9\n",
      "2018-04-22T20:13:26.284510: step 393, loss 0.179817, acc 0.93\n",
      "2018-04-22T20:13:26.406931: step 394, loss 0.279705, acc 0.92\n",
      "2018-04-22T20:13:26.539983: step 395, loss 0.418691, acc 0.895\n",
      "2018-04-22T20:13:26.669899: step 396, loss 0.560172, acc 0.84\n",
      "2018-04-22T20:13:26.804504: step 397, loss 0.175029, acc 0.935\n",
      "2018-04-22T20:13:26.932643: step 398, loss 0.361726, acc 0.885\n",
      "2018-04-22T20:13:27.072489: step 399, loss 0.49659, acc 0.89\n",
      "2018-04-22T20:13:27.200653: step 400, loss 0.466275, acc 0.885\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:27.668553: step 400, loss 0.410744, acc 0.805926, rec 0.991935, pre 0.72508, f1 0.837771\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524427953/checkpoints/model-400\n",
      "\n",
      "2018-04-22T20:13:27.908481: step 401, loss 0.382634, acc 0.895\n",
      "2018-04-22T20:13:28.034596: step 402, loss 0.510417, acc 0.85\n",
      "2018-04-22T20:13:28.162665: step 403, loss 0.440116, acc 0.91\n",
      "2018-04-22T20:13:28.287053: step 404, loss 0.376204, acc 0.895\n",
      "2018-04-22T20:13:28.420487: step 405, loss 0.273186, acc 0.915\n",
      "2018-04-22T20:13:28.549234: step 406, loss 0.163845, acc 0.925\n",
      "2018-04-22T20:13:28.684536: step 407, loss 0.199351, acc 0.94\n",
      "2018-04-22T20:13:28.812490: step 408, loss 0.443437, acc 0.87\n",
      "2018-04-22T20:13:28.945283: step 409, loss 0.206189, acc 0.935\n",
      "2018-04-22T20:13:29.076508: step 410, loss 0.309129, acc 0.93\n",
      "2018-04-22T20:13:29.205366: step 411, loss 0.443049, acc 0.87\n",
      "2018-04-22T20:13:29.333870: step 412, loss 0.315695, acc 0.915\n",
      "2018-04-22T20:13:29.464999: step 413, loss 0.408482, acc 0.9\n",
      "2018-04-22T20:13:29.587150: step 414, loss 0.25832, acc 0.92\n",
      "2018-04-22T20:13:29.721788: step 415, loss 0.26871, acc 0.905\n",
      "2018-04-22T20:13:29.846912: step 416, loss 0.256115, acc 0.915\n",
      "2018-04-22T20:13:29.976514: step 417, loss 0.340804, acc 0.89\n",
      "2018-04-22T20:13:30.100502: step 418, loss 0.18724, acc 0.93\n",
      "2018-04-22T20:13:30.228523: step 419, loss 0.232109, acc 0.915\n",
      "2018-04-22T20:13:30.355095: step 420, loss 0.203688, acc 0.955\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:30.816722: step 420, loss 0.499773, acc 0.748889, rec 0.994868, pre 0.669132, f1 0.800118\n",
      "\n",
      "2018-04-22T20:13:30.947603: step 421, loss 0.326667, acc 0.89\n",
      "2018-04-22T20:13:31.081381: step 422, loss 0.231597, acc 0.93\n",
      "2018-04-22T20:13:31.212538: step 423, loss 0.189234, acc 0.93\n",
      "2018-04-22T20:13:31.347961: step 424, loss 0.25237, acc 0.915\n",
      "2018-04-22T20:13:31.473981: step 425, loss 0.27481, acc 0.89\n",
      "2018-04-22T20:13:31.612481: step 426, loss 0.133329, acc 0.95\n",
      "2018-04-22T20:13:31.736495: step 427, loss 0.270515, acc 0.865\n",
      "2018-04-22T20:13:31.872509: step 428, loss 0.260864, acc 0.88\n",
      "2018-04-22T20:13:31.990851: step 429, loss 0.278802, acc 0.875\n",
      "2018-04-22T20:13:32.118712: step 430, loss 0.314739, acc 0.86\n",
      "2018-04-22T20:13:32.244662: step 431, loss 0.312544, acc 0.905\n",
      "2018-04-22T20:13:32.292511: step 432, loss 0.163809, acc 0.93617\n",
      "2018-04-22T20:13:32.429044: step 433, loss 0.181401, acc 0.925\n",
      "2018-04-22T20:13:32.556843: step 434, loss 0.232646, acc 0.92\n",
      "2018-04-22T20:13:32.690472: step 435, loss 0.218535, acc 0.92\n",
      "2018-04-22T20:13:32.820517: step 436, loss 0.274643, acc 0.885\n",
      "2018-04-22T20:13:32.956537: step 437, loss 0.262952, acc 0.895\n",
      "2018-04-22T20:13:33.084491: step 438, loss 0.181775, acc 0.945\n",
      "2018-04-22T20:13:33.211922: step 439, loss 0.265378, acc 0.915\n",
      "2018-04-22T20:13:33.348524: step 440, loss 0.175952, acc 0.945\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:33.808516: step 440, loss 0.738966, acc 0.658518, rec 0.998534, pre 0.596845, f1 0.74712\n",
      "\n",
      "2018-04-22T20:13:33.927327: step 441, loss 0.243396, acc 0.92\n",
      "2018-04-22T20:13:34.060483: step 442, loss 0.200039, acc 0.93\n",
      "2018-04-22T20:13:34.196558: step 443, loss 0.227114, acc 0.9\n",
      "2018-04-22T20:13:34.328531: step 444, loss 0.184796, acc 0.91\n",
      "2018-04-22T20:13:34.457822: step 445, loss 0.272075, acc 0.89\n",
      "2018-04-22T20:13:34.592517: step 446, loss 0.193445, acc 0.93\n",
      "2018-04-22T20:13:34.724474: step 447, loss 0.296543, acc 0.91\n",
      "2018-04-22T20:13:34.851948: step 448, loss 0.229936, acc 0.91\n",
      "2018-04-22T20:13:34.984525: step 449, loss 0.170198, acc 0.94\n",
      "2018-04-22T20:13:35.111055: step 450, loss 0.281425, acc 0.89\n",
      "2018-04-22T20:13:35.243659: step 451, loss 0.209985, acc 0.915\n",
      "2018-04-22T20:13:35.378075: step 452, loss 0.204408, acc 0.91\n",
      "2018-04-22T20:13:35.508911: step 453, loss 0.231039, acc 0.905\n",
      "2018-04-22T20:13:35.640524: step 454, loss 0.224097, acc 0.95\n",
      "2018-04-22T20:13:35.771373: step 455, loss 0.271889, acc 0.885\n",
      "2018-04-22T20:13:35.903377: step 456, loss 0.32468, acc 0.89\n",
      "2018-04-22T20:13:36.028506: step 457, loss 0.235688, acc 0.915\n",
      "2018-04-22T20:13:36.158954: step 458, loss 0.38843, acc 0.855\n",
      "2018-04-22T20:13:36.289788: step 459, loss 0.199007, acc 0.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-22T20:13:36.420488: step 460, loss 0.373743, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:36.879645: step 460, loss 1.00281, acc 0.611481, rec 0.997067, pre 0.565489, f1 0.721677\n",
      "\n",
      "2018-04-22T20:13:37.012988: step 461, loss 0.268901, acc 0.915\n",
      "2018-04-22T20:13:37.141097: step 462, loss 0.265059, acc 0.91\n",
      "2018-04-22T20:13:37.276853: step 463, loss 0.287743, acc 0.885\n",
      "2018-04-22T20:13:37.408513: step 464, loss 0.283437, acc 0.89\n",
      "2018-04-22T20:13:37.540500: step 465, loss 0.217165, acc 0.925\n",
      "2018-04-22T20:13:37.665814: step 466, loss 0.216113, acc 0.93\n",
      "2018-04-22T20:13:37.796541: step 467, loss 0.292435, acc 0.885\n",
      "2018-04-22T20:13:37.932518: step 468, loss 0.437926, acc 0.845\n",
      "2018-04-22T20:13:38.056507: step 469, loss 0.28762, acc 0.885\n",
      "2018-04-22T20:13:38.196525: step 470, loss 0.360127, acc 0.88\n",
      "2018-04-22T20:13:38.320515: step 471, loss 0.276024, acc 0.87\n",
      "2018-04-22T20:13:38.456522: step 472, loss 0.38524, acc 0.86\n",
      "2018-04-22T20:13:38.584514: step 473, loss 0.282183, acc 0.895\n",
      "2018-04-22T20:13:38.720502: step 474, loss 0.398929, acc 0.895\n",
      "2018-04-22T20:13:38.849023: step 475, loss 0.287336, acc 0.905\n",
      "2018-04-22T20:13:38.978732: step 476, loss 0.288519, acc 0.925\n",
      "2018-04-22T20:13:39.106810: step 477, loss 0.375695, acc 0.875\n",
      "2018-04-22T20:13:39.244519: step 478, loss 0.32179, acc 0.88\n",
      "2018-04-22T20:13:39.376522: step 479, loss 0.319646, acc 0.885\n",
      "2018-04-22T20:13:39.415654: step 480, loss 0.322961, acc 0.893617\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:39.864523: step 480, loss 0.991515, acc 0.619259, rec 0.996334, pre 0.570529, f1 0.725574\n",
      "\n",
      "\n",
      "Test Set:\n",
      "2018-04-22T20:13:40.124536: step 480, loss 0.978831, acc 0.622222, rec 0.994203, pre 0.575503, f1 0.729012\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", 0.75, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "\n",
    "# embedding of 60 is best so far\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\",60, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"1\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 60, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.4, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 200\n",
    "tf.flags.DEFINE_integer(\"batch_size\", batch_size, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 10, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 20, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=train_s.shape[1],\n",
    "            num_classes=2,\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "        \n",
    "         # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        #vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch1, x_batch2, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch1, x_batch2, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            #r \n",
    "            step, summaries, loss, accuracy, recall, precision, f1 = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy,cnn.recall, cnn.precision, cnn.f1_score],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, rec {:g}, pre {:g}, f1 {:g}\".format(time_str, step, loss, accuracy, recall, precision, f1))# recall, precision, f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "        def error_analysis(x_batch1, x_batch2, y_batch, writer=None):\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            #r \n",
    "            step, summaries, loss, accuracy, recall, precision, f1, correct, scores,predictions  = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy,cnn.recall, cnn.precision, cnn.f1_score, cnn.correct_predictions, cnn.scores, cnn.predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, rec {:g}, pre {:g}, f1 {:g}\".format(time_str, step, loss, accuracy, recall, precision, f1))# recall, precision, f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "            return correct, scores, predictions\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(train_s, train_c,  expanded_train_labels)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch1, x_batch2, y_batch = zip(*batch)\n",
    "            train_step(x_batch1, x_batch1, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(validation_s, validation_c, expanded_validation_labels, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "print(\"\\nTest Set:\")\n",
    "correct, logits, predictions = error_analysis(test_s, test_c, expanded_test_labels, writer=dev_summary_writer)\n",
    "print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10.427931, 878],\n",
       " [7.810832, 1134],\n",
       " [7.0946589, 487],\n",
       " [7.0870981, 748],\n",
       " [7.0222578, 1347],\n",
       " [6.9785132, 1186],\n",
       " [6.8468637, 572],\n",
       " [6.7497377, 739],\n",
       " [6.490921, 211],\n",
       " [6.1383171, 263],\n",
       " [6.097518, 637],\n",
       " [5.8480711, 508],\n",
       " [5.7118001, 642],\n",
       " [5.5798311, 462],\n",
       " [5.5189023, 936],\n",
       " [5.4357672, 638],\n",
       " [5.4254031, 1196],\n",
       " [5.2816677, 808],\n",
       " [5.2653756, 1339],\n",
       " [5.2634859, 692],\n",
       " [5.1504669, 431],\n",
       " [5.0838499, 12],\n",
       " [5.0480127, 1179],\n",
       " [5.0367317, 1146],\n",
       " [4.9859791, 1148],\n",
       " [4.97154, 57],\n",
       " [4.9678736, 150],\n",
       " [4.9638543, 293],\n",
       " [4.9600563, 900],\n",
       " [4.9468994, 34],\n",
       " [4.9449244, 479],\n",
       " [4.8697815, 956],\n",
       " [4.8132849, 1239],\n",
       " [4.7893858, 308],\n",
       " [4.7448115, 174],\n",
       " [4.7447433, 955],\n",
       " [4.722105, 1214],\n",
       " [4.6911125, 99],\n",
       " [4.678916, 286],\n",
       " [4.6609955, 200],\n",
       " [4.6553402, 507],\n",
       " [4.6498499, 859],\n",
       " [4.5964808, 454],\n",
       " [4.5920887, 988],\n",
       " [4.5636892, 952],\n",
       " [4.5295076, 1202],\n",
       " [4.5151892, 1231],\n",
       " [4.4920115, 1092],\n",
       " [4.47861, 287],\n",
       " [4.4659796, 876],\n",
       " [4.4151459, 70],\n",
       " [4.3836203, 1237],\n",
       " [4.3342419, 1253],\n",
       " [4.2981286, 515],\n",
       " [4.2800412, 851],\n",
       " [4.2757845, 939],\n",
       " [4.2722178, 396],\n",
       " [4.249608, 689],\n",
       " [4.242919, 229],\n",
       " [4.2181158, 82],\n",
       " [4.2078943, 153],\n",
       " [4.1857967, 448],\n",
       " [4.1759443, 1062],\n",
       " [4.1468601, 1109],\n",
       " [4.1178594, 279],\n",
       " [4.1103754, 213],\n",
       " [4.1086369, 1200],\n",
       " [4.0789905, 807],\n",
       " [4.0619345, 1260],\n",
       " [4.0243196, 76],\n",
       " [4.0210028, 1212],\n",
       " [3.997735, 660],\n",
       " [3.9832721, 927],\n",
       " [3.9763687, 291],\n",
       " [3.9695818, 222],\n",
       " [3.964, 1110],\n",
       " [3.9621696, 926],\n",
       " [3.9316897, 387],\n",
       " [3.8760018, 439],\n",
       " [3.8572423, 901],\n",
       " [3.8515706, 915],\n",
       " [3.8389256, 669],\n",
       " [3.8335309, 49],\n",
       " [3.8213954, 276],\n",
       " [3.8129437, 295],\n",
       " [3.8023045, 647],\n",
       " [3.787524, 84],\n",
       " [3.7640052, 1188],\n",
       " [3.7541046, 199],\n",
       " [3.739681, 1224],\n",
       " [3.7160468, 1331],\n",
       " [3.7091267, 534],\n",
       " [3.7034805, 553],\n",
       " [3.6953523, 209],\n",
       " [3.6896009, 605],\n",
       " [3.6894419, 707],\n",
       " [3.6728442, 1052],\n",
       " [3.6043975, 591],\n",
       " [3.5748508, 1159],\n",
       " [3.5536273, 1257],\n",
       " [3.5367076, 540],\n",
       " [3.48106, 729],\n",
       " [3.4777424, 849],\n",
       " [3.4736845, 843],\n",
       " [3.4532223, 111],\n",
       " [3.4390328, 1232],\n",
       " [3.4044952, 194],\n",
       " [3.4040582, 639],\n",
       " [3.3874731, 856],\n",
       " [3.3846295, 495],\n",
       " [3.377248, 821],\n",
       " [3.3683019, 1067],\n",
       " [3.3669119, 728],\n",
       " [3.3564548, 793],\n",
       " [3.3357797, 433],\n",
       " [3.3244722, 87],\n",
       " [3.3244722, 348],\n",
       " [3.3244722, 616],\n",
       " [3.3240309, 498],\n",
       " [3.3207617, 521],\n",
       " [3.318557, 558],\n",
       " [3.3132522, 1161],\n",
       " [3.3109457, 1313],\n",
       " [3.2995789, 650],\n",
       " [3.2883408, 288],\n",
       " [3.2834449, 152],\n",
       " [3.2808788, 236],\n",
       " [3.2754672, 1336],\n",
       " [3.2625725, 630],\n",
       " [3.2593496, 517],\n",
       " [3.2563369, 86],\n",
       " [3.2426307, 795],\n",
       " [3.2378104, 583],\n",
       " [3.2318296, 833],\n",
       " [3.2277184, 913],\n",
       " [3.2245066, 0],\n",
       " [3.2102282, 1191],\n",
       " [3.2056506, 298],\n",
       " [3.2026348, 704],\n",
       " [3.1995389, 46],\n",
       " [3.1935034, 233],\n",
       " [3.1881011, 444],\n",
       " [3.1740746, 1274],\n",
       " [3.1611903, 693],\n",
       " [3.1379154, 283],\n",
       " [3.1366203, 978],\n",
       " [3.1205952, 201],\n",
       " [3.1191497, 530],\n",
       " [3.099206, 282],\n",
       " [3.0730495, 1018],\n",
       " [3.0654051, 351],\n",
       " [3.06234, 10],\n",
       " [3.05337, 585],\n",
       " [3.0495596, 258],\n",
       " [3.0474825, 1259],\n",
       " [3.0412452, 97],\n",
       " [3.0347095, 493],\n",
       " [3.0257101, 154],\n",
       " [3.0218811, 1245],\n",
       " [3.0178783, 1130],\n",
       " [3.0167556, 993],\n",
       " [3.0148559, 251],\n",
       " [2.9968219, 990],\n",
       " [2.9893382, 1032],\n",
       " [2.9441173, 461],\n",
       " [2.9417062, 892],\n",
       " [2.9304676, 1154],\n",
       " [2.9296722, 45],\n",
       " [2.9153872, 1294],\n",
       " [2.913439, 1102],\n",
       " [2.8999, 675],\n",
       " [2.883357, 349],\n",
       " [2.8663785, 672],\n",
       " [2.8648086, 635],\n",
       " [2.8515224, 1078],\n",
       " [2.8469806, 322],\n",
       " [2.83815, 533],\n",
       " [2.7725868, 835],\n",
       " [2.7698965, 171],\n",
       " [2.7604203, 854],\n",
       " [2.7167022, 1344],\n",
       " [2.7029028, 191],\n",
       " [2.7013972, 305],\n",
       " [2.7008576, 1144],\n",
       " [2.6904233, 773],\n",
       " [2.6902933, 188],\n",
       " [2.6814139, 919],\n",
       " [2.6672754, 501],\n",
       " [2.6577015, 872],\n",
       " [2.6548939, 896],\n",
       " [2.6450186, 964],\n",
       " [2.6324608, 107],\n",
       " [2.6066022, 730],\n",
       " [2.6031494, 265],\n",
       " [2.598578, 994],\n",
       " [2.5953832, 522],\n",
       " [2.5880444, 747],\n",
       " [2.583391, 405],\n",
       " [2.5821483, 178],\n",
       " [2.5737822, 485],\n",
       " [2.5221367, 536],\n",
       " [2.4959671, 1228],\n",
       " [2.4808738, 929],\n",
       " [2.4788604, 1009],\n",
       " [2.4773531, 1210],\n",
       " [2.4653983, 1340],\n",
       " [2.4466276, 141],\n",
       " [2.4389844, 664],\n",
       " [2.4388387, 741],\n",
       " [2.4322009, 1113],\n",
       " [2.4263813, 256],\n",
       " [2.4218345, 74],\n",
       " [2.4110639, 1291],\n",
       " [2.3910208, 36],\n",
       " [2.3770785, 832],\n",
       " [2.3766289, 1034],\n",
       " [2.3741353, 1172],\n",
       " [2.3654807, 761],\n",
       " [2.3574224, 636],\n",
       " [2.3401115, 891],\n",
       " [2.3308496, 1332],\n",
       " [2.3240767, 1175],\n",
       " [2.3133342, 266],\n",
       " [2.3019087, 1189],\n",
       " [2.3006053, 205],\n",
       " [2.3005364, 2],\n",
       " [2.2615652, 1037],\n",
       " [2.2598681, 207],\n",
       " [2.2591031, 705],\n",
       " [2.2569325, 435],\n",
       " [2.2368567, 1011],\n",
       " [2.2315941, 769],\n",
       " [2.2289846, 1027],\n",
       " [2.1932411, 566],\n",
       " [2.1907361, 427],\n",
       " [2.1898279, 1158],\n",
       " [2.1881928, 837],\n",
       " [2.188108, 85],\n",
       " [2.188108, 836],\n",
       " [2.1809609, 381],\n",
       " [2.1791821, 733],\n",
       " [2.1772933, 192],\n",
       " [2.1761489, 541],\n",
       " [2.1705117, 918],\n",
       " [2.1700866, 571],\n",
       " [2.1670957, 29],\n",
       " [2.1664305, 717],\n",
       " [2.1585472, 1045],\n",
       " [2.1454108, 512],\n",
       " [2.1395216, 1072],\n",
       " [2.1304984, 1047],\n",
       " [2.1234627, 83],\n",
       " [2.1093433, 656],\n",
       " [2.108408, 159],\n",
       " [2.1051531, 1199],\n",
       " [2.094511, 1101],\n",
       " [2.0942564, 1070],\n",
       " [2.0911708, 1201],\n",
       " [2.0906272, 861],\n",
       " [2.0871611, 77],\n",
       " [2.08533, 866],\n",
       " [2.0699008, 951],\n",
       " [2.0699008, 995],\n",
       " [2.0610638, 1028],\n",
       " [2.0553095, 329],\n",
       " [2.0528662, 361],\n",
       " [2.051614, 899],\n",
       " [2.0446138, 981],\n",
       " [2.0373476, 1167],\n",
       " [2.0307755, 1204],\n",
       " [2.0297275, 1193],\n",
       " [2.0123434, 219],\n",
       " [2.0102196, 254],\n",
       " [2.0010369, 1083],\n",
       " [1.9772285, 562],\n",
       " [1.9703658, 398],\n",
       " [1.9659737, 1198],\n",
       " [1.9650941, 437],\n",
       " [1.9630542, 723],\n",
       " [1.9303355, 888],\n",
       " [1.9242934, 384],\n",
       " [1.9212152, 290],\n",
       " [1.9208653, 246],\n",
       " [1.9090869, 357],\n",
       " [1.9032885, 972],\n",
       " [1.8969072, 476],\n",
       " [1.8937122, 332],\n",
       " [1.8849809, 712],\n",
       " [1.8740135, 460],\n",
       " [1.870954, 28],\n",
       " [1.8550581, 765],\n",
       " [1.8415356, 883],\n",
       " [1.8402245, 47],\n",
       " [1.817643, 139],\n",
       " [1.8119929, 131],\n",
       " [1.8107042, 784],\n",
       " [1.803133, 26],\n",
       " [1.7894056, 267],\n",
       " [1.7708073, 579],\n",
       " [1.7562904, 1160],\n",
       " [1.7562193, 975],\n",
       " [1.748342, 958],\n",
       " [1.7458365, 371],\n",
       " [1.7427008, 1229],\n",
       " [1.7389936, 9],\n",
       " [1.7371571, 858],\n",
       " [1.7297565, 740],\n",
       " [1.7197571, 1322],\n",
       " [1.7184688, 237],\n",
       " [1.717298, 1089],\n",
       " [1.7017807, 326],\n",
       " [1.701057, 104],\n",
       " [1.6898978, 275],\n",
       " [1.6636691, 1203],\n",
       " [1.65819, 526],\n",
       " [1.6579862, 343],\n",
       " [1.6579862, 640],\n",
       " [1.6482055, 1036],\n",
       " [1.6403732, 8],\n",
       " [1.6248178, 59],\n",
       " [1.6207142, 538],\n",
       " [1.6134009, 703],\n",
       " [1.612221, 980],\n",
       " [1.6120758, 1195],\n",
       " [1.6055912, 1303],\n",
       " [1.5841851, 721],\n",
       " [1.5780998, 143],\n",
       " [1.5576558, 1170],\n",
       " [1.5333886, 573],\n",
       " [1.5103033, 755],\n",
       " [1.5041418, 412],\n",
       " [1.4976015, 1293],\n",
       " [1.4859803, 1133],\n",
       " [1.4859595, 260],\n",
       " [1.482264, 1014],\n",
       " [1.4803909, 130],\n",
       " [1.4762068, 1006],\n",
       " [1.4656994, 206],\n",
       " [1.464246, 802],\n",
       " [1.4579217, 176],\n",
       " [1.4262807, 1267],\n",
       " [1.4241221, 58],\n",
       " [1.4103372, 622],\n",
       " [1.399846, 6],\n",
       " [1.3948691, 984],\n",
       " [1.3899369, 574],\n",
       " [1.3874059, 780],\n",
       " [1.3826172, 69],\n",
       " [1.3781638, 1329],\n",
       " [1.368089, 1140],\n",
       " [1.3650336, 1008],\n",
       " [1.3641706, 1017],\n",
       " [1.3571932, 390],\n",
       " [1.3499775, 794],\n",
       " [1.3213737, 1118],\n",
       " [1.3130219, 875],\n",
       " [1.3035235, 992],\n",
       " [1.3015714, 1298],\n",
       " [1.2978439, 931],\n",
       " [1.2978439, 1003],\n",
       " [1.2871101, 1178],\n",
       " [1.2328315, 907],\n",
       " [1.2321453, 372],\n",
       " [1.2309426, 1258],\n",
       " [1.2244638, 1304],\n",
       " [1.2244233, 478],\n",
       " [1.2208869, 210],\n",
       " [1.2154219, 749],\n",
       " [1.2004437, 962],\n",
       " [1.1748507, 506],\n",
       " [1.1646715, 667],\n",
       " [1.1637416, 700],\n",
       " [1.16101, 1268],\n",
       " [1.1557016, 319],\n",
       " [1.1523173, 177],\n",
       " [1.1504227, 35],\n",
       " [1.1443465, 418],\n",
       " [1.1443465, 475],\n",
       " [1.1443465, 1306],\n",
       " [1.1382802, 1162],\n",
       " [1.1242346, 490],\n",
       " [1.1229937, 1334],\n",
       " [1.1067903, 1050],\n",
       " [1.1005956, 910],\n",
       " [1.0939057, 709],\n",
       " [1.0777812, 834],\n",
       " [1.0736582, 826],\n",
       " [1.0570846, 965],\n",
       " [1.0570846, 1327],\n",
       " [1.0358791, 1187],\n",
       " [1.0349579, 422],\n",
       " [1.0225315, 397],\n",
       " [1.0095143, 402],\n",
       " [0.98986644, 416],\n",
       " [0.97124833, 1147],\n",
       " [0.9573611, 759],\n",
       " [0.95162451, 842],\n",
       " [0.94809633, 264],\n",
       " [0.94803768, 157],\n",
       " [0.93732035, 1125],\n",
       " [0.93111712, 655],\n",
       " [0.92396551, 67],\n",
       " [0.90547723, 1061],\n",
       " [0.90487766, 592],\n",
       " [0.894476, 312],\n",
       " [0.89336342, 30],\n",
       " [0.88905925, 611],\n",
       " [0.88409323, 649],\n",
       " [0.86612916, 1290],\n",
       " [0.86574847, 532],\n",
       " [0.8516019, 961],\n",
       " [0.84562927, 519],\n",
       " [0.81781274, 582],\n",
       " [0.81571013, 21],\n",
       " [0.79057449, 1252],\n",
       " [0.78895688, 261],\n",
       " [0.78544199, 505],\n",
       " [0.76984298, 367],\n",
       " [0.76072723, 376],\n",
       " [0.74174148, 848],\n",
       " [0.74172163, 814],\n",
       " [0.74103951, 299],\n",
       " [0.73293567, 1055],\n",
       " [0.6831134, 1004],\n",
       " [0.68297285, 798],\n",
       " [0.67158699, 1123],\n",
       " [0.66796255, 410],\n",
       " [0.65453261, 464],\n",
       " [0.65453261, 634],\n",
       " [0.65093672, 1048],\n",
       " [0.64674973, 202],\n",
       " [0.6426726, 442],\n",
       " [0.64064813, 1319],\n",
       " [0.63907593, 785],\n",
       " [0.63821566, 597],\n",
       " [0.63345534, 1142],\n",
       " [0.62987345, 967],\n",
       " [0.6288895, 243],\n",
       " [0.61534762, 1217],\n",
       " [0.5911746, 715],\n",
       " [0.58582002, 1335],\n",
       " [0.55860883, 987],\n",
       " [0.5582521, 852],\n",
       " [0.54064882, 554],\n",
       " [0.53177989, 666],\n",
       " [0.52858806, 595],\n",
       " [0.52693325, 959],\n",
       " [0.51979774, 1289],\n",
       " [0.51595604, 496],\n",
       " [0.51211262, 1042],\n",
       " [0.50631112, 142],\n",
       " [0.49430382, 1285],\n",
       " [0.49304694, 220],\n",
       " [0.48545253, 424],\n",
       " [0.46111482, 458],\n",
       " [0.45946044, 108],\n",
       " [0.45946044, 226],\n",
       " [0.45946044, 277],\n",
       " [0.45946044, 354],\n",
       " [0.45946044, 726],\n",
       " [0.45946044, 855],\n",
       " [0.45066202, 714],\n",
       " [0.4407326, 1040],\n",
       " [0.43680537, 1141],\n",
       " [0.43680537, 1166],\n",
       " [0.42864382, 1087],\n",
       " [0.4276287, 1093],\n",
       " [0.41884309, 272],\n",
       " [0.4056474, 549],\n",
       " [0.40149754, 249],\n",
       " [0.38066381, 325],\n",
       " [0.34402275, 1153],\n",
       " [0.33331543, 373],\n",
       " [0.32484198, 436],\n",
       " [0.3225019, 670],\n",
       " [0.31077135, 688],\n",
       " [0.30924988, 633],\n",
       " [0.30742741, 353],\n",
       " [0.30304599, 158],\n",
       " [0.30038995, 935],\n",
       " [0.29965752, 678],\n",
       " [0.29870623, 255],\n",
       " [0.29432893, 102],\n",
       " [0.27541685, 101],\n",
       " [0.25757128, 1145],\n",
       " [0.23834172, 1280],\n",
       " [0.23130667, 969],\n",
       " [0.23080492, 1164],\n",
       " [0.19825268, 867],\n",
       " [0.19562674, 989],\n",
       " [0.17786282, 218],\n",
       " [0.17765903, 40],\n",
       " [0.17765903, 310],\n",
       " [0.16128224, 144],\n",
       " [0.12917495, 743],\n",
       " [0.121759, 1254],\n",
       " [0.11277896, 420],\n",
       " [0.10490608, 1114],\n",
       " [0.098564982, 463],\n",
       " [0.09636271, 1105],\n",
       " [0.094611585, 1183],\n",
       " [0.078581572, 811],\n",
       " [0.069042385, 80],\n",
       " [0.060549021, 776],\n",
       " [0.048381627, 1235],\n",
       " [0.047670782, 356],\n",
       " [0.045074582, 409],\n",
       " [0.033253074, 731],\n",
       " [0.010906339, 284],\n",
       " [0.010906339, 938]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def incorrect_confidence(wrong, logits, predictions):\n",
    "    indeces = np.where(wrong)\n",
    "    wrong_predictions = predictions[indeces]\n",
    "    wrong_logits = logits[indeces]\n",
    "    \n",
    "    return [[wrong_logits[i][value] - wrong_logits[i][1-value], indeces[0][i]] for i, value in enumerate(wrong_predictions)]\n",
    "wrong = correct == False\n",
    "\n",
    "sorted(incorrect_confidence(wrong, logits, predictions), key = lambda logit: -logit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'is', 'with', 'that', '?', 'theyre', 'only', 'going', 'to', 'blame', 'the', 'guns', 'now', '?', 'no', 'people', '?', 'at', 'all', '?', 'oh', 'yeah', 'they', 'will', 'blame', 'trump', '.', 'there', 'is', 'no', 'agenda', 'though', '...', 'LINK']\n",
      "['nan']\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(test_sentences[878])\n",
    "print(test_contexts[878])\n",
    "print(test_labels[878])\n",
    "print(predictions[878])\n",
    "# last senetence is definitely sarcasm. Flaw in our methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['retweet', 'not', 'sure', 'about', 'the', 'cricket', 'ball', ',', 'but', 'bancroft', '’', 's', 'balls', 'must', 'be', 'reverse-swinging', 'quiet', 'a', 'bit', 'after', 'yesterday', '’', 's', 'events', '.', '😂', '😂', '😂', '…']\n",
      "['nan']\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(test_sentences[1347])\n",
    "print(test_contexts[1347])\n",
    "print(test_labels[1347])\n",
    "print(predictions[1347])\n",
    "\n",
    "# the emojis are throwing this one off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'history', 'isn', '’', 't', 'being', 'altered', 'by', 'federal', 'govt', 'education', 'how', 'do', 'i', 'know', 'i', 'heard', 'mrs', 'obama', 'say', 'it', 'right', 'out', 'of', 'her', 'mouth', 'using', 'the', 'word', '(', '(', '(', '#alter', '#history', ')', ')', ')', 'main', 'while', 'in', 'history', 'this', 'was', 'taught', '#vtsen', 'LINK']\n",
      "['nan']\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(test_sentences[293])\n",
    "print(test_contexts[293])\n",
    "print(test_labels[293])\n",
    "print(predictions[293])\n",
    "\n",
    "# hard to say what's going on here. seems to be slightly sarcastic, not really sure though. Throwing me off too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'science', 'form', 'scratch', 'vs', 'python', 'for', 'data', 'analysis', '?', 'LINK', '#datascience', 'what', 'do', 'you', 'guys', 'think', '?', 'i', 'read', 'the', 'first', 'DG', 'chapters', 'of', 'scratch', 'and', 'maybe', 'it', 'was', 'me', 'but', 'i', 'felt', 'he', 'was', 'just', 'putting', 'out', 'code', 'and', 'not', 'explaining', 'it', '.', 'is', 'data', 'analysis', 'just', 'as', 'good', '?', 'subm', '…']\n",
      "['nan']\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(test_sentences[398])\n",
    "print(test_contexts[398])\n",
    "print(test_labels[398])\n",
    "print(predictions[398])\n",
    "\n",
    "# might potentially be sarcastic. Asking if the data analysis book is just as good after saying that the book \n",
    "# was mostly code dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['retweet', 'sport', 'has', 'always', 'been', 'said', 'to', 'be', 'a', 'mirror', 'of', 'society', '.', 'so', 'why', 'all', 'the', 'fuss', 'over', '#cricket', '#cheats', '?', 'it', '’', 's', 'pretty', 'obvious', 'that', 'w', '…']\n",
      "['nan']\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(test_sentences[462])\n",
    "print(test_contexts[462])\n",
    "print(test_labels[462])\n",
    "print(predictions[462])\n",
    "\n",
    "# interesting example. I guess since the tweet is criticising society for encouraging cheating but then\n",
    "# fussing over cheaters in cricket is throwing it off. Maybe this is a form of sarcasm? I'm not totally sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'thought-provoking', 'image', 'of', 'the', 'day', '#tpiotd', '#humor', '#funny', '#laughs', '#absurd', '#football', '#futball', '#soccer', '#thanks', '#god', '#astronaut', '#space', '#thanksgiving', '#sponsors', '#goals', '#match', '#turf', '#celebration', '#motivation', '#someoneiswatching', '#chillingtime', '#fans', '#bandwagon', '#worldcup2018', '#cool', 'LINK']\n",
      "['nan']\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# check if tweet 1280 is improved\n",
    "\n",
    "print(test_sentences[1280])\n",
    "print(test_contexts[1280])\n",
    "print(test_labels[1280])\n",
    "print(predictions[1280])\n",
    "\n",
    "# no longer top 5 biggest issues, but still incorrectly tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
