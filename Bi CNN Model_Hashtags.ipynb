{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saulgrimaldo1/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from w266_common import utils, vocabulary\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "np.random.seed(266)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "tokenizer = TweetTokenizer()\n",
    "x_data = []\n",
    "x_contexts = []\n",
    "labels = []\n",
    "sentences  = []\n",
    "contexts = []\n",
    "with open('merged_data_v4.csv', 'r') as csvfile:\n",
    "    linereader = csv.reader(csvfile, delimiter = '|')\n",
    "    for i, row in enumerate(linereader):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        sentence, context, sarcasm = row\n",
    "        sentence = re.sub(\"RT @[^\\s]+:\", \"retweet\", sentence)\n",
    "        sentences.append(sentence)\n",
    "        contexts.append(context)\n",
    "        x_tokens = utils.canonicalize_words(tokenizer.tokenize(sentence))\n",
    "        context_tokens = utils.canonicalize_words(tokenizer.tokenize(context))\n",
    "        x_data.append(x_tokens)\n",
    "        x_contexts.append(context_tokens)\n",
    "        labels.append(int(sarcasm))\n",
    "\n",
    "\n",
    "#rng = np.random.RandomState(5)\n",
    "#rng.shuffle(x_data)  # in-place\n",
    "#train_split_idx = int(0.7 * len(labels))\n",
    "#test_split_idx  = int(0.9 * len(labels))\n",
    "\n",
    "shuffle_indices = np.random.permutation(np.arange(len(labels)))\n",
    "train_split_idx = int(0.6 * len(labels))\n",
    "test_split_idx  = int(0.9 * len(labels))\n",
    "\n",
    "train_indices = shuffle_indices[:train_split_idx]\n",
    "validation_indices = shuffle_indices[train_split_idx:test_split_idx]\n",
    "test_indices = shuffle_indices[test_split_idx:]\n",
    "\n",
    "\n",
    "train_sentences = np.array(x_data)[train_indices]\n",
    "train_contexts = np.array(x_contexts)[train_indices]\n",
    "train_labels= np.array(labels)[train_indices] \n",
    "validation_sentences = np.array(x_data)[validation_indices]\n",
    "validation_labels = np.array(labels)[validation_indices]\n",
    "validation_contexts = train_contexts = np.array(x_contexts)[validation_indices]\n",
    "test_sentences = np.array(x_data)[test_indices]  \n",
    "test_contexts = train_contexts = np.array(x_contexts)[test_indices]\n",
    "test_labels = np.array(labels)[test_indices]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8098,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_labels(raw_label_set, size):\n",
    "    label_set = []\n",
    "    for label in raw_label_set:\n",
    "        labels = [0] * size\n",
    "        labels[label] = 1\n",
    "        label_set.append(labels)\n",
    "    return np.array(label_set)\n",
    "\n",
    "expanded_train_labels = transform_labels(train_labels, 2)\n",
    "expanded_validation_labels = transform_labels(validation_labels,2)\n",
    "expanded_test_labels = transform_labels(test_labels,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4, 5, 6, 7, 8, 8, 1, 5, 6, 7]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,3,4,5,6,7,8,8,1,5,6,7]\n",
    "a + [\"<PADDING>\"]*0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'angry',\n",
       " 'man',\n",
       " 'is',\n",
       " 'angry',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PaddingAndTruncating:\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def pad_or_truncate(self, sentence):\n",
    "        sen_len = len(sentence)\n",
    "        paddings = self.max_len - sen_len\n",
    "        if paddings >=0:\n",
    "            return list(sentence) + [\"<PADDING>\"] * paddings\n",
    "        return sentence[0:paddings]\n",
    "        \n",
    "PadAndTrunc = PaddingAndTruncating(10)\n",
    "        \n",
    "        \n",
    "PadAndTrunc.pad_or_truncate([\"the\",\"angry\",\"man\",\"is\",\"angry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  11,  923,   20, ...,    3,    3,    3],\n",
       "       [  11,   79,   40, ...,    3,    3,    3],\n",
       "       [  94,  652,    2, ...,    3,    3,    3],\n",
       "       ..., \n",
       "       [  11,    2,  259, ...,    3,    3,    3],\n",
       "       [  11,  996, 1192, ...,    3,    3,    3],\n",
       "       [   5,    5,    5, ...,    3,    3,    3]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.contrib import learn\n",
    "vocab_size = 2500\n",
    "#max_len = max([len(sent) for sent  in train_sentences])\n",
    "PadAndTrunc =PaddingAndTruncating(75)\n",
    "train_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, train_sentences))\n",
    "train_context_padded = list(map(PadAndTrunc.pad_or_truncate, train_contexts))\n",
    "validation_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, validation_sentences))\n",
    "validation_context_padded = list(map(PadAndTrunc.pad_or_truncate, validation_contexts))\n",
    "test_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, test_sentences))\n",
    "test_context_padded = list(map(PadAndTrunc.pad_or_truncate, test_contexts))\n",
    "\n",
    "\n",
    "vocab = vocabulary.Vocabulary(utils.flatten(list(train_sentences_padded) + list(train_context_padded)), vocab_size)\n",
    "train_s = np.array(list(map(vocab.words_to_ids, train_sentences_padded)))\n",
    "train_c = np.array(list(map(vocab.words_to_ids, train_context_padded)))\n",
    "validation_s = np.array(list(map(vocab.words_to_ids, validation_sentences_padded)))\n",
    "validation_c = np.array(list(map(vocab.words_to_ids, validation_context_padded)))\n",
    "test_s = np.array(list(map(vocab.words_to_ids, test_sentences_padded)))\n",
    "test_c = np.array(list(map(vocab.words_to_ids, test_context_padded)))\n",
    "train_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv-maxpool-3'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 3\n",
    "(\"conv-maxpool-%s\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size, \n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x1 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x1\")\n",
    "        self.input_x2 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x2\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars1 = tf.nn.embedding_lookup(self.W, self.input_x1)\n",
    "            self.embedded_chars_expanded1 = tf.expand_dims(self.embedded_chars1, -1)\n",
    "            self.embedded_chars2 = tf.nn.embedding_lookup(self.W, self.input_x2)\n",
    "            self.embedded_chars_expanded2 = tf.expand_dims(self.embedded_chars2, -1)\n",
    "\n",
    "        # Create a convolution + avgpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.variable_scope(\"conv-avgpool-%s\" % i) as scope:\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "              \n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded1,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h1 = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"tanh1\")\n",
    "               \n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.avg_pool(\n",
    "                    h1,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                \n",
    "                pooled_outputs.append(pooled)\n",
    "                #pooled_outputs.append(pooled)\n",
    "                scope.reuse_variables()\n",
    "                 \n",
    "                \n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded2,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h2 = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"tanh2\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled2 = tf.nn.avg_pool(\n",
    "                    h2,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled2)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "\n",
    "        num_filters_total = num_filters * len(filter_sizes) * 2\n",
    "\n",
    "        self.h_pool = tf.concat(pooled_outputs, 1)\n",
    "\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "        \n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "           \n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.labels = tf.argmax(self.input_y, 1)\n",
    "            TP = tf.count_nonzero(self.predictions * self.labels)\n",
    "            TN = tf.count_nonzero((self.predictions - 1) * (self.labels - 1))\n",
    "            FP = tf.count_nonzero(self.predictions * (self.labels - 1))\n",
    "            FN = tf.count_nonzero((self.predictions - 1) * self.labels)\n",
    "            correct_predictions = tf.equal(self.predictions, self.labels)\n",
    "            self.precision = TP / (TP + FP)\n",
    "            self.recall = TP / (TP + FN)\n",
    "            self.f1_score = 2 * self.precision * self.recall / (self.precision + self.recall)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=100\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.75\n",
      "DROPOUT_KEEP_PROB=0.4\n",
      "EMBEDDING_DIM=25\n",
      "EVALUATE_EVERY=20\n",
      "FILTER_SIZES=1\n",
      "L2_REG_LAMBDA=0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.neg\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=50\n",
      "NUM_FILTERS=25\n",
      "POSITIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.pos\n",
      "\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/W:0/grad/hist is illegal; using conv-avgpool-0/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/W:0/grad/sparsity is illegal; using conv-avgpool-0/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/b:0/grad/hist is illegal; using conv-avgpool-0/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/b:0/grad/sparsity is illegal; using conv-avgpool-0/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524369734\n",
      "\n",
      "2018-04-22T04:02:15.544476: step 1, loss 0.748661, acc 0.43\n",
      "2018-04-22T04:02:15.592687: step 2, loss 2.96705, acc 0.44\n",
      "2018-04-22T04:02:15.633689: step 3, loss 1.04952, acc 0.5\n",
      "2018-04-22T04:02:15.674791: step 4, loss 1.97532, acc 0.48\n",
      "2018-04-22T04:02:15.716439: step 5, loss 1.54023, acc 0.44\n",
      "2018-04-22T04:02:15.766655: step 6, loss 0.692488, acc 0.62\n",
      "2018-04-22T04:02:15.807418: step 7, loss 1.1996, acc 0.53\n",
      "2018-04-22T04:02:15.847663: step 8, loss 1.15477, acc 0.5\n",
      "2018-04-22T04:02:15.887940: step 9, loss 1.03053, acc 0.47\n",
      "2018-04-22T04:02:15.931055: step 10, loss 0.711067, acc 0.6\n",
      "2018-04-22T04:02:15.977655: step 11, loss 0.870162, acc 0.61\n",
      "2018-04-22T04:02:16.018272: step 12, loss 0.869814, acc 0.56\n",
      "2018-04-22T04:02:16.058626: step 13, loss 1.10228, acc 0.49\n",
      "2018-04-22T04:02:16.084455: step 14, loss 0.671418, acc 0.64\n",
      "2018-04-22T04:02:16.132476: step 15, loss 0.936261, acc 0.61\n",
      "2018-04-22T04:02:16.169285: step 16, loss 0.850432, acc 0.63\n",
      "2018-04-22T04:02:16.220467: step 17, loss 0.757351, acc 0.57\n",
      "2018-04-22T04:02:16.255333: step 18, loss 0.720927, acc 0.61\n",
      "2018-04-22T04:02:16.296457: step 19, loss 0.790261, acc 0.56\n",
      "2018-04-22T04:02:16.336452: step 20, loss 0.796838, acc 0.54\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:16.969944: step 20, loss 0.63883, acc 0.61225, rec 0.22571, pre 0.965885, f1 0.365913\n",
      "\n",
      "2018-04-22T04:02:17.024541: step 21, loss 0.662875, acc 0.65\n",
      "2018-04-22T04:02:17.064740: step 22, loss 0.625971, acc 0.69\n",
      "2018-04-22T04:02:17.112524: step 23, loss 0.77212, acc 0.54\n",
      "2018-04-22T04:02:17.153617: step 24, loss 0.645829, acc 0.68\n",
      "2018-04-22T04:02:17.206154: step 25, loss 0.64345, acc 0.62\n",
      "2018-04-22T04:02:17.250378: step 26, loss 0.723339, acc 0.53\n",
      "2018-04-22T04:02:17.296511: step 27, loss 0.686085, acc 0.59\n",
      "2018-04-22T04:02:17.319588: step 28, loss 0.550616, acc 0.7\n",
      "2018-04-22T04:02:17.365156: step 29, loss 0.542911, acc 0.68\n",
      "2018-04-22T04:02:17.412514: step 30, loss 0.567114, acc 0.72\n",
      "2018-04-22T04:02:17.454563: step 31, loss 0.585917, acc 0.69\n",
      "2018-04-22T04:02:17.496519: step 32, loss 0.56044, acc 0.73\n",
      "2018-04-22T04:02:17.536539: step 33, loss 0.599551, acc 0.69\n",
      "2018-04-22T04:02:17.580513: step 34, loss 0.642503, acc 0.65\n",
      "2018-04-22T04:02:17.620511: step 35, loss 0.539418, acc 0.72\n",
      "2018-04-22T04:02:17.664535: step 36, loss 0.523804, acc 0.74\n",
      "2018-04-22T04:02:17.704510: step 37, loss 0.562808, acc 0.69\n",
      "2018-04-22T04:02:17.748498: step 38, loss 0.793408, acc 0.67\n",
      "2018-04-22T04:02:17.790890: step 39, loss 0.509422, acc 0.77\n",
      "2018-04-22T04:02:17.832493: step 40, loss 0.522754, acc 0.79\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:18.374695: step 40, loss 0.551798, acc 0.771302, rec 0.592925, pre 0.916089, f1 0.719903\n",
      "\n",
      "2018-04-22T04:02:18.426943: step 41, loss 0.582238, acc 0.7\n",
      "2018-04-22T04:02:18.451383: step 42, loss 0.477303, acc 0.8\n",
      "2018-04-22T04:02:18.496984: step 43, loss 0.521802, acc 0.74\n",
      "2018-04-22T04:02:18.540895: step 44, loss 0.508926, acc 0.74\n",
      "2018-04-22T04:02:18.580484: step 45, loss 0.595906, acc 0.65\n",
      "2018-04-22T04:02:18.621377: step 46, loss 0.610722, acc 0.68\n",
      "2018-04-22T04:02:18.665243: step 47, loss 0.521147, acc 0.78\n",
      "2018-04-22T04:02:18.708539: step 48, loss 0.621313, acc 0.64\n",
      "2018-04-22T04:02:18.745708: step 49, loss 0.428002, acc 0.79\n",
      "2018-04-22T04:02:18.788469: step 50, loss 0.663846, acc 0.66\n",
      "2018-04-22T04:02:18.829475: step 51, loss 0.499748, acc 0.8\n",
      "2018-04-22T04:02:18.872502: step 52, loss 0.473082, acc 0.77\n",
      "2018-04-22T04:02:18.916457: step 53, loss 0.494404, acc 0.78\n",
      "2018-04-22T04:02:18.956476: step 54, loss 0.545046, acc 0.76\n",
      "2018-04-22T04:02:19.004463: step 55, loss 0.580288, acc 0.74\n",
      "2018-04-22T04:02:19.027222: step 56, loss 0.670492, acc 0.74\n",
      "2018-04-22T04:02:19.072525: step 57, loss 0.722956, acc 0.66\n",
      "2018-04-22T04:02:19.117928: step 58, loss 0.575335, acc 0.72\n",
      "2018-04-22T04:02:19.162833: step 59, loss 0.665809, acc 0.69\n",
      "2018-04-22T04:02:19.204350: step 60, loss 0.501384, acc 0.76\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:19.756524: step 60, loss 0.634125, acc 0.532477, rec 0.986547, pre 0.514821, f1 0.676576\n",
      "\n",
      "2018-04-22T04:02:19.804738: step 61, loss 0.597125, acc 0.64\n",
      "2018-04-22T04:02:19.852513: step 62, loss 0.518132, acc 0.75\n",
      "2018-04-22T04:02:19.890841: step 63, loss 0.516051, acc 0.78\n",
      "2018-04-22T04:02:19.932490: step 64, loss 0.507056, acc 0.76\n",
      "2018-04-22T04:02:19.972772: step 65, loss 0.524779, acc 0.76\n",
      "2018-04-22T04:02:20.013756: step 66, loss 0.526693, acc 0.74\n",
      "2018-04-22T04:02:20.060483: step 67, loss 0.391552, acc 0.88\n",
      "2018-04-22T04:02:20.092908: step 68, loss 0.527126, acc 0.72\n",
      "2018-04-22T04:02:20.134064: step 69, loss 0.431538, acc 0.8\n",
      "2018-04-22T04:02:20.160504: step 70, loss 0.421599, acc 0.82\n",
      "2018-04-22T04:02:20.204507: step 71, loss 0.562051, acc 0.75\n",
      "2018-04-22T04:02:20.246569: step 72, loss 0.62281, acc 0.72\n",
      "2018-04-22T04:02:20.292495: step 73, loss 0.497481, acc 0.76\n",
      "2018-04-22T04:02:20.332752: step 74, loss 0.463381, acc 0.75\n",
      "2018-04-22T04:02:20.378810: step 75, loss 0.739371, acc 0.61\n",
      "2018-04-22T04:02:20.430494: step 76, loss 0.495739, acc 0.8\n",
      "2018-04-22T04:02:20.475792: step 77, loss 0.424497, acc 0.8\n",
      "2018-04-22T04:02:20.519051: step 78, loss 0.533162, acc 0.67\n",
      "2018-04-22T04:02:20.561033: step 79, loss 0.432456, acc 0.77\n",
      "2018-04-22T04:02:20.610739: step 80, loss 0.498768, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:21.161475: step 80, loss 0.715681, acc 0.506298, rec 0.992028, pre 0.501007, f1 0.665775\n",
      "\n",
      "2018-04-22T04:02:21.212563: step 81, loss 0.461286, acc 0.76\n",
      "2018-04-22T04:02:21.252534: step 82, loss 0.427675, acc 0.84\n",
      "2018-04-22T04:02:21.300590: step 83, loss 0.727609, acc 0.65\n",
      "2018-04-22T04:02:21.320542: step 84, loss 0.453157, acc 0.82\n",
      "2018-04-22T04:02:21.366410: step 85, loss 0.460942, acc 0.74\n",
      "2018-04-22T04:02:21.413170: step 86, loss 0.389378, acc 0.85\n",
      "2018-04-22T04:02:21.460551: step 87, loss 0.438295, acc 0.81\n",
      "2018-04-22T04:02:21.504546: step 88, loss 0.422029, acc 0.84\n",
      "2018-04-22T04:02:21.548603: step 89, loss 0.378719, acc 0.83\n",
      "2018-04-22T04:02:21.600564: step 90, loss 0.448067, acc 0.74\n",
      "2018-04-22T04:02:21.652546: step 91, loss 0.431058, acc 0.79\n",
      "2018-04-22T04:02:21.696529: step 92, loss 0.64244, acc 0.76\n",
      "2018-04-22T04:02:21.741371: step 93, loss 0.312905, acc 0.86\n",
      "2018-04-22T04:02:21.788535: step 94, loss 0.400077, acc 0.82\n",
      "2018-04-22T04:02:21.837388: step 95, loss 0.444847, acc 0.88\n",
      "2018-04-22T04:02:21.884509: step 96, loss 0.312807, acc 0.89\n",
      "2018-04-22T04:02:21.928527: step 97, loss 0.375906, acc 0.88\n",
      "2018-04-22T04:02:21.956484: step 98, loss 0.338501, acc 0.86\n",
      "2018-04-22T04:02:21.997983: step 99, loss 0.377325, acc 0.88\n",
      "2018-04-22T04:02:22.044461: step 100, loss 0.409631, acc 0.83\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:22.580554: step 100, loss 0.647406, acc 0.565078, rec 0.985052, pre 0.533172, f1 0.691864\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524369734/checkpoints/model-100\n",
      "\n",
      "2018-04-22T04:02:22.737874: step 101, loss 0.361452, acc 0.85\n",
      "2018-04-22T04:02:22.780641: step 102, loss 0.331827, acc 0.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-22T04:02:22.825500: step 103, loss 0.279257, acc 0.89\n",
      "2018-04-22T04:02:22.872482: step 104, loss 0.523036, acc 0.74\n",
      "2018-04-22T04:02:22.916500: step 105, loss 0.335426, acc 0.88\n",
      "2018-04-22T04:02:22.960357: step 106, loss 0.375915, acc 0.85\n",
      "2018-04-22T04:02:23.003201: step 107, loss 0.319478, acc 0.82\n",
      "2018-04-22T04:02:23.046528: step 108, loss 0.419863, acc 0.81\n",
      "2018-04-22T04:02:23.095888: step 109, loss 0.297709, acc 0.87\n",
      "2018-04-22T04:02:23.136877: step 110, loss 0.334228, acc 0.89\n",
      "2018-04-22T04:02:23.178035: step 111, loss 0.40155, acc 0.83\n",
      "2018-04-22T04:02:23.208516: step 112, loss 0.278258, acc 0.92\n",
      "2018-04-22T04:02:23.250880: step 113, loss 0.415549, acc 0.82\n",
      "2018-04-22T04:02:23.300505: step 114, loss 0.332491, acc 0.91\n",
      "2018-04-22T04:02:23.348534: step 115, loss 0.44472, acc 0.81\n",
      "2018-04-22T04:02:23.389246: step 116, loss 0.391633, acc 0.84\n",
      "2018-04-22T04:02:23.430324: step 117, loss 0.371107, acc 0.85\n",
      "2018-04-22T04:02:23.478342: step 118, loss 0.296562, acc 0.86\n",
      "2018-04-22T04:02:23.520504: step 119, loss 0.225457, acc 0.92\n",
      "2018-04-22T04:02:23.566227: step 120, loss 0.258656, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:24.080480: step 120, loss 0.552319, acc 0.653001, rec 0.976084, pre 0.590772, f1 0.736051\n",
      "\n",
      "2018-04-22T04:02:24.128478: step 121, loss 0.319372, acc 0.87\n",
      "2018-04-22T04:02:24.169105: step 122, loss 0.392138, acc 0.87\n",
      "2018-04-22T04:02:24.210095: step 123, loss 0.348951, acc 0.86\n",
      "2018-04-22T04:02:24.254931: step 124, loss 0.358302, acc 0.89\n",
      "2018-04-22T04:02:24.299197: step 125, loss 0.302065, acc 0.88\n",
      "2018-04-22T04:02:24.324465: step 126, loss 0.263002, acc 0.9\n",
      "2018-04-22T04:02:24.365882: step 127, loss 0.346447, acc 0.9\n",
      "2018-04-22T04:02:24.408467: step 128, loss 0.238491, acc 0.96\n",
      "2018-04-22T04:02:24.456487: step 129, loss 0.285138, acc 0.86\n",
      "2018-04-22T04:02:24.497321: step 130, loss 0.432636, acc 0.89\n",
      "2018-04-22T04:02:24.544468: step 131, loss 0.258111, acc 0.9\n",
      "2018-04-22T04:02:24.584488: step 132, loss 0.208033, acc 0.92\n",
      "2018-04-22T04:02:24.628495: step 133, loss 0.333515, acc 0.88\n",
      "2018-04-22T04:02:24.669778: step 134, loss 0.24116, acc 0.92\n",
      "2018-04-22T04:02:24.712464: step 135, loss 0.223303, acc 0.92\n",
      "2018-04-22T04:02:24.756598: step 136, loss 0.271392, acc 0.86\n",
      "2018-04-22T04:02:24.800489: step 137, loss 0.228516, acc 0.93\n",
      "2018-04-22T04:02:24.837422: step 138, loss 0.28378, acc 0.88\n",
      "2018-04-22T04:02:24.878136: step 139, loss 0.346673, acc 0.89\n",
      "2018-04-22T04:02:24.900163: step 140, loss 0.34907, acc 0.82\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:25.428520: step 140, loss 0.447684, acc 0.783156, rec 0.953164, pre 0.709307, f1 0.81335\n",
      "\n",
      "2018-04-22T04:02:25.477481: step 141, loss 0.312781, acc 0.89\n",
      "2018-04-22T04:02:25.524485: step 142, loss 0.27551, acc 0.88\n",
      "2018-04-22T04:02:25.561264: step 143, loss 0.349858, acc 0.86\n",
      "2018-04-22T04:02:25.608505: step 144, loss 0.256829, acc 0.91\n",
      "2018-04-22T04:02:25.652479: step 145, loss 0.340912, acc 0.83\n",
      "2018-04-22T04:02:25.696477: step 146, loss 0.18467, acc 0.95\n",
      "2018-04-22T04:02:25.737805: step 147, loss 0.248421, acc 0.9\n",
      "2018-04-22T04:02:25.779174: step 148, loss 0.254181, acc 0.92\n",
      "2018-04-22T04:02:25.824471: step 149, loss 0.273976, acc 0.9\n",
      "2018-04-22T04:02:25.869173: step 150, loss 0.29212, acc 0.87\n",
      "2018-04-22T04:02:25.920479: step 151, loss 0.161953, acc 0.94\n",
      "2018-04-22T04:02:25.958418: step 152, loss 0.33173, acc 0.8\n",
      "2018-04-22T04:02:26.000466: step 153, loss 0.363868, acc 0.9\n",
      "2018-04-22T04:02:26.021776: step 154, loss 0.281757, acc 0.84\n",
      "2018-04-22T04:02:26.064465: step 155, loss 0.144591, acc 0.96\n",
      "2018-04-22T04:02:26.106678: step 156, loss 0.456066, acc 0.78\n",
      "2018-04-22T04:02:26.148471: step 157, loss 0.224609, acc 0.92\n",
      "2018-04-22T04:02:26.192484: step 158, loss 0.286702, acc 0.87\n",
      "2018-04-22T04:02:26.232481: step 159, loss 0.467796, acc 0.81\n",
      "2018-04-22T04:02:26.271990: step 160, loss 0.286814, acc 0.93\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:26.828684: step 160, loss 0.604487, acc 0.639664, rec 0.280518, pre 0.974048, f1 0.43559\n",
      "\n",
      "2018-04-22T04:02:26.876484: step 161, loss 0.458886, acc 0.76\n",
      "2018-04-22T04:02:26.920497: step 162, loss 0.251538, acc 0.88\n",
      "2018-04-22T04:02:26.968505: step 163, loss 0.266956, acc 0.91\n",
      "2018-04-22T04:02:27.010206: step 164, loss 0.268954, acc 0.86\n",
      "2018-04-22T04:02:27.056757: step 165, loss 0.35538, acc 0.87\n",
      "2018-04-22T04:02:27.097835: step 166, loss 0.229109, acc 0.94\n",
      "2018-04-22T04:02:27.144485: step 167, loss 0.153416, acc 0.97\n",
      "2018-04-22T04:02:27.165500: step 168, loss 0.206185, acc 0.94\n",
      "2018-04-22T04:02:27.210503: step 169, loss 0.209246, acc 0.94\n",
      "2018-04-22T04:02:27.252485: step 170, loss 0.224945, acc 0.93\n",
      "2018-04-22T04:02:27.296507: step 171, loss 0.228069, acc 0.89\n",
      "2018-04-22T04:02:27.340498: step 172, loss 0.207928, acc 0.92\n",
      "2018-04-22T04:02:27.380486: step 173, loss 0.155799, acc 0.98\n",
      "2018-04-22T04:02:27.416494: step 174, loss 0.205914, acc 0.93\n",
      "2018-04-22T04:02:27.460497: step 175, loss 0.248361, acc 0.89\n",
      "2018-04-22T04:02:27.497518: step 176, loss 0.299562, acc 0.87\n",
      "2018-04-22T04:02:27.541958: step 177, loss 0.313832, acc 0.86\n",
      "2018-04-22T04:02:27.588499: step 178, loss 0.291587, acc 0.86\n",
      "2018-04-22T04:02:27.630829: step 179, loss 0.220533, acc 0.9\n",
      "2018-04-22T04:02:27.681373: step 180, loss 0.304518, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:28.232506: step 180, loss 0.387944, acc 0.867128, rec 0.84006, pre 0.88597, f1 0.862404\n",
      "\n",
      "2018-04-22T04:02:28.280481: step 181, loss 0.192433, acc 0.9\n",
      "2018-04-22T04:02:28.305055: step 182, loss 0.275273, acc 0.9\n",
      "2018-04-22T04:02:28.348488: step 183, loss 0.336342, acc 0.88\n",
      "2018-04-22T04:02:28.388932: step 184, loss 0.195099, acc 0.92\n",
      "2018-04-22T04:02:28.430123: step 185, loss 0.324902, acc 0.88\n",
      "2018-04-22T04:02:28.470435: step 186, loss 0.177259, acc 0.95\n",
      "2018-04-22T04:02:28.512523: step 187, loss 0.327575, acc 0.85\n",
      "2018-04-22T04:02:28.552485: step 188, loss 0.294488, acc 0.88\n",
      "2018-04-22T04:02:28.589911: step 189, loss 0.273961, acc 0.88\n",
      "2018-04-22T04:02:28.632508: step 190, loss 0.228927, acc 0.86\n",
      "2018-04-22T04:02:28.676142: step 191, loss 0.193406, acc 0.94\n",
      "2018-04-22T04:02:28.720487: step 192, loss 0.359112, acc 0.9\n",
      "2018-04-22T04:02:28.764497: step 193, loss 0.261483, acc 0.9\n",
      "2018-04-22T04:02:28.804464: step 194, loss 0.190831, acc 0.89\n",
      "2018-04-22T04:02:28.845211: step 195, loss 0.382162, acc 0.89\n",
      "2018-04-22T04:02:28.879956: step 196, loss 0.367272, acc 0.88\n",
      "2018-04-22T04:02:28.924466: step 197, loss 0.120872, acc 0.99\n",
      "2018-04-22T04:02:28.964455: step 198, loss 0.194203, acc 0.92\n",
      "2018-04-22T04:02:29.008463: step 199, loss 0.247104, acc 0.91\n",
      "2018-04-22T04:02:29.049160: step 200, loss 0.245174, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:29.597277: step 200, loss 0.484764, acc 0.737713, rec 0.493772, pre 0.955641, f1 0.651117\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524369734/checkpoints/model-200\n",
      "\n",
      "2018-04-22T04:02:29.744081: step 201, loss 0.370162, acc 0.83\n",
      "2018-04-22T04:02:29.785362: step 202, loss 0.33106, acc 0.89\n",
      "2018-04-22T04:02:29.828511: step 203, loss 0.377057, acc 0.85\n",
      "2018-04-22T04:02:29.870448: step 204, loss 0.226199, acc 0.94\n",
      "2018-04-22T04:02:29.917006: step 205, loss 0.206306, acc 0.95\n",
      "2018-04-22T04:02:29.958972: step 206, loss 0.160318, acc 0.94\n",
      "2018-04-22T04:02:30.004521: step 207, loss 0.188873, acc 0.93\n",
      "2018-04-22T04:02:30.056528: step 208, loss 0.147953, acc 0.96\n",
      "2018-04-22T04:02:30.098670: step 209, loss 0.329836, acc 0.87\n",
      "2018-04-22T04:02:30.128487: step 210, loss 0.224191, acc 0.9\n",
      "2018-04-22T04:02:30.169013: step 211, loss 0.24219, acc 0.91\n",
      "2018-04-22T04:02:30.212496: step 212, loss 0.197241, acc 0.95\n",
      "2018-04-22T04:02:30.256505: step 213, loss 0.230227, acc 0.92\n",
      "2018-04-22T04:02:30.309726: step 214, loss 0.304123, acc 0.92\n",
      "2018-04-22T04:02:30.352478: step 215, loss 0.142478, acc 0.98\n",
      "2018-04-22T04:02:30.394068: step 216, loss 0.284173, acc 0.92\n",
      "2018-04-22T04:02:30.439486: step 217, loss 0.224586, acc 0.93\n",
      "2018-04-22T04:02:30.480852: step 218, loss 0.218777, acc 0.94\n",
      "2018-04-22T04:02:30.534625: step 219, loss 0.19925, acc 0.91\n",
      "2018-04-22T04:02:30.577355: step 220, loss 0.212545, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:31.101233: step 220, loss 0.397752, acc 0.848111, rec 0.763827, pre 0.915771, f1 0.832926\n",
      "\n",
      "2018-04-22T04:02:31.148458: step 221, loss 0.221095, acc 0.92\n",
      "2018-04-22T04:02:31.197026: step 222, loss 0.21631, acc 0.92\n",
      "2018-04-22T04:02:31.237983: step 223, loss 0.353014, acc 0.83\n",
      "2018-04-22T04:02:31.268493: step 224, loss 0.106496, acc 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-22T04:02:31.312488: step 225, loss 0.562168, acc 0.7\n",
      "2018-04-22T04:02:31.354751: step 226, loss 0.208573, acc 0.94\n",
      "2018-04-22T04:02:31.404710: step 227, loss 0.474468, acc 0.77\n",
      "2018-04-22T04:02:31.446495: step 228, loss 0.206737, acc 0.95\n",
      "2018-04-22T04:02:31.497099: step 229, loss 0.172833, acc 0.95\n",
      "2018-04-22T04:02:31.544478: step 230, loss 0.251874, acc 0.87\n",
      "2018-04-22T04:02:31.586015: step 231, loss 0.18356, acc 0.94\n",
      "2018-04-22T04:02:31.628494: step 232, loss 0.314499, acc 0.85\n",
      "2018-04-22T04:02:31.676534: step 233, loss 0.246429, acc 0.93\n",
      "2018-04-22T04:02:31.718143: step 234, loss 0.158259, acc 0.95\n",
      "2018-04-22T04:02:31.769095: step 235, loss 0.182816, acc 0.93\n",
      "2018-04-22T04:02:31.811535: step 236, loss 0.187092, acc 0.93\n",
      "2018-04-22T04:02:31.856801: step 237, loss 0.266364, acc 0.89\n",
      "2018-04-22T04:02:31.877976: step 238, loss 0.303886, acc 0.84\n",
      "2018-04-22T04:02:31.922699: step 239, loss 0.281525, acc 0.95\n",
      "2018-04-22T04:02:31.968074: step 240, loss 0.122694, acc 0.97\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:32.497943: step 240, loss 0.385835, acc 0.854038, rec 0.776283, pre 0.916471, f1 0.840572\n",
      "\n",
      "2018-04-22T04:02:32.552365: step 241, loss 0.208195, acc 0.93\n",
      "2018-04-22T04:02:32.600462: step 242, loss 0.233139, acc 0.92\n",
      "2018-04-22T04:02:32.640457: step 243, loss 0.328403, acc 0.88\n",
      "2018-04-22T04:02:32.684462: step 244, loss 0.17858, acc 0.94\n",
      "2018-04-22T04:02:32.728454: step 245, loss 0.161364, acc 0.95\n",
      "2018-04-22T04:02:32.775652: step 246, loss 0.199951, acc 0.94\n",
      "2018-04-22T04:02:32.821043: step 247, loss 0.167246, acc 0.95\n",
      "2018-04-22T04:02:32.862948: step 248, loss 0.154074, acc 0.96\n",
      "2018-04-22T04:02:32.908478: step 249, loss 0.171374, acc 0.96\n",
      "2018-04-22T04:02:32.953450: step 250, loss 0.172276, acc 0.96\n",
      "2018-04-22T04:02:33.004507: step 251, loss 0.153039, acc 0.95\n",
      "2018-04-22T04:02:33.028490: step 252, loss 0.485368, acc 0.92\n",
      "2018-04-22T04:02:33.070387: step 253, loss 0.228025, acc 0.93\n",
      "2018-04-22T04:02:33.117012: step 254, loss 0.166903, acc 0.94\n",
      "2018-04-22T04:02:33.162028: step 255, loss 0.132736, acc 0.96\n",
      "2018-04-22T04:02:33.208550: step 256, loss 0.169511, acc 0.92\n",
      "2018-04-22T04:02:33.256515: step 257, loss 0.192448, acc 0.94\n",
      "2018-04-22T04:02:33.296497: step 258, loss 0.283423, acc 0.89\n",
      "2018-04-22T04:02:33.333659: step 259, loss 0.282165, acc 0.9\n",
      "2018-04-22T04:02:33.376521: step 260, loss 0.117906, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:34.052006: step 260, loss 0.821835, acc 0.573475, rec 0.992028, pre 0.537817, f1 0.697495\n",
      "\n",
      "2018-04-22T04:02:34.113142: step 261, loss 0.498153, acc 0.85\n",
      "2018-04-22T04:02:34.178013: step 262, loss 0.114654, acc 0.99\n",
      "2018-04-22T04:02:34.252466: step 263, loss 0.242642, acc 0.89\n",
      "2018-04-22T04:02:34.317498: step 264, loss 0.153753, acc 0.96\n",
      "2018-04-22T04:02:34.380509: step 265, loss 0.242674, acc 0.89\n",
      "2018-04-22T04:02:34.420492: step 266, loss 0.106566, acc 0.98\n",
      "2018-04-22T04:02:34.492515: step 267, loss 0.17422, acc 0.91\n",
      "2018-04-22T04:02:34.560943: step 268, loss 0.161208, acc 0.95\n",
      "2018-04-22T04:02:34.617806: step 269, loss 0.225778, acc 0.9\n",
      "2018-04-22T04:02:34.657905: step 270, loss 0.145936, acc 0.97\n",
      "2018-04-22T04:02:34.704511: step 271, loss 0.225561, acc 0.93\n",
      "2018-04-22T04:02:34.748480: step 272, loss 0.390085, acc 0.89\n",
      "2018-04-22T04:02:34.796497: step 273, loss 0.236593, acc 0.86\n",
      "2018-04-22T04:02:34.834038: step 274, loss 0.126757, acc 0.94\n",
      "2018-04-22T04:02:34.876495: step 275, loss 0.135256, acc 0.94\n",
      "2018-04-22T04:02:34.916496: step 276, loss 0.260112, acc 0.9\n",
      "2018-04-22T04:02:34.953957: step 277, loss 0.136433, acc 0.94\n",
      "2018-04-22T04:02:34.996509: step 278, loss 0.27352, acc 0.91\n",
      "2018-04-22T04:02:35.036243: step 279, loss 0.307533, acc 0.91\n",
      "2018-04-22T04:02:35.062029: step 280, loss 0.241646, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:35.614829: step 280, loss 0.621767, acc 0.64979, rec 0.986547, pre 0.587363, f1 0.736333\n",
      "\n",
      "2018-04-22T04:02:35.664459: step 281, loss 0.159247, acc 0.94\n",
      "2018-04-22T04:02:35.720515: step 282, loss 0.187138, acc 0.94\n",
      "2018-04-22T04:02:35.760516: step 283, loss 0.161026, acc 0.95\n",
      "2018-04-22T04:02:35.808525: step 284, loss 0.246368, acc 0.9\n",
      "2018-04-22T04:02:35.856520: step 285, loss 0.129962, acc 0.96\n",
      "2018-04-22T04:02:35.898965: step 286, loss 0.222522, acc 0.91\n",
      "2018-04-22T04:02:35.940493: step 287, loss 0.272861, acc 0.93\n",
      "2018-04-22T04:02:35.984617: step 288, loss 0.218528, acc 0.91\n",
      "2018-04-22T04:02:36.027111: step 289, loss 0.227148, acc 0.94\n",
      "2018-04-22T04:02:36.068533: step 290, loss 0.145435, acc 0.98\n",
      "2018-04-22T04:02:36.112505: step 291, loss 0.210229, acc 0.94\n",
      "2018-04-22T04:02:36.150494: step 292, loss 0.118034, acc 0.95\n",
      "2018-04-22T04:02:36.192503: step 293, loss 0.205772, acc 0.9\n",
      "2018-04-22T04:02:36.220508: step 294, loss 0.297827, acc 0.94\n",
      "2018-04-22T04:02:36.258431: step 295, loss 0.205354, acc 0.92\n",
      "2018-04-22T04:02:36.308554: step 296, loss 0.105266, acc 0.98\n",
      "2018-04-22T04:02:36.352493: step 297, loss 0.190914, acc 0.96\n",
      "2018-04-22T04:02:36.396509: step 298, loss 0.139155, acc 0.94\n",
      "2018-04-22T04:02:36.438338: step 299, loss 0.233488, acc 0.9\n",
      "2018-04-22T04:02:36.485999: step 300, loss 0.137157, acc 0.97\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:37.042716: step 300, loss 0.412575, acc 0.815757, rec 0.933234, pre 0.753722, f1 0.833927\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524369734/checkpoints/model-300\n",
      "\n",
      "2018-04-22T04:02:37.197747: step 301, loss 0.179618, acc 0.95\n",
      "2018-04-22T04:02:37.240513: step 302, loss 0.13015, acc 0.97\n",
      "2018-04-22T04:02:37.285284: step 303, loss 0.136783, acc 0.96\n",
      "2018-04-22T04:02:37.323367: step 304, loss 0.154224, acc 0.95\n",
      "2018-04-22T04:02:37.368532: step 305, loss 0.182028, acc 0.93\n",
      "2018-04-22T04:02:37.408544: step 306, loss 0.167017, acc 0.94\n",
      "2018-04-22T04:02:37.452491: step 307, loss 0.220423, acc 0.9\n",
      "2018-04-22T04:02:37.484480: step 308, loss 0.215523, acc 0.88\n",
      "2018-04-22T04:02:37.526587: step 309, loss 0.311551, acc 0.9\n",
      "2018-04-22T04:02:37.567031: step 310, loss 0.139865, acc 0.95\n",
      "2018-04-22T04:02:37.616512: step 311, loss 0.19988, acc 0.9\n",
      "2018-04-22T04:02:37.662592: step 312, loss 0.148761, acc 0.94\n",
      "2018-04-22T04:02:37.704475: step 313, loss 0.291717, acc 0.89\n",
      "2018-04-22T04:02:37.756522: step 314, loss 0.162453, acc 0.93\n",
      "2018-04-22T04:02:37.796533: step 315, loss 0.138521, acc 0.95\n",
      "2018-04-22T04:02:37.845750: step 316, loss 0.26285, acc 0.89\n",
      "2018-04-22T04:02:37.893224: step 317, loss 0.203784, acc 0.88\n",
      "2018-04-22T04:02:37.936643: step 318, loss 0.143087, acc 0.95\n",
      "2018-04-22T04:02:37.982466: step 319, loss 0.1302, acc 0.96\n",
      "2018-04-22T04:02:38.028500: step 320, loss 0.178578, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:38.570754: step 320, loss 0.58974, acc 0.670042, rec 0.983059, pre 0.602443, f1 0.747066\n",
      "\n",
      "2018-04-22T04:02:38.620516: step 321, loss 0.189337, acc 0.93\n",
      "2018-04-22T04:02:38.641914: step 322, loss 0.343411, acc 0.94\n",
      "2018-04-22T04:02:38.684506: step 323, loss 0.25922, acc 0.95\n",
      "2018-04-22T04:02:38.721748: step 324, loss 0.176581, acc 0.93\n",
      "2018-04-22T04:02:38.764491: step 325, loss 0.155266, acc 0.95\n",
      "2018-04-22T04:02:38.812475: step 326, loss 0.178904, acc 0.93\n",
      "2018-04-22T04:02:38.853718: step 327, loss 0.192757, acc 0.94\n",
      "2018-04-22T04:02:38.893978: step 328, loss 0.204926, acc 0.94\n",
      "2018-04-22T04:02:38.936877: step 329, loss 0.187843, acc 0.92\n",
      "2018-04-22T04:02:38.985691: step 330, loss 0.200761, acc 0.89\n",
      "2018-04-22T04:02:39.036590: step 331, loss 0.193537, acc 0.91\n",
      "2018-04-22T04:02:39.080495: step 332, loss 0.197734, acc 0.89\n",
      "2018-04-22T04:02:39.120479: step 333, loss 0.226778, acc 0.94\n",
      "2018-04-22T04:02:39.161812: step 334, loss 0.225085, acc 0.9\n",
      "2018-04-22T04:02:39.212487: step 335, loss 0.226488, acc 0.94\n",
      "2018-04-22T04:02:39.231419: step 336, loss 0.110803, acc 0.96\n",
      "2018-04-22T04:02:39.280489: step 337, loss 0.144994, acc 0.94\n",
      "2018-04-22T04:02:39.313202: step 338, loss 0.189538, acc 0.93\n",
      "2018-04-22T04:02:39.334573: step 339, loss 0.176473, acc 0.92\n",
      "2018-04-22T04:02:39.354994: step 340, loss 0.104051, acc 0.97\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:39.908576: step 340, loss 0.588685, acc 0.678933, rec 0.980568, pre 0.609477, f1 0.751719\n",
      "\n",
      "2018-04-22T04:02:39.948496: step 341, loss 0.255475, acc 0.93\n",
      "2018-04-22T04:02:39.990558: step 342, loss 0.17959, acc 0.93\n",
      "2018-04-22T04:02:40.036560: step 343, loss 0.100203, acc 0.96\n",
      "2018-04-22T04:02:40.079421: step 344, loss 0.257172, acc 0.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-22T04:02:40.122441: step 345, loss 0.125678, acc 0.98\n",
      "2018-04-22T04:02:40.168500: step 346, loss 0.189466, acc 0.92\n",
      "2018-04-22T04:02:40.216474: step 347, loss 0.163423, acc 0.95\n",
      "2018-04-22T04:02:40.264778: step 348, loss 0.204541, acc 0.92\n",
      "2018-04-22T04:02:40.312522: step 349, loss 0.152936, acc 0.96\n",
      "2018-04-22T04:02:40.335597: step 350, loss 0.168167, acc 0.96\n",
      "2018-04-22T04:02:40.382499: step 351, loss 0.166209, acc 0.98\n",
      "2018-04-22T04:02:40.429985: step 352, loss 0.263557, acc 0.95\n",
      "2018-04-22T04:02:40.475951: step 353, loss 0.124663, acc 0.94\n",
      "2018-04-22T04:02:40.532621: step 354, loss 0.20177, acc 0.9\n",
      "2018-04-22T04:02:40.581208: step 355, loss 0.109378, acc 0.98\n",
      "2018-04-22T04:02:40.625847: step 356, loss 0.213421, acc 0.9\n",
      "2018-04-22T04:02:40.675950: step 357, loss 0.151989, acc 0.95\n",
      "2018-04-22T04:02:40.724474: step 358, loss 0.309743, acc 0.87\n",
      "2018-04-22T04:02:40.768815: step 359, loss 0.161227, acc 0.96\n",
      "2018-04-22T04:02:40.816482: step 360, loss 0.205085, acc 0.89\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:41.346944: step 360, loss 0.569107, acc 0.691035, rec 0.983059, pre 0.618495, f1 0.759284\n",
      "\n",
      "2018-04-22T04:02:41.397158: step 361, loss 0.217152, acc 0.91\n",
      "2018-04-22T04:02:41.444469: step 362, loss 0.0953266, acc 0.97\n",
      "2018-04-22T04:02:41.488496: step 363, loss 0.21668, acc 0.89\n",
      "2018-04-22T04:02:41.516509: step 364, loss 0.104447, acc 0.94\n",
      "2018-04-22T04:02:41.560305: step 365, loss 0.141763, acc 0.95\n",
      "2018-04-22T04:02:41.606403: step 366, loss 0.145144, acc 0.93\n",
      "2018-04-22T04:02:41.657354: step 367, loss 0.143442, acc 0.96\n",
      "2018-04-22T04:02:41.703553: step 368, loss 0.205828, acc 0.92\n",
      "2018-04-22T04:02:41.744985: step 369, loss 0.13494, acc 0.97\n",
      "2018-04-22T04:02:41.792351: step 370, loss 0.28271, acc 0.88\n",
      "2018-04-22T04:02:41.834839: step 371, loss 0.148844, acc 0.96\n",
      "2018-04-22T04:02:41.884485: step 372, loss 0.20334, acc 0.92\n",
      "2018-04-22T04:02:41.924493: step 373, loss 0.422973, acc 0.88\n",
      "2018-04-22T04:02:41.968482: step 374, loss 0.295137, acc 0.92\n",
      "2018-04-22T04:02:42.020508: step 375, loss 0.0958244, acc 0.97\n",
      "2018-04-22T04:02:42.060474: step 376, loss 0.165758, acc 0.92\n",
      "2018-04-22T04:02:42.102167: step 377, loss 0.202778, acc 0.93\n",
      "2018-04-22T04:02:42.132470: step 378, loss 0.162373, acc 0.92\n",
      "2018-04-22T04:02:42.169696: step 379, loss 0.192137, acc 0.9\n",
      "2018-04-22T04:02:42.210427: step 380, loss 0.171424, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:42.738657: step 380, loss 0.382918, acc 0.836503, rec 0.729447, pre 0.924826, f1 0.815599\n",
      "\n",
      "2018-04-22T04:02:42.786958: step 381, loss 0.191359, acc 0.95\n",
      "2018-04-22T04:02:42.829674: step 382, loss 0.342817, acc 0.85\n",
      "2018-04-22T04:02:42.874580: step 383, loss 0.0985772, acc 0.99\n",
      "2018-04-22T04:02:42.916950: step 384, loss 0.302559, acc 0.85\n",
      "2018-04-22T04:02:42.961456: step 385, loss 0.128595, acc 0.95\n",
      "2018-04-22T04:02:43.004429: step 386, loss 0.12898, acc 0.98\n",
      "2018-04-22T04:02:43.048439: step 387, loss 0.165797, acc 0.93\n",
      "2018-04-22T04:02:43.092461: step 388, loss 0.0991848, acc 0.98\n",
      "2018-04-22T04:02:43.133096: step 389, loss 0.134222, acc 0.95\n",
      "2018-04-22T04:02:43.176727: step 390, loss 0.223491, acc 0.91\n",
      "2018-04-22T04:02:43.217513: step 391, loss 0.21191, acc 0.89\n",
      "2018-04-22T04:02:43.242027: step 392, loss 0.181901, acc 0.88\n",
      "2018-04-22T04:02:43.282804: step 393, loss 0.0826867, acc 0.96\n",
      "2018-04-22T04:02:43.326190: step 394, loss 0.245789, acc 0.9\n",
      "2018-04-22T04:02:43.368459: step 395, loss 0.13253, acc 0.95\n",
      "2018-04-22T04:02:43.410728: step 396, loss 0.236611, acc 0.92\n",
      "2018-04-22T04:02:43.452458: step 397, loss 0.213837, acc 0.94\n",
      "2018-04-22T04:02:43.489103: step 398, loss 0.130267, acc 0.93\n",
      "2018-04-22T04:02:43.529982: step 399, loss 0.20298, acc 0.91\n",
      "2018-04-22T04:02:43.572480: step 400, loss 0.0968252, acc 0.99\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:44.097461: step 400, loss 0.436999, acc 0.782662, rec 0.598904, pre 0.941269, f1 0.732034\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524369734/checkpoints/model-400\n",
      "\n",
      "2018-04-22T04:02:44.244447: step 401, loss 0.135345, acc 0.93\n",
      "2018-04-22T04:02:44.292494: step 402, loss 0.219519, acc 0.93\n",
      "2018-04-22T04:02:44.335359: step 403, loss 0.109733, acc 0.97\n",
      "2018-04-22T04:02:44.377133: step 404, loss 0.175846, acc 0.91\n",
      "2018-04-22T04:02:44.418628: step 405, loss 0.355619, acc 0.92\n",
      "2018-04-22T04:02:44.442862: step 406, loss 0.0907726, acc 0.98\n",
      "2018-04-22T04:02:44.492454: step 407, loss 0.107868, acc 0.98\n",
      "2018-04-22T04:02:44.533733: step 408, loss 0.190627, acc 0.91\n",
      "2018-04-22T04:02:44.578003: step 409, loss 0.115929, acc 0.98\n",
      "2018-04-22T04:02:44.620463: step 410, loss 0.0964474, acc 0.97\n",
      "2018-04-22T04:02:44.657126: step 411, loss 0.117067, acc 0.95\n",
      "2018-04-22T04:02:44.700483: step 412, loss 0.167206, acc 0.92\n",
      "2018-04-22T04:02:44.740459: step 413, loss 0.271046, acc 0.92\n",
      "2018-04-22T04:02:44.777534: step 414, loss 0.136264, acc 0.93\n",
      "2018-04-22T04:02:44.824471: step 415, loss 0.124788, acc 0.97\n",
      "2018-04-22T04:02:44.865434: step 416, loss 0.13198, acc 0.95\n",
      "2018-04-22T04:02:44.901750: step 417, loss 0.101494, acc 0.96\n",
      "2018-04-22T04:02:44.945756: step 418, loss 0.167846, acc 0.92\n",
      "2018-04-22T04:02:44.987163: step 419, loss 0.108703, acc 0.96\n",
      "2018-04-22T04:02:45.016455: step 420, loss 0.261539, acc 0.96\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:45.541429: step 420, loss 0.35449, acc 0.856508, rec 0.891878, pre 0.831012, f1 0.86037\n",
      "\n",
      "2018-04-22T04:02:45.588822: step 421, loss 0.276314, acc 0.93\n",
      "2018-04-22T04:02:45.636442: step 422, loss 0.114882, acc 0.95\n",
      "2018-04-22T04:02:45.674337: step 423, loss 0.219964, acc 0.88\n",
      "2018-04-22T04:02:45.716451: step 424, loss 0.0810032, acc 0.97\n",
      "2018-04-22T04:02:45.761029: step 425, loss 0.203004, acc 0.92\n",
      "2018-04-22T04:02:45.804462: step 426, loss 0.0996694, acc 0.98\n",
      "2018-04-22T04:02:45.840839: step 427, loss 0.303796, acc 0.88\n",
      "2018-04-22T04:02:45.884470: step 428, loss 0.124483, acc 0.96\n",
      "2018-04-22T04:02:45.916937: step 429, loss 0.141087, acc 0.91\n",
      "2018-04-22T04:02:45.957736: step 430, loss 0.156832, acc 0.96\n",
      "2018-04-22T04:02:46.001224: step 431, loss 0.11365, acc 0.96\n",
      "2018-04-22T04:02:46.044454: step 432, loss 0.183156, acc 0.93\n",
      "2018-04-22T04:02:46.080707: step 433, loss 0.281953, acc 0.9\n",
      "2018-04-22T04:02:46.108462: step 434, loss 0.124736, acc 0.96\n",
      "2018-04-22T04:02:46.149386: step 435, loss 0.222862, acc 0.93\n",
      "2018-04-22T04:02:46.190091: step 436, loss 0.115999, acc 0.95\n",
      "2018-04-22T04:02:46.238685: step 437, loss 0.138023, acc 0.94\n",
      "2018-04-22T04:02:46.284476: step 438, loss 0.226086, acc 0.91\n",
      "2018-04-22T04:02:46.326046: step 439, loss 0.167915, acc 0.94\n",
      "2018-04-22T04:02:46.366146: step 440, loss 0.156939, acc 0.93\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:46.892627: step 440, loss 0.382478, acc 0.830328, rec 0.933234, pre 0.772053, f1 0.845026\n",
      "\n",
      "2018-04-22T04:02:46.944466: step 441, loss 0.217019, acc 0.94\n",
      "2018-04-22T04:02:46.981215: step 442, loss 0.105607, acc 0.98\n",
      "2018-04-22T04:02:47.024486: step 443, loss 0.244001, acc 0.93\n",
      "2018-04-22T04:02:47.064473: step 444, loss 0.0742621, acc 0.97\n",
      "2018-04-22T04:02:47.101176: step 445, loss 0.165345, acc 0.94\n",
      "2018-04-22T04:02:47.141618: step 446, loss 0.148581, acc 0.95\n",
      "2018-04-22T04:02:47.182215: step 447, loss 0.125912, acc 0.94\n",
      "2018-04-22T04:02:47.212451: step 448, loss 0.290372, acc 0.88\n",
      "2018-04-22T04:02:47.249477: step 449, loss 0.21711, acc 0.94\n",
      "2018-04-22T04:02:47.296458: step 450, loss 0.154724, acc 0.93\n",
      "2018-04-22T04:02:47.340462: step 451, loss 0.123317, acc 0.96\n",
      "2018-04-22T04:02:47.384517: step 452, loss 0.108273, acc 0.96\n",
      "2018-04-22T04:02:47.426057: step 453, loss 0.0772022, acc 0.98\n",
      "2018-04-22T04:02:47.472464: step 454, loss 0.0956834, acc 0.98\n",
      "2018-04-22T04:02:47.508733: step 455, loss 0.25575, acc 0.96\n",
      "2018-04-22T04:02:47.556660: step 456, loss 0.167321, acc 0.96\n",
      "2018-04-22T04:02:47.604471: step 457, loss 0.0981693, acc 0.98\n",
      "2018-04-22T04:02:47.644539: step 458, loss 0.0983244, acc 0.98\n",
      "2018-04-22T04:02:47.688449: step 459, loss 0.111448, acc 0.98\n",
      "2018-04-22T04:02:47.728440: step 460, loss 0.0993009, acc 0.97\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:48.244484: step 460, loss 0.451741, acc 0.776241, rec 0.958146, pre 0.700546, f1 0.809343\n",
      "\n",
      "2018-04-22T04:02:48.283604: step 461, loss 0.109615, acc 0.98\n",
      "2018-04-22T04:02:48.308441: step 462, loss 0.0866595, acc 0.98\n",
      "2018-04-22T04:02:48.348438: step 463, loss 0.168867, acc 0.92\n",
      "2018-04-22T04:02:48.392435: step 464, loss 0.19792, acc 0.95\n",
      "2018-04-22T04:02:48.432680: step 465, loss 0.157295, acc 0.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-22T04:02:48.476444: step 466, loss 0.103904, acc 0.99\n",
      "2018-04-22T04:02:48.520453: step 467, loss 0.14032, acc 0.94\n",
      "2018-04-22T04:02:48.564479: step 468, loss 0.147141, acc 0.96\n",
      "2018-04-22T04:02:48.604452: step 469, loss 0.139929, acc 0.95\n",
      "2018-04-22T04:02:48.648454: step 470, loss 0.0619014, acc 0.98\n",
      "2018-04-22T04:02:48.689924: step 471, loss 0.365943, acc 0.85\n",
      "2018-04-22T04:02:48.738562: step 472, loss 0.208182, acc 0.9\n",
      "2018-04-22T04:02:48.779642: step 473, loss 0.35155, acc 0.87\n",
      "2018-04-22T04:02:48.820708: step 474, loss 0.177511, acc 0.93\n",
      "2018-04-22T04:02:48.861157: step 475, loss 0.444289, acc 0.82\n",
      "2018-04-22T04:02:48.893693: step 476, loss 0.242588, acc 0.9\n",
      "2018-04-22T04:02:48.931453: step 477, loss 0.0984904, acc 0.97\n",
      "2018-04-22T04:02:48.980450: step 478, loss 0.147281, acc 0.97\n",
      "2018-04-22T04:02:49.021596: step 479, loss 0.187677, acc 0.89\n",
      "2018-04-22T04:02:49.062145: step 480, loss 0.153463, acc 0.95\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:49.569399: step 480, loss 0.496997, acc 0.756977, rec 0.533632, pre 0.957105, f1 0.685221\n",
      "\n",
      "2018-04-22T04:02:49.616470: step 481, loss 0.263237, acc 0.9\n",
      "2018-04-22T04:02:49.658325: step 482, loss 0.157515, acc 0.93\n",
      "2018-04-22T04:02:49.700471: step 483, loss 0.286334, acc 0.87\n",
      "2018-04-22T04:02:49.736142: step 484, loss 0.0996115, acc 0.97\n",
      "2018-04-22T04:02:49.755742: step 485, loss 0.0938488, acc 0.96\n",
      "2018-04-22T04:02:49.778459: step 486, loss 0.0537172, acc 0.98\n",
      "2018-04-22T04:02:49.798084: step 487, loss 0.162246, acc 0.92\n",
      "2018-04-22T04:02:49.817710: step 488, loss 0.123488, acc 0.97\n",
      "2018-04-22T04:02:49.837984: step 489, loss 0.136533, acc 0.95\n",
      "2018-04-22T04:02:49.849984: step 490, loss 0.332271, acc 0.9\n",
      "2018-04-22T04:02:49.869919: step 491, loss 0.0854336, acc 0.96\n",
      "2018-04-22T04:02:49.889784: step 492, loss 0.18306, acc 0.91\n",
      "2018-04-22T04:02:49.909685: step 493, loss 0.213037, acc 0.9\n",
      "2018-04-22T04:02:49.929629: step 494, loss 0.126633, acc 0.95\n",
      "2018-04-22T04:02:49.949047: step 495, loss 0.164605, acc 0.92\n",
      "2018-04-22T04:02:49.968514: step 496, loss 0.125381, acc 0.97\n",
      "2018-04-22T04:02:49.992045: step 497, loss 0.218857, acc 0.93\n",
      "2018-04-22T04:02:50.012174: step 498, loss 0.164768, acc 0.92\n",
      "2018-04-22T04:02:50.032329: step 499, loss 0.258996, acc 0.93\n",
      "2018-04-22T04:02:50.052413: step 500, loss 0.208951, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:50.300181: step 500, loss 0.351872, acc 0.863176, rec 0.888391, pre 0.843824, f1 0.865534\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524369734/checkpoints/model-500\n",
      "\n",
      "2018-04-22T04:02:50.368010: step 501, loss 0.250349, acc 0.96\n",
      "2018-04-22T04:02:50.387906: step 502, loss 0.117353, acc 0.96\n",
      "2018-04-22T04:02:50.407488: step 503, loss 0.136426, acc 0.94\n",
      "2018-04-22T04:02:50.419500: step 504, loss 0.257836, acc 0.9\n",
      "2018-04-22T04:02:50.439516: step 505, loss 0.20785, acc 0.91\n",
      "2018-04-22T04:02:50.459333: step 506, loss 0.156899, acc 0.92\n",
      "2018-04-22T04:02:50.479241: step 507, loss 0.211793, acc 0.88\n",
      "2018-04-22T04:02:50.499511: step 508, loss 0.175327, acc 0.93\n",
      "2018-04-22T04:02:50.523023: step 509, loss 0.230705, acc 0.94\n",
      "2018-04-22T04:02:50.543027: step 510, loss 0.155835, acc 0.95\n",
      "2018-04-22T04:02:50.563431: step 511, loss 0.240194, acc 0.93\n",
      "2018-04-22T04:02:50.583134: step 512, loss 0.193235, acc 0.91\n",
      "2018-04-22T04:02:50.603151: step 513, loss 0.141643, acc 0.95\n",
      "2018-04-22T04:02:50.623215: step 514, loss 0.10895, acc 0.96\n",
      "2018-04-22T04:02:50.643359: step 515, loss 0.112502, acc 0.96\n",
      "2018-04-22T04:02:50.663401: step 516, loss 0.088135, acc 0.97\n",
      "2018-04-22T04:02:50.683572: step 517, loss 0.105403, acc 0.97\n",
      "2018-04-22T04:02:50.695804: step 518, loss 0.085805, acc 0.98\n",
      "2018-04-22T04:02:50.717106: step 519, loss 0.106538, acc 0.95\n",
      "2018-04-22T04:02:50.741056: step 520, loss 0.239943, acc 0.93\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:50.987829: step 520, loss 0.360159, acc 0.855026, rec 0.792725, pre 0.902951, f1 0.844256\n",
      "\n",
      "2018-04-22T04:02:51.010167: step 521, loss 0.16011, acc 0.92\n",
      "2018-04-22T04:02:51.030262: step 522, loss 0.28966, acc 0.91\n",
      "2018-04-22T04:02:51.050739: step 523, loss 0.142003, acc 0.94\n",
      "2018-04-22T04:02:51.070476: step 524, loss 0.220803, acc 0.9\n",
      "2018-04-22T04:02:51.090075: step 525, loss 0.101269, acc 0.96\n",
      "2018-04-22T04:02:51.110117: step 526, loss 0.107548, acc 0.95\n",
      "2018-04-22T04:02:51.130507: step 527, loss 0.177162, acc 0.91\n",
      "2018-04-22T04:02:51.150914: step 528, loss 0.161076, acc 0.96\n",
      "2018-04-22T04:02:51.172021: step 529, loss 0.138363, acc 0.97\n",
      "2018-04-22T04:02:51.194976: step 530, loss 0.085273, acc 0.99\n",
      "2018-04-22T04:02:51.215085: step 531, loss 0.142419, acc 0.96\n",
      "2018-04-22T04:02:51.227366: step 532, loss 0.150249, acc 0.92\n",
      "2018-04-22T04:02:51.247696: step 533, loss 0.105332, acc 0.97\n",
      "2018-04-22T04:02:51.267938: step 534, loss 0.180976, acc 0.95\n",
      "2018-04-22T04:02:51.289220: step 535, loss 0.142676, acc 0.97\n",
      "2018-04-22T04:02:51.309099: step 536, loss 0.184486, acc 0.94\n",
      "2018-04-22T04:02:51.328764: step 537, loss 0.11961, acc 0.95\n",
      "2018-04-22T04:02:51.348781: step 538, loss 0.187847, acc 0.94\n",
      "2018-04-22T04:02:51.368867: step 539, loss 0.156694, acc 0.92\n",
      "2018-04-22T04:02:51.389088: step 540, loss 0.296759, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:51.640525: step 540, loss 0.384592, acc 0.827612, rec 0.923767, pre 0.772822, f1 0.84158\n",
      "\n",
      "2018-04-22T04:02:51.663928: step 541, loss 0.0850967, acc 0.98\n",
      "2018-04-22T04:02:51.685864: step 542, loss 0.356868, acc 0.9\n",
      "2018-04-22T04:02:51.706481: step 543, loss 0.157855, acc 0.93\n",
      "2018-04-22T04:02:51.726558: step 544, loss 0.147148, acc 0.93\n",
      "2018-04-22T04:02:51.746730: step 545, loss 0.161474, acc 0.96\n",
      "2018-04-22T04:02:51.758991: step 546, loss 0.122292, acc 0.96\n",
      "2018-04-22T04:02:51.780490: step 547, loss 0.24616, acc 0.87\n",
      "2018-04-22T04:02:51.800939: step 548, loss 0.316314, acc 0.92\n",
      "2018-04-22T04:02:51.821174: step 549, loss 0.217959, acc 0.89\n",
      "2018-04-22T04:02:51.841199: step 550, loss 0.409345, acc 0.79\n",
      "2018-04-22T04:02:51.863877: step 551, loss 0.0911082, acc 0.97\n",
      "2018-04-22T04:02:51.883634: step 552, loss 0.303175, acc 0.9\n",
      "2018-04-22T04:02:51.903891: step 553, loss 0.134502, acc 0.95\n",
      "2018-04-22T04:02:51.924709: step 554, loss 0.131591, acc 0.93\n",
      "2018-04-22T04:02:51.945042: step 555, loss 0.166575, acc 0.96\n",
      "2018-04-22T04:02:51.965037: step 556, loss 0.103295, acc 0.96\n",
      "2018-04-22T04:02:51.985356: step 557, loss 0.204361, acc 0.91\n",
      "2018-04-22T04:02:52.005510: step 558, loss 0.227152, acc 0.93\n",
      "2018-04-22T04:02:52.025793: step 559, loss 0.141229, acc 0.97\n",
      "2018-04-22T04:02:52.037835: step 560, loss 0.147691, acc 0.96\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:52.286899: step 560, loss 0.4741, acc 0.778711, rec 0.581465, pre 0.954211, f1 0.722601\n",
      "\n",
      "2018-04-22T04:02:52.309499: step 561, loss 0.240191, acc 0.9\n",
      "2018-04-22T04:02:52.329204: step 562, loss 0.418846, acc 0.87\n",
      "2018-04-22T04:02:52.349540: step 563, loss 0.103938, acc 0.96\n",
      "2018-04-22T04:02:52.369556: step 564, loss 0.191166, acc 0.95\n",
      "2018-04-22T04:02:52.390539: step 565, loss 0.170371, acc 0.91\n",
      "2018-04-22T04:02:52.410901: step 566, loss 0.15255, acc 0.95\n",
      "2018-04-22T04:02:52.431175: step 567, loss 0.304498, acc 0.87\n",
      "2018-04-22T04:02:52.451075: step 568, loss 0.161735, acc 0.91\n",
      "2018-04-22T04:02:52.471017: step 569, loss 0.133644, acc 0.94\n",
      "2018-04-22T04:02:52.494147: step 570, loss 0.0725642, acc 0.99\n",
      "2018-04-22T04:02:52.514108: step 571, loss 0.136397, acc 0.95\n",
      "2018-04-22T04:02:52.533969: step 572, loss 0.150371, acc 0.95\n",
      "2018-04-22T04:02:52.553644: step 573, loss 0.21339, acc 0.89\n",
      "2018-04-22T04:02:52.565791: step 574, loss 0.064953, acc 1\n",
      "2018-04-22T04:02:52.585919: step 575, loss 0.246615, acc 0.93\n",
      "2018-04-22T04:02:52.606324: step 576, loss 0.285017, acc 0.91\n",
      "2018-04-22T04:02:52.627535: step 577, loss 0.113053, acc 0.96\n",
      "2018-04-22T04:02:52.647915: step 578, loss 0.0768658, acc 0.99\n",
      "2018-04-22T04:02:52.668580: step 579, loss 0.152115, acc 0.92\n",
      "2018-04-22T04:02:52.688981: step 580, loss 0.118153, acc 0.99\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:52.951705: step 580, loss 0.389743, acc 0.835515, rec 0.723966, pre 0.928435, f1 0.81355\n",
      "\n",
      "2018-04-22T04:02:52.975124: step 581, loss 0.124228, acc 0.97\n",
      "2018-04-22T04:02:52.995481: step 582, loss 0.116535, acc 0.97\n",
      "2018-04-22T04:02:53.015656: step 583, loss 0.0897077, acc 0.98\n",
      "2018-04-22T04:02:53.035953: step 584, loss 0.255199, acc 0.86\n",
      "2018-04-22T04:02:53.056501: step 585, loss 0.240197, acc 0.88\n",
      "2018-04-22T04:02:53.076821: step 586, loss 0.0993864, acc 0.98\n",
      "2018-04-22T04:02:53.097077: step 587, loss 0.283524, acc 0.9\n",
      "2018-04-22T04:02:53.109564: step 588, loss 0.0863648, acc 0.96\n",
      "2018-04-22T04:02:53.130089: step 589, loss 0.176683, acc 0.93\n",
      "2018-04-22T04:02:53.150379: step 590, loss 0.11426, acc 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-22T04:02:53.175366: step 591, loss 0.119633, acc 0.96\n",
      "2018-04-22T04:02:53.195847: step 592, loss 0.204331, acc 0.92\n",
      "2018-04-22T04:02:53.216534: step 593, loss 0.152061, acc 0.94\n",
      "2018-04-22T04:02:53.237782: step 594, loss 0.0946603, acc 0.97\n",
      "2018-04-22T04:02:53.257873: step 595, loss 0.151722, acc 0.95\n",
      "2018-04-22T04:02:53.278221: step 596, loss 0.168942, acc 0.94\n",
      "2018-04-22T04:02:53.298621: step 597, loss 0.0640142, acc 1\n",
      "2018-04-22T04:02:53.320188: step 598, loss 0.107071, acc 0.96\n",
      "2018-04-22T04:02:53.340915: step 599, loss 0.125119, acc 0.94\n",
      "2018-04-22T04:02:53.361332: step 600, loss 0.223025, acc 0.89\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:53.617063: step 600, loss 0.741264, acc 0.646085, rec 0.986547, pre 0.584761, f1 0.734285\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524369734/checkpoints/model-600\n",
      "\n",
      "2018-04-22T04:02:53.693451: step 601, loss 0.19809, acc 0.93\n",
      "2018-04-22T04:02:53.706209: step 602, loss 0.263677, acc 0.88\n",
      "2018-04-22T04:02:53.726708: step 603, loss 0.125501, acc 0.98\n",
      "2018-04-22T04:02:53.747312: step 604, loss 0.120006, acc 0.93\n",
      "2018-04-22T04:02:53.768123: step 605, loss 0.137407, acc 0.93\n",
      "2018-04-22T04:02:53.789038: step 606, loss 0.109464, acc 0.96\n",
      "2018-04-22T04:02:53.810285: step 607, loss 0.137591, acc 0.96\n",
      "2018-04-22T04:02:53.835552: step 608, loss 0.0912558, acc 0.97\n",
      "2018-04-22T04:02:53.856561: step 609, loss 0.140668, acc 0.95\n",
      "2018-04-22T04:02:53.877306: step 610, loss 0.0994952, acc 0.97\n",
      "2018-04-22T04:02:53.898987: step 611, loss 0.0874046, acc 0.98\n",
      "2018-04-22T04:02:53.920102: step 612, loss 0.215538, acc 0.96\n",
      "2018-04-22T04:02:53.941252: step 613, loss 0.177752, acc 0.95\n",
      "2018-04-22T04:02:53.961763: step 614, loss 0.137548, acc 0.95\n",
      "2018-04-22T04:02:53.982149: step 615, loss 0.164189, acc 0.92\n",
      "2018-04-22T04:02:53.994886: step 616, loss 0.153167, acc 0.96\n",
      "2018-04-22T04:02:54.015913: step 617, loss 0.276058, acc 0.92\n",
      "2018-04-22T04:02:54.041360: step 618, loss 0.0991053, acc 0.98\n",
      "2018-04-22T04:02:54.062567: step 619, loss 0.136061, acc 0.9\n",
      "2018-04-22T04:02:54.083913: step 620, loss 0.121902, acc 0.95\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:54.338309: step 620, loss 0.716208, acc 0.653989, rec 0.987045, pre 0.590286, f1 0.738766\n",
      "\n",
      "2018-04-22T04:02:54.360905: step 621, loss 0.244965, acc 0.89\n",
      "2018-04-22T04:02:54.381261: step 622, loss 0.128707, acc 0.94\n",
      "2018-04-22T04:02:54.402211: step 623, loss 0.191913, acc 0.94\n",
      "2018-04-22T04:02:54.422872: step 624, loss 0.0662205, acc 0.98\n",
      "2018-04-22T04:02:54.442786: step 625, loss 0.202311, acc 0.95\n",
      "2018-04-22T04:02:54.463116: step 626, loss 0.119755, acc 0.96\n",
      "2018-04-22T04:02:54.484878: step 627, loss 0.158869, acc 0.94\n",
      "2018-04-22T04:02:54.505395: step 628, loss 0.134237, acc 0.97\n",
      "2018-04-22T04:02:54.526120: step 629, loss 0.244749, acc 0.92\n",
      "2018-04-22T04:02:54.539231: step 630, loss 0.118748, acc 0.94\n",
      "2018-04-22T04:02:54.563363: step 631, loss 0.088811, acc 0.95\n",
      "2018-04-22T04:02:54.583722: step 632, loss 0.181825, acc 0.94\n",
      "2018-04-22T04:02:54.603905: step 633, loss 0.0959877, acc 0.95\n",
      "2018-04-22T04:02:54.624067: step 634, loss 0.388068, acc 0.82\n",
      "2018-04-22T04:02:54.645193: step 635, loss 0.28828, acc 0.86\n",
      "2018-04-22T04:02:54.666411: step 636, loss 0.0798207, acc 0.99\n",
      "2018-04-22T04:02:54.686765: step 637, loss 0.30125, acc 0.92\n",
      "2018-04-22T04:02:54.708145: step 638, loss 0.324127, acc 0.85\n",
      "2018-04-22T04:02:54.728988: step 639, loss 0.316338, acc 0.86\n",
      "2018-04-22T04:02:54.750034: step 640, loss 0.257607, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:55.006709: step 640, loss 0.720042, acc 0.649049, rec 0.989038, pre 0.586584, f1 0.736413\n",
      "\n",
      "2018-04-22T04:02:55.029314: step 641, loss 0.35659, acc 0.88\n",
      "2018-04-22T04:02:55.049274: step 642, loss 0.140074, acc 0.96\n",
      "2018-04-22T04:02:55.069774: step 643, loss 0.350252, acc 0.9\n",
      "2018-04-22T04:02:55.082419: step 644, loss 0.0727345, acc 0.98\n",
      "2018-04-22T04:02:55.103404: step 645, loss 0.135073, acc 0.96\n",
      "2018-04-22T04:02:55.124048: step 646, loss 0.480424, acc 0.85\n",
      "2018-04-22T04:02:55.144743: step 647, loss 0.11179, acc 0.97\n",
      "2018-04-22T04:02:55.164934: step 648, loss 0.678801, acc 0.74\n",
      "2018-04-22T04:02:55.185110: step 649, loss 0.851678, acc 0.73\n",
      "2018-04-22T04:02:55.205702: step 650, loss 0.273778, acc 0.9\n",
      "2018-04-22T04:02:55.229629: step 651, loss 0.314896, acc 0.89\n",
      "2018-04-22T04:02:55.249743: step 652, loss 0.342076, acc 0.82\n",
      "2018-04-22T04:02:55.270374: step 653, loss 0.676054, acc 0.8\n",
      "2018-04-22T04:02:55.290933: step 654, loss 0.155689, acc 0.94\n",
      "2018-04-22T04:02:55.310947: step 655, loss 0.292118, acc 0.89\n",
      "2018-04-22T04:02:55.331023: step 656, loss 0.220358, acc 0.94\n",
      "2018-04-22T04:02:55.351105: step 657, loss 0.351227, acc 0.86\n",
      "2018-04-22T04:02:55.363554: step 658, loss 0.207502, acc 0.94\n",
      "2018-04-22T04:02:55.384589: step 659, loss 0.093846, acc 0.99\n",
      "2018-04-22T04:02:55.404851: step 660, loss 0.371329, acc 0.87\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:55.661391: step 660, loss 0.640186, acc 0.663127, rec 0.985052, pre 0.597101, f1 0.743513\n",
      "\n",
      "2018-04-22T04:02:55.684627: step 661, loss 0.197391, acc 0.91\n",
      "2018-04-22T04:02:55.705760: step 662, loss 0.243822, acc 0.88\n",
      "2018-04-22T04:02:55.726342: step 663, loss 0.100647, acc 0.99\n",
      "2018-04-22T04:02:55.747018: step 664, loss 0.188073, acc 0.94\n",
      "2018-04-22T04:02:55.767584: step 665, loss 0.143779, acc 0.98\n",
      "2018-04-22T04:02:55.788034: step 666, loss 0.133172, acc 0.92\n",
      "2018-04-22T04:02:55.808890: step 667, loss 0.24318, acc 0.9\n",
      "2018-04-22T04:02:55.829246: step 668, loss 0.327673, acc 0.82\n",
      "2018-04-22T04:02:55.849439: step 669, loss 0.172598, acc 0.92\n",
      "2018-04-22T04:02:55.873710: step 670, loss 0.17392, acc 0.94\n",
      "2018-04-22T04:02:55.894119: step 671, loss 0.170869, acc 0.94\n",
      "2018-04-22T04:02:55.906404: step 672, loss 0.147323, acc 0.94\n",
      "2018-04-22T04:02:55.926768: step 673, loss 0.129107, acc 0.96\n",
      "2018-04-22T04:02:55.946930: step 674, loss 0.237886, acc 0.93\n",
      "2018-04-22T04:02:55.966973: step 675, loss 0.205578, acc 0.88\n",
      "2018-04-22T04:02:55.986862: step 676, loss 0.16549, acc 0.93\n",
      "2018-04-22T04:02:56.007302: step 677, loss 0.340643, acc 0.91\n",
      "2018-04-22T04:02:56.027320: step 678, loss 0.136974, acc 0.96\n",
      "2018-04-22T04:02:56.047987: step 679, loss 0.11579, acc 0.96\n",
      "2018-04-22T04:02:56.068655: step 680, loss 0.0651036, acc 0.99\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:56.325607: step 680, loss 0.356229, acc 0.864905, rec 0.85999, pre 0.866466, f1 0.863216\n",
      "\n",
      "2018-04-22T04:02:56.348162: step 681, loss 0.138344, acc 0.96\n",
      "2018-04-22T04:02:56.368653: step 682, loss 0.176941, acc 0.92\n",
      "2018-04-22T04:02:56.388764: step 683, loss 0.0973423, acc 0.97\n",
      "2018-04-22T04:02:56.409151: step 684, loss 0.140921, acc 0.93\n",
      "2018-04-22T04:02:56.429611: step 685, loss 0.126615, acc 0.98\n",
      "2018-04-22T04:02:56.442285: step 686, loss 0.0381837, acc 1\n",
      "2018-04-22T04:02:56.463353: step 687, loss 0.154238, acc 0.93\n",
      "2018-04-22T04:02:56.484335: step 688, loss 0.13223, acc 0.97\n",
      "2018-04-22T04:02:56.504884: step 689, loss 0.177, acc 0.9\n",
      "2018-04-22T04:02:56.524915: step 690, loss 0.155886, acc 0.92\n",
      "2018-04-22T04:02:56.548150: step 691, loss 0.251678, acc 0.9\n",
      "2018-04-22T04:02:56.568060: step 692, loss 0.0978493, acc 0.99\n",
      "2018-04-22T04:02:56.587824: step 693, loss 0.263545, acc 0.93\n",
      "2018-04-22T04:02:56.608035: step 694, loss 0.0925685, acc 0.99\n",
      "2018-04-22T04:02:56.628364: step 695, loss 0.116888, acc 0.96\n",
      "2018-04-22T04:02:56.648573: step 696, loss 0.155299, acc 0.91\n",
      "2018-04-22T04:02:56.669301: step 697, loss 0.178933, acc 0.94\n",
      "2018-04-22T04:02:56.692258: step 698, loss 0.202324, acc 0.92\n",
      "2018-04-22T04:02:56.712861: step 699, loss 0.0766271, acc 0.96\n",
      "2018-04-22T04:02:56.725318: step 700, loss 0.14585, acc 0.98\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:56.981003: step 700, loss 0.383535, acc 0.835021, rec 0.91131, pre 0.788702, f1 0.845585\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524369734/checkpoints/model-700\n",
      "\n",
      "\n",
      "Test Set:\n",
      "2018-04-22T04:02:57.114854: step 700, loss 0.365589, acc 0.842222, rec 0.933333, pre 0.794081, f1 0.858095\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", 0.75, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "\n",
    "# embedding of 60 is best so far\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\",25, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"1\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 25, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.4, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 100\n",
    "tf.flags.DEFINE_integer(\"batch_size\", batch_size, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 50, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 20, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=train_s.shape[1],\n",
    "            num_classes=2,\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "        \n",
    "         # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        #vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch1, x_batch2, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch1, x_batch2, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            #r \n",
    "            step, summaries, loss, accuracy, recall, precision, f1 = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy,cnn.recall, cnn.precision, cnn.f1_score],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, rec {:g}, pre {:g}, f1 {:g}\".format(time_str, step, loss, accuracy, recall, precision, f1))# recall, precision, f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(train_s, train_c,  expanded_train_labels)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch1, x_batch2, y_batch = zip(*batch)\n",
    "            train_step(x_batch1, x_batch1, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(validation_s, validation_c, expanded_validation_labels, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "print(\"\\nTest Set:\")\n",
    "dev_step(test_s, test_c, expanded_test_labels, writer=dev_summary_writer)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
