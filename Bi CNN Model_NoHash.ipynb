{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saulgrimaldo1/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from w266_common import utils, vocabulary\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "np.random.seed(266)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "tokenizer = TweetTokenizer()\n",
    "x_data = []\n",
    "x_contexts = []\n",
    "labels = []\n",
    "sentences  = []\n",
    "contexts = []\n",
    "with open('merged_data_v4.csv', 'r') as csvfile:\n",
    "    linereader = csv.reader(csvfile, delimiter = '|')\n",
    "    for i, row in enumerate(linereader):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        sentence, context, sarcasm = row\n",
    "        sentence = re.sub(\"RT @[^\\s]+:\", \"retweet\", sentence)\n",
    "        sentences.append(sentence)\n",
    "        contexts.append(context)\n",
    "        x_tokens = utils.canonicalize_words(tokenizer.tokenize(sentence), hashtags = True)\n",
    "        context_tokens = utils.canonicalize_words(tokenizer.tokenize(context), hashtags = True)\n",
    "        x_data.append(x_tokens)\n",
    "        x_contexts.append(context_tokens)\n",
    "        labels.append(int(sarcasm))\n",
    "\n",
    "\n",
    "#rng = np.random.RandomState(5)\n",
    "#rng.shuffle(x_data)  # in-place\n",
    "#train_split_idx = int(0.7 * len(labels))\n",
    "#test_split_idx  = int(0.9 * len(labels))\n",
    "\n",
    "shuffle_indices = np.random.permutation(np.arange(len(labels)))\n",
    "train_split_idx = int(0.7 * len(labels))\n",
    "test_split_idx  = int(0.9 * len(labels))\n",
    "\n",
    "train_indices = shuffle_indices[:train_split_idx]\n",
    "validation_indices = shuffle_indices[train_split_idx:test_split_idx]\n",
    "test_indices = shuffle_indices[test_split_idx:]\n",
    "\n",
    "\n",
    "train_sentences = np.array(x_data)[train_indices]\n",
    "train_contexts = np.array(x_contexts)[train_indices]\n",
    "train_labels= np.array(labels)[train_indices] \n",
    "validation_sentences = np.array(x_data)[validation_indices]\n",
    "validation_labels = np.array(labels)[validation_indices]\n",
    "validation_contexts = train_contexts = np.array(x_contexts)[validation_indices]\n",
    "test_sentences = np.array(x_data)[test_indices]  \n",
    "test_contexts = train_contexts = np.array(x_contexts)[test_indices]\n",
    "test_labels = np.array(labels)[test_indices]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 6, 2]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [2]*4\n",
    "a[2] = 6\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_labels(raw_label_set, size):\n",
    "    label_set = []\n",
    "    for label in raw_label_set:\n",
    "        labels = [0] * size\n",
    "        labels[label] = 1\n",
    "        label_set.append(labels)\n",
    "    return np.array(label_set)\n",
    "\n",
    "expanded_train_labels = transform_labels(train_labels, 2)\n",
    "expanded_validation_labels = transform_labels(validation_labels,2)\n",
    "expanded_test_labels = transform_labels(test_labels,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,3,4,5,6,7,8,8,1,5,6,7]\n",
    "a + [\"<PADDING>\"]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'angry',\n",
       " 'man',\n",
       " 'is',\n",
       " 'angry',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PaddingAndTruncating:\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def pad_or_truncate(self, sentence):\n",
    "        sen_len = len(sentence)\n",
    "        paddings = self.max_len - sen_len\n",
    "        if paddings >=0:\n",
    "            return list(sentence) + [\"<PADDING>\"] * paddings\n",
    "        return sentence[0:paddings]\n",
    "        \n",
    "PadAndTrunc = PaddingAndTruncating(10)\n",
    "        \n",
    "        \n",
    "PadAndTrunc.pad_or_truncate([\"the\",\"angry\",\"man\",\"is\",\"angry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  11,  910,   20, ...,    3,    3,    3],\n",
       "       [  11,   79,   40, ...,    3,    3,    3],\n",
       "       [  93,  632,    2, ...,    3,    3,    3],\n",
       "       ..., \n",
       "       [  11,    2,  181, ...,    3,    3,    3],\n",
       "       [   2,    2,    2, ...,    3,    3,    3],\n",
       "       [  11,   38, 1022, ...,    3,    3,    3]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.contrib import learn\n",
    "vocab_size = 2500\n",
    "\n",
    "#max_len = max([len(sent) for sent  in train_sentences])\n",
    "PadAndTrunc =PaddingAndTruncating(75)\n",
    "train_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, train_sentences))\n",
    "train_context_padded = list(map(PadAndTrunc.pad_or_truncate, train_contexts))\n",
    "validation_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, validation_sentences))\n",
    "validation_context_padded = list(map(PadAndTrunc.pad_or_truncate, validation_contexts))\n",
    "test_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, test_sentences))\n",
    "test_context_padded = list(map(PadAndTrunc.pad_or_truncate, test_contexts))\n",
    "\n",
    "vocab = vocabulary.Vocabulary(utils.flatten(list(train_sentences_padded) + list(train_context_padded)), vocab_size)\n",
    "train_s = np.array(list(map(vocab.words_to_ids, train_sentences_padded)))\n",
    "train_c = np.array(list(map(vocab.words_to_ids, train_context_padded)))\n",
    "validation_s = np.array(list(map(vocab.words_to_ids, validation_sentences_padded)))\n",
    "validation_c = np.array(list(map(vocab.words_to_ids, validation_context_padded)))\n",
    "test_s = np.array(list(map(vocab.words_to_ids, test_sentences_padded)))\n",
    "test_c = np.array(list(map(vocab.words_to_ids, test_context_padded)))\n",
    "train_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv-maxpool-3'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 3\n",
    "(\"conv-maxpool-%s\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size, \n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x1 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x1\")\n",
    "        self.input_x2 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x2\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars1 = tf.nn.embedding_lookup(self.W, self.input_x1)\n",
    "            self.embedded_chars_expanded1 = tf.expand_dims(self.embedded_chars1, -1)\n",
    "            self.embedded_chars2 = tf.nn.embedding_lookup(self.W, self.input_x2)\n",
    "            self.embedded_chars_expanded2 = tf.expand_dims(self.embedded_chars2, -1)\n",
    "\n",
    "        # Create a convolution + avgpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.variable_scope(\"conv-avgpool-%s\" % i) as scope:\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "              \n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded1,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h1 = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"tanh1\")\n",
    "               \n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.avg_pool(\n",
    "                    h1,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                \n",
    "                pooled_outputs.append(pooled)\n",
    "                #pooled_outputs.append(pooled)\n",
    "                scope.reuse_variables()\n",
    "                 \n",
    "                \n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded2,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h2 = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"tanh2\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled2 = tf.nn.avg_pool(\n",
    "                    h2,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled2)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "\n",
    "        num_filters_total = num_filters * len(filter_sizes) * 2\n",
    "\n",
    "        self.h_pool = tf.concat(pooled_outputs, 1)\n",
    "\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "        \n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "           \n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.labels = tf.argmax(self.input_y, 1)\n",
    "            TP = tf.count_nonzero(self.predictions * self.labels)\n",
    "            TN = tf.count_nonzero((self.predictions - 1) * (self.labels - 1))\n",
    "            FP = tf.count_nonzero(self.predictions * (self.labels - 1))\n",
    "            FN = tf.count_nonzero((self.predictions - 1) * self.labels)\n",
    "            correct_predictions = tf.equal(self.predictions, self.labels)\n",
    "            self.precision = TP / (TP + FP)\n",
    "            self.recall = TP / (TP + FN)\n",
    "            self.f1_score = 2 * self.precision * self.recall / (self.precision + self.recall)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=100\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.75\n",
      "DROPOUT_KEEP_PROB=0.4\n",
      "EMBEDDING_DIM=25\n",
      "EVALUATE_EVERY=20\n",
      "FILTER_SIZES=1\n",
      "L2_REG_LAMBDA=0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.neg\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=50\n",
      "NUM_FILTERS=25\n",
      "POSITIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.pos\n",
      "\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/W:0/grad/hist is illegal; using conv-avgpool-0/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/W:0/grad/sparsity is illegal; using conv-avgpool-0/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/b:0/grad/hist is illegal; using conv-avgpool-0/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/b:0/grad/sparsity is illegal; using conv-avgpool-0/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524369726\n",
      "\n",
      "2018-04-22T04:02:07.152755: step 1, loss 0.762778, acc 0.43\n",
      "2018-04-22T04:02:07.196980: step 2, loss 1.4915, acc 0.43\n",
      "2018-04-22T04:02:07.237611: step 3, loss 1.67651, acc 0.51\n",
      "2018-04-22T04:02:07.276688: step 4, loss 1.60075, acc 0.48\n",
      "2018-04-22T04:02:07.317552: step 5, loss 0.945923, acc 0.58\n",
      "2018-04-22T04:02:07.354052: step 6, loss 1.19405, acc 0.54\n",
      "2018-04-22T04:02:07.396494: step 7, loss 1.34764, acc 0.49\n",
      "2018-04-22T04:02:07.456757: step 8, loss 0.950522, acc 0.58\n",
      "2018-04-22T04:02:07.510524: step 9, loss 0.907482, acc 0.53\n",
      "2018-04-22T04:02:07.564446: step 10, loss 0.976268, acc 0.55\n",
      "2018-04-22T04:02:07.608450: step 11, loss 1.09489, acc 0.62\n",
      "2018-04-22T04:02:07.645468: step 12, loss 1.16944, acc 0.54\n",
      "2018-04-22T04:02:07.686084: step 13, loss 0.916465, acc 0.58\n",
      "2018-04-22T04:02:07.708459: step 14, loss 0.593691, acc 0.66\n",
      "2018-04-22T04:02:07.748447: step 15, loss 0.788171, acc 0.58\n",
      "2018-04-22T04:02:07.791269: step 16, loss 0.994819, acc 0.57\n",
      "2018-04-22T04:02:07.837509: step 17, loss 0.773032, acc 0.6\n",
      "2018-04-22T04:02:07.876477: step 18, loss 0.652554, acc 0.6\n",
      "2018-04-22T04:02:07.917762: step 19, loss 0.771782, acc 0.65\n",
      "2018-04-22T04:02:07.956468: step 20, loss 0.783199, acc 0.61\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:08.369474: step 20, loss 1.07913, acc 0.497407, rec 0.00513196, pre 1, f1 0.0102115\n",
      "\n",
      "2018-04-22T04:02:08.416898: step 21, loss 0.96796, acc 0.53\n",
      "2018-04-22T04:02:08.456476: step 22, loss 0.784487, acc 0.61\n",
      "2018-04-22T04:02:08.495772: step 23, loss 0.604179, acc 0.68\n",
      "2018-04-22T04:02:08.537296: step 24, loss 0.84472, acc 0.57\n",
      "2018-04-22T04:02:08.580472: step 25, loss 0.864815, acc 0.57\n",
      "2018-04-22T04:02:08.622087: step 26, loss 0.824364, acc 0.55\n",
      "2018-04-22T04:02:08.664518: step 27, loss 0.82102, acc 0.54\n",
      "2018-04-22T04:02:08.685755: step 28, loss 0.797027, acc 0.64\n",
      "2018-04-22T04:02:08.728510: step 29, loss 0.703523, acc 0.64\n",
      "2018-04-22T04:02:08.769468: step 30, loss 0.698153, acc 0.67\n",
      "2018-04-22T04:02:08.816521: step 31, loss 0.583315, acc 0.71\n",
      "2018-04-22T04:02:08.853952: step 32, loss 0.690487, acc 0.61\n",
      "2018-04-22T04:02:08.896490: step 33, loss 0.598626, acc 0.66\n",
      "2018-04-22T04:02:08.936475: step 34, loss 0.646843, acc 0.66\n",
      "2018-04-22T04:02:08.976491: step 35, loss 0.609351, acc 0.67\n",
      "2018-04-22T04:02:09.016499: step 36, loss 0.618616, acc 0.67\n",
      "2018-04-22T04:02:09.060488: step 37, loss 0.588432, acc 0.74\n",
      "2018-04-22T04:02:09.101930: step 38, loss 0.841348, acc 0.63\n",
      "2018-04-22T04:02:09.140479: step 39, loss 0.571586, acc 0.65\n",
      "2018-04-22T04:02:09.181725: step 40, loss 0.719081, acc 0.61\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:09.525114: step 40, loss 0.613361, acc 0.591111, rec 0.97434, pre 0.554212, f1 0.706539\n",
      "\n",
      "2018-04-22T04:02:09.568496: step 41, loss 0.578857, acc 0.7\n",
      "2018-04-22T04:02:09.592775: step 42, loss 0.505529, acc 0.78\n",
      "2018-04-22T04:02:09.636494: step 43, loss 0.468559, acc 0.79\n",
      "2018-04-22T04:02:09.674868: step 44, loss 0.615338, acc 0.72\n",
      "2018-04-22T04:02:09.716496: step 45, loss 0.583933, acc 0.76\n",
      "2018-04-22T04:02:09.762803: step 46, loss 0.609577, acc 0.67\n",
      "2018-04-22T04:02:09.804762: step 47, loss 0.553355, acc 0.72\n",
      "2018-04-22T04:02:09.848494: step 48, loss 0.600007, acc 0.72\n",
      "2018-04-22T04:02:09.885968: step 49, loss 0.541834, acc 0.74\n",
      "2018-04-22T04:02:09.928485: step 50, loss 0.545114, acc 0.74\n",
      "2018-04-22T04:02:09.969338: step 51, loss 0.689653, acc 0.65\n",
      "2018-04-22T04:02:10.008486: step 52, loss 0.582657, acc 0.73\n",
      "2018-04-22T04:02:10.049709: step 53, loss 0.566811, acc 0.75\n",
      "2018-04-22T04:02:10.090970: step 54, loss 0.575925, acc 0.71\n",
      "2018-04-22T04:02:10.133277: step 55, loss 0.65389, acc 0.67\n",
      "2018-04-22T04:02:10.160496: step 56, loss 0.734026, acc 0.74\n",
      "2018-04-22T04:02:10.204716: step 57, loss 0.647648, acc 0.66\n",
      "2018-04-22T04:02:10.247819: step 58, loss 0.528496, acc 0.8\n",
      "2018-04-22T04:02:10.285589: step 59, loss 0.648363, acc 0.63\n",
      "2018-04-22T04:02:10.327356: step 60, loss 0.455381, acc 0.76\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:10.681586: step 60, loss 0.61554, acc 0.562593, rec 0.987537, pre 0.53644, f1 0.695226\n",
      "\n",
      "2018-04-22T04:02:10.732466: step 61, loss 0.600103, acc 0.68\n",
      "2018-04-22T04:02:10.768461: step 62, loss 0.627223, acc 0.65\n",
      "2018-04-22T04:02:10.812477: step 63, loss 0.568445, acc 0.67\n",
      "2018-04-22T04:02:10.852500: step 64, loss 0.576489, acc 0.69\n",
      "2018-04-22T04:02:10.894167: step 65, loss 0.632222, acc 0.69\n",
      "2018-04-22T04:02:10.933830: step 66, loss 0.506845, acc 0.75\n",
      "2018-04-22T04:02:10.976474: step 67, loss 0.458265, acc 0.83\n",
      "2018-04-22T04:02:11.012911: step 68, loss 0.550108, acc 0.7\n",
      "2018-04-22T04:02:11.049754: step 69, loss 0.560561, acc 0.69\n",
      "2018-04-22T04:02:11.076443: step 70, loss 0.504638, acc 0.72\n",
      "2018-04-22T04:02:11.120993: step 71, loss 0.663867, acc 0.65\n",
      "2018-04-22T04:02:11.157945: step 72, loss 0.545369, acc 0.72\n",
      "2018-04-22T04:02:11.198722: step 73, loss 0.510839, acc 0.76\n",
      "2018-04-22T04:02:11.240705: step 74, loss 0.583134, acc 0.72\n",
      "2018-04-22T04:02:11.277363: step 75, loss 0.620833, acc 0.69\n",
      "2018-04-22T04:02:11.320780: step 76, loss 0.625824, acc 0.72\n",
      "2018-04-22T04:02:11.362129: step 77, loss 0.595439, acc 0.68\n",
      "2018-04-22T04:02:11.404470: step 78, loss 0.609665, acc 0.69\n",
      "2018-04-22T04:02:11.444496: step 79, loss 0.491017, acc 0.76\n",
      "2018-04-22T04:02:11.485590: step 80, loss 0.568982, acc 0.72\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:11.829598: step 80, loss 0.609345, acc 0.587778, rec 0.186217, pre 0.988327, f1 0.313387\n",
      "\n",
      "2018-04-22T04:02:11.892614: step 81, loss 0.541301, acc 0.73\n",
      "2018-04-22T04:02:11.934619: step 82, loss 0.41841, acc 0.84\n",
      "2018-04-22T04:02:11.976498: step 83, loss 0.77917, acc 0.63\n",
      "2018-04-22T04:02:11.997452: step 84, loss 0.671341, acc 0.72\n",
      "2018-04-22T04:02:12.044474: step 85, loss 0.546381, acc 0.76\n",
      "2018-04-22T04:02:12.085304: step 86, loss 0.451489, acc 0.79\n",
      "2018-04-22T04:02:12.122122: step 87, loss 0.51657, acc 0.77\n",
      "2018-04-22T04:02:12.164441: step 88, loss 0.455029, acc 0.82\n",
      "2018-04-22T04:02:12.200638: step 89, loss 0.486493, acc 0.75\n",
      "2018-04-22T04:02:12.240926: step 90, loss 0.58363, acc 0.72\n",
      "2018-04-22T04:02:12.282162: step 91, loss 0.493909, acc 0.75\n",
      "2018-04-22T04:02:12.320767: step 92, loss 0.685308, acc 0.66\n",
      "2018-04-22T04:02:12.361075: step 93, loss 0.575136, acc 0.75\n",
      "2018-04-22T04:02:12.404445: step 94, loss 0.486489, acc 0.74\n",
      "2018-04-22T04:02:12.440423: step 95, loss 0.681667, acc 0.64\n",
      "2018-04-22T04:02:12.476897: step 96, loss 0.518795, acc 0.75\n",
      "2018-04-22T04:02:12.519628: step 97, loss 0.620331, acc 0.68\n",
      "2018-04-22T04:02:12.544422: step 98, loss 0.632356, acc 0.66\n",
      "2018-04-22T04:02:12.581262: step 99, loss 0.576891, acc 0.72\n",
      "2018-04-22T04:02:12.624503: step 100, loss 0.562706, acc 0.7\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:12.969002: step 100, loss 0.562375, acc 0.704815, rec 0.458944, pre 0.913869, f1 0.61103\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524369726/checkpoints/model-100\n",
      "\n",
      "2018-04-22T04:02:13.107246: step 101, loss 0.654806, acc 0.65\n",
      "2018-04-22T04:02:13.148449: step 102, loss 0.381857, acc 0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-22T04:02:13.188962: step 103, loss 0.563688, acc 0.71\n",
      "2018-04-22T04:02:13.229780: step 104, loss 0.592871, acc 0.71\n",
      "2018-04-22T04:02:13.268448: step 105, loss 0.581294, acc 0.75\n",
      "2018-04-22T04:02:13.310018: step 106, loss 0.51751, acc 0.73\n",
      "2018-04-22T04:02:13.352456: step 107, loss 0.552494, acc 0.71\n",
      "2018-04-22T04:02:13.393245: step 108, loss 0.613892, acc 0.7\n",
      "2018-04-22T04:02:13.429868: step 109, loss 0.48726, acc 0.78\n",
      "2018-04-22T04:02:13.470484: step 110, loss 0.469162, acc 0.8\n",
      "2018-04-22T04:02:13.512448: step 111, loss 0.472908, acc 0.75\n",
      "2018-04-22T04:02:13.532821: step 112, loss 0.687184, acc 0.68\n",
      "2018-04-22T04:02:13.572492: step 113, loss 0.645707, acc 0.69\n",
      "2018-04-22T04:02:13.616439: step 114, loss 0.597211, acc 0.67\n",
      "2018-04-22T04:02:13.656863: step 115, loss 0.657436, acc 0.66\n",
      "2018-04-22T04:02:13.693313: step 116, loss 0.620739, acc 0.66\n",
      "2018-04-22T04:02:13.733831: step 117, loss 0.632769, acc 0.7\n",
      "2018-04-22T04:02:13.773895: step 118, loss 0.476704, acc 0.76\n",
      "2018-04-22T04:02:13.816446: step 119, loss 0.564238, acc 0.7\n",
      "2018-04-22T04:02:13.856432: step 120, loss 0.473586, acc 0.82\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:14.185382: step 120, loss 0.529351, acc 0.842963, rec 0.798387, pre 0.879645, f1 0.837048\n",
      "\n",
      "2018-04-22T04:02:14.232432: step 121, loss 0.55078, acc 0.71\n",
      "2018-04-22T04:02:14.272436: step 122, loss 0.564264, acc 0.74\n",
      "2018-04-22T04:02:14.309049: step 123, loss 0.471882, acc 0.81\n",
      "2018-04-22T04:02:14.351919: step 124, loss 0.53951, acc 0.78\n",
      "2018-04-22T04:02:14.391873: step 125, loss 0.486967, acc 0.74\n",
      "2018-04-22T04:02:14.416763: step 126, loss 0.563491, acc 0.74\n",
      "2018-04-22T04:02:14.453501: step 127, loss 0.661597, acc 0.66\n",
      "2018-04-22T04:02:14.492462: step 128, loss 0.502063, acc 0.78\n",
      "2018-04-22T04:02:14.534594: step 129, loss 0.511367, acc 0.79\n",
      "2018-04-22T04:02:14.575148: step 130, loss 0.590431, acc 0.76\n",
      "2018-04-22T04:02:14.620467: step 131, loss 0.503529, acc 0.77\n",
      "2018-04-22T04:02:14.656444: step 132, loss 0.45227, acc 0.8\n",
      "2018-04-22T04:02:14.696452: step 133, loss 0.630676, acc 0.7\n",
      "2018-04-22T04:02:14.734812: step 134, loss 0.537333, acc 0.81\n",
      "2018-04-22T04:02:14.776743: step 135, loss 0.698844, acc 0.66\n",
      "2018-04-22T04:02:14.817415: step 136, loss 0.607647, acc 0.72\n",
      "2018-04-22T04:02:14.860643: step 137, loss 0.503224, acc 0.8\n",
      "2018-04-22T04:02:14.898752: step 138, loss 0.585759, acc 0.7\n",
      "2018-04-22T04:02:14.949960: step 139, loss 0.571878, acc 0.69\n",
      "2018-04-22T04:02:14.983921: step 140, loss 0.701459, acc 0.66\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:15.319791: step 140, loss 0.561676, acc 0.807037, rec 0.922287, pre 0.751943, f1 0.828449\n",
      "\n",
      "2018-04-22T04:02:15.368445: step 141, loss 0.614878, acc 0.69\n",
      "2018-04-22T04:02:15.404992: step 142, loss 0.567764, acc 0.71\n",
      "2018-04-22T04:02:15.445311: step 143, loss 0.536447, acc 0.76\n",
      "2018-04-22T04:02:15.488456: step 144, loss 0.508855, acc 0.74\n",
      "2018-04-22T04:02:15.532448: step 145, loss 0.500631, acc 0.76\n",
      "2018-04-22T04:02:15.577294: step 146, loss 0.596993, acc 0.76\n",
      "2018-04-22T04:02:15.617856: step 147, loss 0.636946, acc 0.68\n",
      "2018-04-22T04:02:15.659004: step 148, loss 0.501232, acc 0.79\n",
      "2018-04-22T04:02:15.704459: step 149, loss 0.503705, acc 0.78\n",
      "2018-04-22T04:02:15.748462: step 150, loss 0.506894, acc 0.76\n",
      "2018-04-22T04:02:15.790031: step 151, loss 0.433302, acc 0.77\n",
      "2018-04-22T04:02:15.830392: step 152, loss 0.675746, acc 0.64\n",
      "2018-04-22T04:02:15.870363: step 153, loss 0.576316, acc 0.73\n",
      "2018-04-22T04:02:15.896431: step 154, loss 0.573549, acc 0.66\n",
      "2018-04-22T04:02:15.936422: step 155, loss 0.49378, acc 0.78\n",
      "2018-04-22T04:02:15.985946: step 156, loss 0.583771, acc 0.7\n",
      "2018-04-22T04:02:16.025910: step 157, loss 0.543405, acc 0.8\n",
      "2018-04-22T04:02:16.065980: step 158, loss 0.531749, acc 0.74\n",
      "2018-04-22T04:02:16.105822: step 159, loss 0.643793, acc 0.69\n",
      "2018-04-22T04:02:16.147414: step 160, loss 0.589476, acc 0.73\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:16.504461: step 160, loss 0.536694, acc 0.750741, rec 0.541056, pre 0.940127, f1 0.686831\n",
      "\n",
      "2018-04-22T04:02:16.543090: step 161, loss 0.554726, acc 0.74\n",
      "2018-04-22T04:02:16.588505: step 162, loss 0.449455, acc 0.78\n",
      "2018-04-22T04:02:16.625209: step 163, loss 0.683328, acc 0.67\n",
      "2018-04-22T04:02:16.672526: step 164, loss 0.597735, acc 0.68\n",
      "2018-04-22T04:02:16.716498: step 165, loss 0.67784, acc 0.67\n",
      "2018-04-22T04:02:16.760273: step 166, loss 0.646637, acc 0.67\n",
      "2018-04-22T04:02:16.804509: step 167, loss 0.595898, acc 0.7\n",
      "2018-04-22T04:02:16.825782: step 168, loss 0.447055, acc 0.8\n",
      "2018-04-22T04:02:16.868505: step 169, loss 0.590924, acc 0.74\n",
      "2018-04-22T04:02:16.908519: step 170, loss 0.627577, acc 0.73\n",
      "2018-04-22T04:02:16.952510: step 171, loss 0.624521, acc 0.74\n",
      "2018-04-22T04:02:16.994056: step 172, loss 0.517749, acc 0.75\n",
      "2018-04-22T04:02:17.040505: step 173, loss 0.609894, acc 0.71\n",
      "2018-04-22T04:02:17.081567: step 174, loss 0.692114, acc 0.66\n",
      "2018-04-22T04:02:17.128524: step 175, loss 0.638325, acc 0.74\n",
      "2018-04-22T04:02:17.180546: step 176, loss 0.712337, acc 0.68\n",
      "2018-04-22T04:02:17.228524: step 177, loss 0.684595, acc 0.73\n",
      "2018-04-22T04:02:17.272532: step 178, loss 0.496095, acc 0.81\n",
      "2018-04-22T04:02:17.310365: step 179, loss 0.452268, acc 0.79\n",
      "2018-04-22T04:02:17.356518: step 180, loss 0.504852, acc 0.78\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:17.725634: step 180, loss 0.637308, acc 0.53963, rec 0.98607, pre 0.52355, f1 0.683956\n",
      "\n",
      "2018-04-22T04:02:17.772498: step 181, loss 0.669934, acc 0.67\n",
      "2018-04-22T04:02:17.800486: step 182, loss 0.53402, acc 0.72\n",
      "2018-04-22T04:02:17.848476: step 183, loss 0.536281, acc 0.73\n",
      "2018-04-22T04:02:17.884812: step 184, loss 0.5495, acc 0.78\n",
      "2018-04-22T04:02:17.923412: step 185, loss 0.485098, acc 0.8\n",
      "2018-04-22T04:02:17.968488: step 186, loss 0.739451, acc 0.67\n",
      "2018-04-22T04:02:18.004736: step 187, loss 0.4783, acc 0.81\n",
      "2018-04-22T04:02:18.052487: step 188, loss 0.571658, acc 0.71\n",
      "2018-04-22T04:02:18.088498: step 189, loss 0.583305, acc 0.74\n",
      "2018-04-22T04:02:18.124962: step 190, loss 0.567117, acc 0.71\n",
      "2018-04-22T04:02:18.165223: step 191, loss 0.590758, acc 0.7\n",
      "2018-04-22T04:02:18.205196: step 192, loss 0.791061, acc 0.74\n",
      "2018-04-22T04:02:18.248503: step 193, loss 0.558321, acc 0.78\n",
      "2018-04-22T04:02:18.284509: step 194, loss 0.497043, acc 0.76\n",
      "2018-04-22T04:02:18.324488: step 195, loss 0.543692, acc 0.75\n",
      "2018-04-22T04:02:18.345093: step 196, loss 0.76019, acc 0.62\n",
      "2018-04-22T04:02:18.388717: step 197, loss 0.497326, acc 0.79\n",
      "2018-04-22T04:02:18.433503: step 198, loss 0.533376, acc 0.76\n",
      "2018-04-22T04:02:18.474110: step 199, loss 0.439534, acc 0.85\n",
      "2018-04-22T04:02:18.516772: step 200, loss 0.547606, acc 0.74\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:18.888499: step 200, loss 0.615322, acc 0.614444, rec 0.242669, pre 0.976401, f1 0.388726\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524369726/checkpoints/model-200\n",
      "\n",
      "2018-04-22T04:02:19.041649: step 201, loss 0.667549, acc 0.67\n",
      "2018-04-22T04:02:19.082535: step 202, loss 0.576392, acc 0.78\n",
      "2018-04-22T04:02:19.128471: step 203, loss 0.634118, acc 0.7\n",
      "2018-04-22T04:02:19.172493: step 204, loss 0.515131, acc 0.71\n",
      "2018-04-22T04:02:19.216480: step 205, loss 0.566487, acc 0.75\n",
      "2018-04-22T04:02:19.256484: step 206, loss 0.5139, acc 0.79\n",
      "2018-04-22T04:02:19.296559: step 207, loss 0.574409, acc 0.75\n",
      "2018-04-22T04:02:19.340515: step 208, loss 0.517474, acc 0.77\n",
      "2018-04-22T04:02:19.380516: step 209, loss 0.563187, acc 0.73\n",
      "2018-04-22T04:02:19.405474: step 210, loss 0.598913, acc 0.76\n",
      "2018-04-22T04:02:19.448993: step 211, loss 0.59947, acc 0.77\n",
      "2018-04-22T04:02:19.492500: step 212, loss 0.623284, acc 0.7\n",
      "2018-04-22T04:02:19.532505: step 213, loss 0.611305, acc 0.72\n",
      "2018-04-22T04:02:19.576501: step 214, loss 0.53525, acc 0.75\n",
      "2018-04-22T04:02:19.612501: step 215, loss 0.534813, acc 0.77\n",
      "2018-04-22T04:02:19.656510: step 216, loss 0.535053, acc 0.76\n",
      "2018-04-22T04:02:19.696516: step 217, loss 0.569201, acc 0.77\n",
      "2018-04-22T04:02:19.732496: step 218, loss 0.502008, acc 0.75\n",
      "2018-04-22T04:02:19.776484: step 219, loss 0.511453, acc 0.76\n",
      "2018-04-22T04:02:19.824499: step 220, loss 0.712704, acc 0.65\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:20.181392: step 220, loss 0.797716, acc 0.508889, rec 0.994135, pre 0.507105, f1 0.67162\n",
      "\n",
      "2018-04-22T04:02:20.229546: step 221, loss 0.60716, acc 0.71\n",
      "2018-04-22T04:02:20.276536: step 222, loss 0.569645, acc 0.75\n",
      "2018-04-22T04:02:20.317692: step 223, loss 0.460057, acc 0.74\n",
      "2018-04-22T04:02:20.344492: step 224, loss 0.424658, acc 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-22T04:02:20.393257: step 225, loss 0.626366, acc 0.73\n",
      "2018-04-22T04:02:20.438232: step 226, loss 0.595734, acc 0.75\n",
      "2018-04-22T04:02:20.480473: step 227, loss 0.731308, acc 0.71\n",
      "2018-04-22T04:02:20.524483: step 228, loss 0.476942, acc 0.78\n",
      "2018-04-22T04:02:20.568485: step 229, loss 0.616562, acc 0.75\n",
      "2018-04-22T04:02:20.616886: step 230, loss 0.473631, acc 0.76\n",
      "2018-04-22T04:02:20.664504: step 231, loss 0.526618, acc 0.77\n",
      "2018-04-22T04:02:20.708575: step 232, loss 0.544674, acc 0.77\n",
      "2018-04-22T04:02:20.748789: step 233, loss 0.511184, acc 0.8\n",
      "2018-04-22T04:02:20.792527: step 234, loss 0.494322, acc 0.75\n",
      "2018-04-22T04:02:20.838784: step 235, loss 0.628046, acc 0.77\n",
      "2018-04-22T04:02:20.881220: step 236, loss 0.507391, acc 0.78\n",
      "2018-04-22T04:02:20.924541: step 237, loss 0.565495, acc 0.76\n",
      "2018-04-22T04:02:20.950599: step 238, loss 0.713926, acc 0.7\n",
      "2018-04-22T04:02:20.993896: step 239, loss 0.6471, acc 0.73\n",
      "2018-04-22T04:02:21.036564: step 240, loss 0.616419, acc 0.66\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:21.399204: step 240, loss 0.525734, acc 0.808519, rec 0.945015, pre 0.744656, f1 0.832956\n",
      "\n",
      "2018-04-22T04:02:21.451554: step 241, loss 0.58463, acc 0.73\n",
      "2018-04-22T04:02:21.494868: step 242, loss 0.482074, acc 0.78\n",
      "2018-04-22T04:02:21.542451: step 243, loss 0.619505, acc 0.69\n",
      "2018-04-22T04:02:21.588545: step 244, loss 0.594305, acc 0.71\n",
      "2018-04-22T04:02:21.643479: step 245, loss 0.494056, acc 0.73\n",
      "2018-04-22T04:02:21.685908: step 246, loss 0.527283, acc 0.81\n",
      "2018-04-22T04:02:21.732535: step 247, loss 0.483553, acc 0.77\n",
      "2018-04-22T04:02:21.774616: step 248, loss 0.509715, acc 0.79\n",
      "2018-04-22T04:02:21.822039: step 249, loss 0.539997, acc 0.77\n",
      "2018-04-22T04:02:21.872765: step 250, loss 0.597457, acc 0.73\n",
      "2018-04-22T04:02:21.916517: step 251, loss 0.49639, acc 0.77\n",
      "2018-04-22T04:02:21.943286: step 252, loss 0.546008, acc 0.74\n",
      "2018-04-22T04:02:21.988502: step 253, loss 0.566588, acc 0.69\n",
      "2018-04-22T04:02:22.028510: step 254, loss 0.517559, acc 0.69\n",
      "2018-04-22T04:02:22.080474: step 255, loss 0.635347, acc 0.69\n",
      "2018-04-22T04:02:22.124478: step 256, loss 0.552417, acc 0.78\n",
      "2018-04-22T04:02:22.160820: step 257, loss 0.697251, acc 0.64\n",
      "2018-04-22T04:02:22.204494: step 258, loss 0.503275, acc 0.76\n",
      "2018-04-22T04:02:22.240493: step 259, loss 0.531666, acc 0.79\n",
      "2018-04-22T04:02:22.281004: step 260, loss 0.685024, acc 0.72\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:22.633213: step 260, loss 0.589639, acc 0.61037, rec 0.233871, pre 0.978528, f1 0.377515\n",
      "\n",
      "2018-04-22T04:02:22.681270: step 261, loss 0.767558, acc 0.61\n",
      "2018-04-22T04:02:22.722443: step 262, loss 0.538277, acc 0.79\n",
      "2018-04-22T04:02:22.765609: step 263, loss 0.554343, acc 0.75\n",
      "2018-04-22T04:02:22.810541: step 264, loss 0.589814, acc 0.76\n",
      "2018-04-22T04:02:22.855725: step 265, loss 0.708875, acc 0.65\n",
      "2018-04-22T04:02:22.883008: step 266, loss 0.421567, acc 0.86\n",
      "2018-04-22T04:02:22.926192: step 267, loss 0.614057, acc 0.74\n",
      "2018-04-22T04:02:22.968487: step 268, loss 0.546486, acc 0.72\n",
      "2018-04-22T04:02:23.012313: step 269, loss 0.468828, acc 0.8\n",
      "2018-04-22T04:02:23.055466: step 270, loss 0.58316, acc 0.67\n",
      "2018-04-22T04:02:23.104461: step 271, loss 0.640414, acc 0.67\n",
      "2018-04-22T04:02:23.145392: step 272, loss 0.433541, acc 0.79\n",
      "2018-04-22T04:02:23.186397: step 273, loss 0.639168, acc 0.71\n",
      "2018-04-22T04:02:23.232516: step 274, loss 0.571522, acc 0.77\n",
      "2018-04-22T04:02:23.278145: step 275, loss 0.469313, acc 0.78\n",
      "2018-04-22T04:02:23.327240: step 276, loss 0.597406, acc 0.7\n",
      "2018-04-22T04:02:23.372491: step 277, loss 0.56046, acc 0.7\n",
      "2018-04-22T04:02:23.413686: step 278, loss 0.536858, acc 0.76\n",
      "2018-04-22T04:02:23.460550: step 279, loss 0.66529, acc 0.69\n",
      "2018-04-22T04:02:23.485659: step 280, loss 0.661294, acc 0.68\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:23.832820: step 280, loss 0.622144, acc 0.558148, rec 0.98607, pre 0.533942, f1 0.692763\n",
      "\n",
      "2018-04-22T04:02:23.880464: step 281, loss 0.580879, acc 0.7\n",
      "2018-04-22T04:02:23.915202: step 282, loss 0.843703, acc 0.57\n",
      "2018-04-22T04:02:23.957272: step 283, loss 0.610697, acc 0.75\n",
      "2018-04-22T04:02:23.993286: step 284, loss 0.508971, acc 0.75\n",
      "2018-04-22T04:02:24.038491: step 285, loss 0.772019, acc 0.68\n",
      "2018-04-22T04:02:24.076203: step 286, loss 0.646433, acc 0.69\n",
      "2018-04-22T04:02:24.116842: step 287, loss 0.686013, acc 0.68\n",
      "2018-04-22T04:02:24.160946: step 288, loss 0.843582, acc 0.57\n",
      "2018-04-22T04:02:24.201580: step 289, loss 0.823238, acc 0.61\n",
      "2018-04-22T04:02:24.245789: step 290, loss 0.508295, acc 0.75\n",
      "2018-04-22T04:02:24.289582: step 291, loss 0.704862, acc 0.7\n",
      "2018-04-22T04:02:24.333345: step 292, loss 0.654827, acc 0.73\n",
      "2018-04-22T04:02:24.373706: step 293, loss 0.616844, acc 0.72\n",
      "2018-04-22T04:02:24.399918: step 294, loss 0.579242, acc 0.72\n",
      "2018-04-22T04:02:24.436968: step 295, loss 0.794127, acc 0.61\n",
      "2018-04-22T04:02:24.482979: step 296, loss 0.436812, acc 0.79\n",
      "2018-04-22T04:02:24.528809: step 297, loss 0.48154, acc 0.79\n",
      "2018-04-22T04:02:24.569244: step 298, loss 0.551297, acc 0.78\n",
      "2018-04-22T04:02:24.609680: step 299, loss 0.782922, acc 0.71\n",
      "2018-04-22T04:02:24.652472: step 300, loss 0.41827, acc 0.79\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:25.009531: step 300, loss 0.518439, acc 0.751852, rec 0.52566, pre 0.968919, f1 0.681559\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524369726/checkpoints/model-300\n",
      "\n",
      "2018-04-22T04:02:25.152454: step 301, loss 0.518369, acc 0.78\n",
      "2018-04-22T04:02:25.189120: step 302, loss 0.693856, acc 0.71\n",
      "2018-04-22T04:02:25.232459: step 303, loss 0.661266, acc 0.69\n",
      "2018-04-22T04:02:25.273065: step 304, loss 0.48325, acc 0.78\n",
      "2018-04-22T04:02:25.309577: step 305, loss 0.557424, acc 0.73\n",
      "2018-04-22T04:02:25.350016: step 306, loss 0.524479, acc 0.79\n",
      "2018-04-22T04:02:25.389987: step 307, loss 0.456525, acc 0.81\n",
      "2018-04-22T04:02:25.416465: step 308, loss 0.58855, acc 0.74\n",
      "2018-04-22T04:02:25.460479: step 309, loss 0.664637, acc 0.7\n",
      "2018-04-22T04:02:25.504476: step 310, loss 0.714929, acc 0.75\n",
      "2018-04-22T04:02:25.540458: step 311, loss 0.749304, acc 0.65\n",
      "2018-04-22T04:02:25.584474: step 312, loss 0.597229, acc 0.76\n",
      "2018-04-22T04:02:25.624460: step 313, loss 0.86169, acc 0.65\n",
      "2018-04-22T04:02:25.672454: step 314, loss 0.705792, acc 0.73\n",
      "2018-04-22T04:02:25.713367: step 315, loss 0.474861, acc 0.81\n",
      "2018-04-22T04:02:25.754377: step 316, loss 0.623244, acc 0.71\n",
      "2018-04-22T04:02:25.795225: step 317, loss 0.616427, acc 0.75\n",
      "2018-04-22T04:02:25.840473: step 318, loss 0.8401, acc 0.63\n",
      "2018-04-22T04:02:25.889148: step 319, loss 0.520608, acc 0.76\n",
      "2018-04-22T04:02:25.932489: step 320, loss 0.605915, acc 0.76\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:26.280460: step 320, loss 1.3719, acc 0.504444, rec 0.0190616, pre 1, f1 0.0374101\n",
      "\n",
      "2018-04-22T04:02:26.332784: step 321, loss 0.65696, acc 0.73\n",
      "2018-04-22T04:02:26.353057: step 322, loss 0.829856, acc 0.7\n",
      "2018-04-22T04:02:26.396546: step 323, loss 0.786343, acc 0.7\n",
      "2018-04-22T04:02:26.444433: step 324, loss 0.637187, acc 0.72\n",
      "2018-04-22T04:02:26.498324: step 325, loss 0.644772, acc 0.72\n",
      "2018-04-22T04:02:26.540464: step 326, loss 0.822732, acc 0.64\n",
      "2018-04-22T04:02:26.578279: step 327, loss 0.497716, acc 0.82\n",
      "2018-04-22T04:02:26.618749: step 328, loss 0.68804, acc 0.73\n",
      "2018-04-22T04:02:26.660456: step 329, loss 0.634405, acc 0.73\n",
      "2018-04-22T04:02:26.696729: step 330, loss 0.495845, acc 0.79\n",
      "2018-04-22T04:02:26.744477: step 331, loss 0.55305, acc 0.8\n",
      "2018-04-22T04:02:26.780751: step 332, loss 0.548979, acc 0.75\n",
      "2018-04-22T04:02:26.822345: step 333, loss 0.537847, acc 0.73\n",
      "2018-04-22T04:02:26.864503: step 334, loss 0.664244, acc 0.72\n",
      "2018-04-22T04:02:26.905560: step 335, loss 0.629214, acc 0.74\n",
      "2018-04-22T04:02:26.930253: step 336, loss 0.708193, acc 0.74\n",
      "2018-04-22T04:02:26.977258: step 337, loss 0.636586, acc 0.71\n",
      "2018-04-22T04:02:27.018089: step 338, loss 0.54547, acc 0.78\n",
      "2018-04-22T04:02:27.065806: step 339, loss 0.518201, acc 0.77\n",
      "2018-04-22T04:02:27.106163: step 340, loss 0.695963, acc 0.69\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:27.472756: step 340, loss 0.544314, acc 0.647407, rec 0.978739, pre 0.591231, f1 0.737162\n",
      "\n",
      "2018-04-22T04:02:27.520117: step 341, loss 0.479741, acc 0.8\n",
      "2018-04-22T04:02:27.565449: step 342, loss 0.519969, acc 0.76\n",
      "2018-04-22T04:02:27.610652: step 343, loss 0.480829, acc 0.76\n",
      "2018-04-22T04:02:27.653246: step 344, loss 0.600985, acc 0.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-22T04:02:27.708511: step 345, loss 0.535255, acc 0.8\n",
      "2018-04-22T04:02:27.752488: step 346, loss 0.51725, acc 0.74\n",
      "2018-04-22T04:02:27.792512: step 347, loss 0.462159, acc 0.78\n",
      "2018-04-22T04:02:27.829350: step 348, loss 0.420846, acc 0.79\n",
      "2018-04-22T04:02:27.872501: step 349, loss 0.616165, acc 0.71\n",
      "2018-04-22T04:02:27.891104: step 350, loss 0.569099, acc 0.66\n",
      "2018-04-22T04:02:27.935723: step 351, loss 0.492053, acc 0.76\n",
      "2018-04-22T04:02:27.972500: step 352, loss 0.565728, acc 0.71\n",
      "2018-04-22T04:02:28.014441: step 353, loss 0.559163, acc 0.74\n",
      "2018-04-22T04:02:28.053609: step 354, loss 0.558236, acc 0.77\n",
      "2018-04-22T04:02:28.092500: step 355, loss 0.477188, acc 0.79\n",
      "2018-04-22T04:02:28.133004: step 356, loss 0.527365, acc 0.78\n",
      "2018-04-22T04:02:28.174616: step 357, loss 0.649865, acc 0.7\n",
      "2018-04-22T04:02:28.216510: step 358, loss 0.570991, acc 0.73\n",
      "2018-04-22T04:02:28.257598: step 359, loss 0.494853, acc 0.8\n",
      "2018-04-22T04:02:28.297909: step 360, loss 0.569915, acc 0.74\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:28.664741: step 360, loss 0.488981, acc 0.814444, rec 0.689883, pre 0.923454, f1 0.789761\n",
      "\n",
      "2018-04-22T04:02:28.715862: step 361, loss 0.475113, acc 0.76\n",
      "2018-04-22T04:02:28.758077: step 362, loss 0.413633, acc 0.83\n",
      "2018-04-22T04:02:28.799880: step 363, loss 0.796801, acc 0.66\n",
      "2018-04-22T04:02:28.824478: step 364, loss 0.355333, acc 0.82\n",
      "2018-04-22T04:02:28.872470: step 365, loss 0.54363, acc 0.78\n",
      "2018-04-22T04:02:28.918352: step 366, loss 0.523466, acc 0.8\n",
      "2018-04-22T04:02:28.960069: step 367, loss 0.510478, acc 0.77\n",
      "2018-04-22T04:02:29.001266: step 368, loss 0.762997, acc 0.66\n",
      "2018-04-22T04:02:29.043422: step 369, loss 0.56783, acc 0.73\n",
      "2018-04-22T04:02:29.096989: step 370, loss 0.441374, acc 0.85\n",
      "2018-04-22T04:02:29.133937: step 371, loss 0.565077, acc 0.79\n",
      "2018-04-22T04:02:29.176509: step 372, loss 0.585347, acc 0.77\n",
      "2018-04-22T04:02:29.213414: step 373, loss 0.704635, acc 0.71\n",
      "2018-04-22T04:02:29.251060: step 374, loss 0.564238, acc 0.72\n",
      "2018-04-22T04:02:29.292502: step 375, loss 0.41804, acc 0.84\n",
      "2018-04-22T04:02:29.340517: step 376, loss 0.567948, acc 0.77\n",
      "2018-04-22T04:02:29.377933: step 377, loss 0.460769, acc 0.8\n",
      "2018-04-22T04:02:29.404514: step 378, loss 0.619383, acc 0.66\n",
      "2018-04-22T04:02:29.442196: step 379, loss 0.547226, acc 0.71\n",
      "2018-04-22T04:02:29.484514: step 380, loss 0.576264, acc 0.78\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:29.844507: step 380, loss 0.571368, acc 0.605556, rec 0.982405, pre 0.562789, f1 0.715621\n",
      "\n",
      "2018-04-22T04:02:29.891349: step 381, loss 0.606163, acc 0.75\n",
      "2018-04-22T04:02:29.933299: step 382, loss 0.583618, acc 0.76\n",
      "2018-04-22T04:02:29.980514: step 383, loss 0.467233, acc 0.71\n",
      "2018-04-22T04:02:30.020232: step 384, loss 0.518924, acc 0.76\n",
      "2018-04-22T04:02:30.072524: step 385, loss 0.681894, acc 0.69\n",
      "2018-04-22T04:02:30.114544: step 386, loss 0.491307, acc 0.74\n",
      "2018-04-22T04:02:30.160510: step 387, loss 0.544025, acc 0.75\n",
      "2018-04-22T04:02:30.201362: step 388, loss 0.558639, acc 0.72\n",
      "2018-04-22T04:02:30.240356: step 389, loss 0.652432, acc 0.7\n",
      "2018-04-22T04:02:30.293229: step 390, loss 0.697736, acc 0.64\n",
      "2018-04-22T04:02:30.340504: step 391, loss 0.532834, acc 0.74\n",
      "2018-04-22T04:02:30.362051: step 392, loss 0.532647, acc 0.74\n",
      "2018-04-22T04:02:30.403420: step 393, loss 0.584581, acc 0.74\n",
      "2018-04-22T04:02:30.444478: step 394, loss 0.495026, acc 0.74\n",
      "2018-04-22T04:02:30.488490: step 395, loss 0.607641, acc 0.74\n",
      "2018-04-22T04:02:30.540484: step 396, loss 0.475182, acc 0.77\n",
      "2018-04-22T04:02:30.584491: step 397, loss 0.535284, acc 0.76\n",
      "2018-04-22T04:02:30.624499: step 398, loss 0.465192, acc 0.81\n",
      "2018-04-22T04:02:30.661898: step 399, loss 0.60801, acc 0.74\n",
      "2018-04-22T04:02:30.704508: step 400, loss 0.585006, acc 0.78\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:31.048515: step 400, loss 0.485002, acc 0.875185, rec 0.849707, pre 0.897754, f1 0.87307\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524369726/checkpoints/model-400\n",
      "\n",
      "2018-04-22T04:02:31.190060: step 401, loss 0.538759, acc 0.75\n",
      "2018-04-22T04:02:31.230739: step 402, loss 0.569586, acc 0.72\n",
      "2018-04-22T04:02:31.280468: step 403, loss 0.538508, acc 0.76\n",
      "2018-04-22T04:02:31.332625: step 404, loss 0.489282, acc 0.79\n",
      "2018-04-22T04:02:31.370141: step 405, loss 0.634721, acc 0.69\n",
      "2018-04-22T04:02:31.396571: step 406, loss 0.433173, acc 0.8\n",
      "2018-04-22T04:02:31.438669: step 407, loss 0.545917, acc 0.75\n",
      "2018-04-22T04:02:31.479311: step 408, loss 0.592423, acc 0.74\n",
      "2018-04-22T04:02:31.536244: step 409, loss 0.567398, acc 0.76\n",
      "2018-04-22T04:02:31.577478: step 410, loss 0.418687, acc 0.83\n",
      "2018-04-22T04:02:31.623286: step 411, loss 0.639174, acc 0.73\n",
      "2018-04-22T04:02:31.667258: step 412, loss 0.544643, acc 0.71\n",
      "2018-04-22T04:02:31.708596: step 413, loss 0.638782, acc 0.73\n",
      "2018-04-22T04:02:31.760503: step 414, loss 0.486131, acc 0.73\n",
      "2018-04-22T04:02:31.800513: step 415, loss 0.469693, acc 0.78\n",
      "2018-04-22T04:02:31.841601: step 416, loss 0.602941, acc 0.76\n",
      "2018-04-22T04:02:31.886987: step 417, loss 0.555492, acc 0.77\n",
      "2018-04-22T04:02:31.928450: step 418, loss 0.774222, acc 0.69\n",
      "2018-04-22T04:02:31.977181: step 419, loss 0.56308, acc 0.74\n",
      "2018-04-22T04:02:32.001824: step 420, loss 0.496672, acc 0.7\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:32.341581: step 420, loss 0.54041, acc 0.673704, rec 0.97654, pre 0.610729, f1 0.751481\n",
      "\n",
      "2018-04-22T04:02:32.385377: step 421, loss 0.601204, acc 0.75\n",
      "2018-04-22T04:02:32.428506: step 422, loss 0.490619, acc 0.75\n",
      "2018-04-22T04:02:32.464999: step 423, loss 0.695661, acc 0.68\n",
      "2018-04-22T04:02:32.506549: step 424, loss 0.61814, acc 0.75\n",
      "2018-04-22T04:02:32.556467: step 425, loss 0.541978, acc 0.74\n",
      "2018-04-22T04:02:32.594817: step 426, loss 0.607044, acc 0.77\n",
      "2018-04-22T04:02:32.636642: step 427, loss 0.527391, acc 0.76\n",
      "2018-04-22T04:02:32.680167: step 428, loss 0.60646, acc 0.68\n",
      "2018-04-22T04:02:32.724996: step 429, loss 0.472607, acc 0.75\n",
      "2018-04-22T04:02:32.770157: step 430, loss 0.545051, acc 0.73\n",
      "2018-04-22T04:02:32.813229: step 431, loss 0.588865, acc 0.73\n",
      "2018-04-22T04:02:32.854399: step 432, loss 0.46134, acc 0.79\n",
      "2018-04-22T04:02:32.900242: step 433, loss 0.50289, acc 0.76\n",
      "2018-04-22T04:02:32.924973: step 434, loss 0.469301, acc 0.8\n",
      "2018-04-22T04:02:32.970614: step 435, loss 0.547569, acc 0.76\n",
      "2018-04-22T04:02:33.016478: step 436, loss 0.497648, acc 0.77\n",
      "2018-04-22T04:02:33.060771: step 437, loss 0.556893, acc 0.74\n",
      "2018-04-22T04:02:33.102843: step 438, loss 0.502788, acc 0.81\n",
      "2018-04-22T04:02:33.148504: step 439, loss 0.504194, acc 0.77\n",
      "2018-04-22T04:02:33.193242: step 440, loss 0.56561, acc 0.69\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:33.547966: step 440, loss 0.646917, acc 0.541111, rec 0.989736, pre 0.524272, f1 0.685453\n",
      "\n",
      "2018-04-22T04:02:33.600506: step 441, loss 0.741434, acc 0.66\n",
      "2018-04-22T04:02:33.644555: step 442, loss 0.406103, acc 0.8\n",
      "2018-04-22T04:02:33.692001: step 443, loss 0.533544, acc 0.84\n",
      "2018-04-22T04:02:33.748482: step 444, loss 0.452109, acc 0.75\n",
      "2018-04-22T04:02:33.808481: step 445, loss 0.53659, acc 0.78\n",
      "2018-04-22T04:02:33.862945: step 446, loss 0.469018, acc 0.8\n",
      "2018-04-22T04:02:33.922211: step 447, loss 0.593189, acc 0.76\n",
      "2018-04-22T04:02:33.957333: step 448, loss 0.770591, acc 0.7\n",
      "2018-04-22T04:02:34.022455: step 449, loss 0.593827, acc 0.77\n",
      "2018-04-22T04:02:34.080496: step 450, loss 0.534472, acc 0.75\n",
      "2018-04-22T04:02:34.137723: step 451, loss 0.484374, acc 0.81\n",
      "2018-04-22T04:02:34.200521: step 452, loss 0.540641, acc 0.77\n",
      "2018-04-22T04:02:34.269227: step 453, loss 0.455501, acc 0.82\n",
      "2018-04-22T04:02:34.333337: step 454, loss 0.586026, acc 0.73\n",
      "2018-04-22T04:02:34.398988: step 455, loss 0.566547, acc 0.74\n",
      "2018-04-22T04:02:34.468494: step 456, loss 0.573739, acc 0.71\n",
      "2018-04-22T04:02:34.540500: step 457, loss 0.515684, acc 0.75\n",
      "2018-04-22T04:02:34.600489: step 458, loss 0.615803, acc 0.69\n",
      "2018-04-22T04:02:34.648452: step 459, loss 0.585114, acc 0.76\n",
      "2018-04-22T04:02:34.684717: step 460, loss 0.535964, acc 0.76\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:35.053635: step 460, loss 0.530626, acc 0.715556, rec 0.972141, pre 0.644942, f1 0.775439\n",
      "\n",
      "2018-04-22T04:02:35.101146: step 461, loss 0.542966, acc 0.72\n",
      "2018-04-22T04:02:35.124482: step 462, loss 0.393555, acc 0.84\n",
      "2018-04-22T04:02:35.165791: step 463, loss 0.525423, acc 0.79\n",
      "2018-04-22T04:02:35.202297: step 464, loss 0.485083, acc 0.78\n",
      "2018-04-22T04:02:35.248530: step 465, loss 0.56894, acc 0.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-22T04:02:35.292498: step 466, loss 0.50216, acc 0.77\n",
      "2018-04-22T04:02:35.332761: step 467, loss 0.536917, acc 0.72\n",
      "2018-04-22T04:02:35.374200: step 468, loss 0.581761, acc 0.71\n",
      "2018-04-22T04:02:35.416523: step 469, loss 0.566645, acc 0.68\n",
      "2018-04-22T04:02:35.460528: step 470, loss 0.439799, acc 0.8\n",
      "2018-04-22T04:02:35.501449: step 471, loss 0.503926, acc 0.78\n",
      "2018-04-22T04:02:35.544522: step 472, loss 0.515015, acc 0.78\n",
      "2018-04-22T04:02:35.581972: step 473, loss 0.663678, acc 0.67\n",
      "2018-04-22T04:02:35.624484: step 474, loss 0.72744, acc 0.69\n",
      "2018-04-22T04:02:35.668473: step 475, loss 0.658418, acc 0.66\n",
      "2018-04-22T04:02:35.693266: step 476, loss 0.580951, acc 0.72\n",
      "2018-04-22T04:02:35.744541: step 477, loss 0.395511, acc 0.8\n",
      "2018-04-22T04:02:35.785841: step 478, loss 0.601084, acc 0.74\n",
      "2018-04-22T04:02:35.832504: step 479, loss 0.655539, acc 0.73\n",
      "2018-04-22T04:02:35.873432: step 480, loss 0.613272, acc 0.72\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:36.241231: step 480, loss 0.549811, acc 0.647778, rec 0.97654, pre 0.591737, f1 0.736929\n",
      "\n",
      "2018-04-22T04:02:36.296435: step 481, loss 0.601471, acc 0.67\n",
      "2018-04-22T04:02:36.337639: step 482, loss 0.52865, acc 0.77\n",
      "2018-04-22T04:02:36.380500: step 483, loss 0.67098, acc 0.74\n",
      "2018-04-22T04:02:36.421427: step 484, loss 0.677481, acc 0.72\n",
      "2018-04-22T04:02:36.469299: step 485, loss 0.460607, acc 0.82\n",
      "2018-04-22T04:02:36.520536: step 486, loss 0.337844, acc 0.84\n",
      "2018-04-22T04:02:36.558407: step 487, loss 0.723071, acc 0.66\n",
      "2018-04-22T04:02:36.600490: step 488, loss 0.501512, acc 0.82\n",
      "2018-04-22T04:02:36.637224: step 489, loss 0.411007, acc 0.82\n",
      "2018-04-22T04:02:36.664521: step 490, loss 0.705702, acc 0.74\n",
      "2018-04-22T04:02:36.712787: step 491, loss 0.606608, acc 0.72\n",
      "2018-04-22T04:02:36.749735: step 492, loss 0.603333, acc 0.71\n",
      "2018-04-22T04:02:36.788526: step 493, loss 0.683836, acc 0.72\n",
      "2018-04-22T04:02:36.829614: step 494, loss 0.637584, acc 0.73\n",
      "2018-04-22T04:02:36.872513: step 495, loss 0.418017, acc 0.79\n",
      "2018-04-22T04:02:36.910050: step 496, loss 0.592846, acc 0.68\n",
      "2018-04-22T04:02:36.960533: step 497, loss 0.68729, acc 0.71\n",
      "2018-04-22T04:02:37.000510: step 498, loss 0.438026, acc 0.86\n",
      "2018-04-22T04:02:37.034673: step 499, loss 0.663116, acc 0.69\n",
      "2018-04-22T04:02:37.079067: step 500, loss 0.658543, acc 0.81\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:37.447364: step 500, loss 0.569007, acc 0.650741, rec 0.319648, pre 0.966741, f1 0.480441\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524369726/checkpoints/model-500\n",
      "\n",
      "2018-04-22T04:02:37.608197: step 501, loss 0.620135, acc 0.75\n",
      "2018-04-22T04:02:37.654437: step 502, loss 0.455609, acc 0.79\n",
      "2018-04-22T04:02:37.699342: step 503, loss 0.559244, acc 0.77\n",
      "2018-04-22T04:02:37.724537: step 504, loss 0.821086, acc 0.66\n",
      "2018-04-22T04:02:37.772506: step 505, loss 0.558767, acc 0.73\n",
      "2018-04-22T04:02:37.820506: step 506, loss 0.486356, acc 0.79\n",
      "2018-04-22T04:02:37.868504: step 507, loss 0.762286, acc 0.67\n",
      "2018-04-22T04:02:37.912511: step 508, loss 0.727165, acc 0.75\n",
      "2018-04-22T04:02:37.952486: step 509, loss 0.734254, acc 0.72\n",
      "2018-04-22T04:02:37.997565: step 510, loss 0.408556, acc 0.82\n",
      "2018-04-22T04:02:38.040510: step 511, loss 0.719316, acc 0.7\n",
      "2018-04-22T04:02:38.088497: step 512, loss 0.605463, acc 0.77\n",
      "2018-04-22T04:02:38.125699: step 513, loss 0.643183, acc 0.71\n",
      "2018-04-22T04:02:38.166671: step 514, loss 0.633833, acc 0.73\n",
      "2018-04-22T04:02:38.210497: step 515, loss 0.489647, acc 0.77\n",
      "2018-04-22T04:02:38.252501: step 516, loss 0.462737, acc 0.82\n",
      "2018-04-22T04:02:38.294021: step 517, loss 0.565162, acc 0.74\n",
      "2018-04-22T04:02:38.319466: step 518, loss 0.76437, acc 0.64\n",
      "2018-04-22T04:02:38.362048: step 519, loss 0.570847, acc 0.78\n",
      "2018-04-22T04:02:38.404529: step 520, loss 0.604482, acc 0.7\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:38.752512: step 520, loss 0.511216, acc 0.767407, rec 0.561584, pre 0.962312, f1 0.709259\n",
      "\n",
      "2018-04-22T04:02:38.803510: step 521, loss 0.550314, acc 0.77\n",
      "2018-04-22T04:02:38.844705: step 522, loss 0.637285, acc 0.73\n",
      "2018-04-22T04:02:38.885493: step 523, loss 0.57529, acc 0.74\n",
      "2018-04-22T04:02:38.926736: step 524, loss 0.667313, acc 0.69\n",
      "2018-04-22T04:02:38.976515: step 525, loss 0.527864, acc 0.71\n",
      "2018-04-22T04:02:39.020497: step 526, loss 0.673773, acc 0.69\n",
      "2018-04-22T04:02:39.061202: step 527, loss 0.724068, acc 0.7\n",
      "2018-04-22T04:02:39.104496: step 528, loss 0.654866, acc 0.74\n",
      "2018-04-22T04:02:39.145965: step 529, loss 0.502813, acc 0.77\n",
      "2018-04-22T04:02:39.191572: step 530, loss 0.778009, acc 0.67\n",
      "2018-04-22T04:02:39.236457: step 531, loss 0.456727, acc 0.81\n",
      "2018-04-22T04:02:39.264477: step 532, loss 0.589074, acc 0.76\n",
      "2018-04-22T04:02:39.306107: step 533, loss 0.800338, acc 0.6\n",
      "2018-04-22T04:02:39.400491: step 534, loss 0.532827, acc 0.8\n",
      "2018-04-22T04:02:39.441810: step 535, loss 0.623973, acc 0.76\n",
      "2018-04-22T04:02:39.485179: step 536, loss 0.590408, acc 0.77\n",
      "2018-04-22T04:02:39.527011: step 537, loss 0.561443, acc 0.72\n",
      "2018-04-22T04:02:39.572537: step 538, loss 0.497996, acc 0.77\n",
      "2018-04-22T04:02:39.612526: step 539, loss 0.616601, acc 0.74\n",
      "2018-04-22T04:02:39.656526: step 540, loss 0.358718, acc 0.81\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:39.997632: step 540, loss 0.502802, acc 0.858519, rec 0.879765, pre 0.846262, f1 0.862689\n",
      "\n",
      "2018-04-22T04:02:40.045139: step 541, loss 0.539304, acc 0.69\n",
      "2018-04-22T04:02:40.088490: step 542, loss 0.600792, acc 0.81\n",
      "2018-04-22T04:02:40.132463: step 543, loss 0.442683, acc 0.78\n",
      "2018-04-22T04:02:40.176626: step 544, loss 0.665152, acc 0.72\n",
      "2018-04-22T04:02:40.224507: step 545, loss 0.551111, acc 0.74\n",
      "2018-04-22T04:02:40.252514: step 546, loss 0.618107, acc 0.74\n",
      "2018-04-22T04:02:40.292519: step 547, loss 0.613292, acc 0.71\n",
      "2018-04-22T04:02:40.342252: step 548, loss 0.592437, acc 0.79\n",
      "2018-04-22T04:02:40.388477: step 549, loss 0.459195, acc 0.76\n",
      "2018-04-22T04:02:40.437146: step 550, loss 0.545584, acc 0.77\n",
      "2018-04-22T04:02:40.480501: step 551, loss 0.533726, acc 0.77\n",
      "2018-04-22T04:02:40.540051: step 552, loss 0.618345, acc 0.75\n",
      "2018-04-22T04:02:40.584485: step 553, loss 0.509123, acc 0.79\n",
      "2018-04-22T04:02:40.632505: step 554, loss 0.436885, acc 0.81\n",
      "2018-04-22T04:02:40.680513: step 555, loss 0.556601, acc 0.71\n",
      "2018-04-22T04:02:40.720800: step 556, loss 0.56709, acc 0.73\n",
      "2018-04-22T04:02:40.764830: step 557, loss 0.591575, acc 0.77\n",
      "2018-04-22T04:02:40.811870: step 558, loss 0.496866, acc 0.75\n",
      "2018-04-22T04:02:40.857160: step 559, loss 0.625188, acc 0.69\n",
      "2018-04-22T04:02:40.885841: step 560, loss 0.588147, acc 0.74\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:41.225626: step 560, loss 0.565502, acc 0.642222, rec 0.980205, pre 0.587434, f1 0.734615\n",
      "\n",
      "2018-04-22T04:02:41.276506: step 561, loss 0.521957, acc 0.78\n",
      "2018-04-22T04:02:41.308959: step 562, loss 0.610706, acc 0.73\n",
      "2018-04-22T04:02:41.352479: step 563, loss 0.502749, acc 0.78\n",
      "2018-04-22T04:02:41.391195: step 564, loss 0.62565, acc 0.71\n",
      "2018-04-22T04:02:41.439048: step 565, loss 0.495193, acc 0.75\n",
      "2018-04-22T04:02:41.480517: step 566, loss 0.462163, acc 0.79\n",
      "2018-04-22T04:02:41.528510: step 567, loss 0.569841, acc 0.69\n",
      "2018-04-22T04:02:41.573953: step 568, loss 0.414455, acc 0.83\n",
      "2018-04-22T04:02:41.620562: step 569, loss 0.510343, acc 0.77\n",
      "2018-04-22T04:02:41.668491: step 570, loss 0.496072, acc 0.76\n",
      "2018-04-22T04:02:41.712521: step 571, loss 0.473437, acc 0.76\n",
      "2018-04-22T04:02:41.753793: step 572, loss 0.692664, acc 0.65\n",
      "2018-04-22T04:02:41.800510: step 573, loss 0.482782, acc 0.79\n",
      "2018-04-22T04:02:41.824484: step 574, loss 0.677209, acc 0.58\n",
      "2018-04-22T04:02:41.865410: step 575, loss 0.50221, acc 0.73\n",
      "2018-04-22T04:02:41.916083: step 576, loss 0.582631, acc 0.77\n",
      "2018-04-22T04:02:41.956505: step 577, loss 0.681461, acc 0.71\n",
      "2018-04-22T04:02:42.000501: step 578, loss 0.567372, acc 0.77\n",
      "2018-04-22T04:02:42.044505: step 579, loss 0.51176, acc 0.73\n",
      "2018-04-22T04:02:42.085005: step 580, loss 0.526844, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:42.437599: step 580, loss 0.50791, acc 0.837407, rec 0.926686, pre 0.788522, f1 0.852039\n",
      "\n",
      "2018-04-22T04:02:42.481287: step 581, loss 0.649806, acc 0.71\n",
      "2018-04-22T04:02:42.522864: step 582, loss 0.452805, acc 0.84\n",
      "2018-04-22T04:02:42.564501: step 583, loss 0.711784, acc 0.72\n",
      "2018-04-22T04:02:42.601295: step 584, loss 0.459935, acc 0.76\n",
      "2018-04-22T04:02:42.648547: step 585, loss 0.573363, acc 0.72\n",
      "2018-04-22T04:02:42.692511: step 586, loss 0.48598, acc 0.73\n",
      "2018-04-22T04:02:42.728837: step 587, loss 0.583643, acc 0.72\n",
      "2018-04-22T04:02:42.753417: step 588, loss 0.587211, acc 0.7\n",
      "2018-04-22T04:02:42.794161: step 589, loss 0.508102, acc 0.8\n",
      "2018-04-22T04:02:42.836440: step 590, loss 0.53605, acc 0.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-22T04:02:42.880854: step 591, loss 0.612409, acc 0.74\n",
      "2018-04-22T04:02:42.920426: step 592, loss 0.523272, acc 0.72\n",
      "2018-04-22T04:02:42.964419: step 593, loss 0.492629, acc 0.8\n",
      "2018-04-22T04:02:43.001962: step 594, loss 0.608239, acc 0.69\n",
      "2018-04-22T04:02:43.042261: step 595, loss 0.620278, acc 0.7\n",
      "2018-04-22T04:02:43.088200: step 596, loss 0.643728, acc 0.7\n",
      "2018-04-22T04:02:43.128605: step 597, loss 0.569269, acc 0.72\n",
      "2018-04-22T04:02:43.172461: step 598, loss 0.506981, acc 0.77\n",
      "2018-04-22T04:02:43.212788: step 599, loss 0.536887, acc 0.74\n",
      "2018-04-22T04:02:43.257040: step 600, loss 0.652089, acc 0.67\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:43.609519: step 600, loss 0.564983, acc 0.618889, rec 0.980938, pre 0.571551, f1 0.722267\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524369726/checkpoints/model-600\n",
      "\n",
      "2018-04-22T04:02:43.756448: step 601, loss 0.495781, acc 0.74\n",
      "2018-04-22T04:02:43.781102: step 602, loss 0.60406, acc 0.66\n",
      "2018-04-22T04:02:43.824451: step 603, loss 0.571218, acc 0.73\n",
      "2018-04-22T04:02:43.861036: step 604, loss 0.536017, acc 0.76\n",
      "2018-04-22T04:02:43.904471: step 605, loss 0.56309, acc 0.74\n",
      "2018-04-22T04:02:43.941817: step 606, loss 0.583032, acc 0.75\n",
      "2018-04-22T04:02:43.984454: step 607, loss 0.553551, acc 0.76\n",
      "2018-04-22T04:02:44.021123: step 608, loss 0.477807, acc 0.73\n",
      "2018-04-22T04:02:44.060974: step 609, loss 0.589688, acc 0.7\n",
      "2018-04-22T04:02:44.101297: step 610, loss 0.600383, acc 0.72\n",
      "2018-04-22T04:02:44.142418: step 611, loss 0.566971, acc 0.73\n",
      "2018-04-22T04:02:44.184448: step 612, loss 0.712877, acc 0.7\n",
      "2018-04-22T04:02:44.224466: step 613, loss 0.560823, acc 0.73\n",
      "2018-04-22T04:02:44.272928: step 614, loss 0.62551, acc 0.7\n",
      "2018-04-22T04:02:44.317305: step 615, loss 0.582493, acc 0.72\n",
      "2018-04-22T04:02:44.341582: step 616, loss 0.458247, acc 0.78\n",
      "2018-04-22T04:02:44.380856: step 617, loss 0.648172, acc 0.74\n",
      "2018-04-22T04:02:44.424427: step 618, loss 0.777627, acc 0.71\n",
      "2018-04-22T04:02:44.464586: step 619, loss 0.56159, acc 0.76\n",
      "2018-04-22T04:02:44.508460: step 620, loss 0.631468, acc 0.71\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:44.858726: step 620, loss 0.903973, acc 0.508519, rec 0.996334, pre 0.5069, f1 0.671941\n",
      "\n",
      "2018-04-22T04:02:44.908437: step 621, loss 0.710802, acc 0.65\n",
      "2018-04-22T04:02:44.952443: step 622, loss 0.702764, acc 0.68\n",
      "2018-04-22T04:02:44.992418: step 623, loss 0.585044, acc 0.76\n",
      "2018-04-22T04:02:45.034874: step 624, loss 0.524604, acc 0.84\n",
      "2018-04-22T04:02:45.080449: step 625, loss 0.742783, acc 0.71\n",
      "2018-04-22T04:02:45.116460: step 626, loss 0.58103, acc 0.76\n",
      "2018-04-22T04:02:45.152685: step 627, loss 0.628687, acc 0.72\n",
      "2018-04-22T04:02:45.192704: step 628, loss 0.636449, acc 0.71\n",
      "2018-04-22T04:02:45.228778: step 629, loss 0.740685, acc 0.68\n",
      "2018-04-22T04:02:45.253206: step 630, loss 0.835993, acc 0.62\n",
      "2018-04-22T04:02:45.296454: step 631, loss 0.508054, acc 0.79\n",
      "2018-04-22T04:02:45.336453: step 632, loss 0.761753, acc 0.69\n",
      "2018-04-22T04:02:45.372696: step 633, loss 0.741459, acc 0.71\n",
      "2018-04-22T04:02:45.411310: step 634, loss 0.561828, acc 0.73\n",
      "2018-04-22T04:02:45.452451: step 635, loss 0.501216, acc 0.8\n",
      "2018-04-22T04:02:45.488445: step 636, loss 0.644836, acc 0.7\n",
      "2018-04-22T04:02:45.528590: step 637, loss 0.474931, acc 0.81\n",
      "2018-04-22T04:02:45.568703: step 638, loss 0.566249, acc 0.79\n",
      "2018-04-22T04:02:45.608805: step 639, loss 0.451098, acc 0.76\n",
      "2018-04-22T04:02:45.650578: step 640, loss 0.528266, acc 0.76\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:46.009501: step 640, loss 0.664944, acc 0.577037, rec 0.164223, pre 0.99115, f1 0.281761\n",
      "\n",
      "2018-04-22T04:02:46.053809: step 641, loss 0.589299, acc 0.75\n",
      "2018-04-22T04:02:46.099449: step 642, loss 0.616968, acc 0.75\n",
      "2018-04-22T04:02:46.135592: step 643, loss 0.657384, acc 0.76\n",
      "2018-04-22T04:02:46.159705: step 644, loss 0.507397, acc 0.78\n",
      "2018-04-22T04:02:46.200448: step 645, loss 0.625656, acc 0.7\n",
      "2018-04-22T04:02:46.248456: step 646, loss 0.544044, acc 0.74\n",
      "2018-04-22T04:02:46.292468: step 647, loss 0.588515, acc 0.76\n",
      "2018-04-22T04:02:46.333052: step 648, loss 0.547318, acc 0.71\n",
      "2018-04-22T04:02:46.375862: step 649, loss 0.53037, acc 0.75\n",
      "2018-04-22T04:02:46.416869: step 650, loss 0.602675, acc 0.73\n",
      "2018-04-22T04:02:46.460464: step 651, loss 0.499935, acc 0.77\n",
      "2018-04-22T04:02:46.501001: step 652, loss 0.481969, acc 0.79\n",
      "2018-04-22T04:02:46.538699: step 653, loss 0.603311, acc 0.73\n",
      "2018-04-22T04:02:46.580456: step 654, loss 0.501409, acc 0.76\n",
      "2018-04-22T04:02:46.616585: step 655, loss 0.537001, acc 0.72\n",
      "2018-04-22T04:02:46.656774: step 656, loss 0.550131, acc 0.75\n",
      "2018-04-22T04:02:46.704457: step 657, loss 0.57743, acc 0.71\n",
      "2018-04-22T04:02:46.720692: step 658, loss 0.431287, acc 0.8\n",
      "2018-04-22T04:02:46.761513: step 659, loss 0.584057, acc 0.75\n",
      "2018-04-22T04:02:46.804459: step 660, loss 0.652422, acc 0.71\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:47.148446: step 660, loss 0.540997, acc 0.717037, rec 0.969941, pre 0.646628, f1 0.775953\n",
      "\n",
      "2018-04-22T04:02:47.192447: step 661, loss 0.502949, acc 0.72\n",
      "2018-04-22T04:02:47.235109: step 662, loss 0.576785, acc 0.69\n",
      "2018-04-22T04:02:47.276446: step 663, loss 0.491099, acc 0.78\n",
      "2018-04-22T04:02:47.320753: step 664, loss 0.529968, acc 0.72\n",
      "2018-04-22T04:02:47.368466: step 665, loss 0.348701, acc 0.91\n",
      "2018-04-22T04:02:47.408623: step 666, loss 0.470661, acc 0.78\n",
      "2018-04-22T04:02:47.449503: step 667, loss 0.570683, acc 0.77\n",
      "2018-04-22T04:02:47.488454: step 668, loss 0.607367, acc 0.7\n",
      "2018-04-22T04:02:47.528794: step 669, loss 0.513675, acc 0.76\n",
      "2018-04-22T04:02:47.577463: step 670, loss 0.572366, acc 0.71\n",
      "2018-04-22T04:02:47.620453: step 671, loss 0.508607, acc 0.75\n",
      "2018-04-22T04:02:47.640667: step 672, loss 0.465397, acc 0.78\n",
      "2018-04-22T04:02:47.681447: step 673, loss 0.534553, acc 0.76\n",
      "2018-04-22T04:02:47.723721: step 674, loss 0.595231, acc 0.69\n",
      "2018-04-22T04:02:47.767271: step 675, loss 0.544729, acc 0.76\n",
      "2018-04-22T04:02:47.812460: step 676, loss 0.505324, acc 0.76\n",
      "2018-04-22T04:02:47.848949: step 677, loss 0.535217, acc 0.78\n",
      "2018-04-22T04:02:47.889256: step 678, loss 0.5429, acc 0.75\n",
      "2018-04-22T04:02:47.932462: step 679, loss 0.497365, acc 0.74\n",
      "2018-04-22T04:02:47.964671: step 680, loss 0.523966, acc 0.69\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:48.296763: step 680, loss 0.556029, acc 0.658889, rec 0.972874, pre 0.600181, f1 0.742378\n",
      "\n",
      "2018-04-22T04:02:48.342973: step 681, loss 0.551538, acc 0.74\n",
      "2018-04-22T04:02:48.381516: step 682, loss 0.503222, acc 0.8\n",
      "2018-04-22T04:02:48.424458: step 683, loss 0.513374, acc 0.77\n",
      "2018-04-22T04:02:48.465185: step 684, loss 0.546166, acc 0.75\n",
      "2018-04-22T04:02:48.510733: step 685, loss 0.602765, acc 0.73\n",
      "2018-04-22T04:02:48.540443: step 686, loss 0.490404, acc 0.78\n",
      "2018-04-22T04:02:48.580434: step 687, loss 0.470403, acc 0.77\n",
      "2018-04-22T04:02:48.620470: step 688, loss 0.582354, acc 0.74\n",
      "2018-04-22T04:02:48.660737: step 689, loss 0.60216, acc 0.74\n",
      "2018-04-22T04:02:48.705373: step 690, loss 0.533618, acc 0.71\n",
      "2018-04-22T04:02:48.752575: step 691, loss 0.518974, acc 0.8\n",
      "2018-04-22T04:02:48.792673: step 692, loss 0.625478, acc 0.71\n",
      "2018-04-22T04:02:48.832969: step 693, loss 0.537413, acc 0.73\n",
      "2018-04-22T04:02:48.873303: step 694, loss 0.474695, acc 0.79\n",
      "2018-04-22T04:02:48.920459: step 695, loss 0.702295, acc 0.71\n",
      "2018-04-22T04:02:48.964597: step 696, loss 0.610303, acc 0.73\n",
      "2018-04-22T04:02:49.005082: step 697, loss 0.520072, acc 0.77\n",
      "2018-04-22T04:02:49.050074: step 698, loss 0.589459, acc 0.72\n",
      "2018-04-22T04:02:49.089895: step 699, loss 0.466351, acc 0.77\n",
      "2018-04-22T04:02:49.116421: step 700, loss 0.522398, acc 0.8\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T04:02:49.444457: step 700, loss 0.51213, acc 0.815556, rec 0.778592, pre 0.844197, f1 0.810069\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524369726/checkpoints/model-700\n",
      "\n",
      "\n",
      "Test Set:\n",
      "2018-04-22T04:02:49.717920: step 700, loss 0.510826, acc 0.815556, rec 0.782609, pre 0.84507, f1 0.812641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", 0.75, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 25, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"1\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 25, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.4, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 100\n",
    "tf.flags.DEFINE_integer(\"batch_size\", batch_size, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 50, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 20, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=train_s.shape[1],\n",
    "            num_classes=2,\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "        \n",
    "         # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        #vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch1, x_batch2, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch1, x_batch2, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            #r \n",
    "            step, summaries, loss, accuracy, recall, precision, f1 = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy,cnn.recall, cnn.precision, cnn.f1_score],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, rec {:g}, pre {:g}, f1 {:g}\".format(time_str, step, loss, accuracy, recall, precision, f1))# recall, precision, f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(train_s, train_c,  expanded_train_labels)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch1, x_batch2, y_batch = zip(*batch)\n",
    "            train_step(x_batch1, x_batch1, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(validation_s, validation_c, expanded_validation_labels, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "print(\"\\nTest Set:\")\n",
    "dev_step(test_s, test_c, expanded_test_labels, writer=dev_summary_writer)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
