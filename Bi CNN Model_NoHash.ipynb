{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saulgrimaldo1/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from w266_common import utils, vocabulary\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "np.random.seed(266)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "tokenizer = TweetTokenizer()\n",
    "x_data = []\n",
    "x_contexts = []\n",
    "labels = []\n",
    "sentences  = []\n",
    "contexts = []\n",
    "with open('merged_data_v4.csv', 'r') as csvfile:\n",
    "    linereader = csv.reader(csvfile, delimiter = '|')\n",
    "    for i, row in enumerate(linereader):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        sentence, context, sarcasm = row\n",
    "        sentence = re.sub(\"RT @[^\\s]+:\", \"retweet\", sentence)\n",
    "        sentences.append(sentence)\n",
    "        contexts.append(context)\n",
    "        x_tokens = utils.canonicalize_words(tokenizer.tokenize(sentence), hashtags = True)\n",
    "        context_tokens = utils.canonicalize_words(tokenizer.tokenize(context), hashtags = True)\n",
    "        x_data.append(x_tokens)\n",
    "        x_contexts.append(context_tokens)\n",
    "        labels.append(int(sarcasm))\n",
    "\n",
    "\n",
    "\n",
    "shuffle_indices = np.random.permutation(np.arange(len(labels)))\n",
    "train_split_idx = int(0.7 * len(labels))\n",
    "test_split_idx  = int(0.9 * len(labels))\n",
    "\n",
    "train_indices = shuffle_indices[:train_split_idx]\n",
    "validation_indices = shuffle_indices[train_split_idx:test_split_idx]\n",
    "test_indices = shuffle_indices[test_split_idx:]\n",
    "\n",
    "\n",
    "train_sentences = np.array(x_data)[train_indices]\n",
    "train_contexts = np.array(x_contexts)[train_indices]\n",
    "train_labels= np.array(labels)[train_indices] \n",
    "validation_sentences = np.array(x_data)[validation_indices]\n",
    "validation_labels = np.array(labels)[validation_indices]\n",
    "validation_contexts = np.array(x_contexts)[validation_indices]\n",
    "test_sentences = np.array(x_data)[test_indices]  \n",
    "test_contexts =  np.array(x_contexts)[test_indices]\n",
    "test_labels = np.array(labels)[test_indices]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 6, 2]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [2]*4\n",
    "a[2] = 6\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_labels(raw_label_set, size):\n",
    "    label_set = []\n",
    "    for label in raw_label_set:\n",
    "        labels = [0] * size\n",
    "        labels[label] = 1\n",
    "        label_set.append(labels)\n",
    "    return np.array(label_set)\n",
    "\n",
    "expanded_train_labels = transform_labels(train_labels, 2)\n",
    "expanded_validation_labels = transform_labels(validation_labels,2)\n",
    "expanded_test_labels = transform_labels(test_labels,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,3,4,5,6,7,8,8,1,5,6,7]\n",
    "a + [\"<PADDING>\"]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'angry',\n",
       " 'man',\n",
       " 'is',\n",
       " 'angry',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PaddingAndTruncating:\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def pad_or_truncate(self, sentence):\n",
    "        sen_len = len(sentence)\n",
    "        paddings = self.max_len - sen_len\n",
    "        if paddings >=0:\n",
    "            return list(sentence) + [\"<PADDING>\"] * paddings\n",
    "        return sentence[0:paddings]\n",
    "        \n",
    "PadAndTrunc = PaddingAndTruncating(10)\n",
    "        \n",
    "        \n",
    "PadAndTrunc.pad_or_truncate([\"the\",\"angry\",\"man\",\"is\",\"angry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  12,  877,   22, ...,    3,    3,    3],\n",
       "       [  12,   80,   41, ...,    3,    3,    3],\n",
       "       [  93,  618,    2, ...,    3,    3,    3],\n",
       "       ..., \n",
       "       [  12,    2,  182, ...,    3,    3,    3],\n",
       "       [   2, 2646, 3847, ...,    3,    3,    3],\n",
       "       [  12,   39,  983, ...,    3,    3,    3]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.contrib import learn\n",
    "vocab_size = 5000\n",
    "\n",
    "#max_len = max([len(sent) for sent  in train_sentences])\n",
    "PadAndTrunc =PaddingAndTruncating(40)\n",
    "train_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, train_sentences))\n",
    "train_context_padded = list(map(PadAndTrunc.pad_or_truncate, train_contexts))\n",
    "validation_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, validation_sentences))\n",
    "validation_context_padded = list(map(PadAndTrunc.pad_or_truncate, validation_contexts))\n",
    "test_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, test_sentences))\n",
    "test_context_padded = list(map(PadAndTrunc.pad_or_truncate, test_contexts))\n",
    "\n",
    "vocab = vocabulary.Vocabulary(utils.flatten(list(train_sentences_padded) + list(train_context_padded)), vocab_size)\n",
    "train_s = np.array(list(map(vocab.words_to_ids, train_sentences_padded)))\n",
    "train_c = np.array(list(map(vocab.words_to_ids, train_context_padded)))\n",
    "validation_s = np.array(list(map(vocab.words_to_ids, validation_sentences_padded)))\n",
    "validation_c = np.array(list(map(vocab.words_to_ids, validation_context_padded)))\n",
    "test_s = np.array(list(map(vocab.words_to_ids, test_sentences_padded)))\n",
    "test_c = np.array(list(map(vocab.words_to_ids, test_context_padded)))\n",
    "train_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv-maxpool-3'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 3\n",
    "(\"conv-maxpool-%s\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size, \n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x1 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x1\")\n",
    "        self.input_x2 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x2\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars1 = tf.nn.embedding_lookup(self.W, self.input_x1)\n",
    "            self.embedded_chars_expanded1 = tf.expand_dims(self.embedded_chars1, -1)\n",
    "            self.embedded_chars2 = tf.nn.embedding_lookup(self.W, self.input_x2)\n",
    "            self.embedded_chars_expanded2 = tf.expand_dims(self.embedded_chars2, -1)\n",
    "\n",
    "        # Create a convolution + avgpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.variable_scope(\"conv-avgpool-%s\" % i) as scope:\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "              \n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded1,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h1 = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"tanh1\")\n",
    "               \n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.avg_pool(\n",
    "                    h1,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                \n",
    "                pooled_outputs.append(pooled)\n",
    "                #pooled_outputs.append(pooled)\n",
    "                scope.reuse_variables()\n",
    "                 \n",
    "                \n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded2,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h2 = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"tanh2\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled2 = tf.nn.avg_pool(\n",
    "                    h2,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled2)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "\n",
    "        num_filters_total = num_filters * len(filter_sizes) * 2\n",
    "\n",
    "        self.h_pool = tf.concat(pooled_outputs, 1)\n",
    "\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "        \n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "           \n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.labels = tf.argmax(self.input_y, 1)\n",
    "            TP = tf.count_nonzero(self.predictions * self.labels)\n",
    "            TN = tf.count_nonzero((self.predictions - 1) * (self.labels - 1))\n",
    "            FP = tf.count_nonzero(self.predictions * (self.labels - 1))\n",
    "            FN = tf.count_nonzero((self.predictions - 1) * self.labels)\n",
    "            self.correct_predictions = tf.equal(self.predictions, self.labels)\n",
    "            self.precision = TP / (TP + FP)\n",
    "            self.recall = TP / (TP + FN)\n",
    "            self.f1_score = 2 * self.precision * self.recall / (self.precision + self.recall)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=200\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.75\n",
      "DROPOUT_KEEP_PROB=0.4\n",
      "EMBEDDING_DIM=40\n",
      "EVALUATE_EVERY=20\n",
      "FILTER_SIZES=1\n",
      "L2_REG_LAMBDA=0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.neg\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=10\n",
      "NUM_FILTERS=40\n",
      "POSITIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.pos\n",
      "\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/W:0/grad/hist is illegal; using conv-avgpool-0/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/W:0/grad/sparsity is illegal; using conv-avgpool-0/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/b:0/grad/hist is illegal; using conv-avgpool-0/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/b:0/grad/sparsity is illegal; using conv-avgpool-0/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524427983\n",
      "\n",
      "2018-04-22T20:13:04.300492: step 1, loss 0.716011, acc 0.47\n",
      "2018-04-22T20:13:04.384460: step 2, loss 1.48887, acc 0.54\n",
      "2018-04-22T20:13:04.464453: step 3, loss 1.70451, acc 0.515\n",
      "2018-04-22T20:13:04.539268: step 4, loss 0.886647, acc 0.505\n",
      "2018-04-22T20:13:04.624455: step 5, loss 1.00332, acc 0.6\n",
      "2018-04-22T20:13:04.704470: step 6, loss 1.02038, acc 0.645\n",
      "2018-04-22T20:13:04.785186: step 7, loss 0.665916, acc 0.735\n",
      "2018-04-22T20:13:04.864477: step 8, loss 0.78398, acc 0.6\n",
      "2018-04-22T20:13:04.939749: step 9, loss 0.574587, acc 0.74\n",
      "2018-04-22T20:13:05.024485: step 10, loss 0.603165, acc 0.725\n",
      "2018-04-22T20:13:05.105131: step 11, loss 0.529702, acc 0.775\n",
      "2018-04-22T20:13:05.186735: step 12, loss 0.616981, acc 0.7\n",
      "2018-04-22T20:13:05.276479: step 13, loss 0.505244, acc 0.785\n",
      "2018-04-22T20:13:05.350181: step 14, loss 0.452446, acc 0.79\n",
      "2018-04-22T20:13:05.428455: step 15, loss 0.503527, acc 0.79\n",
      "2018-04-22T20:13:05.512453: step 16, loss 0.440225, acc 0.815\n",
      "2018-04-22T20:13:05.584451: step 17, loss 0.504977, acc 0.77\n",
      "2018-04-22T20:13:05.660471: step 18, loss 0.400293, acc 0.84\n",
      "2018-04-22T20:13:05.739163: step 19, loss 0.568673, acc 0.755\n",
      "2018-04-22T20:13:05.824949: step 20, loss 0.422225, acc 0.79\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:06.193478: step 20, loss 0.44102, acc 0.827037, rec 0.89956, pre 0.788054, f1 0.840123\n",
      "\n",
      "2018-04-22T20:13:06.276466: step 21, loss 0.397593, acc 0.805\n",
      "2018-04-22T20:13:06.356458: step 22, loss 0.411703, acc 0.835\n",
      "2018-04-22T20:13:06.432935: step 23, loss 0.612455, acc 0.805\n",
      "2018-04-22T20:13:06.516463: step 24, loss 0.405191, acc 0.82\n",
      "2018-04-22T20:13:06.592479: step 25, loss 0.488256, acc 0.83\n",
      "2018-04-22T20:13:06.678123: step 26, loss 0.519297, acc 0.75\n",
      "2018-04-22T20:13:06.758112: step 27, loss 0.521582, acc 0.77\n",
      "2018-04-22T20:13:06.836478: step 28, loss 0.427278, acc 0.815\n",
      "2018-04-22T20:13:06.924945: step 29, loss 0.429839, acc 0.825\n",
      "2018-04-22T20:13:07.005110: step 30, loss 0.36799, acc 0.87\n",
      "2018-04-22T20:13:07.080493: step 31, loss 0.376428, acc 0.855\n",
      "2018-04-22T20:13:07.164525: step 32, loss 0.382741, acc 0.85\n",
      "2018-04-22T20:13:07.244499: step 33, loss 0.504557, acc 0.775\n",
      "2018-04-22T20:13:07.324530: step 34, loss 0.437765, acc 0.85\n",
      "2018-04-22T20:13:07.408525: step 35, loss 0.445158, acc 0.815\n",
      "2018-04-22T20:13:07.486408: step 36, loss 0.37681, acc 0.845\n",
      "2018-04-22T20:13:07.565145: step 37, loss 0.451417, acc 0.82\n",
      "2018-04-22T20:13:07.648512: step 38, loss 0.477082, acc 0.83\n",
      "2018-04-22T20:13:07.728500: step 39, loss 0.365178, acc 0.845\n",
      "2018-04-22T20:13:07.808502: step 40, loss 0.429657, acc 0.8\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:08.148510: step 40, loss 0.443879, acc 0.798519, rec 0.659091, pre 0.919223, f1 0.76772\n",
      "\n",
      "2018-04-22T20:13:08.236537: step 41, loss 0.497529, acc 0.81\n",
      "2018-04-22T20:13:08.320636: step 42, loss 0.487781, acc 0.815\n",
      "2018-04-22T20:13:08.400693: step 43, loss 0.481964, acc 0.8\n",
      "2018-04-22T20:13:08.478154: step 44, loss 0.393767, acc 0.82\n",
      "2018-04-22T20:13:08.560591: step 45, loss 0.366074, acc 0.845\n",
      "2018-04-22T20:13:08.640524: step 46, loss 0.466574, acc 0.825\n",
      "2018-04-22T20:13:08.716865: step 47, loss 0.456067, acc 0.815\n",
      "2018-04-22T20:13:08.745115: step 48, loss 0.755947, acc 0.829787\n",
      "2018-04-22T20:13:08.828523: step 49, loss 0.386612, acc 0.825\n",
      "2018-04-22T20:13:08.912527: step 50, loss 0.365293, acc 0.835\n",
      "2018-04-22T20:13:08.990538: step 51, loss 0.361744, acc 0.835\n",
      "2018-04-22T20:13:09.073032: step 52, loss 0.426885, acc 0.83\n",
      "2018-04-22T20:13:09.156485: step 53, loss 0.367845, acc 0.86\n",
      "2018-04-22T20:13:09.240511: step 54, loss 0.422114, acc 0.82\n",
      "2018-04-22T20:13:09.316917: step 55, loss 0.5279, acc 0.81\n",
      "2018-04-22T20:13:09.400523: step 56, loss 0.397655, acc 0.84\n",
      "2018-04-22T20:13:09.480521: step 57, loss 0.317799, acc 0.855\n",
      "2018-04-22T20:13:09.564500: step 58, loss 0.469451, acc 0.805\n",
      "2018-04-22T20:13:09.644543: step 59, loss 0.388381, acc 0.845\n",
      "2018-04-22T20:13:09.725548: step 60, loss 0.393767, acc 0.83\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:10.060487: step 60, loss 0.623577, acc 0.645185, rec 0.978006, pre 0.589744, f1 0.735797\n",
      "\n",
      "2018-04-22T20:13:10.146346: step 61, loss 0.357478, acc 0.875\n",
      "2018-04-22T20:13:10.220815: step 62, loss 0.430418, acc 0.85\n",
      "2018-04-22T20:13:10.301980: step 63, loss 0.243517, acc 0.91\n",
      "2018-04-22T20:13:10.384479: step 64, loss 0.298341, acc 0.87\n",
      "2018-04-22T20:13:10.459202: step 65, loss 0.331875, acc 0.875\n",
      "2018-04-22T20:13:10.568483: step 66, loss 0.294725, acc 0.875\n",
      "2018-04-22T20:13:10.651562: step 67, loss 0.470152, acc 0.86\n",
      "2018-04-22T20:13:10.728549: step 68, loss 0.405409, acc 0.85\n",
      "2018-04-22T20:13:10.814293: step 69, loss 0.283136, acc 0.895\n",
      "2018-04-22T20:13:10.893265: step 70, loss 0.305019, acc 0.89\n",
      "2018-04-22T20:13:10.972620: step 71, loss 0.458976, acc 0.86\n",
      "2018-04-22T20:13:11.056467: step 72, loss 0.419971, acc 0.85\n",
      "2018-04-22T20:13:11.135343: step 73, loss 0.284929, acc 0.89\n",
      "2018-04-22T20:13:11.220506: step 74, loss 0.315889, acc 0.875\n",
      "2018-04-22T20:13:11.300503: step 75, loss 0.294678, acc 0.875\n",
      "2018-04-22T20:13:11.378456: step 76, loss 0.243718, acc 0.905\n",
      "2018-04-22T20:13:11.457462: step 77, loss 0.332172, acc 0.865\n",
      "2018-04-22T20:13:11.540512: step 78, loss 0.264145, acc 0.88\n",
      "2018-04-22T20:13:11.616567: step 79, loss 0.26157, acc 0.875\n",
      "2018-04-22T20:13:11.693822: step 80, loss 0.382709, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:12.025457: step 80, loss 0.509703, acc 0.721111, rec 0.982405, pre 0.647656, f1 0.780658\n",
      "\n",
      "2018-04-22T20:13:12.112456: step 81, loss 0.350387, acc 0.9\n",
      "2018-04-22T20:13:12.188458: step 82, loss 0.407937, acc 0.86\n",
      "2018-04-22T20:13:12.270361: step 83, loss 0.355157, acc 0.88\n",
      "2018-04-22T20:13:12.348461: step 84, loss 0.339927, acc 0.86\n",
      "2018-04-22T20:13:12.422979: step 85, loss 0.3571, acc 0.88\n",
      "2018-04-22T20:13:12.504451: step 86, loss 0.366061, acc 0.82\n",
      "2018-04-22T20:13:12.584472: step 87, loss 0.340052, acc 0.875\n",
      "2018-04-22T20:13:12.660474: step 88, loss 0.258724, acc 0.9\n",
      "2018-04-22T20:13:12.752516: step 89, loss 0.365678, acc 0.835\n",
      "2018-04-22T20:13:12.852523: step 90, loss 0.408102, acc 0.845\n",
      "2018-04-22T20:13:12.975524: step 91, loss 0.457306, acc 0.835\n",
      "2018-04-22T20:13:13.105059: step 92, loss 0.426727, acc 0.84\n",
      "2018-04-22T20:13:13.212818: step 93, loss 0.344445, acc 0.85\n",
      "2018-04-22T20:13:13.332760: step 94, loss 0.278529, acc 0.875\n",
      "2018-04-22T20:13:13.465018: step 95, loss 0.417132, acc 0.86\n",
      "2018-04-22T20:13:13.510011: step 96, loss 0.27476, acc 0.914894\n",
      "2018-04-22T20:13:13.642979: step 97, loss 0.289999, acc 0.91\n",
      "2018-04-22T20:13:13.762170: step 98, loss 0.322778, acc 0.9\n",
      "2018-04-22T20:13:13.892521: step 99, loss 0.433647, acc 0.845\n",
      "2018-04-22T20:13:14.020771: step 100, loss 0.324163, acc 0.89\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:14.586147: step 100, loss 0.366461, acc 0.859259, rec 0.954545, pre 0.803704, f1 0.872654\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524427983/checkpoints/model-100\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-22T20:13:14.796482: step 101, loss 0.358958, acc 0.87\n",
      "2018-04-22T20:13:14.872468: step 102, loss 0.272576, acc 0.875\n",
      "2018-04-22T20:13:14.952475: step 103, loss 0.488639, acc 0.84\n",
      "2018-04-22T20:13:15.036443: step 104, loss 0.379693, acc 0.87\n",
      "2018-04-22T20:13:15.116482: step 105, loss 0.316909, acc 0.88\n",
      "2018-04-22T20:13:15.192462: step 106, loss 0.271103, acc 0.885\n",
      "2018-04-22T20:13:15.271731: step 107, loss 0.39276, acc 0.865\n",
      "2018-04-22T20:13:15.356475: step 108, loss 0.274302, acc 0.89\n",
      "2018-04-22T20:13:15.428450: step 109, loss 0.37812, acc 0.85\n",
      "2018-04-22T20:13:15.506340: step 110, loss 0.391465, acc 0.865\n",
      "2018-04-22T20:13:15.588458: step 111, loss 0.40381, acc 0.885\n",
      "2018-04-22T20:13:15.666693: step 112, loss 0.262221, acc 0.89\n",
      "2018-04-22T20:13:15.748398: step 113, loss 0.247644, acc 0.91\n",
      "2018-04-22T20:13:15.829038: step 114, loss 0.434305, acc 0.875\n",
      "2018-04-22T20:13:15.909967: step 115, loss 0.291643, acc 0.88\n",
      "2018-04-22T20:13:15.989761: step 116, loss 0.38604, acc 0.85\n",
      "2018-04-22T20:13:16.068462: step 117, loss 0.281916, acc 0.91\n",
      "2018-04-22T20:13:16.150563: step 118, loss 0.3377, acc 0.895\n",
      "2018-04-22T20:13:16.232467: step 119, loss 0.338361, acc 0.84\n",
      "2018-04-22T20:13:16.306182: step 120, loss 0.366788, acc 0.865\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:16.641496: step 120, loss 0.575794, acc 0.672222, rec 0.983138, pre 0.608715, f1 0.751892\n",
      "\n",
      "2018-04-22T20:13:16.728469: step 121, loss 0.28242, acc 0.885\n",
      "2018-04-22T20:13:16.808462: step 122, loss 0.361535, acc 0.87\n",
      "2018-04-22T20:13:16.892452: step 123, loss 0.382029, acc 0.825\n",
      "2018-04-22T20:13:16.972467: step 124, loss 0.30665, acc 0.885\n",
      "2018-04-22T20:13:17.046700: step 125, loss 0.355206, acc 0.87\n",
      "2018-04-22T20:13:17.128473: step 126, loss 0.300144, acc 0.875\n",
      "2018-04-22T20:13:17.212473: step 127, loss 0.375791, acc 0.865\n",
      "2018-04-22T20:13:17.288458: step 128, loss 0.36327, acc 0.85\n",
      "2018-04-22T20:13:17.368458: step 129, loss 0.386204, acc 0.855\n",
      "2018-04-22T20:13:17.446412: step 130, loss 0.330476, acc 0.925\n",
      "2018-04-22T20:13:17.524457: step 131, loss 0.333182, acc 0.855\n",
      "2018-04-22T20:13:17.609004: step 132, loss 0.334564, acc 0.865\n",
      "2018-04-22T20:13:17.689290: step 133, loss 0.347685, acc 0.87\n",
      "2018-04-22T20:13:17.768746: step 134, loss 0.371955, acc 0.84\n",
      "2018-04-22T20:13:17.852482: step 135, loss 0.352462, acc 0.84\n",
      "2018-04-22T20:13:17.928466: step 136, loss 0.329119, acc 0.87\n",
      "2018-04-22T20:13:18.008478: step 137, loss 0.334926, acc 0.865\n",
      "2018-04-22T20:13:18.088476: step 138, loss 0.413934, acc 0.835\n",
      "2018-04-22T20:13:18.161391: step 139, loss 0.288573, acc 0.87\n",
      "2018-04-22T20:13:18.240578: step 140, loss 0.441772, acc 0.82\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:18.564477: step 140, loss 0.969325, acc 0.57, rec 0.989003, pre 0.540681, f1 0.699145\n",
      "\n",
      "2018-04-22T20:13:18.645252: step 141, loss 0.554263, acc 0.79\n",
      "2018-04-22T20:13:18.720495: step 142, loss 0.362926, acc 0.865\n",
      "2018-04-22T20:13:18.805362: step 143, loss 0.380415, acc 0.84\n",
      "2018-04-22T20:13:18.840473: step 144, loss 0.332039, acc 0.87234\n",
      "2018-04-22T20:13:18.916470: step 145, loss 0.391282, acc 0.83\n",
      "2018-04-22T20:13:18.990903: step 146, loss 0.357907, acc 0.845\n",
      "2018-04-22T20:13:19.076447: step 147, loss 0.381842, acc 0.87\n",
      "2018-04-22T20:13:19.150879: step 148, loss 0.370143, acc 0.84\n",
      "2018-04-22T20:13:19.229871: step 149, loss 0.265646, acc 0.93\n",
      "2018-04-22T20:13:19.310141: step 150, loss 0.368835, acc 0.845\n",
      "2018-04-22T20:13:19.388685: step 151, loss 0.397569, acc 0.84\n",
      "2018-04-22T20:13:19.464475: step 152, loss 0.304976, acc 0.855\n",
      "2018-04-22T20:13:19.544993: step 153, loss 0.379055, acc 0.855\n",
      "2018-04-22T20:13:19.628491: step 154, loss 0.243609, acc 0.91\n",
      "2018-04-22T20:13:19.704146: step 155, loss 0.421606, acc 0.845\n",
      "2018-04-22T20:13:19.788481: step 156, loss 0.275428, acc 0.89\n",
      "2018-04-22T20:13:19.866654: step 157, loss 0.264602, acc 0.895\n",
      "2018-04-22T20:13:19.940951: step 158, loss 0.311545, acc 0.875\n",
      "2018-04-22T20:13:20.024474: step 159, loss 0.413444, acc 0.885\n",
      "2018-04-22T20:13:20.100438: step 160, loss 0.250178, acc 0.91\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:20.429073: step 160, loss 0.373772, acc 0.85037, rec 0.950147, pre 0.794118, f1 0.865154\n",
      "\n",
      "2018-04-22T20:13:20.513116: step 161, loss 0.354739, acc 0.88\n",
      "2018-04-22T20:13:20.593031: step 162, loss 0.339978, acc 0.82\n",
      "2018-04-22T20:13:20.673678: step 163, loss 0.354948, acc 0.82\n",
      "2018-04-22T20:13:20.752837: step 164, loss 0.289621, acc 0.885\n",
      "2018-04-22T20:13:20.832466: step 165, loss 0.349192, acc 0.875\n",
      "2018-04-22T20:13:20.910319: step 166, loss 0.34624, acc 0.845\n",
      "2018-04-22T20:13:20.992452: step 167, loss 0.438397, acc 0.84\n",
      "2018-04-22T20:13:21.068459: step 168, loss 0.328637, acc 0.885\n",
      "2018-04-22T20:13:21.148118: step 169, loss 0.284086, acc 0.865\n",
      "2018-04-22T20:13:21.228464: step 170, loss 0.323845, acc 0.855\n",
      "2018-04-22T20:13:21.304460: step 171, loss 0.307813, acc 0.89\n",
      "2018-04-22T20:13:21.388462: step 172, loss 0.256669, acc 0.895\n",
      "2018-04-22T20:13:21.460466: step 173, loss 0.30291, acc 0.885\n",
      "2018-04-22T20:13:21.536481: step 174, loss 0.342383, acc 0.865\n",
      "2018-04-22T20:13:21.615337: step 175, loss 0.293451, acc 0.875\n",
      "2018-04-22T20:13:21.694952: step 176, loss 0.353787, acc 0.865\n",
      "2018-04-22T20:13:21.770558: step 177, loss 0.275772, acc 0.91\n",
      "2018-04-22T20:13:21.855772: step 178, loss 0.290896, acc 0.895\n",
      "2018-04-22T20:13:21.930499: step 179, loss 0.45076, acc 0.81\n",
      "2018-04-22T20:13:22.009284: step 180, loss 0.24541, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:22.340444: step 180, loss 0.342221, acc 0.89037, rec 0.942815, pre 0.855053, f1 0.896792\n",
      "\n",
      "2018-04-22T20:13:22.424472: step 181, loss 0.494288, acc 0.84\n",
      "2018-04-22T20:13:22.496745: step 182, loss 0.286499, acc 0.875\n",
      "2018-04-22T20:13:22.577181: step 183, loss 0.297891, acc 0.89\n",
      "2018-04-22T20:13:22.656486: step 184, loss 0.376568, acc 0.865\n",
      "2018-04-22T20:13:22.732473: step 185, loss 0.376344, acc 0.845\n",
      "2018-04-22T20:13:22.816634: step 186, loss 0.274386, acc 0.91\n",
      "2018-04-22T20:13:22.894437: step 187, loss 0.319096, acc 0.865\n",
      "2018-04-22T20:13:22.975683: step 188, loss 0.309959, acc 0.875\n",
      "2018-04-22T20:13:23.065031: step 189, loss 0.440627, acc 0.835\n",
      "2018-04-22T20:13:23.144518: step 190, loss 0.360839, acc 0.86\n",
      "2018-04-22T20:13:23.220531: step 191, loss 0.315528, acc 0.875\n",
      "2018-04-22T20:13:23.252489: step 192, loss 0.218386, acc 0.93617\n",
      "2018-04-22T20:13:23.338139: step 193, loss 0.274334, acc 0.885\n",
      "2018-04-22T20:13:23.417490: step 194, loss 0.313476, acc 0.865\n",
      "2018-04-22T20:13:23.492499: step 195, loss 0.34252, acc 0.89\n",
      "2018-04-22T20:13:23.575891: step 196, loss 0.312673, acc 0.87\n",
      "2018-04-22T20:13:23.660509: step 197, loss 0.344838, acc 0.855\n",
      "2018-04-22T20:13:23.737542: step 198, loss 0.449118, acc 0.84\n",
      "2018-04-22T20:13:23.821292: step 199, loss 0.264197, acc 0.9\n",
      "2018-04-22T20:13:23.898754: step 200, loss 0.356515, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:24.244529: step 200, loss 0.557413, acc 0.671111, rec 0.989003, pre 0.607111, f1 0.75237\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524427983/checkpoints/model-200\n",
      "\n",
      "2018-04-22T20:13:24.436520: step 201, loss 0.284389, acc 0.89\n",
      "2018-04-22T20:13:24.516540: step 202, loss 0.417784, acc 0.88\n",
      "2018-04-22T20:13:24.592532: step 203, loss 0.40134, acc 0.865\n",
      "2018-04-22T20:13:24.672533: step 204, loss 0.293257, acc 0.885\n",
      "2018-04-22T20:13:24.760526: step 205, loss 0.355908, acc 0.87\n",
      "2018-04-22T20:13:24.840721: step 206, loss 0.249847, acc 0.93\n",
      "2018-04-22T20:13:24.928539: step 207, loss 0.402714, acc 0.83\n",
      "2018-04-22T20:13:25.009232: step 208, loss 0.377076, acc 0.845\n",
      "2018-04-22T20:13:25.096480: step 209, loss 0.359791, acc 0.86\n",
      "2018-04-22T20:13:25.181776: step 210, loss 0.312977, acc 0.875\n",
      "2018-04-22T20:13:25.265136: step 211, loss 0.315213, acc 0.88\n",
      "2018-04-22T20:13:25.341107: step 212, loss 0.335046, acc 0.865\n",
      "2018-04-22T20:13:25.424539: step 213, loss 0.341711, acc 0.9\n",
      "2018-04-22T20:13:25.512515: step 214, loss 0.222208, acc 0.905\n",
      "2018-04-22T20:13:25.588519: step 215, loss 0.341606, acc 0.865\n",
      "2018-04-22T20:13:25.664639: step 216, loss 0.315964, acc 0.895\n",
      "2018-04-22T20:13:25.748500: step 217, loss 0.322665, acc 0.845\n",
      "2018-04-22T20:13:25.828516: step 218, loss 0.346653, acc 0.88\n",
      "2018-04-22T20:13:25.903559: step 219, loss 0.271004, acc 0.905\n",
      "2018-04-22T20:13:25.987129: step 220, loss 0.254981, acc 0.875\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-22T20:13:26.326866: step 220, loss 0.410722, acc 0.826296, rec 0.971408, pre 0.754986, f1 0.849631\n",
      "\n",
      "2018-04-22T20:13:26.413047: step 221, loss 0.387872, acc 0.86\n",
      "2018-04-22T20:13:26.492517: step 222, loss 0.257965, acc 0.91\n",
      "2018-04-22T20:13:26.576517: step 223, loss 0.275406, acc 0.895\n",
      "2018-04-22T20:13:26.652493: step 224, loss 0.348566, acc 0.89\n",
      "2018-04-22T20:13:26.732599: step 225, loss 0.317488, acc 0.855\n",
      "2018-04-22T20:13:26.815433: step 226, loss 0.365887, acc 0.865\n",
      "2018-04-22T20:13:26.892510: step 227, loss 0.3531, acc 0.865\n",
      "2018-04-22T20:13:26.972519: step 228, loss 0.286236, acc 0.865\n",
      "2018-04-22T20:13:27.060503: step 229, loss 0.292478, acc 0.895\n",
      "2018-04-22T20:13:27.137760: step 230, loss 0.406115, acc 0.86\n",
      "2018-04-22T20:13:27.217266: step 231, loss 0.320368, acc 0.875\n",
      "2018-04-22T20:13:27.298246: step 232, loss 0.445906, acc 0.87\n",
      "2018-04-22T20:13:27.375435: step 233, loss 0.246671, acc 0.91\n",
      "2018-04-22T20:13:27.456544: step 234, loss 0.381265, acc 0.86\n",
      "2018-04-22T20:13:27.535125: step 235, loss 0.339973, acc 0.855\n",
      "2018-04-22T20:13:27.616533: step 236, loss 0.281185, acc 0.9\n",
      "2018-04-22T20:13:27.696533: step 237, loss 0.3595, acc 0.865\n",
      "2018-04-22T20:13:27.782054: step 238, loss 0.282207, acc 0.89\n",
      "2018-04-22T20:13:27.860519: step 239, loss 0.359946, acc 0.875\n",
      "2018-04-22T20:13:27.896507: step 240, loss 0.240533, acc 0.893617\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:28.240529: step 240, loss 0.496281, acc 0.707778, rec 0.43695, pre 0.965964, f1 0.601716\n",
      "\n",
      "2018-04-22T20:13:28.328852: step 241, loss 0.452517, acc 0.85\n",
      "2018-04-22T20:13:28.408514: step 242, loss 0.302755, acc 0.87\n",
      "2018-04-22T20:13:28.492528: step 243, loss 0.219218, acc 0.91\n",
      "2018-04-22T20:13:28.567367: step 244, loss 0.368759, acc 0.85\n",
      "2018-04-22T20:13:28.646239: step 245, loss 0.370862, acc 0.885\n",
      "2018-04-22T20:13:28.732535: step 246, loss 0.432353, acc 0.83\n",
      "2018-04-22T20:13:28.803888: step 247, loss 0.422785, acc 0.84\n",
      "2018-04-22T20:13:28.888865: step 248, loss 0.268069, acc 0.88\n",
      "2018-04-22T20:13:28.970898: step 249, loss 0.367651, acc 0.88\n",
      "2018-04-22T20:13:29.045694: step 250, loss 0.344981, acc 0.87\n",
      "2018-04-22T20:13:29.124554: step 251, loss 0.35535, acc 0.88\n",
      "2018-04-22T20:13:29.206967: step 252, loss 0.471276, acc 0.84\n",
      "2018-04-22T20:13:29.288518: step 253, loss 0.316743, acc 0.88\n",
      "2018-04-22T20:13:29.364500: step 254, loss 0.321487, acc 0.845\n",
      "2018-04-22T20:13:29.441006: step 255, loss 0.310711, acc 0.865\n",
      "2018-04-22T20:13:29.515595: step 256, loss 0.253116, acc 0.885\n",
      "2018-04-22T20:13:29.593316: step 257, loss 0.285059, acc 0.87\n",
      "2018-04-22T20:13:29.674834: step 258, loss 0.42277, acc 0.84\n",
      "2018-04-22T20:13:29.753877: step 259, loss 0.419277, acc 0.845\n",
      "2018-04-22T20:13:29.828546: step 260, loss 0.302485, acc 0.865\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:30.165562: step 260, loss 0.402962, acc 0.804815, rec 0.945748, pre 0.740103, f1 0.830383\n",
      "\n",
      "2018-04-22T20:13:30.254598: step 261, loss 0.351392, acc 0.88\n",
      "2018-04-22T20:13:30.331798: step 262, loss 0.284352, acc 0.88\n",
      "2018-04-22T20:13:30.412503: step 263, loss 0.242112, acc 0.875\n",
      "2018-04-22T20:13:30.492516: step 264, loss 0.328616, acc 0.84\n",
      "2018-04-22T20:13:30.564508: step 265, loss 0.264113, acc 0.89\n",
      "2018-04-22T20:13:30.640501: step 266, loss 0.304226, acc 0.875\n",
      "2018-04-22T20:13:30.717394: step 267, loss 0.363446, acc 0.845\n",
      "2018-04-22T20:13:30.792511: step 268, loss 0.267743, acc 0.875\n",
      "2018-04-22T20:13:30.877082: step 269, loss 0.238653, acc 0.905\n",
      "2018-04-22T20:13:30.956535: step 270, loss 0.249725, acc 0.9\n",
      "2018-04-22T20:13:31.036510: step 271, loss 0.397385, acc 0.85\n",
      "2018-04-22T20:13:31.116504: step 272, loss 0.301676, acc 0.88\n",
      "2018-04-22T20:13:31.191482: step 273, loss 0.36228, acc 0.865\n",
      "2018-04-22T20:13:31.272051: step 274, loss 0.294869, acc 0.875\n",
      "2018-04-22T20:13:31.358538: step 275, loss 0.335044, acc 0.875\n",
      "2018-04-22T20:13:31.432527: step 276, loss 0.318436, acc 0.89\n",
      "2018-04-22T20:13:31.512520: step 277, loss 0.421873, acc 0.89\n",
      "2018-04-22T20:13:31.593823: step 278, loss 0.363977, acc 0.86\n",
      "2018-04-22T20:13:31.674689: step 279, loss 0.268006, acc 0.875\n",
      "2018-04-22T20:13:31.752517: step 280, loss 0.324853, acc 0.885\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:32.096611: step 280, loss 0.715922, acc 0.583333, rec 0.998534, pre 0.548089, f1 0.707716\n",
      "\n",
      "2018-04-22T20:13:32.180536: step 281, loss 0.347536, acc 0.86\n",
      "2018-04-22T20:13:32.255249: step 282, loss 0.223306, acc 0.91\n",
      "2018-04-22T20:13:32.344526: step 283, loss 0.318531, acc 0.865\n",
      "2018-04-22T20:13:32.420505: step 284, loss 0.309735, acc 0.885\n",
      "2018-04-22T20:13:32.496508: step 285, loss 0.290341, acc 0.865\n",
      "2018-04-22T20:13:32.574809: step 286, loss 0.307198, acc 0.89\n",
      "2018-04-22T20:13:32.652514: step 287, loss 0.315574, acc 0.85\n",
      "2018-04-22T20:13:32.680491: step 288, loss 0.305833, acc 0.87234\n",
      "2018-04-22T20:13:32.761681: step 289, loss 0.388079, acc 0.86\n",
      "2018-04-22T20:13:32.844509: step 290, loss 0.263217, acc 0.885\n",
      "2018-04-22T20:13:32.921571: step 291, loss 0.310754, acc 0.89\n",
      "2018-04-22T20:13:33.002140: step 292, loss 0.415066, acc 0.85\n",
      "2018-04-22T20:13:33.078525: step 293, loss 0.323596, acc 0.86\n",
      "2018-04-22T20:13:33.157042: step 294, loss 0.241181, acc 0.915\n",
      "2018-04-22T20:13:33.240495: step 295, loss 0.292865, acc 0.885\n",
      "2018-04-22T20:13:33.320524: step 296, loss 0.379287, acc 0.85\n",
      "2018-04-22T20:13:33.400518: step 297, loss 0.37542, acc 0.87\n",
      "2018-04-22T20:13:33.480503: step 298, loss 0.272161, acc 0.905\n",
      "2018-04-22T20:13:33.560517: step 299, loss 0.427264, acc 0.83\n",
      "2018-04-22T20:13:33.636496: step 300, loss 0.359431, acc 0.89\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:33.966867: step 300, loss 0.631338, acc 0.638148, rec 0.994868, pre 0.583154, f1 0.735302\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524427983/checkpoints/model-300\n",
      "\n",
      "2018-04-22T20:13:34.153296: step 301, loss 0.388621, acc 0.845\n",
      "2018-04-22T20:13:34.236544: step 302, loss 0.305091, acc 0.92\n",
      "2018-04-22T20:13:34.315774: step 303, loss 0.317282, acc 0.855\n",
      "2018-04-22T20:13:34.396518: step 304, loss 0.306309, acc 0.87\n",
      "2018-04-22T20:13:34.480860: step 305, loss 0.309448, acc 0.85\n",
      "2018-04-22T20:13:34.561269: step 306, loss 0.433834, acc 0.8\n",
      "2018-04-22T20:13:34.641469: step 307, loss 0.299953, acc 0.885\n",
      "2018-04-22T20:13:34.718894: step 308, loss 0.292098, acc 0.855\n",
      "2018-04-22T20:13:34.804493: step 309, loss 0.302542, acc 0.86\n",
      "2018-04-22T20:13:34.879419: step 310, loss 0.324537, acc 0.86\n",
      "2018-04-22T20:13:34.957045: step 311, loss 0.280338, acc 0.91\n",
      "2018-04-22T20:13:35.040518: step 312, loss 0.419895, acc 0.82\n",
      "2018-04-22T20:13:35.120515: step 313, loss 0.384023, acc 0.86\n",
      "2018-04-22T20:13:35.201269: step 314, loss 0.33916, acc 0.865\n",
      "2018-04-22T20:13:35.281512: step 315, loss 0.234172, acc 0.9\n",
      "2018-04-22T20:13:35.361725: step 316, loss 0.487276, acc 0.825\n",
      "2018-04-22T20:13:35.444736: step 317, loss 0.266861, acc 0.885\n",
      "2018-04-22T20:13:35.524530: step 318, loss 0.343523, acc 0.86\n",
      "2018-04-22T20:13:35.604035: step 319, loss 0.341674, acc 0.895\n",
      "2018-04-22T20:13:35.692530: step 320, loss 0.341059, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:36.024770: step 320, loss 0.422506, acc 0.781482, rec 0.982405, pre 0.703043, f1 0.819572\n",
      "\n",
      "2018-04-22T20:13:36.116530: step 321, loss 0.396636, acc 0.83\n",
      "2018-04-22T20:13:36.196511: step 322, loss 0.433839, acc 0.86\n",
      "2018-04-22T20:13:36.272823: step 323, loss 0.293367, acc 0.865\n",
      "2018-04-22T20:13:36.349181: step 324, loss 0.367807, acc 0.855\n",
      "2018-04-22T20:13:36.432468: step 325, loss 0.337899, acc 0.865\n",
      "2018-04-22T20:13:36.512531: step 326, loss 0.241871, acc 0.91\n",
      "2018-04-22T20:13:36.587809: step 327, loss 0.269673, acc 0.9\n",
      "2018-04-22T20:13:36.663094: step 328, loss 0.272028, acc 0.87\n",
      "2018-04-22T20:13:36.740922: step 329, loss 0.230557, acc 0.925\n",
      "2018-04-22T20:13:36.820512: step 330, loss 0.30592, acc 0.865\n",
      "2018-04-22T20:13:36.894362: step 331, loss 0.290583, acc 0.895\n",
      "2018-04-22T20:13:36.972519: step 332, loss 0.290229, acc 0.91\n",
      "2018-04-22T20:13:37.052542: step 333, loss 0.303774, acc 0.87\n",
      "2018-04-22T20:13:37.132518: step 334, loss 0.367445, acc 0.885\n",
      "2018-04-22T20:13:37.212506: step 335, loss 0.33545, acc 0.895\n",
      "2018-04-22T20:13:37.241863: step 336, loss 0.426199, acc 0.765957\n",
      "2018-04-22T20:13:37.322317: step 337, loss 0.304527, acc 0.875\n",
      "2018-04-22T20:13:37.402566: step 338, loss 0.282105, acc 0.875\n",
      "2018-04-22T20:13:37.485350: step 339, loss 0.373062, acc 0.87\n",
      "2018-04-22T20:13:37.564503: step 340, loss 0.230357, acc 0.89\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-22T20:13:37.903600: step 340, loss 0.485888, acc 0.744074, rec 0.953079, pre 0.674624, f1 0.790033\n",
      "\n",
      "2018-04-22T20:13:37.992511: step 341, loss 0.217934, acc 0.905\n",
      "2018-04-22T20:13:38.068513: step 342, loss 0.261298, acc 0.92\n",
      "2018-04-22T20:13:38.156554: step 343, loss 0.337933, acc 0.895\n",
      "2018-04-22T20:13:38.234050: step 344, loss 0.466395, acc 0.855\n",
      "2018-04-22T20:13:38.309231: step 345, loss 0.23481, acc 0.92\n",
      "2018-04-22T20:13:38.395751: step 346, loss 0.275848, acc 0.89\n",
      "2018-04-22T20:13:38.473026: step 347, loss 0.423859, acc 0.84\n",
      "2018-04-22T20:13:38.552505: step 348, loss 0.2281, acc 0.905\n",
      "2018-04-22T20:13:38.630401: step 349, loss 0.306794, acc 0.905\n",
      "2018-04-22T20:13:38.713659: step 350, loss 0.39391, acc 0.865\n",
      "2018-04-22T20:13:38.792530: step 351, loss 0.385508, acc 0.855\n",
      "2018-04-22T20:13:38.873262: step 352, loss 0.378045, acc 0.875\n",
      "2018-04-22T20:13:38.956533: step 353, loss 0.383974, acc 0.84\n",
      "2018-04-22T20:13:39.032519: step 354, loss 0.265593, acc 0.9\n",
      "2018-04-22T20:13:39.113370: step 355, loss 0.431408, acc 0.86\n",
      "2018-04-22T20:13:39.192861: step 356, loss 0.428315, acc 0.85\n",
      "2018-04-22T20:13:39.274359: step 357, loss 0.357879, acc 0.88\n",
      "2018-04-22T20:13:39.353713: step 358, loss 0.257238, acc 0.9\n",
      "2018-04-22T20:13:39.436517: step 359, loss 0.400965, acc 0.835\n",
      "2018-04-22T20:13:39.510484: step 360, loss 0.334646, acc 0.89\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:39.837586: step 360, loss 0.356816, acc 0.870741, rec 0.945748, pre 0.824281, f1 0.880847\n",
      "\n",
      "2018-04-22T20:13:39.932518: step 361, loss 0.322051, acc 0.835\n",
      "2018-04-22T20:13:40.008519: step 362, loss 0.379134, acc 0.855\n",
      "2018-04-22T20:13:40.096547: step 363, loss 0.308791, acc 0.87\n",
      "2018-04-22T20:13:40.181320: step 364, loss 0.216396, acc 0.925\n",
      "2018-04-22T20:13:40.298840: step 365, loss 0.309891, acc 0.89\n",
      "2018-04-22T20:13:40.375406: step 366, loss 0.38071, acc 0.875\n",
      "2018-04-22T20:13:40.413576: step 367, loss 0.231219, acc 0.92\n",
      "2018-04-22T20:13:40.452166: step 368, loss 0.33394, acc 0.875\n",
      "2018-04-22T20:13:40.491489: step 369, loss 0.279547, acc 0.88\n",
      "2018-04-22T20:13:40.533899: step 370, loss 0.271968, acc 0.88\n",
      "2018-04-22T20:13:40.571879: step 371, loss 0.349453, acc 0.86\n",
      "2018-04-22T20:13:40.610035: step 372, loss 0.295182, acc 0.92\n",
      "2018-04-22T20:13:40.647758: step 373, loss 0.315147, acc 0.9\n",
      "2018-04-22T20:13:40.687007: step 374, loss 0.34668, acc 0.86\n",
      "2018-04-22T20:13:40.724607: step 375, loss 0.339495, acc 0.885\n",
      "2018-04-22T20:13:40.764801: step 376, loss 0.28449, acc 0.85\n",
      "2018-04-22T20:13:40.801709: step 377, loss 0.324109, acc 0.88\n",
      "2018-04-22T20:13:40.838838: step 378, loss 0.283088, acc 0.865\n",
      "2018-04-22T20:13:40.875890: step 379, loss 0.3399, acc 0.85\n",
      "2018-04-22T20:13:40.913408: step 380, loss 0.39146, acc 0.855\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:41.078861: step 380, loss 0.339894, acc 0.885185, rec 0.939883, pre 0.849007, f1 0.892136\n",
      "\n",
      "2018-04-22T20:13:41.120388: step 381, loss 0.283278, acc 0.89\n",
      "2018-04-22T20:13:41.157788: step 382, loss 0.384416, acc 0.85\n",
      "2018-04-22T20:13:41.195596: step 383, loss 0.245775, acc 0.895\n",
      "2018-04-22T20:13:41.211352: step 384, loss 0.204106, acc 0.93617\n",
      "2018-04-22T20:13:41.251795: step 385, loss 0.353619, acc 0.865\n",
      "2018-04-22T20:13:41.293587: step 386, loss 0.281823, acc 0.895\n",
      "2018-04-22T20:13:41.331262: step 387, loss 0.261996, acc 0.91\n",
      "2018-04-22T20:13:41.368425: step 388, loss 0.375101, acc 0.875\n",
      "2018-04-22T20:13:41.405248: step 389, loss 0.302368, acc 0.89\n",
      "2018-04-22T20:13:41.442623: step 390, loss 0.375863, acc 0.86\n",
      "2018-04-22T20:13:41.480517: step 391, loss 0.291265, acc 0.885\n",
      "2018-04-22T20:13:41.522227: step 392, loss 0.343888, acc 0.9\n",
      "2018-04-22T20:13:41.560410: step 393, loss 0.315519, acc 0.91\n",
      "2018-04-22T20:13:41.598035: step 394, loss 0.270107, acc 0.89\n",
      "2018-04-22T20:13:41.636070: step 395, loss 0.333983, acc 0.87\n",
      "2018-04-22T20:13:41.674426: step 396, loss 0.335857, acc 0.86\n",
      "2018-04-22T20:13:41.712497: step 397, loss 0.308059, acc 0.88\n",
      "2018-04-22T20:13:41.754164: step 398, loss 0.342371, acc 0.865\n",
      "2018-04-22T20:13:41.791972: step 399, loss 0.324881, acc 0.88\n",
      "2018-04-22T20:13:41.830025: step 400, loss 0.346677, acc 0.87\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:41.997046: step 400, loss 0.630727, acc 0.633333, rec 0.990469, pre 0.580326, f1 0.731853\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524427983/checkpoints/model-400\n",
      "\n",
      "2018-04-22T20:13:42.085329: step 401, loss 0.407279, acc 0.82\n",
      "2018-04-22T20:13:42.123471: step 402, loss 0.388968, acc 0.84\n",
      "2018-04-22T20:13:42.160681: step 403, loss 0.359903, acc 0.86\n",
      "2018-04-22T20:13:42.197361: step 404, loss 0.388081, acc 0.83\n",
      "2018-04-22T20:13:42.237998: step 405, loss 0.356348, acc 0.86\n",
      "2018-04-22T20:13:42.274278: step 406, loss 0.240636, acc 0.915\n",
      "2018-04-22T20:13:42.311343: step 407, loss 0.31028, acc 0.865\n",
      "2018-04-22T20:13:42.349560: step 408, loss 0.328375, acc 0.855\n",
      "2018-04-22T20:13:42.387191: step 409, loss 0.318287, acc 0.86\n",
      "2018-04-22T20:13:42.423747: step 410, loss 0.506192, acc 0.845\n",
      "2018-04-22T20:13:42.463542: step 411, loss 0.374514, acc 0.885\n",
      "2018-04-22T20:13:42.501330: step 412, loss 0.330944, acc 0.865\n",
      "2018-04-22T20:13:42.537740: step 413, loss 0.514621, acc 0.855\n",
      "2018-04-22T20:13:42.574287: step 414, loss 0.326158, acc 0.875\n",
      "2018-04-22T20:13:42.612146: step 415, loss 0.306899, acc 0.87\n",
      "2018-04-22T20:13:42.649207: step 416, loss 0.411653, acc 0.845\n",
      "2018-04-22T20:13:42.692624: step 417, loss 0.451881, acc 0.87\n",
      "2018-04-22T20:13:42.737512: step 418, loss 0.327674, acc 0.9\n",
      "2018-04-22T20:13:42.775357: step 419, loss 0.378987, acc 0.855\n",
      "2018-04-22T20:13:42.814815: step 420, loss 0.293072, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:42.983703: step 420, loss 0.504835, acc 0.752593, rec 0.96261, pre 0.680311, f1 0.797207\n",
      "\n",
      "2018-04-22T20:13:43.024414: step 421, loss 0.433728, acc 0.865\n",
      "2018-04-22T20:13:43.062524: step 422, loss 0.297635, acc 0.86\n",
      "2018-04-22T20:13:43.100074: step 423, loss 0.403464, acc 0.895\n",
      "2018-04-22T20:13:43.138927: step 424, loss 0.40658, acc 0.835\n",
      "2018-04-22T20:13:43.178617: step 425, loss 0.324135, acc 0.86\n",
      "2018-04-22T20:13:43.220948: step 426, loss 0.165022, acc 0.955\n",
      "2018-04-22T20:13:43.258382: step 427, loss 0.480958, acc 0.86\n",
      "2018-04-22T20:13:43.295826: step 428, loss 0.351973, acc 0.885\n",
      "2018-04-22T20:13:43.334561: step 429, loss 0.256592, acc 0.9\n",
      "2018-04-22T20:13:43.371594: step 430, loss 0.325442, acc 0.86\n",
      "2018-04-22T20:13:43.409150: step 431, loss 0.28245, acc 0.905\n",
      "2018-04-22T20:13:43.428597: step 432, loss 0.397637, acc 0.829787\n",
      "2018-04-22T20:13:43.470333: step 433, loss 0.231022, acc 0.91\n",
      "2018-04-22T20:13:43.508691: step 434, loss 0.338129, acc 0.885\n",
      "2018-04-22T20:13:43.545832: step 435, loss 0.25765, acc 0.89\n",
      "2018-04-22T20:13:43.584108: step 436, loss 0.463652, acc 0.855\n",
      "2018-04-22T20:13:43.621841: step 437, loss 0.312682, acc 0.88\n",
      "2018-04-22T20:13:43.663283: step 438, loss 0.31839, acc 0.86\n",
      "2018-04-22T20:13:43.700668: step 439, loss 0.39834, acc 0.865\n",
      "2018-04-22T20:13:43.739592: step 440, loss 0.293005, acc 0.865\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:43.902038: step 440, loss 0.484502, acc 0.745185, rec 0.971408, pre 0.671226, f1 0.793889\n",
      "\n",
      "2018-04-22T20:13:43.943396: step 441, loss 0.351869, acc 0.87\n",
      "2018-04-22T20:13:43.980478: step 442, loss 0.351158, acc 0.875\n",
      "2018-04-22T20:13:44.017637: step 443, loss 0.32621, acc 0.89\n",
      "2018-04-22T20:13:44.054865: step 444, loss 0.324502, acc 0.865\n",
      "2018-04-22T20:13:44.091707: step 445, loss 0.437148, acc 0.82\n",
      "2018-04-22T20:13:44.132089: step 446, loss 0.263595, acc 0.885\n",
      "2018-04-22T20:13:44.169018: step 447, loss 0.440661, acc 0.86\n",
      "2018-04-22T20:13:44.206371: step 448, loss 0.343029, acc 0.88\n",
      "2018-04-22T20:13:44.243715: step 449, loss 0.245899, acc 0.92\n",
      "2018-04-22T20:13:44.280753: step 450, loss 0.451956, acc 0.84\n",
      "2018-04-22T20:13:44.318037: step 451, loss 0.296908, acc 0.88\n",
      "2018-04-22T20:13:44.358101: step 452, loss 0.350004, acc 0.9\n",
      "2018-04-22T20:13:44.395665: step 453, loss 0.299924, acc 0.865\n",
      "2018-04-22T20:13:44.432374: step 454, loss 0.310291, acc 0.89\n",
      "2018-04-22T20:13:44.469270: step 455, loss 0.355449, acc 0.845\n",
      "2018-04-22T20:13:44.506217: step 456, loss 0.403039, acc 0.845\n",
      "2018-04-22T20:13:44.543071: step 457, loss 0.323102, acc 0.87\n",
      "2018-04-22T20:13:44.584349: step 458, loss 0.371296, acc 0.825\n",
      "2018-04-22T20:13:44.622319: step 459, loss 0.307854, acc 0.875\n",
      "2018-04-22T20:13:44.660427: step 460, loss 0.288687, acc 0.895\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-22T20:13:44.825164: step 460, loss 0.793958, acc 0.594074, rec 0.997801, pre 0.554605, f1 0.712939\n",
      "\n",
      "2018-04-22T20:13:44.866564: step 461, loss 0.304632, acc 0.875\n",
      "2018-04-22T20:13:44.903101: step 462, loss 0.298867, acc 0.875\n",
      "2018-04-22T20:13:44.939953: step 463, loss 0.346685, acc 0.895\n",
      "2018-04-22T20:13:44.976517: step 464, loss 0.339781, acc 0.85\n",
      "2018-04-22T20:13:45.013619: step 465, loss 0.264422, acc 0.9\n",
      "2018-04-22T20:13:45.054454: step 466, loss 0.324825, acc 0.89\n",
      "2018-04-22T20:13:45.092088: step 467, loss 0.238773, acc 0.925\n",
      "2018-04-22T20:13:45.129153: step 468, loss 0.444983, acc 0.855\n",
      "2018-04-22T20:13:45.165856: step 469, loss 0.387745, acc 0.86\n",
      "2018-04-22T20:13:45.202556: step 470, loss 0.411659, acc 0.84\n",
      "2018-04-22T20:13:45.240176: step 471, loss 0.287369, acc 0.87\n",
      "2018-04-22T20:13:45.281847: step 472, loss 0.250736, acc 0.91\n",
      "2018-04-22T20:13:45.319396: step 473, loss 0.3068, acc 0.85\n",
      "2018-04-22T20:13:45.358794: step 474, loss 0.236286, acc 0.92\n",
      "2018-04-22T20:13:45.399508: step 475, loss 0.312631, acc 0.885\n",
      "2018-04-22T20:13:45.436891: step 476, loss 0.224974, acc 0.89\n",
      "2018-04-22T20:13:45.473985: step 477, loss 0.324015, acc 0.885\n",
      "2018-04-22T20:13:45.516132: step 478, loss 0.26564, acc 0.92\n",
      "2018-04-22T20:13:45.553973: step 479, loss 0.312714, acc 0.865\n",
      "2018-04-22T20:13:45.569712: step 480, loss 0.299635, acc 0.893617\n",
      "\n",
      "Evaluation:\n",
      "2018-04-22T20:13:45.741380: step 480, loss 0.372785, acc 0.852963, rec 0.961144, pre 0.792145, f1 0.8685\n",
      "\n",
      "\n",
      "Test Set:\n",
      "2018-04-22T20:13:45.831236: step 480, loss 0.365901, acc 0.853333, rec 0.976812, pre 0.787383, f1 0.871928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", 0.75, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 40, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"1\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 40, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.4, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 200\n",
    "tf.flags.DEFINE_integer(\"batch_size\", batch_size, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 10, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 20, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=train_s.shape[1],\n",
    "            num_classes=2,\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "        \n",
    "         # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        #vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch1, x_batch2, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch1, x_batch2, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            #r \n",
    "            step, summaries, loss, accuracy, recall, precision, f1 = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy,cnn.recall, cnn.precision, cnn.f1_score],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, rec {:g}, pre {:g}, f1 {:g}\".format(time_str, step, loss, accuracy, recall, precision, f1))# recall, precision, f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "        \n",
    "        def error_analysis(x_batch1, x_batch2, y_batch, writer=None):\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            #r \n",
    "            step, summaries, loss, accuracy, recall, precision, f1, correct, scores,predictions  = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy,cnn.recall, cnn.precision, cnn.f1_score, cnn.correct_predictions, cnn.scores, cnn.predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, rec {:g}, pre {:g}, f1 {:g}\".format(time_str, step, loss, accuracy, recall, precision, f1))# recall, precision, f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "            return correct, scores, predictions\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(train_s, train_c,  expanded_train_labels)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch1, x_batch2, y_batch = zip(*batch)\n",
    "            train_step(x_batch1, x_batch1, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(validation_s, validation_c, expanded_validation_labels, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "print(\"\\nTest Set:\")\n",
    "correct, logits, predictions = error_analysis(test_s, test_c, expanded_test_labels, writer=dev_summary_writer)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3.8800895, 798],\n",
       " [3.6586144, 878],\n",
       " [2.7627788, 739],\n",
       " [2.4937358, 1347],\n",
       " [2.2221501, 1196],\n",
       " [2.2016802, 1186],\n",
       " [2.0854609, 462],\n",
       " [2.038799, 748],\n",
       " [2.0335665, 1280],\n",
       " [2.0248444, 1134],\n",
       " [1.9460719, 572],\n",
       " [1.7840009, 1052],\n",
       " [1.7175657, 926],\n",
       " [1.691916, 487],\n",
       " [1.6451406, 293],\n",
       " [1.6396594, 211],\n",
       " [1.5169232, 939],\n",
       " [1.4470859, 956],\n",
       " [1.3997362, 263],\n",
       " [1.325048, 1146],\n",
       " [1.3107753, 150],\n",
       " [1.2799933, 34],\n",
       " [1.264863, 84],\n",
       " [1.2616477, 936],\n",
       " [1.2471952, 174],\n",
       " [1.2355683, 1062],\n",
       " [1.2076569, 308],\n",
       " [1.1605282, 398],\n",
       " [1.1555448, 479],\n",
       " [1.1374881, 508],\n",
       " [1.1240113, 1148],\n",
       " [1.1211736, 454],\n",
       " [1.1208985, 191],\n",
       " [1.1124182, 70],\n",
       " [1.1031786, 952],\n",
       " [1.0616415, 505],\n",
       " [1.0329688, 637],\n",
       " [1.0207822, 286],\n",
       " [0.99851751, 291],\n",
       " [0.99112976, 675],\n",
       " [0.94670254, 382],\n",
       " [0.93777567, 835],\n",
       " [0.9345172, 955],\n",
       " [0.92688459, 1239],\n",
       " [0.90270978, 692],\n",
       " [0.88538486, 1092],\n",
       " [0.87993592, 605],\n",
       " [0.87174743, 707],\n",
       " [0.86846215, 894],\n",
       " [0.85658669, 638],\n",
       " [0.85612482, 229],\n",
       " [0.82828426, 1198],\n",
       " [0.81020188, 507],\n",
       " [0.8076908, 107],\n",
       " [0.76978594, 910],\n",
       " [0.75674236, 729],\n",
       " [0.75582367, 371],\n",
       " [0.75547272, 717],\n",
       " [0.7405346, 430],\n",
       " [0.72354412, 431],\n",
       " [0.71306974, 534],\n",
       " [0.71196842, 642],\n",
       " [0.7118727, 295],\n",
       " [0.70439506, 1232],\n",
       " [0.69556171, 12],\n",
       " [0.6929605, 298],\n",
       " [0.68690753, 1339],\n",
       " [0.66820741, 1274],\n",
       " [0.66378289, 520],\n",
       " [0.65705884, 153],\n",
       " [0.64590096, 283],\n",
       " [0.64274234, 111],\n",
       " [0.638565, 1231],\n",
       " [0.63786834, 639],\n",
       " [0.61320788, 219],\n",
       " [0.57008672, 859],\n",
       " [0.57008219, 1102],\n",
       " [0.56107062, 927],\n",
       " [0.55297756, 988],\n",
       " [0.55107713, 287],\n",
       " [0.55032182, 1179],\n",
       " [0.54600501, 485],\n",
       " [0.53594428, 258],\n",
       " [0.52444947, 833],\n",
       " [0.51036596, 689],\n",
       " [0.50227904, 99],\n",
       " [0.48422486, 502],\n",
       " [0.4834137, 583],\n",
       " [0.4639436, 1109],\n",
       " [0.46380639, 1067],\n",
       " [0.44789076, 322],\n",
       " [0.44051671, 849],\n",
       " [0.431979, 471],\n",
       " [0.42749363, 29],\n",
       " [0.42069936, 33],\n",
       " [0.42060399, 728],\n",
       " [0.41950643, 496],\n",
       " [0.41413569, 832],\n",
       " [0.4131, 1257],\n",
       " [0.40844816, 288],\n",
       " [0.39577454, 1188],\n",
       " [0.39345306, 1202],\n",
       " [0.36667323, 493],\n",
       " [0.36072218, 201],\n",
       " [0.35482639, 1214],\n",
       " [0.35476851, 901],\n",
       " [0.34178448, 876],\n",
       " [0.34049511, 276],\n",
       " [0.3347559, 387],\n",
       " [0.31824762, 579],\n",
       " [0.31790543, 705],\n",
       " [0.31595534, 891],\n",
       " [0.30848932, 896],\n",
       " [0.30648994, 672],\n",
       " [0.29904675, 57],\n",
       " [0.29904652, 851],\n",
       " [0.29811597, 439],\n",
       " [0.29575825, 251],\n",
       " [0.27557969, 733],\n",
       " [0.27012682, 1072],\n",
       " [0.2696116, 1018],\n",
       " [0.26858532, 990],\n",
       " [0.25730801, 332],\n",
       " [0.24705219, 416],\n",
       " [0.24021232, 178],\n",
       " [0.23993373, 899],\n",
       " [0.23791337, 194],\n",
       " [0.23266363, 872],\n",
       " [0.2285918, 808],\n",
       " [0.22186732, 37],\n",
       " [0.21784687, 213],\n",
       " [0.21516681, 1128],\n",
       " [0.21290815, 558],\n",
       " [0.20984507, 1260],\n",
       " [0.2054925, 82],\n",
       " [0.19592029, 741],\n",
       " [0.19002008, 650],\n",
       " [0.18716025, 704],\n",
       " [0.18549728, 45],\n",
       " [0.18543506, 1253],\n",
       " [0.18301892, 87],\n",
       " [0.18301892, 348],\n",
       " [0.18301892, 616],\n",
       " [0.17319989, 1237],\n",
       " [0.16861343, 994],\n",
       " [0.16768289, 498],\n",
       " [0.16690552, 1036],\n",
       " [0.16012239, 222],\n",
       " [0.15777141, 86],\n",
       " [0.15728378, 205],\n",
       " [0.15012169, 553],\n",
       " [0.14932311, 785],\n",
       " [0.14874721, 199],\n",
       " [0.14670038, 573],\n",
       " [0.14454222, 919],\n",
       " [0.14454007, 236],\n",
       " [0.14410114, 693],\n",
       " [0.14337146, 1157],\n",
       " [0.13507575, 96],\n",
       " [0.13159925, 1295],\n",
       " [0.13105601, 299],\n",
       " [0.12771833, 461],\n",
       " [0.11981225, 533],\n",
       " [0.11427474, 200],\n",
       " [0.11098909, 1294],\n",
       " [0.10896564, 591],\n",
       " [0.10774875, 495],\n",
       " [0.095098972, 1154],\n",
       " [0.093271732, 1313],\n",
       " [0.085581899, 1336],\n",
       " [0.079485655, 866],\n",
       " [0.077903986, 1200],\n",
       " [0.069938183, 635],\n",
       " [0.069842815, 351],\n",
       " [0.058982611, 188],\n",
       " [0.058900118, 1144],\n",
       " [0.057709694, 1224],\n",
       " [0.055962861, 821],\n",
       " [0.055341482, 1175],\n",
       " [0.048785448, 1228],\n",
       " [0.047371209, 282],\n",
       " [0.044832826, 1065],\n",
       " [0.043114901, 858],\n",
       " [0.042502403, 530],\n",
       " [0.042206287, 660],\n",
       " [0.041575193, 0],\n",
       " [0.039858818, 856],\n",
       " [0.038537741, 793],\n",
       " [0.035725832, 279],\n",
       " [0.033586025, 448],\n",
       " [0.028875113, 795],\n",
       " [0.027213573, 881],\n",
       " [0.023098946, 349],\n",
       " [0.017249346, 571],\n",
       " [0.015125334, 152],\n",
       " [0.011829853, 76],\n",
       " [0.010740399, 207],\n",
       " [0.01074028, 512]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong = correct == False\n",
    "def incorrect_confidence(wrong, logits, predictions):\n",
    "    indeces = np.where(wrong)\n",
    "    wrong_predictions = predictions[indeces]\n",
    "    wrong_logits = logits[indeces]\n",
    "    \n",
    "    return [[wrong_logits[i][value] - wrong_logits[i][1-value], indeces[0][i]] for i, value in enumerate(wrong_predictions)]\n",
    "\n",
    "\n",
    "sorted(incorrect_confidence(wrong, logits, predictions), key = lambda logit: -logit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'is', 'with', 'that', '?', 'theyre', 'only', 'going', 'to', 'blame', 'the', 'guns', 'now', '?', 'no', 'people', '?', 'at', 'all', '?', 'oh', 'yeah', 'they', 'will', 'blame', 'trump', '.', 'there', 'is', 'no', 'agenda', 'though', '...', 'LINK']\n",
      "['nan']\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(test_sentences[878])\n",
    "print(test_contexts[878])\n",
    "print(test_labels[878])\n",
    "print(predictions[878])\n",
    "#definitely sarcastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'history', 'isn', '', 't', 'being', 'altered', 'by', 'federal', 'govt', 'education', 'how', 'do', 'i', 'know', 'i', 'heard', 'mrs', 'obama', 'say', 'it', 'right', 'out', 'of', 'her', 'mouth', 'using', 'the', 'word', '(', '(', '(', 'HASHTAG', 'HASHTAG', ')', ')', ')', 'main', 'while', 'in', 'history', 'this', 'was', 'taught', 'HASHTAG', 'LINK']\n",
      "['nan']\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(test_sentences[293])\n",
    "print(test_contexts[293])\n",
    "print(test_labels[293])\n",
    "print(predictions[293])\n",
    "\n",
    "# hard to say what's going on here. seems to be slightly sarcastic, not really sure though. Throwing me off too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LINK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'once', 'upon', 'a', 'time', '(', 'spin', ')', 'featuring', 'ACCOUNT', 'of', 'd12', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG']\n",
      "['nan']\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(test_sentences[798])\n",
    "print(test_contexts[798])\n",
    "print(test_labels[798])\n",
    "print(predictions[798])\n",
    "# heavy use of hashtags is throwing this one off too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['retweet', 'not', 'sure', 'about', 'the', 'cricket', 'ball', ',', 'but', 'bancroft', '', 's', 'balls', 'must', 'be', 'reverse-swinging', 'quiet', 'a', 'bit', 'after', 'yesterday', '', 's', 'events', '.', '', '', '', '']\n",
      "['nan']\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(test_sentences[1347])\n",
    "print(test_contexts[1347])\n",
    "print(test_labels[1347])\n",
    "print(predictions[1347])\n",
    "# emoji is throwing it off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'thought-provoking', 'image', 'of', 'the', 'day', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'LINK']\n",
      "['nan']\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(test_sentences[1280])\n",
    "print(test_contexts[1280])\n",
    "print(test_labels[1280])\n",
    "print(predictions[1280])\n",
    "\n",
    "# heavy use of hashtags is throwing this one off. Majority of tweets with heavy hashtag usage (as decribed in EDA)\n",
    "# are non-sarcatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
