{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saulgrimaldo1/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from w266_common import utils, vocabulary\n",
    "import time\n",
    "import datetime\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "tokenizer = TweetTokenizer()\n",
    "x_data = []\n",
    "x_contexts = []\n",
    "labels = []\n",
    "sentences  = []\n",
    "contexts = []\n",
    "with open('merged_data_v3.csv', 'r') as csvfile:\n",
    "    linereader = csv.reader(csvfile, delimiter = '|')\n",
    "    for i, row in enumerate(linereader):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        sentence, context, sarcasm = row\n",
    "        sentences.append(sentence)\n",
    "        contexts.append(context)\n",
    "        x_tokens = utils.canonicalize_words(tokenizer.tokenize(sentence), hashtags = True)\n",
    "        context_tokens = utils.canonicalize_words(tokenizer.tokenize(context), hashtags = True)\n",
    "        x_data.append(x_tokens)\n",
    "        x_contexts.append(context_tokens)\n",
    "        labels.append(int(sarcasm))\n",
    "\n",
    "\n",
    "#rng = np.random.RandomState(5)\n",
    "#rng.shuffle(x_data)  # in-place\n",
    "#train_split_idx = int(0.7 * len(labels))\n",
    "#test_split_idx  = int(0.9 * len(labels))\n",
    "\n",
    "shuffle_indices = np.random.permutation(np.arange(len(labels)))\n",
    "train_split_idx = int(0.7 * len(labels))\n",
    "test_split_idx  = int(0.9 * len(labels))\n",
    "\n",
    "train_indices = shuffle_indices[:train_split_idx]\n",
    "validation_indices = shuffle_indices[train_split_idx:test_split_idx]\n",
    "test_indices = shuffle_indices[test_split_idx:]\n",
    "\n",
    "\n",
    "train_sentences = np.array(x_data)[train_indices]\n",
    "train_contexts = np.array(x_contexts)[train_indices]\n",
    "train_labels= np.array(labels)[train_indices] \n",
    "validation_sentences = np.array(x_data)[validation_indices]\n",
    "validation_labels = np.array(labels)[validation_indices]\n",
    "validation_contexts = train_contexts = np.array(x_contexts)[validation_indices]\n",
    "test_sentences = np.array(x_data)[test_indices]  \n",
    "test_contexts = train_contexts = np.array(x_contexts)[test_indices]\n",
    "test_labels = np.array(labels)[test_indices]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 6, 2]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [2]*4\n",
    "a[2] = 6\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_labels(raw_label_set, size):\n",
    "    label_set = []\n",
    "    for label in raw_label_set:\n",
    "        labels = [0] * size\n",
    "        labels[label] = 1\n",
    "        label_set.append(labels)\n",
    "    return np.array(label_set)\n",
    "\n",
    "expanded_train_labels = transform_labels(train_labels, 2)\n",
    "expanded_validation_labels = transform_labels(validation_labels,2)\n",
    "expanded_test_labels = transform_labels(test_labels,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4, 5, 6, 7, 8, 8, 1, 5, 6, 7]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,3,4,5,6,7,8,8,1,5,6,7]\n",
    "a + [\"<PADDING>\"]*0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'angry',\n",
       " 'man',\n",
       " 'is',\n",
       " 'angry',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PaddingAndTruncating:\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def pad_or_truncate(self, sentence):\n",
    "        sen_len = len(sentence)\n",
    "        paddings = self.max_len - sen_len\n",
    "        if paddings >=0:\n",
    "            return sentence + [\"<PADDING>\"] * paddings\n",
    "        return sentence[0:paddings]\n",
    "        \n",
    "PadAndTrunc = PaddingAndTruncating(10)\n",
    "        \n",
    "        \n",
    "PadAndTrunc.pad_or_truncate([\"the\",\"angry\",\"man\",\"is\",\"angry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2, 1040,  267, ...,    3,    3,    3],\n",
       "       [ 947,  275,  947, ...,    4,    3,    3],\n",
       "       [2203,   20,    5, ...,    4,    4,    4],\n",
       "       ..., \n",
       "       [  10,    5,    7, ...,    3,    3,    3],\n",
       "       [  70,   19,   44, ...,   24,    2,    2],\n",
       "       [  10,    5,    7, ...,    3,    3,    3]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.contrib import learn\n",
    "\n",
    "#max_len = max([len(sent) for sent  in train_sentences])\n",
    "PadAndTrunc =PaddingAndTruncating(30)\n",
    "train_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, train_sentences))\n",
    "train_context_padded = list(map(PadAndTrunc.pad_or_truncate, train_contexts))\n",
    "validation_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, validation_sentences))\n",
    "validation_context_padded = list(map(PadAndTrunc.pad_or_truncate, validation_contexts))\n",
    "test_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, test_sentences))\n",
    "test_context_padded = list(map(PadAndTrunc.pad_or_truncate, test_contexts))\n",
    "\n",
    "vocab = vocabulary.Vocabulary(utils.flatten(list(train_sentences_padded) + list(train_context_padded)), 2500)\n",
    "train_s = np.array(list(map(vocab.words_to_ids, train_sentences_padded)))\n",
    "train_c = np.array(list(map(vocab.words_to_ids, train_context_padded)))\n",
    "validation_s = np.array(list(map(vocab.words_to_ids, validation_sentences_padded)))\n",
    "validation_c = np.array(list(map(vocab.words_to_ids, validation_context_padded)))\n",
    "test_s = np.array(list(map(vocab.words_to_ids, test_sentences_padded)))\n",
    "test_c = np.array(list(map(vocab.words_to_ids, test_context_padded)))\n",
    "train_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv-maxpool-3'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 3\n",
    "(\"conv-maxpool-%s\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size, \n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x1 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x1\")\n",
    "        self.input_x2 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x2\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars1 = tf.nn.embedding_lookup(self.W, self.input_x1)\n",
    "            self.embedded_chars_expanded1 = tf.expand_dims(self.embedded_chars1, -1)\n",
    "            self.embedded_chars2 = tf.nn.embedding_lookup(self.W, self.input_x2)\n",
    "            self.embedded_chars_expanded2 = tf.expand_dims(self.embedded_chars2, -1)\n",
    "\n",
    "        # Create a convolution + avgpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.variable_scope(\"conv-avgpool-%s\" % i) as scope:\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "              \n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded1,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h1 = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"tanh1\")\n",
    "               \n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.avg_pool(\n",
    "                    h1,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                \n",
    "                pooled_outputs.append(pooled)\n",
    "                #pooled_outputs.append(pooled)\n",
    "                scope.reuse_variables()\n",
    "                 \n",
    "                \n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded2,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h2 = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"tanh2\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled2 = tf.nn.avg_pool(\n",
    "                    h2,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled2)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "\n",
    "        num_filters_total = num_filters * len(filter_sizes) * 2\n",
    "\n",
    "        self.h_pool = tf.concat(pooled_outputs, 1)\n",
    "\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "        \n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "           \n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.labels = tf.argmax(self.input_y, 1)\n",
    "            TP = tf.count_nonzero(self.predictions * self.labels)\n",
    "            TN = tf.count_nonzero((self.predictions - 1) * (self.labels - 1))\n",
    "            FP = tf.count_nonzero(self.predictions * (self.labels - 1))\n",
    "            FN = tf.count_nonzero((self.predictions - 1) * self.labels)\n",
    "            correct_predictions = tf.equal(self.predictions, self.labels)\n",
    "            self.precision = TP / (TP + FP)\n",
    "            self.recall = TP / (TP + FN)\n",
    "            self.f1_score = 2 * self.precision * self.recall / (self.precision + self.recall)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=100\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.75\n",
      "DROPOUT_KEEP_PROB=0.4\n",
      "EMBEDDING_DIM=50\n",
      "EVALUATE_EVERY=20\n",
      "FILTER_SIZES=1\n",
      "L2_REG_LAMBDA=0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.neg\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=200\n",
      "NUM_FILTERS=50\n",
      "POSITIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.pos\n",
      "\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/W:0/grad/hist is illegal; using conv-avgpool-0/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/W:0/grad/sparsity is illegal; using conv-avgpool-0/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/b:0/grad/hist is illegal; using conv-avgpool-0/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/b:0/grad/sparsity is illegal; using conv-avgpool-0/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523944169\n",
      "\n",
      "2018-04-17T05:49:30.058073: step 1, loss 0.692099, acc 0.54\n",
      "2018-04-17T05:49:30.083875: step 2, loss 0.68142, acc 0.58\n",
      "2018-04-17T05:49:30.106172: step 3, loss 0.636269, acc 0.62\n",
      "2018-04-17T05:49:30.128162: step 4, loss 0.713, acc 0.55\n",
      "2018-04-17T05:49:30.150822: step 5, loss 0.688871, acc 0.55\n",
      "2018-04-17T05:49:30.171908: step 6, loss 0.642095, acc 0.66\n",
      "2018-04-17T05:49:30.193159: step 7, loss 0.643385, acc 0.61\n",
      "2018-04-17T05:49:30.214527: step 8, loss 0.614636, acc 0.7\n",
      "2018-04-17T05:49:30.231657: step 9, loss 0.654698, acc 0.616438\n",
      "2018-04-17T05:49:30.253201: step 10, loss 0.620878, acc 0.72\n",
      "2018-04-17T05:49:30.278478: step 11, loss 0.627289, acc 0.66\n",
      "2018-04-17T05:49:30.300988: step 12, loss 0.685029, acc 0.55\n",
      "2018-04-17T05:49:30.322229: step 13, loss 0.625544, acc 0.64\n",
      "2018-04-17T05:49:30.344100: step 14, loss 0.668584, acc 0.62\n",
      "2018-04-17T05:49:30.369485: step 15, loss 0.616738, acc 0.67\n",
      "2018-04-17T05:49:30.391092: step 16, loss 0.672803, acc 0.6\n",
      "2018-04-17T05:49:30.419734: step 17, loss 0.611156, acc 0.68\n",
      "2018-04-17T05:49:30.437264: step 18, loss 0.631939, acc 0.671233\n",
      "2018-04-17T05:49:30.458407: step 19, loss 0.600385, acc 0.68\n",
      "2018-04-17T05:49:30.479266: step 20, loss 0.594499, acc 0.65\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:30.603895: step 20, loss 0.660431, acc 0.602292, rec 0.855188, pre 0.569476, f1 0.683683\n",
      "\n",
      "2018-04-17T05:49:30.627346: step 21, loss 0.602615, acc 0.72\n",
      "2018-04-17T05:49:30.648760: step 22, loss 0.634748, acc 0.69\n",
      "2018-04-17T05:49:30.670105: step 23, loss 0.609452, acc 0.72\n",
      "2018-04-17T05:49:30.695070: step 24, loss 0.630367, acc 0.62\n",
      "2018-04-17T05:49:30.719851: step 25, loss 0.703832, acc 0.58\n",
      "2018-04-17T05:49:30.740350: step 26, loss 0.63147, acc 0.64\n",
      "2018-04-17T05:49:30.756973: step 27, loss 0.587597, acc 0.726027\n",
      "2018-04-17T05:49:30.778189: step 28, loss 0.624001, acc 0.62\n",
      "2018-04-17T05:49:30.799538: step 29, loss 0.578311, acc 0.69\n",
      "2018-04-17T05:49:30.820835: step 30, loss 0.683359, acc 0.64\n",
      "2018-04-17T05:49:30.842500: step 31, loss 0.589391, acc 0.7\n",
      "2018-04-17T05:49:30.864165: step 32, loss 0.557769, acc 0.7\n",
      "2018-04-17T05:49:30.884919: step 33, loss 0.57573, acc 0.75\n",
      "2018-04-17T05:49:30.910426: step 34, loss 0.634513, acc 0.66\n",
      "2018-04-17T05:49:30.931505: step 35, loss 0.594227, acc 0.7\n",
      "2018-04-17T05:49:30.949188: step 36, loss 0.543356, acc 0.69863\n",
      "2018-04-17T05:49:30.970366: step 37, loss 0.623414, acc 0.65\n",
      "2018-04-17T05:49:30.991135: step 38, loss 0.566889, acc 0.73\n",
      "2018-04-17T05:49:31.011718: step 39, loss 0.556548, acc 0.69\n",
      "2018-04-17T05:49:31.032484: step 40, loss 0.655971, acc 0.68\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:31.139095: step 40, loss 0.663686, acc 0.560458, rec 0.915621, pre 0.536765, f1 0.67678\n",
      "\n",
      "2018-04-17T05:49:31.162206: step 41, loss 0.631223, acc 0.75\n",
      "2018-04-17T05:49:31.182729: step 42, loss 0.596422, acc 0.67\n",
      "2018-04-17T05:49:31.203359: step 43, loss 0.597862, acc 0.73\n",
      "2018-04-17T05:49:31.224284: step 44, loss 0.587198, acc 0.68\n",
      "2018-04-17T05:49:31.241014: step 45, loss 0.576336, acc 0.69863\n",
      "2018-04-17T05:49:31.261999: step 46, loss 0.649076, acc 0.65\n",
      "2018-04-17T05:49:31.282935: step 47, loss 0.613838, acc 0.74\n",
      "2018-04-17T05:49:31.304590: step 48, loss 0.617615, acc 0.67\n",
      "2018-04-17T05:49:31.325476: step 49, loss 0.66263, acc 0.59\n",
      "2018-04-17T05:49:31.350107: step 50, loss 0.639667, acc 0.65\n",
      "2018-04-17T05:49:31.371523: step 51, loss 0.584088, acc 0.69\n",
      "2018-04-17T05:49:31.392543: step 52, loss 0.51999, acc 0.84\n",
      "2018-04-17T05:49:31.414712: step 53, loss 0.533721, acc 0.73\n",
      "2018-04-17T05:49:31.431993: step 54, loss 0.539385, acc 0.712329\n",
      "2018-04-17T05:49:31.453326: step 55, loss 0.574137, acc 0.78\n",
      "2018-04-17T05:49:31.474575: step 56, loss 0.581959, acc 0.71\n",
      "2018-04-17T05:49:31.496268: step 57, loss 0.602648, acc 0.69\n",
      "2018-04-17T05:49:31.517166: step 58, loss 0.528776, acc 0.77\n",
      "2018-04-17T05:49:31.538585: step 59, loss 0.592509, acc 0.72\n",
      "2018-04-17T05:49:31.564179: step 60, loss 0.541809, acc 0.71\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:31.669052: step 60, loss 0.613841, acc 0.748424, rec 0.782212, pre 0.734475, f1 0.757592\n",
      "\n",
      "2018-04-17T05:49:31.693218: step 61, loss 0.646269, acc 0.62\n",
      "2018-04-17T05:49:31.714517: step 62, loss 0.550791, acc 0.74\n",
      "2018-04-17T05:49:31.731711: step 63, loss 0.555908, acc 0.69863\n",
      "2018-04-17T05:49:31.752962: step 64, loss 0.603407, acc 0.76\n",
      "2018-04-17T05:49:31.777481: step 65, loss 0.573306, acc 0.69\n",
      "2018-04-17T05:49:31.799139: step 66, loss 0.609966, acc 0.68\n",
      "2018-04-17T05:49:31.820664: step 67, loss 0.541635, acc 0.72\n",
      "2018-04-17T05:49:31.841958: step 68, loss 0.523585, acc 0.81\n",
      "2018-04-17T05:49:31.862744: step 69, loss 0.579284, acc 0.73\n",
      "2018-04-17T05:49:31.883854: step 70, loss 0.541958, acc 0.72\n",
      "2018-04-17T05:49:31.905331: step 71, loss 0.511478, acc 0.79\n",
      "2018-04-17T05:49:31.922503: step 72, loss 0.573823, acc 0.69863\n",
      "2018-04-17T05:49:31.944095: step 73, loss 0.628326, acc 0.66\n",
      "2018-04-17T05:49:31.966175: step 74, loss 0.569801, acc 0.72\n",
      "2018-04-17T05:49:31.991961: step 75, loss 0.543481, acc 0.71\n",
      "2018-04-17T05:49:32.014075: step 76, loss 0.580623, acc 0.65\n",
      "2018-04-17T05:49:32.035782: step 77, loss 0.58764, acc 0.72\n",
      "2018-04-17T05:49:32.057780: step 78, loss 0.599723, acc 0.71\n",
      "2018-04-17T05:49:32.079331: step 79, loss 0.551392, acc 0.7\n",
      "2018-04-17T05:49:32.100948: step 80, loss 0.535829, acc 0.79\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:32.206063: step 80, loss 0.603628, acc 0.730086, rec 0.880274, pre 0.678383, f1 0.766253\n",
      "\n",
      "2018-04-17T05:49:32.224414: step 81, loss 0.637218, acc 0.712329\n",
      "2018-04-17T05:49:32.246598: step 82, loss 0.572266, acc 0.65\n",
      "2018-04-17T05:49:32.267285: step 83, loss 0.496749, acc 0.8\n",
      "2018-04-17T05:49:32.288169: step 84, loss 0.541496, acc 0.79\n",
      "2018-04-17T05:49:32.309334: step 85, loss 0.576677, acc 0.71\n",
      "2018-04-17T05:49:32.330834: step 86, loss 0.601935, acc 0.68\n",
      "2018-04-17T05:49:32.351698: step 87, loss 0.511673, acc 0.78\n",
      "2018-04-17T05:49:32.372816: step 88, loss 0.552997, acc 0.76\n",
      "2018-04-17T05:49:32.393808: step 89, loss 0.568952, acc 0.73\n",
      "2018-04-17T05:49:32.414631: step 90, loss 0.49618, acc 0.808219\n",
      "2018-04-17T05:49:32.436307: step 91, loss 0.658594, acc 0.66\n",
      "2018-04-17T05:49:32.457930: step 92, loss 0.546308, acc 0.73\n",
      "2018-04-17T05:49:32.479065: step 93, loss 0.434469, acc 0.83\n",
      "2018-04-17T05:49:32.500123: step 94, loss 0.576763, acc 0.72\n",
      "2018-04-17T05:49:32.521124: step 95, loss 0.479731, acc 0.76\n",
      "2018-04-17T05:49:32.542159: step 96, loss 0.635566, acc 0.7\n",
      "2018-04-17T05:49:32.563563: step 97, loss 0.604183, acc 0.69\n",
      "2018-04-17T05:49:32.584649: step 98, loss 0.582759, acc 0.7\n",
      "2018-04-17T05:49:32.601277: step 99, loss 0.517252, acc 0.780822\n",
      "2018-04-17T05:49:32.625551: step 100, loss 0.551985, acc 0.73\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:32.726639: step 100, loss 0.649386, acc 0.574212, rec 0.952109, pre 0.54362, f1 0.692085\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523944169/checkpoints/model-100\n",
      "\n",
      "2018-04-17T05:49:32.797511: step 101, loss 0.594826, acc 0.69\n",
      "2018-04-17T05:49:32.818672: step 102, loss 0.47721, acc 0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17T05:49:32.843699: step 103, loss 0.55133, acc 0.72\n",
      "2018-04-17T05:49:32.865051: step 104, loss 0.512383, acc 0.76\n",
      "2018-04-17T05:49:32.886173: step 105, loss 0.502151, acc 0.8\n",
      "2018-04-17T05:49:32.907644: step 106, loss 0.582191, acc 0.74\n",
      "2018-04-17T05:49:32.929923: step 107, loss 0.475634, acc 0.83\n",
      "2018-04-17T05:49:32.947547: step 108, loss 0.463896, acc 0.780822\n",
      "2018-04-17T05:49:32.969393: step 109, loss 0.520532, acc 0.78\n",
      "2018-04-17T05:49:32.990923: step 110, loss 0.510371, acc 0.86\n",
      "2018-04-17T05:49:33.012590: step 111, loss 0.464261, acc 0.8\n",
      "2018-04-17T05:49:33.033863: step 112, loss 0.588559, acc 0.74\n",
      "2018-04-17T05:49:33.059580: step 113, loss 0.53799, acc 0.68\n",
      "2018-04-17T05:49:33.081081: step 114, loss 0.522884, acc 0.74\n",
      "2018-04-17T05:49:33.103291: step 115, loss 0.599659, acc 0.73\n",
      "2018-04-17T05:49:33.125080: step 116, loss 0.561278, acc 0.73\n",
      "2018-04-17T05:49:33.142735: step 117, loss 0.511252, acc 0.739726\n",
      "2018-04-17T05:49:33.164267: step 118, loss 0.526402, acc 0.79\n",
      "2018-04-17T05:49:33.186119: step 119, loss 0.563896, acc 0.71\n",
      "2018-04-17T05:49:33.208510: step 120, loss 0.515409, acc 0.72\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:33.314859: step 120, loss 0.641276, acc 0.590258, rec 0.941847, pre 0.554362, f1 0.69793\n",
      "\n",
      "2018-04-17T05:49:33.339418: step 121, loss 0.512348, acc 0.75\n",
      "2018-04-17T05:49:33.360297: step 122, loss 0.479401, acc 0.74\n",
      "2018-04-17T05:49:33.381703: step 123, loss 0.542235, acc 0.71\n",
      "2018-04-17T05:49:33.402942: step 124, loss 0.539504, acc 0.75\n",
      "2018-04-17T05:49:33.424315: step 125, loss 0.618422, acc 0.73\n",
      "2018-04-17T05:49:33.441351: step 126, loss 0.415783, acc 0.863014\n",
      "2018-04-17T05:49:33.463155: step 127, loss 0.460151, acc 0.78\n",
      "2018-04-17T05:49:33.485550: step 128, loss 0.597048, acc 0.71\n",
      "2018-04-17T05:49:33.507358: step 129, loss 0.478236, acc 0.83\n",
      "2018-04-17T05:49:33.532266: step 130, loss 0.583024, acc 0.66\n",
      "2018-04-17T05:49:33.555777: step 131, loss 0.739069, acc 0.63\n",
      "2018-04-17T05:49:33.581687: step 132, loss 0.540021, acc 0.7\n",
      "2018-04-17T05:49:33.606662: step 133, loss 0.561177, acc 0.74\n",
      "2018-04-17T05:49:33.627973: step 134, loss 0.497316, acc 0.79\n",
      "2018-04-17T05:49:33.645274: step 135, loss 0.458631, acc 0.821918\n",
      "2018-04-17T05:49:33.669267: step 136, loss 0.497577, acc 0.8\n",
      "2018-04-17T05:49:33.691540: step 137, loss 0.52387, acc 0.78\n",
      "2018-04-17T05:49:33.713595: step 138, loss 0.558654, acc 0.75\n",
      "2018-04-17T05:49:33.738777: step 139, loss 0.461949, acc 0.87\n",
      "2018-04-17T05:49:33.759808: step 140, loss 0.463627, acc 0.71\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:33.871032: step 140, loss 0.591915, acc 0.696848, rec 0.91106, pre 0.6392, f1 0.751293\n",
      "\n",
      "2018-04-17T05:49:33.897912: step 141, loss 0.539992, acc 0.73\n",
      "2018-04-17T05:49:33.920503: step 142, loss 0.480812, acc 0.75\n",
      "2018-04-17T05:49:33.946791: step 143, loss 0.531096, acc 0.71\n",
      "2018-04-17T05:49:33.963866: step 144, loss 0.553045, acc 0.684932\n",
      "2018-04-17T05:49:33.985747: step 145, loss 0.555913, acc 0.73\n",
      "2018-04-17T05:49:34.007370: step 146, loss 0.63917, acc 0.68\n",
      "2018-04-17T05:49:34.028843: step 147, loss 0.519341, acc 0.75\n",
      "2018-04-17T05:49:34.049828: step 148, loss 0.44437, acc 0.86\n",
      "2018-04-17T05:49:34.070851: step 149, loss 0.522003, acc 0.79\n",
      "2018-04-17T05:49:34.091894: step 150, loss 0.469419, acc 0.79\n",
      "2018-04-17T05:49:34.113233: step 151, loss 0.475041, acc 0.82\n",
      "2018-04-17T05:49:34.133530: step 152, loss 0.529416, acc 0.77\n",
      "2018-04-17T05:49:34.153929: step 153, loss 0.454772, acc 0.767123\n",
      "2018-04-17T05:49:34.174894: step 154, loss 0.466158, acc 0.77\n",
      "2018-04-17T05:49:34.196138: step 155, loss 0.529159, acc 0.73\n",
      "2018-04-17T05:49:34.217570: step 156, loss 0.567445, acc 0.78\n",
      "2018-04-17T05:49:34.238756: step 157, loss 0.479045, acc 0.78\n",
      "2018-04-17T05:49:34.260100: step 158, loss 0.520818, acc 0.8\n",
      "2018-04-17T05:49:34.281521: step 159, loss 0.49432, acc 0.73\n",
      "2018-04-17T05:49:34.302643: step 160, loss 0.410981, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:34.412893: step 160, loss 0.583206, acc 0.704871, rec 0.906499, pre 0.647394, f1 0.755344\n",
      "\n",
      "2018-04-17T05:49:34.436589: step 161, loss 0.423985, acc 0.82\n",
      "2018-04-17T05:49:34.453748: step 162, loss 0.510835, acc 0.780822\n",
      "2018-04-17T05:49:34.475152: step 163, loss 0.479209, acc 0.81\n",
      "2018-04-17T05:49:34.496320: step 164, loss 0.591146, acc 0.71\n",
      "2018-04-17T05:49:34.517874: step 165, loss 0.476641, acc 0.8\n",
      "2018-04-17T05:49:34.539147: step 166, loss 0.491573, acc 0.75\n",
      "2018-04-17T05:49:34.560557: step 167, loss 0.558006, acc 0.75\n",
      "2018-04-17T05:49:34.582074: step 168, loss 0.449972, acc 0.76\n",
      "2018-04-17T05:49:34.605780: step 169, loss 0.453474, acc 0.8\n",
      "2018-04-17T05:49:34.631540: step 170, loss 0.493596, acc 0.81\n",
      "2018-04-17T05:49:34.650983: step 171, loss 0.550506, acc 0.739726\n",
      "2018-04-17T05:49:34.672638: step 172, loss 0.51668, acc 0.74\n",
      "2018-04-17T05:49:34.694245: step 173, loss 0.506415, acc 0.79\n",
      "2018-04-17T05:49:34.715247: step 174, loss 0.517535, acc 0.77\n",
      "2018-04-17T05:49:34.736675: step 175, loss 0.509355, acc 0.75\n",
      "2018-04-17T05:49:34.758073: step 176, loss 0.505887, acc 0.74\n",
      "2018-04-17T05:49:34.779906: step 177, loss 0.44363, acc 0.84\n",
      "2018-04-17T05:49:34.801144: step 178, loss 0.436858, acc 0.86\n",
      "2018-04-17T05:49:34.822788: step 179, loss 0.447548, acc 0.84\n",
      "2018-04-17T05:49:34.844160: step 180, loss 0.431586, acc 0.835616\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:34.946193: step 180, loss 0.625281, acc 0.624069, rec 0.939567, pre 0.577435, f1 0.715278\n",
      "\n",
      "2018-04-17T05:49:34.969736: step 181, loss 0.429973, acc 0.81\n",
      "2018-04-17T05:49:34.991120: step 182, loss 0.467876, acc 0.75\n",
      "2018-04-17T05:49:35.012609: step 183, loss 0.544985, acc 0.7\n",
      "2018-04-17T05:49:35.033629: step 184, loss 0.442738, acc 0.84\n",
      "2018-04-17T05:49:35.059304: step 185, loss 0.50264, acc 0.79\n",
      "2018-04-17T05:49:35.081137: step 186, loss 0.477603, acc 0.78\n",
      "2018-04-17T05:49:35.103265: step 187, loss 0.444423, acc 0.81\n",
      "2018-04-17T05:49:35.125176: step 188, loss 0.579753, acc 0.73\n",
      "2018-04-17T05:49:35.142496: step 189, loss 0.459993, acc 0.767123\n",
      "2018-04-17T05:49:35.163893: step 190, loss 0.507291, acc 0.81\n",
      "2018-04-17T05:49:35.185299: step 191, loss 0.400301, acc 0.82\n",
      "2018-04-17T05:49:35.206631: step 192, loss 0.46583, acc 0.79\n",
      "2018-04-17T05:49:35.230115: step 193, loss 0.502975, acc 0.72\n",
      "2018-04-17T05:49:35.251044: step 194, loss 0.512668, acc 0.73\n",
      "2018-04-17T05:49:35.276530: step 195, loss 0.472982, acc 0.75\n",
      "2018-04-17T05:49:35.298129: step 196, loss 0.509482, acc 0.74\n",
      "2018-04-17T05:49:35.320004: step 197, loss 0.552939, acc 0.72\n",
      "2018-04-17T05:49:35.337170: step 198, loss 0.615991, acc 0.69863\n",
      "2018-04-17T05:49:35.359901: step 199, loss 0.543443, acc 0.77\n",
      "2018-04-17T05:49:35.382227: step 200, loss 0.481644, acc 0.8\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:35.490039: step 200, loss 0.674964, acc 0.570774, rec 0.155074, pre 0.944444, f1 0.266405\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523944169/checkpoints/model-200\n",
      "\n",
      "2018-04-17T05:49:35.557076: step 201, loss 0.631278, acc 0.69\n",
      "2018-04-17T05:49:35.578433: step 202, loss 0.534075, acc 0.75\n",
      "2018-04-17T05:49:35.600789: step 203, loss 0.516176, acc 0.72\n",
      "2018-04-17T05:49:35.622909: step 204, loss 0.479234, acc 0.78\n",
      "2018-04-17T05:49:35.647685: step 205, loss 0.445126, acc 0.78\n",
      "2018-04-17T05:49:35.669312: step 206, loss 0.412205, acc 0.86\n",
      "2018-04-17T05:49:35.687001: step 207, loss 0.462565, acc 0.767123\n",
      "2018-04-17T05:49:35.712460: step 208, loss 0.42579, acc 0.79\n",
      "2018-04-17T05:49:35.733978: step 209, loss 0.4371, acc 0.79\n",
      "2018-04-17T05:49:35.755636: step 210, loss 0.500523, acc 0.79\n",
      "2018-04-17T05:49:35.777075: step 211, loss 0.50636, acc 0.77\n",
      "2018-04-17T05:49:35.798381: step 212, loss 0.500953, acc 0.73\n",
      "2018-04-17T05:49:35.819924: step 213, loss 0.466227, acc 0.78\n",
      "2018-04-17T05:49:35.840921: step 214, loss 0.422052, acc 0.81\n",
      "2018-04-17T05:49:35.862627: step 215, loss 0.504812, acc 0.79\n",
      "2018-04-17T05:49:35.880236: step 216, loss 0.534085, acc 0.726027\n",
      "2018-04-17T05:49:35.901587: step 217, loss 0.493075, acc 0.73\n",
      "2018-04-17T05:49:35.927154: step 218, loss 0.415376, acc 0.86\n",
      "2018-04-17T05:49:35.948256: step 219, loss 0.469164, acc 0.78\n",
      "2018-04-17T05:49:35.969257: step 220, loss 0.539431, acc 0.78\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:36.071605: step 220, loss 0.718966, acc 0.554155, rec 0.970353, pre 0.53088, f1 0.68629\n",
      "\n",
      "2018-04-17T05:49:36.094709: step 221, loss 0.478569, acc 0.75\n",
      "2018-04-17T05:49:36.116265: step 222, loss 0.381511, acc 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17T05:49:36.141896: step 223, loss 0.488231, acc 0.72\n",
      "2018-04-17T05:49:36.163023: step 224, loss 0.469266, acc 0.74\n",
      "2018-04-17T05:49:36.180093: step 225, loss 0.486819, acc 0.808219\n",
      "2018-04-17T05:49:36.202113: step 226, loss 0.518929, acc 0.76\n",
      "2018-04-17T05:49:36.223664: step 227, loss 0.380652, acc 0.86\n",
      "2018-04-17T05:49:36.246616: step 228, loss 0.537248, acc 0.76\n",
      "2018-04-17T05:49:36.267660: step 229, loss 0.398349, acc 0.86\n",
      "2018-04-17T05:49:36.288729: step 230, loss 0.486653, acc 0.76\n",
      "2018-04-17T05:49:36.310541: step 231, loss 0.53663, acc 0.75\n",
      "2018-04-17T05:49:36.332306: step 232, loss 0.427031, acc 0.78\n",
      "2018-04-17T05:49:36.358095: step 233, loss 0.440561, acc 0.8\n",
      "2018-04-17T05:49:36.375703: step 234, loss 0.458256, acc 0.780822\n",
      "2018-04-17T05:49:36.397834: step 235, loss 0.501407, acc 0.79\n",
      "2018-04-17T05:49:36.418761: step 236, loss 0.458079, acc 0.82\n",
      "2018-04-17T05:49:36.439667: step 237, loss 0.488686, acc 0.79\n",
      "2018-04-17T05:49:36.460584: step 238, loss 0.704387, acc 0.62\n",
      "2018-04-17T05:49:36.481547: step 239, loss 0.626946, acc 0.72\n",
      "2018-04-17T05:49:36.502612: step 240, loss 0.57605, acc 0.68\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:36.608183: step 240, loss 0.562535, acc 0.732951, rec 0.898518, pre 0.676395, f1 0.771792\n",
      "\n",
      "2018-04-17T05:49:36.631179: step 241, loss 0.392578, acc 0.82\n",
      "2018-04-17T05:49:36.651931: step 242, loss 0.384411, acc 0.82\n",
      "2018-04-17T05:49:36.668891: step 243, loss 0.585218, acc 0.671233\n",
      "2018-04-17T05:49:36.690295: step 244, loss 0.60354, acc 0.7\n",
      "2018-04-17T05:49:36.711937: step 245, loss 0.454504, acc 0.82\n",
      "2018-04-17T05:49:36.733110: step 246, loss 0.437883, acc 0.82\n",
      "2018-04-17T05:49:36.754396: step 247, loss 0.38493, acc 0.86\n",
      "2018-04-17T05:49:36.775482: step 248, loss 0.458887, acc 0.78\n",
      "2018-04-17T05:49:36.796706: step 249, loss 0.489367, acc 0.75\n",
      "2018-04-17T05:49:36.823187: step 250, loss 0.483465, acc 0.76\n",
      "2018-04-17T05:49:36.844436: step 251, loss 0.434752, acc 0.79\n",
      "2018-04-17T05:49:36.861081: step 252, loss 0.541321, acc 0.767123\n",
      "2018-04-17T05:49:36.882479: step 253, loss 0.475883, acc 0.76\n",
      "2018-04-17T05:49:36.904157: step 254, loss 0.486504, acc 0.79\n",
      "2018-04-17T05:49:36.925646: step 255, loss 0.529193, acc 0.72\n",
      "2018-04-17T05:49:36.947014: step 256, loss 0.527361, acc 0.74\n",
      "2018-04-17T05:49:36.968470: step 257, loss 0.373325, acc 0.88\n",
      "2018-04-17T05:49:36.989658: step 258, loss 0.492521, acc 0.8\n",
      "2018-04-17T05:49:37.011049: step 259, loss 0.434715, acc 0.81\n",
      "2018-04-17T05:49:37.035624: step 260, loss 0.50244, acc 0.78\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:37.137532: step 260, loss 0.645669, acc 0.605731, rec 0.957811, pre 0.56338, f1 0.709459\n",
      "\n",
      "2018-04-17T05:49:37.155698: step 261, loss 0.478266, acc 0.780822\n",
      "2018-04-17T05:49:37.176876: step 262, loss 0.428851, acc 0.79\n",
      "2018-04-17T05:49:37.197382: step 263, loss 0.456261, acc 0.8\n",
      "2018-04-17T05:49:37.218105: step 264, loss 0.558632, acc 0.71\n",
      "2018-04-17T05:49:37.243152: step 265, loss 0.646496, acc 0.72\n",
      "2018-04-17T05:49:37.264480: step 266, loss 0.458625, acc 0.81\n",
      "2018-04-17T05:49:37.285376: step 267, loss 0.374182, acc 0.84\n",
      "2018-04-17T05:49:37.306170: step 268, loss 0.417284, acc 0.82\n",
      "2018-04-17T05:49:37.327040: step 269, loss 0.585672, acc 0.68\n",
      "2018-04-17T05:49:37.344232: step 270, loss 0.626363, acc 0.712329\n",
      "2018-04-17T05:49:37.365060: step 271, loss 0.589073, acc 0.72\n",
      "2018-04-17T05:49:37.385907: step 272, loss 0.396515, acc 0.84\n",
      "2018-04-17T05:49:37.407885: step 273, loss 0.491256, acc 0.72\n",
      "2018-04-17T05:49:37.428983: step 274, loss 0.505957, acc 0.79\n",
      "2018-04-17T05:49:37.453446: step 275, loss 0.48406, acc 0.77\n",
      "2018-04-17T05:49:37.475095: step 276, loss 0.487669, acc 0.73\n",
      "2018-04-17T05:49:37.496255: step 277, loss 0.407677, acc 0.84\n",
      "2018-04-17T05:49:37.517339: step 278, loss 0.428503, acc 0.82\n",
      "2018-04-17T05:49:37.534226: step 279, loss 0.45477, acc 0.767123\n",
      "2018-04-17T05:49:37.555825: step 280, loss 0.418075, acc 0.78\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:37.660405: step 280, loss 0.698825, acc 0.566762, rec 0.965792, pre 0.538462, f1 0.691429\n",
      "\n",
      "2018-04-17T05:49:37.683870: step 281, loss 0.478753, acc 0.78\n",
      "2018-04-17T05:49:37.705016: step 282, loss 0.433387, acc 0.82\n",
      "2018-04-17T05:49:37.725834: step 283, loss 0.472693, acc 0.76\n",
      "2018-04-17T05:49:37.746882: step 284, loss 0.35584, acc 0.85\n",
      "2018-04-17T05:49:37.767893: step 285, loss 0.455303, acc 0.81\n",
      "2018-04-17T05:49:37.788845: step 286, loss 0.468616, acc 0.81\n",
      "2018-04-17T05:49:37.810180: step 287, loss 0.413495, acc 0.81\n",
      "2018-04-17T05:49:37.827172: step 288, loss 0.486728, acc 0.780822\n",
      "2018-04-17T05:49:37.848577: step 289, loss 0.431683, acc 0.77\n",
      "2018-04-17T05:49:37.873626: step 290, loss 0.451196, acc 0.83\n",
      "2018-04-17T05:49:37.895098: step 291, loss 0.458237, acc 0.78\n",
      "2018-04-17T05:49:37.916631: step 292, loss 0.444262, acc 0.78\n",
      "2018-04-17T05:49:37.938532: step 293, loss 0.432737, acc 0.79\n",
      "2018-04-17T05:49:37.959439: step 294, loss 0.508388, acc 0.8\n",
      "2018-04-17T05:49:37.981548: step 295, loss 0.464666, acc 0.82\n",
      "2018-04-17T05:49:38.003577: step 296, loss 0.424839, acc 0.75\n",
      "2018-04-17T05:49:38.020973: step 297, loss 0.476594, acc 0.808219\n",
      "2018-04-17T05:49:38.042476: step 298, loss 0.35655, acc 0.86\n",
      "2018-04-17T05:49:38.063866: step 299, loss 0.477297, acc 0.82\n",
      "2018-04-17T05:49:38.088709: step 300, loss 0.357882, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:38.189245: step 300, loss 0.837369, acc 0.516905, rec 0.976055, pre 0.510131, f1 0.670059\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523944169/checkpoints/model-300\n",
      "\n",
      "2018-04-17T05:49:38.257242: step 301, loss 0.482405, acc 0.76\n",
      "2018-04-17T05:49:38.278764: step 302, loss 0.464601, acc 0.74\n",
      "2018-04-17T05:49:38.304405: step 303, loss 0.457848, acc 0.79\n",
      "2018-04-17T05:49:38.325770: step 304, loss 0.369706, acc 0.81\n",
      "2018-04-17T05:49:38.346621: step 305, loss 0.495499, acc 0.81\n",
      "2018-04-17T05:49:38.363292: step 306, loss 0.461784, acc 0.808219\n",
      "2018-04-17T05:49:38.384481: step 307, loss 0.482943, acc 0.76\n",
      "2018-04-17T05:49:38.405771: step 308, loss 0.413902, acc 0.84\n",
      "2018-04-17T05:49:38.426438: step 309, loss 0.392936, acc 0.84\n",
      "2018-04-17T05:49:38.447420: step 310, loss 0.419726, acc 0.77\n",
      "2018-04-17T05:49:38.467557: step 311, loss 0.41222, acc 0.79\n",
      "2018-04-17T05:49:38.488096: step 312, loss 0.472403, acc 0.78\n",
      "2018-04-17T05:49:38.512537: step 313, loss 0.39296, acc 0.85\n",
      "2018-04-17T05:49:38.533471: step 314, loss 0.392075, acc 0.82\n",
      "2018-04-17T05:49:38.550048: step 315, loss 0.526714, acc 0.726027\n",
      "2018-04-17T05:49:38.570859: step 316, loss 0.612801, acc 0.74\n",
      "2018-04-17T05:49:38.591762: step 317, loss 0.443772, acc 0.81\n",
      "2018-04-17T05:49:38.613340: step 318, loss 0.490917, acc 0.77\n",
      "2018-04-17T05:49:38.634982: step 319, loss 0.517013, acc 0.73\n",
      "2018-04-17T05:49:38.655937: step 320, loss 0.441139, acc 0.79\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:38.757978: step 320, loss 0.524733, acc 0.786246, rec 0.797035, pre 0.781879, f1 0.789385\n",
      "\n",
      "2018-04-17T05:49:38.781274: step 321, loss 0.45955, acc 0.77\n",
      "2018-04-17T05:49:38.802624: step 322, loss 0.517907, acc 0.78\n",
      "2018-04-17T05:49:38.824202: step 323, loss 0.45666, acc 0.82\n",
      "2018-04-17T05:49:38.840891: step 324, loss 0.383131, acc 0.863014\n",
      "2018-04-17T05:49:38.862528: step 325, loss 0.387381, acc 0.85\n",
      "2018-04-17T05:49:38.883967: step 326, loss 0.437847, acc 0.74\n",
      "2018-04-17T05:49:38.906102: step 327, loss 0.390677, acc 0.84\n",
      "2018-04-17T05:49:38.927487: step 328, loss 0.404639, acc 0.79\n",
      "2018-04-17T05:49:38.949053: step 329, loss 0.455339, acc 0.78\n",
      "2018-04-17T05:49:38.974409: step 330, loss 0.417997, acc 0.79\n",
      "2018-04-17T05:49:38.995967: step 331, loss 0.50546, acc 0.76\n",
      "2018-04-17T05:49:39.016989: step 332, loss 0.405068, acc 0.84\n",
      "2018-04-17T05:49:39.034054: step 333, loss 0.512723, acc 0.767123\n",
      "2018-04-17T05:49:39.055269: step 334, loss 0.482613, acc 0.72\n",
      "2018-04-17T05:49:39.076560: step 335, loss 0.448778, acc 0.79\n",
      "2018-04-17T05:49:39.098123: step 336, loss 0.477213, acc 0.79\n",
      "2018-04-17T05:49:39.119381: step 337, loss 0.374344, acc 0.83\n",
      "2018-04-17T05:49:39.140675: step 338, loss 0.375688, acc 0.83\n",
      "2018-04-17T05:49:39.161687: step 339, loss 0.452465, acc 0.79\n",
      "2018-04-17T05:49:39.187151: step 340, loss 0.396796, acc 0.82\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:39.288251: step 340, loss 0.729709, acc 0.558166, rec 0.976055, pre 0.533001, f1 0.689489\n",
      "\n",
      "2018-04-17T05:49:39.311100: step 341, loss 0.470609, acc 0.77\n",
      "2018-04-17T05:49:39.328556: step 342, loss 0.473733, acc 0.835616\n",
      "2018-04-17T05:49:39.350082: step 343, loss 0.421724, acc 0.81\n",
      "2018-04-17T05:49:39.371306: step 344, loss 0.405172, acc 0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17T05:49:39.396931: step 345, loss 0.443701, acc 0.8\n",
      "2018-04-17T05:49:39.418289: step 346, loss 0.474488, acc 0.8\n",
      "2018-04-17T05:49:39.439309: step 347, loss 0.443613, acc 0.8\n",
      "2018-04-17T05:49:39.459973: step 348, loss 0.427242, acc 0.77\n",
      "2018-04-17T05:49:39.481532: step 349, loss 0.456031, acc 0.78\n",
      "2018-04-17T05:49:39.502286: step 350, loss 0.387956, acc 0.84\n",
      "2018-04-17T05:49:39.519163: step 351, loss 0.354696, acc 0.863014\n",
      "2018-04-17T05:49:39.540423: step 352, loss 0.449158, acc 0.81\n",
      "2018-04-17T05:49:39.564158: step 353, loss 0.405711, acc 0.77\n",
      "2018-04-17T05:49:39.585011: step 354, loss 0.394432, acc 0.85\n",
      "2018-04-17T05:49:39.612226: step 355, loss 0.416413, acc 0.83\n",
      "2018-04-17T05:49:39.633879: step 356, loss 0.504906, acc 0.75\n",
      "2018-04-17T05:49:39.655090: step 357, loss 0.366683, acc 0.83\n",
      "2018-04-17T05:49:39.676386: step 358, loss 0.338311, acc 0.83\n",
      "2018-04-17T05:49:39.698050: step 359, loss 0.478288, acc 0.8\n",
      "2018-04-17T05:49:39.715001: step 360, loss 0.483864, acc 0.753425\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:39.820986: step 360, loss 0.552021, acc 0.737536, rec 0.90878, pre 0.678298, f1 0.776803\n",
      "\n",
      "2018-04-17T05:49:39.847363: step 361, loss 0.371294, acc 0.82\n",
      "2018-04-17T05:49:39.868853: step 362, loss 0.40059, acc 0.79\n",
      "2018-04-17T05:49:39.890132: step 363, loss 0.46485, acc 0.79\n",
      "2018-04-17T05:49:39.911374: step 364, loss 0.426268, acc 0.82\n",
      "2018-04-17T05:49:39.932766: step 365, loss 0.382522, acc 0.84\n",
      "2018-04-17T05:49:39.953635: step 366, loss 0.372397, acc 0.86\n",
      "2018-04-17T05:49:39.974291: step 367, loss 0.493684, acc 0.8\n",
      "2018-04-17T05:49:39.995437: step 368, loss 0.43212, acc 0.79\n",
      "2018-04-17T05:49:40.012357: step 369, loss 0.414777, acc 0.794521\n",
      "2018-04-17T05:49:40.037067: step 370, loss 0.4522, acc 0.71\n",
      "2018-04-17T05:49:40.058022: step 371, loss 0.41847, acc 0.82\n",
      "2018-04-17T05:49:40.079117: step 372, loss 0.362457, acc 0.85\n",
      "2018-04-17T05:49:40.099977: step 373, loss 0.390096, acc 0.82\n",
      "2018-04-17T05:49:40.121351: step 374, loss 0.311148, acc 0.9\n",
      "2018-04-17T05:49:40.142190: step 375, loss 0.335628, acc 0.85\n",
      "2018-04-17T05:49:40.163083: step 376, loss 0.449754, acc 0.77\n",
      "2018-04-17T05:49:40.184658: step 377, loss 0.496902, acc 0.78\n",
      "2018-04-17T05:49:40.201551: step 378, loss 0.442127, acc 0.767123\n",
      "2018-04-17T05:49:40.222510: step 379, loss 0.417884, acc 0.84\n",
      "2018-04-17T05:49:40.247592: step 380, loss 0.417076, acc 0.81\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:40.349504: step 380, loss 0.514112, acc 0.789685, rec 0.77423, pre 0.800708, f1 0.787246\n",
      "\n",
      "2018-04-17T05:49:40.372764: step 381, loss 0.400172, acc 0.8\n",
      "2018-04-17T05:49:40.394703: step 382, loss 0.407021, acc 0.8\n",
      "2018-04-17T05:49:40.416561: step 383, loss 0.455827, acc 0.78\n",
      "2018-04-17T05:49:40.437795: step 384, loss 0.338278, acc 0.85\n",
      "2018-04-17T05:49:40.462370: step 385, loss 0.415674, acc 0.78\n",
      "2018-04-17T05:49:40.483331: step 386, loss 0.393844, acc 0.81\n",
      "2018-04-17T05:49:40.500507: step 387, loss 0.463254, acc 0.835616\n",
      "2018-04-17T05:49:40.521854: step 388, loss 0.477435, acc 0.8\n",
      "2018-04-17T05:49:40.543143: step 389, loss 0.414093, acc 0.83\n",
      "2018-04-17T05:49:40.563928: step 390, loss 0.523333, acc 0.79\n",
      "2018-04-17T05:49:40.585259: step 391, loss 0.482242, acc 0.76\n",
      "2018-04-17T05:49:40.605988: step 392, loss 0.350111, acc 0.85\n",
      "2018-04-17T05:49:40.626817: step 393, loss 0.40711, acc 0.8\n",
      "2018-04-17T05:49:40.648469: step 394, loss 0.473789, acc 0.76\n",
      "2018-04-17T05:49:40.674664: step 395, loss 0.405635, acc 0.83\n",
      "2018-04-17T05:49:40.692109: step 396, loss 0.350175, acc 0.849315\n",
      "2018-04-17T05:49:40.713892: step 397, loss 0.465014, acc 0.79\n",
      "2018-04-17T05:49:40.734970: step 398, loss 0.509329, acc 0.78\n",
      "2018-04-17T05:49:40.755533: step 399, loss 0.367742, acc 0.87\n",
      "2018-04-17T05:49:40.776282: step 400, loss 0.394036, acc 0.8\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:40.879576: step 400, loss 0.53739, acc 0.727221, rec 0.517674, pre 0.895464, f1 0.656069\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523944169/checkpoints/model-400\n",
      "\n",
      "2018-04-17T05:49:40.945930: step 401, loss 0.5027, acc 0.77\n",
      "2018-04-17T05:49:40.967589: step 402, loss 0.441208, acc 0.76\n",
      "2018-04-17T05:49:40.988731: step 403, loss 0.475225, acc 0.75\n",
      "2018-04-17T05:49:41.010671: step 404, loss 0.368969, acc 0.82\n",
      "2018-04-17T05:49:41.027640: step 405, loss 0.368317, acc 0.821918\n",
      "2018-04-17T05:49:41.048379: step 406, loss 0.379022, acc 0.84\n",
      "2018-04-17T05:49:41.069006: step 407, loss 0.416677, acc 0.85\n",
      "2018-04-17T05:49:41.094267: step 408, loss 0.334949, acc 0.88\n",
      "2018-04-17T05:49:41.115593: step 409, loss 0.452123, acc 0.76\n",
      "2018-04-17T05:49:41.136813: step 410, loss 0.355137, acc 0.84\n",
      "2018-04-17T05:49:41.157853: step 411, loss 0.355327, acc 0.85\n",
      "2018-04-17T05:49:41.178873: step 412, loss 0.354459, acc 0.85\n",
      "2018-04-17T05:49:41.199899: step 413, loss 0.516906, acc 0.79\n",
      "2018-04-17T05:49:41.216802: step 414, loss 0.464762, acc 0.739726\n",
      "2018-04-17T05:49:41.238177: step 415, loss 0.383761, acc 0.81\n",
      "2018-04-17T05:49:41.259091: step 416, loss 0.482557, acc 0.79\n",
      "2018-04-17T05:49:41.280032: step 417, loss 0.443082, acc 0.75\n",
      "2018-04-17T05:49:41.304730: step 418, loss 0.420601, acc 0.81\n",
      "2018-04-17T05:49:41.325431: step 419, loss 0.355132, acc 0.87\n",
      "2018-04-17T05:49:41.346539: step 420, loss 0.404979, acc 0.85\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:41.446450: step 420, loss 0.551732, acc 0.726075, rec 0.924743, pre 0.663123, f1 0.772381\n",
      "\n",
      "2018-04-17T05:49:41.469667: step 421, loss 0.379288, acc 0.8\n",
      "2018-04-17T05:49:41.491873: step 422, loss 0.413319, acc 0.83\n",
      "2018-04-17T05:49:41.513239: step 423, loss 0.332976, acc 0.835616\n",
      "2018-04-17T05:49:41.535128: step 424, loss 0.310976, acc 0.88\n",
      "2018-04-17T05:49:41.556631: step 425, loss 0.453516, acc 0.82\n",
      "2018-04-17T05:49:41.577292: step 426, loss 0.348665, acc 0.8\n",
      "2018-04-17T05:49:41.598504: step 427, loss 0.49405, acc 0.76\n",
      "2018-04-17T05:49:41.619793: step 428, loss 0.421717, acc 0.78\n",
      "2018-04-17T05:49:41.641393: step 429, loss 0.464185, acc 0.79\n",
      "2018-04-17T05:49:41.662526: step 430, loss 0.352194, acc 0.87\n",
      "2018-04-17T05:49:41.683724: step 431, loss 0.382769, acc 0.83\n",
      "2018-04-17T05:49:41.700586: step 432, loss 0.41183, acc 0.821918\n",
      "2018-04-17T05:49:41.725766: step 433, loss 0.369221, acc 0.83\n",
      "2018-04-17T05:49:41.746421: step 434, loss 0.400967, acc 0.84\n",
      "2018-04-17T05:49:41.767125: step 435, loss 0.35075, acc 0.84\n",
      "2018-04-17T05:49:41.788502: step 436, loss 0.510113, acc 0.77\n",
      "2018-04-17T05:49:41.809823: step 437, loss 0.428954, acc 0.82\n",
      "2018-04-17T05:49:41.831149: step 438, loss 0.374118, acc 0.87\n",
      "2018-04-17T05:49:41.852245: step 439, loss 0.303774, acc 0.88\n",
      "2018-04-17T05:49:41.873369: step 440, loss 0.441356, acc 0.79\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:41.977778: step 440, loss 0.686873, acc 0.598854, rec 0.960091, pre 0.558726, f1 0.706376\n",
      "\n",
      "2018-04-17T05:49:41.996313: step 441, loss 0.385541, acc 0.835616\n",
      "2018-04-17T05:49:42.018730: step 442, loss 0.356571, acc 0.85\n",
      "2018-04-17T05:49:42.039732: step 443, loss 0.493838, acc 0.75\n",
      "2018-04-17T05:49:42.060641: step 444, loss 0.390149, acc 0.86\n",
      "2018-04-17T05:49:42.081030: step 445, loss 0.415982, acc 0.8\n",
      "2018-04-17T05:49:42.101342: step 446, loss 0.397824, acc 0.83\n",
      "2018-04-17T05:49:42.121784: step 447, loss 0.425108, acc 0.83\n",
      "2018-04-17T05:49:42.142281: step 448, loss 0.415844, acc 0.82\n",
      "2018-04-17T05:49:42.162867: step 449, loss 0.328456, acc 0.85\n",
      "2018-04-17T05:49:42.182998: step 450, loss 0.352202, acc 0.835616\n",
      "2018-04-17T05:49:42.204176: step 451, loss 0.396754, acc 0.83\n",
      "2018-04-17T05:49:42.224814: step 452, loss 0.410642, acc 0.84\n",
      "2018-04-17T05:49:42.245797: step 453, loss 0.249204, acc 0.93\n",
      "2018-04-17T05:49:42.266692: step 454, loss 0.498491, acc 0.79\n",
      "2018-04-17T05:49:42.287527: step 455, loss 0.339873, acc 0.86\n",
      "2018-04-17T05:49:42.308691: step 456, loss 0.372699, acc 0.84\n",
      "2018-04-17T05:49:42.329959: step 457, loss 0.451558, acc 0.77\n",
      "2018-04-17T05:49:42.350460: step 458, loss 0.35339, acc 0.83\n",
      "2018-04-17T05:49:42.366675: step 459, loss 0.533293, acc 0.726027\n",
      "2018-04-17T05:49:42.391491: step 460, loss 0.416244, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:42.493022: step 460, loss 0.517243, acc 0.769054, rec 0.889396, pre 0.718232, f1 0.794702\n",
      "\n",
      "2018-04-17T05:49:42.516562: step 461, loss 0.386128, acc 0.85\n",
      "2018-04-17T05:49:42.537501: step 462, loss 0.382674, acc 0.83\n",
      "2018-04-17T05:49:42.559485: step 463, loss 0.381354, acc 0.86\n",
      "2018-04-17T05:49:42.580463: step 464, loss 0.527639, acc 0.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17T05:49:42.604275: step 465, loss 0.375132, acc 0.87\n",
      "2018-04-17T05:49:42.624683: step 466, loss 0.34823, acc 0.84\n",
      "2018-04-17T05:49:42.647951: step 467, loss 0.329652, acc 0.84\n",
      "2018-04-17T05:49:42.665112: step 468, loss 0.323308, acc 0.835616\n",
      "2018-04-17T05:49:42.686547: step 469, loss 0.333851, acc 0.87\n",
      "2018-04-17T05:49:42.707922: step 470, loss 0.343726, acc 0.85\n",
      "2018-04-17T05:49:42.729168: step 471, loss 0.454869, acc 0.82\n",
      "2018-04-17T05:49:42.750262: step 472, loss 0.374604, acc 0.85\n",
      "2018-04-17T05:49:42.771867: step 473, loss 0.418089, acc 0.8\n",
      "2018-04-17T05:49:42.792736: step 474, loss 0.450577, acc 0.81\n",
      "2018-04-17T05:49:42.817700: step 475, loss 0.389007, acc 0.79\n",
      "2018-04-17T05:49:42.839185: step 476, loss 0.404742, acc 0.82\n",
      "2018-04-17T05:49:42.856116: step 477, loss 0.391134, acc 0.835616\n",
      "2018-04-17T05:49:42.878698: step 478, loss 0.343, acc 0.86\n",
      "2018-04-17T05:49:42.905893: step 479, loss 0.422796, acc 0.8\n",
      "2018-04-17T05:49:42.927448: step 480, loss 0.359596, acc 0.79\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:43.038358: step 480, loss 0.508002, acc 0.778797, rec 0.879133, pre 0.733587, f1 0.799793\n",
      "\n",
      "2018-04-17T05:49:43.062017: step 481, loss 0.356429, acc 0.84\n",
      "2018-04-17T05:49:43.082941: step 482, loss 0.439342, acc 0.83\n",
      "2018-04-17T05:49:43.104254: step 483, loss 0.416141, acc 0.81\n",
      "2018-04-17T05:49:43.125138: step 484, loss 0.370588, acc 0.83\n",
      "2018-04-17T05:49:43.145678: step 485, loss 0.336798, acc 0.87\n",
      "2018-04-17T05:49:43.162713: step 486, loss 0.346168, acc 0.876712\n",
      "2018-04-17T05:49:43.183559: step 487, loss 0.285088, acc 0.88\n",
      "2018-04-17T05:49:43.204301: step 488, loss 0.395142, acc 0.86\n",
      "2018-04-17T05:49:43.225060: step 489, loss 0.417107, acc 0.77\n",
      "2018-04-17T05:49:43.249378: step 490, loss 0.422075, acc 0.79\n",
      "2018-04-17T05:49:43.270157: step 491, loss 0.363902, acc 0.86\n",
      "2018-04-17T05:49:43.290699: step 492, loss 0.406276, acc 0.82\n",
      "2018-04-17T05:49:43.311647: step 493, loss 0.398799, acc 0.83\n",
      "2018-04-17T05:49:43.332367: step 494, loss 0.368879, acc 0.84\n",
      "2018-04-17T05:49:43.349070: step 495, loss 0.390397, acc 0.849315\n",
      "2018-04-17T05:49:43.369823: step 496, loss 0.414982, acc 0.82\n",
      "2018-04-17T05:49:43.390688: step 497, loss 0.390951, acc 0.86\n",
      "2018-04-17T05:49:43.412659: step 498, loss 0.316842, acc 0.86\n",
      "2018-04-17T05:49:43.433586: step 499, loss 0.493253, acc 0.81\n",
      "2018-04-17T05:49:43.458890: step 500, loss 0.350157, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:43.560917: step 500, loss 0.571927, acc 0.705444, rec 0.932725, pre 0.642577, f1 0.76093\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523944169/checkpoints/model-500\n",
      "\n",
      "2018-04-17T05:49:43.628578: step 501, loss 0.369758, acc 0.81\n",
      "2018-04-17T05:49:43.649417: step 502, loss 0.356281, acc 0.84\n",
      "2018-04-17T05:49:43.674541: step 503, loss 0.367678, acc 0.85\n",
      "2018-04-17T05:49:43.691768: step 504, loss 0.326575, acc 0.890411\n",
      "2018-04-17T05:49:43.713304: step 505, loss 0.432207, acc 0.81\n",
      "2018-04-17T05:49:43.734210: step 506, loss 0.421134, acc 0.81\n",
      "2018-04-17T05:49:43.754781: step 507, loss 0.379025, acc 0.83\n",
      "2018-04-17T05:49:43.775731: step 508, loss 0.354483, acc 0.86\n",
      "2018-04-17T05:49:43.796728: step 509, loss 0.345845, acc 0.86\n",
      "2018-04-17T05:49:43.817208: step 510, loss 0.355903, acc 0.83\n",
      "2018-04-17T05:49:43.838139: step 511, loss 0.298923, acc 0.87\n",
      "2018-04-17T05:49:43.859212: step 512, loss 0.37602, acc 0.83\n",
      "2018-04-17T05:49:43.879489: step 513, loss 0.547146, acc 0.767123\n",
      "2018-04-17T05:49:43.901351: step 514, loss 0.37942, acc 0.85\n",
      "2018-04-17T05:49:43.922543: step 515, loss 0.333524, acc 0.87\n",
      "2018-04-17T05:49:43.943450: step 516, loss 0.364895, acc 0.83\n",
      "2018-04-17T05:49:43.964363: step 517, loss 0.341579, acc 0.82\n",
      "2018-04-17T05:49:43.985142: step 518, loss 0.364966, acc 0.81\n",
      "2018-04-17T05:49:44.006173: step 519, loss 0.36621, acc 0.85\n",
      "2018-04-17T05:49:44.026988: step 520, loss 0.350491, acc 0.85\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:44.134412: step 520, loss 0.510246, acc 0.77765, rec 0.888255, pre 0.728718, f1 0.800617\n",
      "\n",
      "2018-04-17T05:49:44.158205: step 521, loss 0.424927, acc 0.8\n",
      "2018-04-17T05:49:44.175207: step 522, loss 0.489639, acc 0.753425\n",
      "2018-04-17T05:49:44.196408: step 523, loss 0.33207, acc 0.86\n",
      "2018-04-17T05:49:44.218164: step 524, loss 0.381153, acc 0.83\n",
      "2018-04-17T05:49:44.239820: step 525, loss 0.326275, acc 0.86\n",
      "2018-04-17T05:49:44.260973: step 526, loss 0.364855, acc 0.86\n",
      "2018-04-17T05:49:44.282015: step 527, loss 0.39007, acc 0.84\n",
      "2018-04-17T05:49:44.304174: step 528, loss 0.377002, acc 0.82\n",
      "2018-04-17T05:49:44.326092: step 529, loss 0.405345, acc 0.83\n",
      "2018-04-17T05:49:44.351982: step 530, loss 0.519814, acc 0.76\n",
      "2018-04-17T05:49:44.369072: step 531, loss 0.433313, acc 0.808219\n",
      "2018-04-17T05:49:44.390390: step 532, loss 0.43643, acc 0.81\n",
      "2018-04-17T05:49:44.411193: step 533, loss 0.31137, acc 0.83\n",
      "2018-04-17T05:49:44.432404: step 534, loss 0.368365, acc 0.8\n",
      "2018-04-17T05:49:44.453173: step 535, loss 0.318867, acc 0.88\n",
      "2018-04-17T05:49:44.474733: step 536, loss 0.278766, acc 0.86\n",
      "2018-04-17T05:49:44.495571: step 537, loss 0.451912, acc 0.84\n",
      "2018-04-17T05:49:44.516149: step 538, loss 0.460039, acc 0.8\n",
      "2018-04-17T05:49:44.536932: step 539, loss 0.342638, acc 0.87\n",
      "2018-04-17T05:49:44.556751: step 540, loss 0.364237, acc 0.835616\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:44.658160: step 540, loss 0.564633, acc 0.713467, rec 0.937286, pre 0.648777, f1 0.766791\n",
      "\n",
      "2018-04-17T05:49:44.681186: step 541, loss 0.310239, acc 0.9\n",
      "2018-04-17T05:49:44.703635: step 542, loss 0.385063, acc 0.79\n",
      "2018-04-17T05:49:44.724546: step 543, loss 0.422989, acc 0.82\n",
      "2018-04-17T05:49:44.744715: step 544, loss 0.430417, acc 0.83\n",
      "2018-04-17T05:49:44.768309: step 545, loss 0.485383, acc 0.71\n",
      "2018-04-17T05:49:44.788661: step 546, loss 0.383024, acc 0.88\n",
      "2018-04-17T05:49:44.809135: step 547, loss 0.417114, acc 0.81\n",
      "2018-04-17T05:49:44.829730: step 548, loss 0.487277, acc 0.8\n",
      "2018-04-17T05:49:44.846206: step 549, loss 0.39568, acc 0.808219\n",
      "2018-04-17T05:49:44.866547: step 550, loss 0.42511, acc 0.78\n",
      "2018-04-17T05:49:44.887333: step 551, loss 0.376367, acc 0.86\n",
      "2018-04-17T05:49:44.908356: step 552, loss 0.370889, acc 0.84\n",
      "2018-04-17T05:49:44.929588: step 553, loss 0.415221, acc 0.83\n",
      "2018-04-17T05:49:44.950343: step 554, loss 0.342549, acc 0.87\n",
      "2018-04-17T05:49:44.975574: step 555, loss 0.335944, acc 0.87\n",
      "2018-04-17T05:49:44.996805: step 556, loss 0.377118, acc 0.81\n",
      "2018-04-17T05:49:45.018310: step 557, loss 0.503083, acc 0.76\n",
      "2018-04-17T05:49:45.035314: step 558, loss 0.562747, acc 0.726027\n",
      "2018-04-17T05:49:45.056389: step 559, loss 0.383283, acc 0.83\n",
      "2018-04-17T05:49:45.078001: step 560, loss 0.311546, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:45.181992: step 560, loss 0.635021, acc 0.645845, rec 0.95325, pre 0.591649, f1 0.730131\n",
      "\n",
      "2018-04-17T05:49:45.205146: step 561, loss 0.368835, acc 0.88\n",
      "2018-04-17T05:49:45.225546: step 562, loss 0.334338, acc 0.86\n",
      "2018-04-17T05:49:45.246022: step 563, loss 0.35055, acc 0.84\n",
      "2018-04-17T05:49:45.266369: step 564, loss 0.383189, acc 0.85\n",
      "2018-04-17T05:49:45.287463: step 565, loss 0.428332, acc 0.79\n",
      "2018-04-17T05:49:45.308657: step 566, loss 0.292783, acc 0.9\n",
      "2018-04-17T05:49:45.325299: step 567, loss 0.432874, acc 0.835616\n",
      "2018-04-17T05:49:45.346160: step 568, loss 0.368413, acc 0.85\n",
      "2018-04-17T05:49:45.366662: step 569, loss 0.339709, acc 0.8\n",
      "2018-04-17T05:49:45.391636: step 570, loss 0.395311, acc 0.86\n",
      "2018-04-17T05:49:45.412579: step 571, loss 0.361933, acc 0.83\n",
      "2018-04-17T05:49:45.433005: step 572, loss 0.281475, acc 0.9\n",
      "2018-04-17T05:49:45.453501: step 573, loss 0.327282, acc 0.88\n",
      "2018-04-17T05:49:45.474749: step 574, loss 0.444362, acc 0.8\n",
      "2018-04-17T05:49:45.497117: step 575, loss 0.373308, acc 0.84\n",
      "2018-04-17T05:49:45.514557: step 576, loss 0.339293, acc 0.890411\n",
      "2018-04-17T05:49:45.535882: step 577, loss 0.271864, acc 0.86\n",
      "2018-04-17T05:49:45.558324: step 578, loss 0.3667, acc 0.81\n",
      "2018-04-17T05:49:45.579837: step 579, loss 0.428273, acc 0.75\n",
      "2018-04-17T05:49:45.605902: step 580, loss 0.412483, acc 0.82\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:45.710356: step 580, loss 0.703686, acc 0.608023, rec 0.95553, pre 0.565071, f1 0.710169\n",
      "\n",
      "2018-04-17T05:49:45.733541: step 581, loss 0.313278, acc 0.9\n",
      "2018-04-17T05:49:45.755032: step 582, loss 0.335449, acc 0.88\n",
      "2018-04-17T05:49:45.776365: step 583, loss 0.321159, acc 0.86\n",
      "2018-04-17T05:49:45.797511: step 584, loss 0.343251, acc 0.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17T05:49:45.818132: step 585, loss 0.428513, acc 0.808219\n",
      "2018-04-17T05:49:45.839962: step 586, loss 0.432296, acc 0.81\n",
      "2018-04-17T05:49:45.860724: step 587, loss 0.475665, acc 0.75\n",
      "2018-04-17T05:49:45.881606: step 588, loss 0.542667, acc 0.78\n",
      "2018-04-17T05:49:45.902442: step 589, loss 0.414179, acc 0.81\n",
      "2018-04-17T05:49:45.923211: step 590, loss 0.424975, acc 0.8\n",
      "2018-04-17T05:49:45.943857: step 591, loss 0.331179, acc 0.85\n",
      "2018-04-17T05:49:45.964683: step 592, loss 0.359519, acc 0.85\n",
      "2018-04-17T05:49:45.985066: step 593, loss 0.339706, acc 0.86\n",
      "2018-04-17T05:49:46.002292: step 594, loss 0.326473, acc 0.863014\n",
      "2018-04-17T05:49:46.027845: step 595, loss 0.308898, acc 0.86\n",
      "2018-04-17T05:49:46.048850: step 596, loss 0.349837, acc 0.86\n",
      "2018-04-17T05:49:46.069648: step 597, loss 0.395515, acc 0.84\n",
      "2018-04-17T05:49:46.090188: step 598, loss 0.368072, acc 0.84\n",
      "2018-04-17T05:49:46.110584: step 599, loss 0.451091, acc 0.79\n",
      "2018-04-17T05:49:46.130855: step 600, loss 0.42561, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:46.235403: step 600, loss 0.780544, acc 0.571347, rec 0.977195, pre 0.540694, f1 0.696182\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523944169/checkpoints/model-600\n",
      "\n",
      "2018-04-17T05:49:46.305981: step 601, loss 0.350043, acc 0.89\n",
      "2018-04-17T05:49:46.327774: step 602, loss 0.317132, acc 0.9\n",
      "2018-04-17T05:49:46.344847: step 603, loss 0.357209, acc 0.808219\n",
      "2018-04-17T05:49:46.367004: step 604, loss 0.31192, acc 0.89\n",
      "2018-04-17T05:49:46.388790: step 605, loss 0.377173, acc 0.84\n",
      "2018-04-17T05:49:46.410605: step 606, loss 0.377963, acc 0.83\n",
      "2018-04-17T05:49:46.431643: step 607, loss 0.480372, acc 0.78\n",
      "2018-04-17T05:49:46.456730: step 608, loss 0.339323, acc 0.87\n",
      "2018-04-17T05:49:46.477865: step 609, loss 0.364531, acc 0.84\n",
      "2018-04-17T05:49:46.498682: step 610, loss 0.332077, acc 0.86\n",
      "2018-04-17T05:49:46.519290: step 611, loss 0.321855, acc 0.87\n",
      "2018-04-17T05:49:46.536172: step 612, loss 0.291486, acc 0.917808\n",
      "2018-04-17T05:49:46.557112: step 613, loss 0.326208, acc 0.86\n",
      "2018-04-17T05:49:46.578067: step 614, loss 0.414251, acc 0.84\n",
      "2018-04-17T05:49:46.598975: step 615, loss 0.373085, acc 0.82\n",
      "2018-04-17T05:49:46.619795: step 616, loss 0.422749, acc 0.83\n",
      "2018-04-17T05:49:46.640627: step 617, loss 0.439907, acc 0.81\n",
      "2018-04-17T05:49:46.665114: step 618, loss 0.371917, acc 0.86\n",
      "2018-04-17T05:49:46.686557: step 619, loss 0.309914, acc 0.89\n",
      "2018-04-17T05:49:46.707622: step 620, loss 0.326022, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:46.804195: step 620, loss 0.643423, acc 0.646418, rec 0.958951, pre 0.591421, f1 0.731622\n",
      "\n",
      "2018-04-17T05:49:46.822004: step 621, loss 0.301763, acc 0.863014\n",
      "2018-04-17T05:49:46.843313: step 622, loss 0.359943, acc 0.85\n",
      "2018-04-17T05:49:46.863671: step 623, loss 0.270599, acc 0.91\n",
      "2018-04-17T05:49:46.887583: step 624, loss 0.323954, acc 0.86\n",
      "2018-04-17T05:49:46.908661: step 625, loss 0.403039, acc 0.81\n",
      "2018-04-17T05:49:46.929560: step 626, loss 0.361536, acc 0.87\n",
      "2018-04-17T05:49:46.950456: step 627, loss 0.322134, acc 0.83\n",
      "2018-04-17T05:49:46.971339: step 628, loss 0.354796, acc 0.83\n",
      "2018-04-17T05:49:46.992342: step 629, loss 0.419195, acc 0.76\n",
      "2018-04-17T05:49:47.008867: step 630, loss 0.405536, acc 0.863014\n",
      "2018-04-17T05:49:47.030092: step 631, loss 0.381358, acc 0.82\n",
      "2018-04-17T05:49:47.050782: step 632, loss 0.328707, acc 0.84\n",
      "2018-04-17T05:49:47.071716: step 633, loss 0.349535, acc 0.81\n",
      "2018-04-17T05:49:47.097254: step 634, loss 0.39528, acc 0.81\n",
      "2018-04-17T05:49:47.119305: step 635, loss 0.311399, acc 0.86\n",
      "2018-04-17T05:49:47.140885: step 636, loss 0.369922, acc 0.83\n",
      "2018-04-17T05:49:47.161802: step 637, loss 0.440017, acc 0.81\n",
      "2018-04-17T05:49:47.183257: step 638, loss 0.303612, acc 0.88\n",
      "2018-04-17T05:49:47.200725: step 639, loss 0.353971, acc 0.849315\n",
      "2018-04-17T05:49:47.221790: step 640, loss 0.292688, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:47.325518: step 640, loss 0.492086, acc 0.790831, rec 0.718358, pre 0.842246, f1 0.775385\n",
      "\n",
      "2018-04-17T05:49:47.349396: step 641, loss 0.418037, acc 0.79\n",
      "2018-04-17T05:49:47.370823: step 642, loss 0.296352, acc 0.88\n",
      "2018-04-17T05:49:47.392508: step 643, loss 0.346932, acc 0.87\n",
      "2018-04-17T05:49:47.413658: step 644, loss 0.30494, acc 0.87\n",
      "2018-04-17T05:49:47.434713: step 645, loss 0.352052, acc 0.85\n",
      "2018-04-17T05:49:47.455996: step 646, loss 0.367562, acc 0.82\n",
      "2018-04-17T05:49:47.478594: step 647, loss 0.301336, acc 0.88\n",
      "2018-04-17T05:49:47.495896: step 648, loss 0.384184, acc 0.849315\n",
      "2018-04-17T05:49:47.518205: step 649, loss 0.417389, acc 0.81\n",
      "2018-04-17T05:49:47.544245: step 650, loss 0.311156, acc 0.9\n",
      "2018-04-17T05:49:47.566015: step 651, loss 0.417246, acc 0.83\n",
      "2018-04-17T05:49:47.588139: step 652, loss 0.350371, acc 0.83\n",
      "2018-04-17T05:49:47.609309: step 653, loss 0.28169, acc 0.91\n",
      "2018-04-17T05:49:47.631746: step 654, loss 0.294957, acc 0.89\n",
      "2018-04-17T05:49:47.652774: step 655, loss 0.347904, acc 0.86\n",
      "2018-04-17T05:49:47.673700: step 656, loss 0.388962, acc 0.82\n",
      "2018-04-17T05:49:47.690620: step 657, loss 0.383525, acc 0.821918\n",
      "2018-04-17T05:49:47.711853: step 658, loss 0.365376, acc 0.84\n",
      "2018-04-17T05:49:47.733536: step 659, loss 0.393102, acc 0.88\n",
      "2018-04-17T05:49:47.759283: step 660, loss 0.405539, acc 0.78\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:47.860699: step 660, loss 0.663866, acc 0.63553, rec 0.95325, pre 0.584207, f1 0.724437\n",
      "\n",
      "2018-04-17T05:49:47.883738: step 661, loss 0.274117, acc 0.91\n",
      "2018-04-17T05:49:47.904784: step 662, loss 0.326205, acc 0.88\n",
      "2018-04-17T05:49:47.926055: step 663, loss 0.312303, acc 0.87\n",
      "2018-04-17T05:49:47.947273: step 664, loss 0.364108, acc 0.87\n",
      "2018-04-17T05:49:47.971646: step 665, loss 0.341961, acc 0.85\n",
      "2018-04-17T05:49:47.988849: step 666, loss 0.39093, acc 0.835616\n",
      "2018-04-17T05:49:48.010424: step 667, loss 0.427173, acc 0.81\n",
      "2018-04-17T05:49:48.031581: step 668, loss 0.230143, acc 0.89\n",
      "2018-04-17T05:49:48.052364: step 669, loss 0.340133, acc 0.85\n",
      "2018-04-17T05:49:48.073428: step 670, loss 0.361735, acc 0.81\n",
      "2018-04-17T05:49:48.094442: step 671, loss 0.333471, acc 0.87\n",
      "2018-04-17T05:49:48.115290: step 672, loss 0.336564, acc 0.86\n",
      "2018-04-17T05:49:48.136082: step 673, loss 0.345668, acc 0.85\n",
      "2018-04-17T05:49:48.157063: step 674, loss 0.288865, acc 0.88\n",
      "2018-04-17T05:49:48.178063: step 675, loss 0.369682, acc 0.794521\n",
      "2018-04-17T05:49:48.204488: step 676, loss 0.346506, acc 0.86\n",
      "2018-04-17T05:49:48.225656: step 677, loss 0.254955, acc 0.92\n",
      "2018-04-17T05:49:48.246803: step 678, loss 0.35275, acc 0.83\n",
      "2018-04-17T05:49:48.267901: step 679, loss 0.421265, acc 0.81\n",
      "2018-04-17T05:49:48.288887: step 680, loss 0.325649, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:48.394585: step 680, loss 0.524796, acc 0.75702, rec 0.899658, pre 0.701333, f1 0.788212\n",
      "\n",
      "2018-04-17T05:49:48.417976: step 681, loss 0.372762, acc 0.81\n",
      "2018-04-17T05:49:48.438998: step 682, loss 0.360997, acc 0.86\n",
      "2018-04-17T05:49:48.460476: step 683, loss 0.360689, acc 0.84\n",
      "2018-04-17T05:49:48.477325: step 684, loss 0.361008, acc 0.863014\n",
      "2018-04-17T05:49:48.498676: step 685, loss 0.267853, acc 0.88\n",
      "2018-04-17T05:49:48.519923: step 686, loss 0.374129, acc 0.81\n",
      "2018-04-17T05:49:48.541166: step 687, loss 0.356541, acc 0.83\n",
      "2018-04-17T05:49:48.562433: step 688, loss 0.318389, acc 0.85\n",
      "2018-04-17T05:49:48.583275: step 689, loss 0.351551, acc 0.85\n",
      "2018-04-17T05:49:48.608336: step 690, loss 0.293455, acc 0.84\n",
      "2018-04-17T05:49:48.629403: step 691, loss 0.387102, acc 0.83\n",
      "2018-04-17T05:49:48.650871: step 692, loss 0.321245, acc 0.89\n",
      "2018-04-17T05:49:48.668178: step 693, loss 0.33121, acc 0.835616\n",
      "2018-04-17T05:49:48.689355: step 694, loss 0.246259, acc 0.9\n",
      "2018-04-17T05:49:48.710305: step 695, loss 0.421678, acc 0.81\n",
      "2018-04-17T05:49:48.731256: step 696, loss 0.521534, acc 0.76\n",
      "2018-04-17T05:49:48.752589: step 697, loss 0.334275, acc 0.81\n",
      "2018-04-17T05:49:48.773639: step 698, loss 0.303479, acc 0.86\n",
      "2018-04-17T05:49:48.795126: step 699, loss 0.398919, acc 0.81\n",
      "2018-04-17T05:49:48.819427: step 700, loss 0.295979, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:48.921503: step 700, loss 0.496641, acc 0.787966, rec 0.838084, pre 0.76324, f1 0.798913\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523944169/checkpoints/model-700\n",
      "\n",
      "2018-04-17T05:49:48.993148: step 701, loss 0.306438, acc 0.84\n",
      "2018-04-17T05:49:49.010676: step 702, loss 0.317639, acc 0.876712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17T05:49:49.037156: step 703, loss 0.32982, acc 0.88\n",
      "2018-04-17T05:49:49.058597: step 704, loss 0.339307, acc 0.87\n",
      "2018-04-17T05:49:49.079924: step 705, loss 0.33136, acc 0.89\n",
      "2018-04-17T05:49:49.101818: step 706, loss 0.313007, acc 0.86\n",
      "2018-04-17T05:49:49.122832: step 707, loss 0.307228, acc 0.89\n",
      "2018-04-17T05:49:49.143596: step 708, loss 0.427899, acc 0.81\n",
      "2018-04-17T05:49:49.164641: step 709, loss 0.361477, acc 0.84\n",
      "2018-04-17T05:49:49.185267: step 710, loss 0.24977, acc 0.87\n",
      "2018-04-17T05:49:49.201964: step 711, loss 0.363036, acc 0.863014\n",
      "2018-04-17T05:49:49.223137: step 712, loss 0.259117, acc 0.9\n",
      "2018-04-17T05:49:49.247989: step 713, loss 0.297089, acc 0.88\n",
      "2018-04-17T05:49:49.270150: step 714, loss 0.26805, acc 0.89\n",
      "2018-04-17T05:49:49.291003: step 715, loss 0.407373, acc 0.82\n",
      "2018-04-17T05:49:49.311803: step 716, loss 0.569465, acc 0.74\n",
      "2018-04-17T05:49:49.333420: step 717, loss 0.531258, acc 0.75\n",
      "2018-04-17T05:49:49.354667: step 718, loss 0.359966, acc 0.85\n",
      "2018-04-17T05:49:49.375404: step 719, loss 0.304951, acc 0.88\n",
      "2018-04-17T05:49:49.392554: step 720, loss 0.473329, acc 0.767123\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:49.497870: step 720, loss 0.818246, acc 0.574212, rec 0.962372, pre 0.543115, f1 0.694364\n",
      "\n",
      "2018-04-17T05:49:49.523852: step 721, loss 0.360283, acc 0.85\n",
      "2018-04-17T05:49:49.546920: step 722, loss 0.277541, acc 0.89\n",
      "2018-04-17T05:49:49.570035: step 723, loss 0.371047, acc 0.83\n",
      "2018-04-17T05:49:49.591775: step 724, loss 0.305401, acc 0.88\n",
      "2018-04-17T05:49:49.614372: step 725, loss 0.377865, acc 0.82\n",
      "2018-04-17T05:49:49.636491: step 726, loss 0.364196, acc 0.82\n",
      "2018-04-17T05:49:49.657084: step 727, loss 0.396461, acc 0.84\n",
      "2018-04-17T05:49:49.678183: step 728, loss 0.269306, acc 0.89\n",
      "2018-04-17T05:49:49.695517: step 729, loss 0.203666, acc 0.958904\n",
      "2018-04-17T05:49:49.725087: step 730, loss 0.390774, acc 0.82\n",
      "2018-04-17T05:49:49.748014: step 731, loss 0.28702, acc 0.9\n",
      "2018-04-17T05:49:49.770753: step 732, loss 0.466551, acc 0.76\n",
      "2018-04-17T05:49:49.793482: step 733, loss 0.256328, acc 0.9\n",
      "2018-04-17T05:49:49.817346: step 734, loss 0.430146, acc 0.79\n",
      "2018-04-17T05:49:49.838980: step 735, loss 0.378919, acc 0.83\n",
      "2018-04-17T05:49:49.860441: step 736, loss 0.416694, acc 0.81\n",
      "2018-04-17T05:49:49.882996: step 737, loss 0.215354, acc 0.92\n",
      "2018-04-17T05:49:49.901075: step 738, loss 0.347334, acc 0.849315\n",
      "2018-04-17T05:49:49.922662: step 739, loss 0.357944, acc 0.85\n",
      "2018-04-17T05:49:49.947881: step 740, loss 0.362429, acc 0.82\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:50.046861: step 740, loss 0.507718, acc 0.774785, rec 0.858609, pre 0.736791, f1 0.793049\n",
      "\n",
      "2018-04-17T05:49:50.070036: step 741, loss 0.254827, acc 0.91\n",
      "2018-04-17T05:49:50.092424: step 742, loss 0.352578, acc 0.85\n",
      "2018-04-17T05:49:50.113723: step 743, loss 0.392218, acc 0.84\n",
      "2018-04-17T05:49:50.135965: step 744, loss 0.339772, acc 0.85\n",
      "2018-04-17T05:49:50.161658: step 745, loss 0.255969, acc 0.91\n",
      "2018-04-17T05:49:50.184056: step 746, loss 0.388729, acc 0.86\n",
      "2018-04-17T05:49:50.201653: step 747, loss 0.270646, acc 0.863014\n",
      "2018-04-17T05:49:50.223550: step 748, loss 0.201962, acc 0.93\n",
      "2018-04-17T05:49:50.245872: step 749, loss 0.304868, acc 0.9\n",
      "2018-04-17T05:49:50.267476: step 750, loss 0.344957, acc 0.86\n",
      "2018-04-17T05:49:50.289144: step 751, loss 0.312771, acc 0.89\n",
      "2018-04-17T05:49:50.310645: step 752, loss 0.264532, acc 0.91\n",
      "2018-04-17T05:49:50.332591: step 753, loss 0.453641, acc 0.82\n",
      "2018-04-17T05:49:50.355744: step 754, loss 0.432831, acc 0.83\n",
      "2018-04-17T05:49:50.381154: step 755, loss 0.296744, acc 0.87\n",
      "2018-04-17T05:49:50.399013: step 756, loss 0.348945, acc 0.849315\n",
      "2018-04-17T05:49:50.421469: step 757, loss 0.472426, acc 0.76\n",
      "2018-04-17T05:49:50.443023: step 758, loss 0.426471, acc 0.79\n",
      "2018-04-17T05:49:50.464086: step 759, loss 0.360143, acc 0.83\n",
      "2018-04-17T05:49:50.485884: step 760, loss 0.435809, acc 0.8\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:50.587588: step 760, loss 0.91049, acc 0.547851, rec 0.974914, pre 0.527127, f1 0.684274\n",
      "\n",
      "2018-04-17T05:49:50.610836: step 761, loss 0.362318, acc 0.85\n",
      "2018-04-17T05:49:50.632041: step 762, loss 0.335897, acc 0.84\n",
      "2018-04-17T05:49:50.653612: step 763, loss 0.344176, acc 0.86\n",
      "2018-04-17T05:49:50.675619: step 764, loss 0.274853, acc 0.88\n",
      "2018-04-17T05:49:50.693654: step 765, loss 0.421337, acc 0.821918\n",
      "2018-04-17T05:49:50.716233: step 766, loss 0.348318, acc 0.84\n",
      "2018-04-17T05:49:50.738447: step 767, loss 0.333212, acc 0.82\n",
      "2018-04-17T05:49:50.758998: step 768, loss 0.440949, acc 0.82\n",
      "2018-04-17T05:49:50.780015: step 769, loss 0.32167, acc 0.84\n",
      "2018-04-17T05:49:50.803621: step 770, loss 0.353834, acc 0.88\n",
      "2018-04-17T05:49:50.824553: step 771, loss 0.32456, acc 0.85\n",
      "2018-04-17T05:49:50.846522: step 772, loss 0.328842, acc 0.86\n",
      "2018-04-17T05:49:50.867596: step 773, loss 0.300773, acc 0.87\n",
      "2018-04-17T05:49:50.884881: step 774, loss 0.285197, acc 0.917808\n",
      "2018-04-17T05:49:50.906150: step 775, loss 0.298167, acc 0.89\n",
      "2018-04-17T05:49:50.927522: step 776, loss 0.259134, acc 0.94\n",
      "2018-04-17T05:49:50.949341: step 777, loss 0.260022, acc 0.9\n",
      "2018-04-17T05:49:50.970903: step 778, loss 0.336141, acc 0.87\n",
      "2018-04-17T05:49:50.992054: step 779, loss 0.338763, acc 0.83\n",
      "2018-04-17T05:49:51.017502: step 780, loss 0.373401, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:51.116096: step 780, loss 0.546776, acc 0.733524, rec 0.923603, pre 0.67053, f1 0.776978\n",
      "\n",
      "2018-04-17T05:49:51.139292: step 781, loss 0.248819, acc 0.92\n",
      "2018-04-17T05:49:51.160368: step 782, loss 0.349297, acc 0.82\n",
      "2018-04-17T05:49:51.176956: step 783, loss 0.255081, acc 0.917808\n",
      "2018-04-17T05:49:51.198206: step 784, loss 0.359579, acc 0.84\n",
      "2018-04-17T05:49:51.222291: step 785, loss 0.391832, acc 0.82\n",
      "2018-04-17T05:49:51.242905: step 786, loss 0.307672, acc 0.88\n",
      "2018-04-17T05:49:51.263427: step 787, loss 0.19921, acc 0.94\n",
      "2018-04-17T05:49:51.284047: step 788, loss 0.309186, acc 0.87\n",
      "2018-04-17T05:49:51.304751: step 789, loss 0.305278, acc 0.9\n",
      "2018-04-17T05:49:51.325163: step 790, loss 0.347049, acc 0.87\n",
      "2018-04-17T05:49:51.346267: step 791, loss 0.207532, acc 0.94\n",
      "2018-04-17T05:49:51.363217: step 792, loss 0.365026, acc 0.808219\n",
      "2018-04-17T05:49:51.384426: step 793, loss 0.290642, acc 0.88\n",
      "2018-04-17T05:49:51.405426: step 794, loss 0.291714, acc 0.91\n",
      "2018-04-17T05:49:51.431228: step 795, loss 0.268076, acc 0.92\n",
      "2018-04-17T05:49:51.452990: step 796, loss 0.326899, acc 0.89\n",
      "2018-04-17T05:49:51.475498: step 797, loss 0.297943, acc 0.87\n",
      "2018-04-17T05:49:51.497155: step 798, loss 0.353725, acc 0.83\n",
      "2018-04-17T05:49:51.518210: step 799, loss 0.358719, acc 0.86\n",
      "2018-04-17T05:49:51.540125: step 800, loss 0.351279, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:51.649960: step 800, loss 0.483414, acc 0.797708, rec 0.841505, pre 0.77521, f1 0.806998\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523944169/checkpoints/model-800\n",
      "\n",
      "2018-04-17T05:49:51.717865: step 801, loss 0.408512, acc 0.821918\n",
      "2018-04-17T05:49:51.746321: step 802, loss 0.316029, acc 0.86\n",
      "2018-04-17T05:49:51.769195: step 803, loss 0.319163, acc 0.86\n",
      "2018-04-17T05:49:51.793132: step 804, loss 0.227858, acc 0.95\n",
      "2018-04-17T05:49:51.815781: step 805, loss 0.354302, acc 0.86\n",
      "2018-04-17T05:49:51.838133: step 806, loss 0.289337, acc 0.86\n",
      "2018-04-17T05:49:51.863624: step 807, loss 0.330896, acc 0.89\n",
      "2018-04-17T05:49:51.885383: step 808, loss 0.384836, acc 0.86\n",
      "2018-04-17T05:49:51.906892: step 809, loss 0.329099, acc 0.88\n",
      "2018-04-17T05:49:51.925051: step 810, loss 0.247682, acc 0.917808\n",
      "2018-04-17T05:49:51.948029: step 811, loss 0.277512, acc 0.89\n",
      "2018-04-17T05:49:51.972077: step 812, loss 0.22932, acc 0.92\n",
      "2018-04-17T05:49:52.000276: step 813, loss 0.295665, acc 0.92\n",
      "2018-04-17T05:49:52.031166: step 814, loss 0.266126, acc 0.89\n",
      "2018-04-17T05:49:52.055617: step 815, loss 0.342309, acc 0.85\n",
      "2018-04-17T05:49:52.088801: step 816, loss 0.339914, acc 0.87\n",
      "2018-04-17T05:49:52.112737: step 817, loss 0.381947, acc 0.81\n",
      "2018-04-17T05:49:52.135038: step 818, loss 0.344976, acc 0.83\n",
      "2018-04-17T05:49:52.152654: step 819, loss 0.398418, acc 0.767123\n",
      "2018-04-17T05:49:52.183644: step 820, loss 0.278008, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:52.285633: step 820, loss 0.481086, acc 0.803438, rec 0.799316, pre 0.807604, f1 0.803438\n",
      "\n",
      "2018-04-17T05:49:52.312726: step 821, loss 0.472938, acc 0.78\n",
      "2018-04-17T05:49:52.334131: step 822, loss 0.206023, acc 0.92\n",
      "2018-04-17T05:49:52.355556: step 823, loss 0.325221, acc 0.84\n",
      "2018-04-17T05:49:52.376082: step 824, loss 0.293604, acc 0.88\n",
      "2018-04-17T05:49:52.397640: step 825, loss 0.300752, acc 0.86\n",
      "2018-04-17T05:49:52.419175: step 826, loss 0.379146, acc 0.85\n",
      "2018-04-17T05:49:52.440154: step 827, loss 0.259677, acc 0.91\n",
      "2018-04-17T05:49:52.457574: step 828, loss 0.270535, acc 0.876712\n",
      "2018-04-17T05:49:52.479466: step 829, loss 0.268111, acc 0.88\n",
      "2018-04-17T05:49:52.501009: step 830, loss 0.291426, acc 0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17T05:49:52.525939: step 831, loss 0.34655, acc 0.86\n",
      "2018-04-17T05:49:52.547108: step 832, loss 0.246829, acc 0.88\n",
      "2018-04-17T05:49:52.567922: step 833, loss 0.337517, acc 0.83\n",
      "2018-04-17T05:49:52.589944: step 834, loss 0.374633, acc 0.82\n",
      "2018-04-17T05:49:52.610872: step 835, loss 0.38418, acc 0.84\n",
      "2018-04-17T05:49:52.632891: step 836, loss 0.241425, acc 0.93\n",
      "2018-04-17T05:49:52.649066: step 837, loss 0.328248, acc 0.890411\n",
      "2018-04-17T05:49:52.671441: step 838, loss 0.403875, acc 0.81\n",
      "2018-04-17T05:49:52.692090: step 839, loss 0.297008, acc 0.86\n",
      "2018-04-17T05:49:52.712755: step 840, loss 0.354959, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:52.815422: step 840, loss 0.66228, acc 0.654441, rec 0.944128, pre 0.599132, f1 0.733068\n",
      "\n",
      "2018-04-17T05:49:52.838347: step 841, loss 0.337722, acc 0.87\n",
      "2018-04-17T05:49:52.859414: step 842, loss 0.31836, acc 0.86\n",
      "2018-04-17T05:49:52.880536: step 843, loss 0.300849, acc 0.89\n",
      "2018-04-17T05:49:52.901582: step 844, loss 0.287173, acc 0.87\n",
      "2018-04-17T05:49:52.930196: step 845, loss 0.251271, acc 0.92\n",
      "2018-04-17T05:49:52.948021: step 846, loss 0.214137, acc 0.917808\n",
      "2018-04-17T05:49:52.971882: step 847, loss 0.311854, acc 0.84\n",
      "2018-04-17T05:49:52.995374: step 848, loss 0.349418, acc 0.87\n",
      "2018-04-17T05:49:53.021069: step 849, loss 0.323342, acc 0.88\n",
      "2018-04-17T05:49:53.043270: step 850, loss 0.414637, acc 0.84\n",
      "2018-04-17T05:49:53.064895: step 851, loss 0.344208, acc 0.85\n",
      "2018-04-17T05:49:53.087195: step 852, loss 0.259531, acc 0.92\n",
      "2018-04-17T05:49:53.109936: step 853, loss 0.331047, acc 0.86\n",
      "2018-04-17T05:49:53.132767: step 854, loss 0.2312, acc 0.91\n",
      "2018-04-17T05:49:53.151039: step 855, loss 0.253657, acc 0.890411\n",
      "2018-04-17T05:49:53.173930: step 856, loss 0.378792, acc 0.86\n",
      "2018-04-17T05:49:53.197149: step 857, loss 0.328819, acc 0.87\n",
      "2018-04-17T05:49:53.218118: step 858, loss 0.246667, acc 0.91\n",
      "2018-04-17T05:49:53.242275: step 859, loss 0.351634, acc 0.83\n",
      "2018-04-17T05:49:53.262827: step 860, loss 0.180947, acc 0.93\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:53.364363: step 860, loss 0.768664, acc 0.600573, rec 0.957811, pre 0.56, f1 0.706773\n",
      "\n",
      "2018-04-17T05:49:53.387842: step 861, loss 0.263997, acc 0.89\n",
      "2018-04-17T05:49:53.409304: step 862, loss 0.297503, acc 0.89\n",
      "2018-04-17T05:49:53.430556: step 863, loss 0.263943, acc 0.88\n",
      "2018-04-17T05:49:53.451082: step 864, loss 0.432274, acc 0.780822\n",
      "2018-04-17T05:49:53.472713: step 865, loss 0.264036, acc 0.87\n",
      "2018-04-17T05:49:53.494783: step 866, loss 0.415358, acc 0.81\n",
      "2018-04-17T05:49:53.516499: step 867, loss 0.269586, acc 0.9\n",
      "2018-04-17T05:49:53.537634: step 868, loss 0.203838, acc 0.94\n",
      "2018-04-17T05:49:53.558904: step 869, loss 0.187128, acc 0.94\n",
      "2018-04-17T05:49:53.582743: step 870, loss 0.404096, acc 0.79\n",
      "2018-04-17T05:49:53.605371: step 871, loss 0.299586, acc 0.9\n",
      "2018-04-17T05:49:53.628077: step 872, loss 0.239462, acc 0.91\n",
      "2018-04-17T05:49:53.651177: step 873, loss 0.345173, acc 0.808219\n",
      "2018-04-17T05:49:53.686001: step 874, loss 0.329422, acc 0.84\n",
      "2018-04-17T05:49:53.711501: step 875, loss 0.288294, acc 0.88\n",
      "2018-04-17T05:49:53.733992: step 876, loss 0.304269, acc 0.86\n",
      "2018-04-17T05:49:53.757620: step 877, loss 0.281967, acc 0.88\n",
      "2018-04-17T05:49:53.779624: step 878, loss 0.281783, acc 0.86\n",
      "2018-04-17T05:49:53.801816: step 879, loss 0.295141, acc 0.91\n",
      "2018-04-17T05:49:53.823864: step 880, loss 0.341101, acc 0.87\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:53.927836: step 880, loss 0.473889, acc 0.802292, rec 0.791334, pre 0.810748, f1 0.800923\n",
      "\n",
      "2018-04-17T05:49:53.951052: step 881, loss 0.364322, acc 0.87\n",
      "2018-04-17T05:49:53.969259: step 882, loss 0.169092, acc 0.931507\n",
      "2018-04-17T05:49:53.991368: step 883, loss 0.246846, acc 0.89\n",
      "2018-04-17T05:49:54.012874: step 884, loss 0.327096, acc 0.85\n",
      "2018-04-17T05:49:54.034851: step 885, loss 0.243819, acc 0.95\n",
      "2018-04-17T05:49:54.056571: step 886, loss 0.352822, acc 0.88\n",
      "2018-04-17T05:49:54.078443: step 887, loss 0.225453, acc 0.91\n",
      "2018-04-17T05:49:54.100709: step 888, loss 0.282995, acc 0.89\n",
      "2018-04-17T05:49:54.121830: step 889, loss 0.239327, acc 0.92\n",
      "2018-04-17T05:49:54.147060: step 890, loss 0.400063, acc 0.86\n",
      "2018-04-17T05:49:54.164492: step 891, loss 0.285081, acc 0.863014\n",
      "2018-04-17T05:49:54.186535: step 892, loss 0.300527, acc 0.87\n",
      "2018-04-17T05:49:54.207809: step 893, loss 0.224039, acc 0.91\n",
      "2018-04-17T05:49:54.229033: step 894, loss 0.29128, acc 0.9\n",
      "2018-04-17T05:49:54.250650: step 895, loss 0.338019, acc 0.88\n",
      "2018-04-17T05:49:54.271855: step 896, loss 0.342787, acc 0.87\n",
      "2018-04-17T05:49:54.293620: step 897, loss 0.283248, acc 0.84\n",
      "2018-04-17T05:49:54.315487: step 898, loss 0.327147, acc 0.9\n",
      "2018-04-17T05:49:54.336463: step 899, loss 0.347348, acc 0.85\n",
      "2018-04-17T05:49:54.356274: step 900, loss 0.266605, acc 0.917808\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:54.458232: step 900, loss 0.518094, acc 0.76447, rec 0.901938, pre 0.708781, f1 0.793778\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523944169/checkpoints/model-900\n",
      "\n",
      "2018-04-17T05:49:54.541786: step 901, loss 0.284473, acc 0.88\n",
      "2018-04-17T05:49:54.569654: step 902, loss 0.3665, acc 0.84\n",
      "2018-04-17T05:49:54.590987: step 903, loss 0.242077, acc 0.94\n",
      "2018-04-17T05:49:54.615075: step 904, loss 0.295286, acc 0.89\n",
      "2018-04-17T05:49:54.636698: step 905, loss 0.274995, acc 0.9\n",
      "2018-04-17T05:49:54.658938: step 906, loss 0.291901, acc 0.89\n",
      "2018-04-17T05:49:54.681352: step 907, loss 0.280449, acc 0.88\n",
      "2018-04-17T05:49:54.703742: step 908, loss 0.281634, acc 0.89\n",
      "2018-04-17T05:49:54.723170: step 909, loss 0.281736, acc 0.876712\n",
      "2018-04-17T05:49:54.744794: step 910, loss 0.252694, acc 0.91\n",
      "2018-04-17T05:49:54.766368: step 911, loss 0.287401, acc 0.9\n",
      "2018-04-17T05:49:54.792692: step 912, loss 0.371695, acc 0.82\n",
      "2018-04-17T05:49:54.815746: step 913, loss 0.240753, acc 0.91\n",
      "2018-04-17T05:49:54.837638: step 914, loss 0.223653, acc 0.9\n",
      "2018-04-17T05:49:54.859289: step 915, loss 0.269584, acc 0.9\n",
      "2018-04-17T05:49:54.881006: step 916, loss 0.236328, acc 0.91\n",
      "2018-04-17T05:49:54.902618: step 917, loss 0.367082, acc 0.82\n",
      "2018-04-17T05:49:54.920964: step 918, loss 0.373235, acc 0.849315\n",
      "2018-04-17T05:49:54.943553: step 919, loss 0.382747, acc 0.81\n",
      "2018-04-17T05:49:54.964532: step 920, loss 0.28684, acc 0.87\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:55.065778: step 920, loss 0.785866, acc 0.602865, rec 0.95667, pre 0.56158, f1 0.707718\n",
      "\n",
      "2018-04-17T05:49:55.088971: step 921, loss 0.253762, acc 0.89\n",
      "2018-04-17T05:49:55.110190: step 922, loss 0.290416, acc 0.89\n",
      "2018-04-17T05:49:55.132286: step 923, loss 0.189055, acc 0.95\n",
      "2018-04-17T05:49:55.155705: step 924, loss 0.31859, acc 0.83\n",
      "2018-04-17T05:49:55.178305: step 925, loss 0.24508, acc 0.88\n",
      "2018-04-17T05:49:55.205160: step 926, loss 0.35913, acc 0.88\n",
      "2018-04-17T05:49:55.223202: step 927, loss 0.235601, acc 0.876712\n",
      "2018-04-17T05:49:55.245596: step 928, loss 0.227612, acc 0.92\n",
      "2018-04-17T05:49:55.270970: step 929, loss 0.332044, acc 0.87\n",
      "2018-04-17T05:49:55.293673: step 930, loss 0.233005, acc 0.9\n",
      "2018-04-17T05:49:55.315154: step 931, loss 0.316594, acc 0.85\n",
      "2018-04-17T05:49:55.336574: step 932, loss 0.27377, acc 0.89\n",
      "2018-04-17T05:49:55.358234: step 933, loss 0.362728, acc 0.86\n",
      "2018-04-17T05:49:55.380253: step 934, loss 0.458879, acc 0.78\n",
      "2018-04-17T05:49:55.403494: step 935, loss 0.335871, acc 0.85\n",
      "2018-04-17T05:49:55.421282: step 936, loss 0.22586, acc 0.931507\n",
      "2018-04-17T05:49:55.443940: step 937, loss 0.253394, acc 0.87\n",
      "2018-04-17T05:49:55.465533: step 938, loss 0.270758, acc 0.86\n",
      "2018-04-17T05:49:55.491713: step 939, loss 0.331838, acc 0.87\n",
      "2018-04-17T05:49:55.513305: step 940, loss 0.366718, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:55.613374: step 940, loss 0.565383, acc 0.726075, rec 0.912201, pre 0.666112, f1 0.769971\n",
      "\n",
      "2018-04-17T05:49:55.635890: step 941, loss 0.262244, acc 0.89\n",
      "2018-04-17T05:49:55.657031: step 942, loss 0.295849, acc 0.9\n",
      "2018-04-17T05:49:55.678544: step 943, loss 0.3552, acc 0.86\n",
      "2018-04-17T05:49:55.702760: step 944, loss 0.307951, acc 0.86\n",
      "2018-04-17T05:49:55.720161: step 945, loss 0.298125, acc 0.849315\n",
      "2018-04-17T05:49:55.742759: step 946, loss 0.296546, acc 0.84\n",
      "2018-04-17T05:49:55.763803: step 947, loss 0.339148, acc 0.87\n",
      "2018-04-17T05:49:55.784751: step 948, loss 0.241191, acc 0.9\n",
      "2018-04-17T05:49:55.806472: step 949, loss 0.222992, acc 0.91\n",
      "2018-04-17T05:49:55.828547: step 950, loss 0.303942, acc 0.85\n",
      "2018-04-17T05:49:55.850003: step 951, loss 0.388529, acc 0.81\n",
      "2018-04-17T05:49:55.873035: step 952, loss 0.349778, acc 0.86\n",
      "2018-04-17T05:49:55.896277: step 953, loss 0.351066, acc 0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17T05:49:55.917482: step 954, loss 0.356461, acc 0.835616\n",
      "2018-04-17T05:49:55.939415: step 955, loss 0.218754, acc 0.9\n",
      "2018-04-17T05:49:55.961502: step 956, loss 0.373258, acc 0.85\n",
      "2018-04-17T05:49:55.983591: step 957, loss 0.311334, acc 0.85\n",
      "2018-04-17T05:49:56.006409: step 958, loss 0.337921, acc 0.86\n",
      "2018-04-17T05:49:56.028886: step 959, loss 0.292896, acc 0.89\n",
      "2018-04-17T05:49:56.050776: step 960, loss 0.252968, acc 0.87\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:56.162399: step 960, loss 0.797502, acc 0.600573, rec 0.95667, pre 0.56008, f1 0.706526\n",
      "\n",
      "2018-04-17T05:49:56.187032: step 961, loss 0.241782, acc 0.91\n",
      "2018-04-17T05:49:56.210267: step 962, loss 0.246965, acc 0.9\n",
      "2018-04-17T05:49:56.229156: step 963, loss 0.25846, acc 0.876712\n",
      "2018-04-17T05:49:56.251116: step 964, loss 0.249116, acc 0.89\n",
      "2018-04-17T05:49:56.273572: step 965, loss 0.304113, acc 0.87\n",
      "2018-04-17T05:49:56.295587: step 966, loss 0.26177, acc 0.92\n",
      "2018-04-17T05:49:56.319050: step 967, loss 0.318315, acc 0.86\n",
      "2018-04-17T05:49:56.341784: step 968, loss 0.300451, acc 0.86\n",
      "2018-04-17T05:49:56.369416: step 969, loss 0.306913, acc 0.84\n",
      "2018-04-17T05:49:56.391648: step 970, loss 0.358166, acc 0.86\n",
      "2018-04-17T05:49:56.413644: step 971, loss 0.412255, acc 0.8\n",
      "2018-04-17T05:49:56.431431: step 972, loss 0.37823, acc 0.835616\n",
      "2018-04-17T05:49:56.452600: step 973, loss 0.352494, acc 0.8\n",
      "2018-04-17T05:49:56.474611: step 974, loss 0.307179, acc 0.9\n",
      "2018-04-17T05:49:56.496609: step 975, loss 0.30292, acc 0.87\n",
      "2018-04-17T05:49:56.517785: step 976, loss 0.23426, acc 0.88\n",
      "2018-04-17T05:49:56.539771: step 977, loss 0.254364, acc 0.87\n",
      "2018-04-17T05:49:56.561548: step 978, loss 0.265426, acc 0.89\n",
      "2018-04-17T05:49:56.586149: step 979, loss 0.337654, acc 0.86\n",
      "2018-04-17T05:49:56.607588: step 980, loss 0.269775, acc 0.89\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:56.706137: step 980, loss 0.65314, acc 0.665903, rec 0.952109, pre 0.606831, f1 0.741234\n",
      "\n",
      "2018-04-17T05:49:56.724596: step 981, loss 0.263304, acc 0.890411\n",
      "2018-04-17T05:49:56.747243: step 982, loss 0.249757, acc 0.86\n",
      "2018-04-17T05:49:56.768999: step 983, loss 0.327681, acc 0.89\n",
      "2018-04-17T05:49:56.793386: step 984, loss 0.46064, acc 0.75\n",
      "2018-04-17T05:49:56.815594: step 985, loss 0.332372, acc 0.85\n",
      "2018-04-17T05:49:56.838553: step 986, loss 0.177785, acc 0.95\n",
      "2018-04-17T05:49:56.859686: step 987, loss 0.302465, acc 0.91\n",
      "2018-04-17T05:49:56.880733: step 988, loss 0.262548, acc 0.88\n",
      "2018-04-17T05:49:56.902348: step 989, loss 0.310392, acc 0.86\n",
      "2018-04-17T05:49:56.918917: step 990, loss 0.230932, acc 0.890411\n",
      "2018-04-17T05:49:56.940389: step 991, loss 0.21356, acc 0.89\n",
      "2018-04-17T05:49:56.961524: step 992, loss 0.265105, acc 0.88\n",
      "2018-04-17T05:49:56.982647: step 993, loss 0.290744, acc 0.9\n",
      "2018-04-17T05:49:57.007427: step 994, loss 0.345776, acc 0.85\n",
      "2018-04-17T05:49:57.028727: step 995, loss 0.290757, acc 0.87\n",
      "2018-04-17T05:49:57.050691: step 996, loss 0.277678, acc 0.9\n",
      "2018-04-17T05:49:57.071924: step 997, loss 0.271159, acc 0.91\n",
      "2018-04-17T05:49:57.094037: step 998, loss 0.220629, acc 0.92\n",
      "2018-04-17T05:49:57.111576: step 999, loss 0.33342, acc 0.890411\n",
      "2018-04-17T05:49:57.132944: step 1000, loss 0.294362, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:57.235859: step 1000, loss 0.549421, acc 0.74212, rec 0.920182, pre 0.679865, f1 0.781977\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523944169/checkpoints/model-1000\n",
      "\n",
      "2018-04-17T05:49:57.304703: step 1001, loss 0.303647, acc 0.91\n",
      "2018-04-17T05:49:57.326193: step 1002, loss 0.254095, acc 0.9\n",
      "2018-04-17T05:49:57.348428: step 1003, loss 0.2632, acc 0.92\n",
      "2018-04-17T05:49:57.371331: step 1004, loss 0.351848, acc 0.82\n",
      "2018-04-17T05:49:57.394462: step 1005, loss 0.268589, acc 0.88\n",
      "2018-04-17T05:49:57.417694: step 1006, loss 0.29147, acc 0.84\n",
      "2018-04-17T05:49:57.442195: step 1007, loss 0.252341, acc 0.89\n",
      "2018-04-17T05:49:57.459835: step 1008, loss 0.357134, acc 0.863014\n",
      "2018-04-17T05:49:57.482635: step 1009, loss 0.342447, acc 0.84\n",
      "2018-04-17T05:49:57.504439: step 1010, loss 0.375618, acc 0.87\n",
      "2018-04-17T05:49:57.526521: step 1011, loss 0.343873, acc 0.84\n",
      "2018-04-17T05:49:57.549468: step 1012, loss 0.209037, acc 0.9\n",
      "2018-04-17T05:49:57.570154: step 1013, loss 0.238041, acc 0.91\n",
      "2018-04-17T05:49:57.590643: step 1014, loss 0.311689, acc 0.87\n",
      "2018-04-17T05:49:57.611377: step 1015, loss 0.291921, acc 0.88\n",
      "2018-04-17T05:49:57.632030: step 1016, loss 0.26618, acc 0.9\n",
      "2018-04-17T05:49:57.651145: step 1017, loss 0.267424, acc 0.90411\n",
      "2018-04-17T05:49:57.671886: step 1018, loss 0.281644, acc 0.86\n",
      "2018-04-17T05:49:57.692618: step 1019, loss 0.3276, acc 0.86\n",
      "2018-04-17T05:49:57.713865: step 1020, loss 0.249005, acc 0.89\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:57.810038: step 1020, loss 0.474294, acc 0.797135, rec 0.805017, pre 0.794151, f1 0.799547\n",
      "\n",
      "2018-04-17T05:49:57.832317: step 1021, loss 0.268787, acc 0.87\n",
      "2018-04-17T05:49:57.856490: step 1022, loss 0.408283, acc 0.83\n",
      "2018-04-17T05:49:57.877575: step 1023, loss 0.263283, acc 0.9\n",
      "2018-04-17T05:49:57.898784: step 1024, loss 0.170131, acc 0.95\n",
      "2018-04-17T05:49:57.921255: step 1025, loss 0.27323, acc 0.93\n",
      "2018-04-17T05:49:57.938529: step 1026, loss 0.238862, acc 0.890411\n",
      "2018-04-17T05:49:57.961337: step 1027, loss 0.200911, acc 0.93\n",
      "2018-04-17T05:49:57.983117: step 1028, loss 0.224925, acc 0.92\n",
      "2018-04-17T05:49:58.004543: step 1029, loss 0.16321, acc 0.95\n",
      "2018-04-17T05:49:58.026069: step 1030, loss 0.265204, acc 0.91\n",
      "2018-04-17T05:49:58.047573: step 1031, loss 0.412861, acc 0.8\n",
      "2018-04-17T05:49:58.073101: step 1032, loss 0.444586, acc 0.81\n",
      "2018-04-17T05:49:58.094735: step 1033, loss 0.307993, acc 0.89\n",
      "2018-04-17T05:49:58.115386: step 1034, loss 0.27311, acc 0.89\n",
      "2018-04-17T05:49:58.132706: step 1035, loss 0.232746, acc 0.876712\n",
      "2018-04-17T05:49:58.158342: step 1036, loss 0.253416, acc 0.9\n",
      "2018-04-17T05:49:58.179568: step 1037, loss 0.248489, acc 0.92\n",
      "2018-04-17T05:49:58.200820: step 1038, loss 0.252185, acc 0.93\n",
      "2018-04-17T05:49:58.222217: step 1039, loss 0.292134, acc 0.85\n",
      "2018-04-17T05:49:58.246308: step 1040, loss 0.49071, acc 0.8\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:58.366598: step 1040, loss 1.14497, acc 0.532951, rec 0.973774, pre 0.518834, f1 0.676972\n",
      "\n",
      "2018-04-17T05:49:58.394111: step 1041, loss 0.277928, acc 0.88\n",
      "2018-04-17T05:49:58.415705: step 1042, loss 0.253981, acc 0.86\n",
      "2018-04-17T05:49:58.436609: step 1043, loss 0.251725, acc 0.88\n",
      "2018-04-17T05:49:58.453059: step 1044, loss 0.272412, acc 0.890411\n",
      "2018-04-17T05:49:58.473863: step 1045, loss 0.247611, acc 0.9\n",
      "2018-04-17T05:49:58.494431: step 1046, loss 0.312973, acc 0.86\n",
      "2018-04-17T05:49:58.515191: step 1047, loss 0.312888, acc 0.85\n",
      "2018-04-17T05:49:58.535903: step 1048, loss 0.329924, acc 0.85\n",
      "2018-04-17T05:49:58.556587: step 1049, loss 0.305177, acc 0.85\n",
      "2018-04-17T05:49:58.580590: step 1050, loss 0.379384, acc 0.88\n",
      "2018-04-17T05:49:58.601673: step 1051, loss 0.17539, acc 0.93\n",
      "2018-04-17T05:49:58.622429: step 1052, loss 0.287724, acc 0.87\n",
      "2018-04-17T05:49:58.638870: step 1053, loss 0.249678, acc 0.890411\n",
      "2018-04-17T05:49:58.659616: step 1054, loss 0.187503, acc 0.94\n",
      "2018-04-17T05:49:58.682293: step 1055, loss 0.267157, acc 0.88\n",
      "2018-04-17T05:49:58.712847: step 1056, loss 0.259961, acc 0.94\n",
      "2018-04-17T05:49:58.734231: step 1057, loss 0.223561, acc 0.92\n",
      "2018-04-17T05:49:58.755487: step 1058, loss 0.333145, acc 0.88\n",
      "2018-04-17T05:49:58.776708: step 1059, loss 0.265138, acc 0.9\n",
      "2018-04-17T05:49:58.801599: step 1060, loss 0.281738, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:58.899000: step 1060, loss 0.724009, acc 0.639542, rec 0.958951, pre 0.586471, f1 0.727823\n",
      "\n",
      "2018-04-17T05:49:58.921643: step 1061, loss 0.273249, acc 0.85\n",
      "2018-04-17T05:49:58.939097: step 1062, loss 0.321545, acc 0.835616\n",
      "2018-04-17T05:49:58.963671: step 1063, loss 0.222274, acc 0.89\n",
      "2018-04-17T05:49:58.986761: step 1064, loss 0.227079, acc 0.92\n",
      "2018-04-17T05:49:59.011924: step 1065, loss 0.268898, acc 0.88\n",
      "2018-04-17T05:49:59.035161: step 1066, loss 0.252749, acc 0.9\n",
      "2018-04-17T05:49:59.056049: step 1067, loss 0.298291, acc 0.88\n",
      "2018-04-17T05:49:59.077259: step 1068, loss 0.186068, acc 0.93\n",
      "2018-04-17T05:49:59.098559: step 1069, loss 0.360401, acc 0.88\n",
      "2018-04-17T05:49:59.119588: step 1070, loss 0.267335, acc 0.89\n",
      "2018-04-17T05:49:59.136562: step 1071, loss 0.179435, acc 0.917808\n",
      "2018-04-17T05:49:59.157949: step 1072, loss 0.185653, acc 0.94\n",
      "2018-04-17T05:49:59.178841: step 1073, loss 0.256873, acc 0.89\n",
      "2018-04-17T05:49:59.199999: step 1074, loss 0.202031, acc 0.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17T05:49:59.223645: step 1075, loss 0.317307, acc 0.88\n",
      "2018-04-17T05:49:59.244840: step 1076, loss 0.239603, acc 0.88\n",
      "2018-04-17T05:49:59.266131: step 1077, loss 0.28244, acc 0.88\n",
      "2018-04-17T05:49:59.287100: step 1078, loss 0.304854, acc 0.86\n",
      "2018-04-17T05:49:59.309989: step 1079, loss 0.336363, acc 0.83\n",
      "2018-04-17T05:49:59.327638: step 1080, loss 0.39765, acc 0.821918\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:59.427974: step 1080, loss 0.468693, acc 0.800573, rec 0.776511, pre 0.817527, f1 0.796491\n",
      "\n",
      "2018-04-17T05:49:59.451855: step 1081, loss 0.270261, acc 0.86\n",
      "2018-04-17T05:49:59.473364: step 1082, loss 0.374429, acc 0.82\n",
      "2018-04-17T05:49:59.496760: step 1083, loss 0.169839, acc 0.96\n",
      "2018-04-17T05:49:59.517602: step 1084, loss 0.198398, acc 0.92\n",
      "2018-04-17T05:49:59.539239: step 1085, loss 0.164742, acc 0.96\n",
      "2018-04-17T05:49:59.560322: step 1086, loss 0.247946, acc 0.9\n",
      "2018-04-17T05:49:59.581041: step 1087, loss 0.370944, acc 0.84\n",
      "2018-04-17T05:49:59.602147: step 1088, loss 0.401929, acc 0.87\n",
      "2018-04-17T05:49:59.620161: step 1089, loss 0.175786, acc 0.945205\n",
      "2018-04-17T05:49:59.645528: step 1090, loss 0.28478, acc 0.86\n",
      "2018-04-17T05:49:59.667997: step 1091, loss 0.212047, acc 0.9\n",
      "2018-04-17T05:49:59.689995: step 1092, loss 0.234733, acc 0.92\n",
      "2018-04-17T05:49:59.712760: step 1093, loss 0.231428, acc 0.92\n",
      "2018-04-17T05:49:59.736063: step 1094, loss 0.230916, acc 0.9\n",
      "2018-04-17T05:49:59.756996: step 1095, loss 0.287113, acc 0.91\n",
      "2018-04-17T05:49:59.778730: step 1096, loss 0.310561, acc 0.86\n",
      "2018-04-17T05:49:59.800336: step 1097, loss 0.195234, acc 0.93\n",
      "2018-04-17T05:49:59.816909: step 1098, loss 0.407317, acc 0.821918\n",
      "2018-04-17T05:49:59.837669: step 1099, loss 0.220158, acc 0.9\n",
      "2018-04-17T05:49:59.861415: step 1100, loss 0.235292, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:49:59.959032: step 1100, loss 0.489035, acc 0.787393, rec 0.844926, pre 0.759221, f1 0.799784\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523944169/checkpoints/model-1100\n",
      "\n",
      "2018-04-17T05:50:00.025934: step 1101, loss 0.318367, acc 0.88\n",
      "2018-04-17T05:50:00.047183: step 1102, loss 0.407016, acc 0.86\n",
      "2018-04-17T05:50:00.071415: step 1103, loss 0.331818, acc 0.84\n",
      "2018-04-17T05:50:00.093582: step 1104, loss 0.274142, acc 0.88\n",
      "2018-04-17T05:50:00.114726: step 1105, loss 0.279537, acc 0.9\n",
      "2018-04-17T05:50:00.135982: step 1106, loss 0.174002, acc 0.94\n",
      "2018-04-17T05:50:00.153104: step 1107, loss 0.227095, acc 0.890411\n",
      "2018-04-17T05:50:00.175107: step 1108, loss 0.226871, acc 0.93\n",
      "2018-04-17T05:50:00.196962: step 1109, loss 0.272763, acc 0.88\n",
      "2018-04-17T05:50:00.218588: step 1110, loss 0.221892, acc 0.87\n",
      "2018-04-17T05:50:00.240155: step 1111, loss 0.253899, acc 0.9\n",
      "2018-04-17T05:50:00.262061: step 1112, loss 0.222196, acc 0.92\n",
      "2018-04-17T05:50:00.286335: step 1113, loss 0.336295, acc 0.86\n",
      "2018-04-17T05:50:00.307298: step 1114, loss 0.284507, acc 0.88\n",
      "2018-04-17T05:50:00.328473: step 1115, loss 0.245743, acc 0.9\n",
      "2018-04-17T05:50:00.345530: step 1116, loss 0.246375, acc 0.876712\n",
      "2018-04-17T05:50:00.366616: step 1117, loss 0.239742, acc 0.89\n",
      "2018-04-17T05:50:00.388522: step 1118, loss 0.259137, acc 0.88\n",
      "2018-04-17T05:50:00.409536: step 1119, loss 0.242435, acc 0.86\n",
      "2018-04-17T05:50:00.430467: step 1120, loss 0.23814, acc 0.89\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:00.531927: step 1120, loss 0.504257, acc 0.783954, rec 0.893957, pre 0.734082, f1 0.80617\n",
      "\n",
      "2018-04-17T05:50:00.555587: step 1121, loss 0.33633, acc 0.84\n",
      "2018-04-17T05:50:00.578285: step 1122, loss 0.301985, acc 0.89\n",
      "2018-04-17T05:50:00.599933: step 1123, loss 0.208788, acc 0.9\n",
      "2018-04-17T05:50:00.620787: step 1124, loss 0.348764, acc 0.87\n",
      "2018-04-17T05:50:00.637794: step 1125, loss 0.255441, acc 0.863014\n",
      "2018-04-17T05:50:00.659609: step 1126, loss 0.317781, acc 0.85\n",
      "2018-04-17T05:50:00.680637: step 1127, loss 0.218571, acc 0.9\n",
      "2018-04-17T05:50:00.701853: step 1128, loss 0.229469, acc 0.89\n",
      "2018-04-17T05:50:00.722659: step 1129, loss 0.223833, acc 0.88\n",
      "2018-04-17T05:50:00.750833: step 1130, loss 0.256086, acc 0.91\n",
      "2018-04-17T05:50:00.772564: step 1131, loss 0.293901, acc 0.87\n",
      "2018-04-17T05:50:00.795932: step 1132, loss 0.217166, acc 0.93\n",
      "2018-04-17T05:50:00.816536: step 1133, loss 0.204172, acc 0.93\n",
      "2018-04-17T05:50:00.833300: step 1134, loss 0.300305, acc 0.90411\n",
      "2018-04-17T05:50:00.853757: step 1135, loss 0.294159, acc 0.92\n",
      "2018-04-17T05:50:00.874395: step 1136, loss 0.257851, acc 0.9\n",
      "2018-04-17T05:50:00.895458: step 1137, loss 0.345568, acc 0.83\n",
      "2018-04-17T05:50:00.915896: step 1138, loss 0.235491, acc 0.92\n",
      "2018-04-17T05:50:00.936131: step 1139, loss 0.20295, acc 0.91\n",
      "2018-04-17T05:50:00.960057: step 1140, loss 0.182551, acc 0.91\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:01.056722: step 1140, loss 0.708107, acc 0.651576, rec 0.950969, pre 0.59614, f1 0.732865\n",
      "\n",
      "2018-04-17T05:50:01.079049: step 1141, loss 0.245227, acc 0.92\n",
      "2018-04-17T05:50:01.100171: step 1142, loss 0.25869, acc 0.91\n",
      "2018-04-17T05:50:01.116607: step 1143, loss 0.264064, acc 0.917808\n",
      "2018-04-17T05:50:01.137312: step 1144, loss 0.342756, acc 0.81\n",
      "2018-04-17T05:50:01.158018: step 1145, loss 0.243747, acc 0.89\n",
      "2018-04-17T05:50:01.181493: step 1146, loss 0.245364, acc 0.86\n",
      "2018-04-17T05:50:01.202670: step 1147, loss 0.218028, acc 0.91\n",
      "2018-04-17T05:50:01.223526: step 1148, loss 0.223944, acc 0.93\n",
      "2018-04-17T05:50:01.244330: step 1149, loss 0.211172, acc 0.92\n",
      "2018-04-17T05:50:01.264596: step 1150, loss 0.265371, acc 0.9\n",
      "2018-04-17T05:50:01.284943: step 1151, loss 0.321725, acc 0.85\n",
      "2018-04-17T05:50:01.301276: step 1152, loss 0.296148, acc 0.863014\n",
      "2018-04-17T05:50:01.322140: step 1153, loss 0.240648, acc 0.91\n",
      "2018-04-17T05:50:01.343174: step 1154, loss 0.136642, acc 0.96\n",
      "2018-04-17T05:50:01.363850: step 1155, loss 0.254897, acc 0.9\n",
      "2018-04-17T05:50:01.387813: step 1156, loss 0.204325, acc 0.91\n",
      "2018-04-17T05:50:01.409266: step 1157, loss 0.280498, acc 0.88\n",
      "2018-04-17T05:50:01.431493: step 1158, loss 0.29328, acc 0.91\n",
      "2018-04-17T05:50:01.455172: step 1159, loss 0.260228, acc 0.88\n",
      "2018-04-17T05:50:01.478774: step 1160, loss 0.28659, acc 0.91\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:01.596300: step 1160, loss 0.761002, acc 0.632665, rec 0.95439, pre 0.582058, f1 0.72311\n",
      "\n",
      "2018-04-17T05:50:01.619066: step 1161, loss 0.306753, acc 0.863014\n",
      "2018-04-17T05:50:01.647865: step 1162, loss 0.36038, acc 0.85\n",
      "2018-04-17T05:50:01.673149: step 1163, loss 0.336787, acc 0.87\n",
      "2018-04-17T05:50:01.697964: step 1164, loss 0.255105, acc 0.89\n",
      "2018-04-17T05:50:01.723032: step 1165, loss 0.29217, acc 0.88\n",
      "2018-04-17T05:50:01.747744: step 1166, loss 0.152684, acc 0.95\n",
      "2018-04-17T05:50:01.772680: step 1167, loss 0.301019, acc 0.91\n",
      "2018-04-17T05:50:01.801219: step 1168, loss 0.416039, acc 0.79\n",
      "2018-04-17T05:50:01.826613: step 1169, loss 0.177562, acc 0.94\n",
      "2018-04-17T05:50:01.845052: step 1170, loss 0.202212, acc 0.917808\n",
      "2018-04-17T05:50:01.869260: step 1171, loss 0.218988, acc 0.9\n",
      "2018-04-17T05:50:01.893037: step 1172, loss 0.218088, acc 0.94\n",
      "2018-04-17T05:50:01.917761: step 1173, loss 0.168824, acc 0.93\n",
      "2018-04-17T05:50:01.940763: step 1174, loss 0.184784, acc 0.92\n",
      "2018-04-17T05:50:01.968327: step 1175, loss 0.334638, acc 0.86\n",
      "2018-04-17T05:50:01.990931: step 1176, loss 0.341844, acc 0.8\n",
      "2018-04-17T05:50:02.021342: step 1177, loss 0.193954, acc 0.94\n",
      "2018-04-17T05:50:02.056355: step 1178, loss 0.282398, acc 0.87\n",
      "2018-04-17T05:50:02.080264: step 1179, loss 0.222221, acc 0.90411\n",
      "2018-04-17T05:50:02.105199: step 1180, loss 0.197462, acc 0.93\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:02.213642: step 1180, loss 0.648556, acc 0.687106, rec 0.936146, pre 0.62624, f1 0.750457\n",
      "\n",
      "2018-04-17T05:50:02.245020: step 1181, loss 0.322347, acc 0.9\n",
      "2018-04-17T05:50:02.270562: step 1182, loss 0.172019, acc 0.93\n",
      "2018-04-17T05:50:02.295850: step 1183, loss 0.268913, acc 0.89\n",
      "2018-04-17T05:50:02.327638: step 1184, loss 0.301148, acc 0.89\n",
      "2018-04-17T05:50:02.360478: step 1185, loss 0.241969, acc 0.92\n",
      "2018-04-17T05:50:02.393348: step 1186, loss 0.216206, acc 0.9\n",
      "2018-04-17T05:50:02.425204: step 1187, loss 0.218876, acc 0.91\n",
      "2018-04-17T05:50:02.454068: step 1188, loss 0.25839, acc 0.876712\n",
      "2018-04-17T05:50:02.479011: step 1189, loss 0.233912, acc 0.9\n",
      "2018-04-17T05:50:02.503895: step 1190, loss 0.228807, acc 0.89\n",
      "2018-04-17T05:50:02.525277: step 1191, loss 0.223656, acc 0.91\n",
      "2018-04-17T05:50:02.548147: step 1192, loss 0.170978, acc 0.93\n",
      "2018-04-17T05:50:02.570692: step 1193, loss 0.276655, acc 0.89\n",
      "2018-04-17T05:50:02.593180: step 1194, loss 0.229719, acc 0.91\n",
      "2018-04-17T05:50:02.614977: step 1195, loss 0.244387, acc 0.91\n",
      "2018-04-17T05:50:02.637737: step 1196, loss 0.267021, acc 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17T05:50:02.658537: step 1197, loss 0.191844, acc 0.972603\n",
      "2018-04-17T05:50:02.681688: step 1198, loss 0.161293, acc 0.96\n",
      "2018-04-17T05:50:02.710555: step 1199, loss 0.271234, acc 0.9\n",
      "2018-04-17T05:50:02.733812: step 1200, loss 0.242942, acc 0.91\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:02.841175: step 1200, loss 0.535257, acc 0.762178, rec 0.905359, pre 0.705151, f1 0.792811\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523944169/checkpoints/model-1200\n",
      "\n",
      "2018-04-17T05:50:02.927858: step 1201, loss 0.226609, acc 0.92\n",
      "2018-04-17T05:50:02.952711: step 1202, loss 0.268929, acc 0.9\n",
      "2018-04-17T05:50:02.981713: step 1203, loss 0.294223, acc 0.86\n",
      "2018-04-17T05:50:03.005060: step 1204, loss 0.338228, acc 0.88\n",
      "2018-04-17T05:50:03.030095: step 1205, loss 0.219673, acc 0.92\n",
      "2018-04-17T05:50:03.048757: step 1206, loss 0.305695, acc 0.876712\n",
      "2018-04-17T05:50:03.072921: step 1207, loss 0.269563, acc 0.91\n",
      "2018-04-17T05:50:03.097073: step 1208, loss 0.15846, acc 0.93\n",
      "2018-04-17T05:50:03.124993: step 1209, loss 0.259557, acc 0.89\n",
      "2018-04-17T05:50:03.149791: step 1210, loss 0.259029, acc 0.89\n",
      "2018-04-17T05:50:03.174417: step 1211, loss 0.295024, acc 0.88\n",
      "2018-04-17T05:50:03.200930: step 1212, loss 0.147471, acc 0.96\n",
      "2018-04-17T05:50:03.225955: step 1213, loss 0.323516, acc 0.87\n",
      "2018-04-17T05:50:03.249675: step 1214, loss 0.412811, acc 0.79\n",
      "2018-04-17T05:50:03.267963: step 1215, loss 0.25826, acc 0.90411\n",
      "2018-04-17T05:50:03.291875: step 1216, loss 0.28856, acc 0.86\n",
      "2018-04-17T05:50:03.317860: step 1217, loss 0.19707, acc 0.94\n",
      "2018-04-17T05:50:03.344090: step 1218, loss 0.232682, acc 0.9\n",
      "2018-04-17T05:50:03.369159: step 1219, loss 0.186362, acc 0.92\n",
      "2018-04-17T05:50:03.394088: step 1220, loss 0.17604, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:03.498262: step 1220, loss 0.679665, acc 0.672779, rec 0.944128, pre 0.613333, f1 0.743601\n",
      "\n",
      "2018-04-17T05:50:03.523564: step 1221, loss 0.206571, acc 0.92\n",
      "2018-04-17T05:50:03.550084: step 1222, loss 0.290944, acc 0.91\n",
      "2018-04-17T05:50:03.573355: step 1223, loss 0.218685, acc 0.89\n",
      "2018-04-17T05:50:03.597609: step 1224, loss 0.193047, acc 0.917808\n",
      "2018-04-17T05:50:03.625721: step 1225, loss 0.135516, acc 0.96\n",
      "2018-04-17T05:50:03.646949: step 1226, loss 0.239633, acc 0.88\n",
      "2018-04-17T05:50:03.667686: step 1227, loss 0.24891, acc 0.89\n",
      "2018-04-17T05:50:03.688931: step 1228, loss 0.19372, acc 0.94\n",
      "2018-04-17T05:50:03.711813: step 1229, loss 0.226158, acc 0.89\n",
      "2018-04-17T05:50:03.736974: step 1230, loss 0.258847, acc 0.91\n",
      "2018-04-17T05:50:03.764459: step 1231, loss 0.283254, acc 0.89\n",
      "2018-04-17T05:50:03.786115: step 1232, loss 0.239383, acc 0.91\n",
      "2018-04-17T05:50:03.806224: step 1233, loss 0.223296, acc 0.917808\n",
      "2018-04-17T05:50:03.828197: step 1234, loss 0.264303, acc 0.88\n",
      "2018-04-17T05:50:03.849935: step 1235, loss 0.290172, acc 0.9\n",
      "2018-04-17T05:50:03.871228: step 1236, loss 0.327964, acc 0.8\n",
      "2018-04-17T05:50:03.894954: step 1237, loss 0.365314, acc 0.87\n",
      "2018-04-17T05:50:03.917675: step 1238, loss 0.36146, acc 0.85\n",
      "2018-04-17T05:50:03.939495: step 1239, loss 0.277006, acc 0.93\n",
      "2018-04-17T05:50:03.960765: step 1240, loss 0.180588, acc 0.95\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:04.064793: step 1240, loss 0.580771, acc 0.733524, rec 0.921323, pre 0.671096, f1 0.77655\n",
      "\n",
      "2018-04-17T05:50:04.089350: step 1241, loss 0.216793, acc 0.87\n",
      "2018-04-17T05:50:04.106771: step 1242, loss 0.156105, acc 0.958904\n",
      "2018-04-17T05:50:04.128895: step 1243, loss 0.221495, acc 0.91\n",
      "2018-04-17T05:50:04.150058: step 1244, loss 0.236271, acc 0.87\n",
      "2018-04-17T05:50:04.171048: step 1245, loss 0.206717, acc 0.92\n",
      "2018-04-17T05:50:04.192440: step 1246, loss 0.255686, acc 0.88\n",
      "2018-04-17T05:50:04.214843: step 1247, loss 0.283476, acc 0.84\n",
      "2018-04-17T05:50:04.236555: step 1248, loss 0.200606, acc 0.93\n",
      "2018-04-17T05:50:04.257384: step 1249, loss 0.189663, acc 0.93\n",
      "2018-04-17T05:50:04.282531: step 1250, loss 0.218691, acc 0.91\n",
      "2018-04-17T05:50:04.301287: step 1251, loss 0.211654, acc 0.917808\n",
      "2018-04-17T05:50:04.322820: step 1252, loss 0.20737, acc 0.93\n",
      "2018-04-17T05:50:04.343893: step 1253, loss 0.163216, acc 0.93\n",
      "2018-04-17T05:50:04.365484: step 1254, loss 0.247436, acc 0.91\n",
      "2018-04-17T05:50:04.387914: step 1255, loss 0.247357, acc 0.92\n",
      "2018-04-17T05:50:04.409187: step 1256, loss 0.234957, acc 0.91\n",
      "2018-04-17T05:50:04.430373: step 1257, loss 0.220167, acc 0.92\n",
      "2018-04-17T05:50:04.453068: step 1258, loss 0.252741, acc 0.87\n",
      "2018-04-17T05:50:04.474949: step 1259, loss 0.272619, acc 0.87\n",
      "2018-04-17T05:50:04.496145: step 1260, loss 0.271126, acc 0.890411\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:04.597159: step 1260, loss 1.11342, acc 0.552436, rec 0.965792, pre 0.530038, f1 0.684444\n",
      "\n",
      "2018-04-17T05:50:04.622890: step 1261, loss 0.195885, acc 0.91\n",
      "2018-04-17T05:50:04.646616: step 1262, loss 0.240467, acc 0.87\n",
      "2018-04-17T05:50:04.672091: step 1263, loss 0.361084, acc 0.84\n",
      "2018-04-17T05:50:04.700446: step 1264, loss 0.28344, acc 0.92\n",
      "2018-04-17T05:50:04.848309: step 1265, loss 0.249641, acc 0.89\n",
      "2018-04-17T05:50:04.873702: step 1266, loss 0.238506, acc 0.92\n",
      "2018-04-17T05:50:04.899701: step 1267, loss 0.186684, acc 0.94\n",
      "2018-04-17T05:50:04.924854: step 1268, loss 0.212348, acc 0.91\n",
      "2018-04-17T05:50:04.941992: step 1269, loss 0.17649, acc 0.931507\n",
      "2018-04-17T05:50:04.964545: step 1270, loss 0.237101, acc 0.9\n",
      "2018-04-17T05:50:04.987016: step 1271, loss 0.28967, acc 0.88\n",
      "2018-04-17T05:50:05.010219: step 1272, loss 0.206755, acc 0.91\n",
      "2018-04-17T05:50:05.038176: step 1273, loss 0.247268, acc 0.9\n",
      "2018-04-17T05:50:05.061388: step 1274, loss 0.227148, acc 0.91\n",
      "2018-04-17T05:50:05.083548: step 1275, loss 0.190596, acc 0.9\n",
      "2018-04-17T05:50:05.106242: step 1276, loss 0.238622, acc 0.92\n",
      "2018-04-17T05:50:05.127837: step 1277, loss 0.229661, acc 0.91\n",
      "2018-04-17T05:50:05.145561: step 1278, loss 0.186589, acc 0.931507\n",
      "2018-04-17T05:50:05.169158: step 1279, loss 0.186261, acc 0.93\n",
      "2018-04-17T05:50:05.192978: step 1280, loss 0.263693, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:05.293048: step 1280, loss 0.591603, acc 0.723209, rec 0.925884, pre 0.660163, f1 0.770764\n",
      "\n",
      "2018-04-17T05:50:05.316105: step 1281, loss 0.206391, acc 0.93\n",
      "2018-04-17T05:50:05.337503: step 1282, loss 0.223038, acc 0.9\n",
      "2018-04-17T05:50:05.359266: step 1283, loss 0.311195, acc 0.83\n",
      "2018-04-17T05:50:05.380390: step 1284, loss 0.319551, acc 0.89\n",
      "2018-04-17T05:50:05.401931: step 1285, loss 0.196059, acc 0.95\n",
      "2018-04-17T05:50:05.423907: step 1286, loss 0.159478, acc 0.94\n",
      "2018-04-17T05:50:05.441276: step 1287, loss 0.169985, acc 0.945205\n",
      "2018-04-17T05:50:05.463908: step 1288, loss 0.228636, acc 0.9\n",
      "2018-04-17T05:50:05.485127: step 1289, loss 0.176144, acc 0.97\n",
      "2018-04-17T05:50:05.510412: step 1290, loss 0.214891, acc 0.9\n",
      "2018-04-17T05:50:05.532698: step 1291, loss 0.186127, acc 0.94\n",
      "2018-04-17T05:50:05.556254: step 1292, loss 0.111482, acc 0.99\n",
      "2018-04-17T05:50:05.577705: step 1293, loss 0.300597, acc 0.88\n",
      "2018-04-17T05:50:05.599818: step 1294, loss 0.247461, acc 0.88\n",
      "2018-04-17T05:50:05.621971: step 1295, loss 0.269375, acc 0.9\n",
      "2018-04-17T05:50:05.639106: step 1296, loss 0.219551, acc 0.90411\n",
      "2018-04-17T05:50:05.660119: step 1297, loss 0.267163, acc 0.88\n",
      "2018-04-17T05:50:05.681609: step 1298, loss 0.299672, acc 0.88\n",
      "2018-04-17T05:50:05.703431: step 1299, loss 0.244823, acc 0.9\n",
      "2018-04-17T05:50:05.736801: step 1300, loss 0.222756, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:05.849310: step 1300, loss 0.531504, acc 0.761605, rec 0.884835, pre 0.711274, f1 0.788618\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523944169/checkpoints/model-1300\n",
      "\n",
      "2018-04-17T05:50:05.918370: step 1301, loss 0.187886, acc 0.89\n",
      "2018-04-17T05:50:05.944159: step 1302, loss 0.236503, acc 0.9\n",
      "2018-04-17T05:50:05.965755: step 1303, loss 0.286801, acc 0.87\n",
      "2018-04-17T05:50:05.988679: step 1304, loss 0.126232, acc 0.97\n",
      "2018-04-17T05:50:06.006601: step 1305, loss 0.281873, acc 0.917808\n",
      "2018-04-17T05:50:06.028972: step 1306, loss 0.297767, acc 0.88\n",
      "2018-04-17T05:50:06.051668: step 1307, loss 0.269572, acc 0.89\n",
      "2018-04-17T05:50:06.074879: step 1308, loss 0.378944, acc 0.82\n",
      "2018-04-17T05:50:06.097294: step 1309, loss 0.319828, acc 0.88\n",
      "2018-04-17T05:50:06.118949: step 1310, loss 0.203145, acc 0.92\n",
      "2018-04-17T05:50:06.141707: step 1311, loss 0.249926, acc 0.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17T05:50:06.168946: step 1312, loss 0.202433, acc 0.9\n",
      "2018-04-17T05:50:06.192263: step 1313, loss 0.175878, acc 0.94\n",
      "2018-04-17T05:50:06.208968: step 1314, loss 0.14303, acc 0.972603\n",
      "2018-04-17T05:50:06.230457: step 1315, loss 0.17476, acc 0.94\n",
      "2018-04-17T05:50:06.251317: step 1316, loss 0.205197, acc 0.91\n",
      "2018-04-17T05:50:06.272107: step 1317, loss 0.18178, acc 0.94\n",
      "2018-04-17T05:50:06.293644: step 1318, loss 0.194595, acc 0.96\n",
      "2018-04-17T05:50:06.316006: step 1319, loss 0.199892, acc 0.91\n",
      "2018-04-17T05:50:06.340439: step 1320, loss 0.286794, acc 0.91\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:06.445512: step 1320, loss 0.596024, acc 0.72149, rec 0.927024, pre 0.6583, f1 0.769886\n",
      "\n",
      "2018-04-17T05:50:06.470331: step 1321, loss 0.205829, acc 0.91\n",
      "2018-04-17T05:50:06.496126: step 1322, loss 0.216745, acc 0.93\n",
      "2018-04-17T05:50:06.517528: step 1323, loss 0.211068, acc 0.917808\n",
      "2018-04-17T05:50:06.541607: step 1324, loss 0.185689, acc 0.94\n",
      "2018-04-17T05:50:06.563399: step 1325, loss 0.202652, acc 0.91\n",
      "2018-04-17T05:50:06.591146: step 1326, loss 0.227034, acc 0.88\n",
      "2018-04-17T05:50:06.614103: step 1327, loss 0.285042, acc 0.86\n",
      "2018-04-17T05:50:06.637518: step 1328, loss 0.431854, acc 0.83\n",
      "2018-04-17T05:50:06.665143: step 1329, loss 0.426665, acc 0.8\n",
      "2018-04-17T05:50:06.688110: step 1330, loss 0.236963, acc 0.91\n",
      "2018-04-17T05:50:06.712037: step 1331, loss 0.211841, acc 0.92\n",
      "2018-04-17T05:50:06.736923: step 1332, loss 0.146408, acc 0.958904\n",
      "2018-04-17T05:50:06.764284: step 1333, loss 0.202189, acc 0.93\n",
      "2018-04-17T05:50:06.787940: step 1334, loss 0.190381, acc 0.92\n",
      "2018-04-17T05:50:06.811487: step 1335, loss 0.278414, acc 0.88\n",
      "2018-04-17T05:50:06.835835: step 1336, loss 0.175061, acc 0.91\n",
      "2018-04-17T05:50:06.859849: step 1337, loss 0.188789, acc 0.93\n",
      "2018-04-17T05:50:06.884910: step 1338, loss 0.233281, acc 0.9\n",
      "2018-04-17T05:50:06.908360: step 1339, loss 0.208545, acc 0.93\n",
      "2018-04-17T05:50:06.931572: step 1340, loss 0.294767, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:07.041701: step 1340, loss 0.510811, acc 0.781662, rec 0.892816, pre 0.731776, f1 0.804314\n",
      "\n",
      "2018-04-17T05:50:07.061405: step 1341, loss 0.117796, acc 0.958904\n",
      "2018-04-17T05:50:07.089584: step 1342, loss 0.241412, acc 0.9\n",
      "2018-04-17T05:50:07.111130: step 1343, loss 0.185071, acc 0.95\n",
      "2018-04-17T05:50:07.134807: step 1344, loss 0.211726, acc 0.95\n",
      "2018-04-17T05:50:07.156831: step 1345, loss 0.188219, acc 0.9\n",
      "2018-04-17T05:50:07.179120: step 1346, loss 0.218457, acc 0.92\n",
      "2018-04-17T05:50:07.200231: step 1347, loss 0.268474, acc 0.89\n",
      "2018-04-17T05:50:07.222459: step 1348, loss 0.128669, acc 0.97\n",
      "2018-04-17T05:50:07.244469: step 1349, loss 0.250471, acc 0.88\n",
      "2018-04-17T05:50:07.261246: step 1350, loss 0.242277, acc 0.849315\n",
      "2018-04-17T05:50:07.282268: step 1351, loss 0.153994, acc 0.96\n",
      "2018-04-17T05:50:07.306564: step 1352, loss 0.165404, acc 0.93\n",
      "2018-04-17T05:50:07.328099: step 1353, loss 0.282271, acc 0.91\n",
      "2018-04-17T05:50:07.348854: step 1354, loss 0.358056, acc 0.83\n",
      "2018-04-17T05:50:07.371401: step 1355, loss 0.166953, acc 0.95\n",
      "2018-04-17T05:50:07.392822: step 1356, loss 0.178021, acc 0.92\n",
      "2018-04-17T05:50:07.416336: step 1357, loss 0.180695, acc 0.88\n",
      "2018-04-17T05:50:07.437341: step 1358, loss 0.246799, acc 0.89\n",
      "2018-04-17T05:50:07.454812: step 1359, loss 0.158174, acc 0.945205\n",
      "2018-04-17T05:50:07.476983: step 1360, loss 0.184874, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:07.578646: step 1360, loss 0.752703, acc 0.655014, rec 0.952109, pre 0.598566, f1 0.735035\n",
      "\n",
      "2018-04-17T05:50:07.608286: step 1361, loss 0.268733, acc 0.91\n",
      "2018-04-17T05:50:07.637557: step 1362, loss 0.238092, acc 0.91\n",
      "2018-04-17T05:50:07.666324: step 1363, loss 0.177873, acc 0.93\n",
      "2018-04-17T05:50:07.694171: step 1364, loss 0.206857, acc 0.94\n",
      "2018-04-17T05:50:07.716349: step 1365, loss 0.202789, acc 0.9\n",
      "2018-04-17T05:50:07.738218: step 1366, loss 0.216152, acc 0.92\n",
      "2018-04-17T05:50:07.760914: step 1367, loss 0.236743, acc 0.91\n",
      "2018-04-17T05:50:07.778389: step 1368, loss 0.179129, acc 0.931507\n",
      "2018-04-17T05:50:07.803900: step 1369, loss 0.161151, acc 0.94\n",
      "2018-04-17T05:50:07.825860: step 1370, loss 0.161419, acc 0.94\n",
      "2018-04-17T05:50:07.847098: step 1371, loss 0.149123, acc 0.94\n",
      "2018-04-17T05:50:07.870444: step 1372, loss 0.167737, acc 0.92\n",
      "2018-04-17T05:50:07.892997: step 1373, loss 0.323741, acc 0.87\n",
      "2018-04-17T05:50:07.915549: step 1374, loss 0.219902, acc 0.92\n",
      "2018-04-17T05:50:07.938579: step 1375, loss 0.400908, acc 0.83\n",
      "2018-04-17T05:50:07.960297: step 1376, loss 0.410736, acc 0.83\n",
      "2018-04-17T05:50:07.977714: step 1377, loss 0.220936, acc 0.917808\n",
      "2018-04-17T05:50:07.998509: step 1378, loss 0.392527, acc 0.87\n",
      "2018-04-17T05:50:08.023193: step 1379, loss 0.283029, acc 0.87\n",
      "2018-04-17T05:50:08.055220: step 1380, loss 0.284686, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:08.161254: step 1380, loss 0.48105, acc 0.785673, rec 0.81756, pre 0.77014, f1 0.793142\n",
      "\n",
      "2018-04-17T05:50:08.191241: step 1381, loss 0.212827, acc 0.91\n",
      "2018-04-17T05:50:08.215293: step 1382, loss 0.204436, acc 0.91\n",
      "2018-04-17T05:50:08.242958: step 1383, loss 0.139182, acc 0.96\n",
      "2018-04-17T05:50:08.265498: step 1384, loss 0.20308, acc 0.89\n",
      "2018-04-17T05:50:08.289893: step 1385, loss 0.174425, acc 0.94\n",
      "2018-04-17T05:50:08.307027: step 1386, loss 0.199253, acc 0.890411\n",
      "2018-04-17T05:50:08.328975: step 1387, loss 0.204466, acc 0.94\n",
      "2018-04-17T05:50:08.351374: step 1388, loss 0.177154, acc 0.93\n",
      "2018-04-17T05:50:08.372569: step 1389, loss 0.15753, acc 0.95\n",
      "2018-04-17T05:50:08.394020: step 1390, loss 0.243555, acc 0.91\n",
      "2018-04-17T05:50:08.415232: step 1391, loss 0.174852, acc 0.95\n",
      "2018-04-17T05:50:08.436234: step 1392, loss 0.248971, acc 0.87\n",
      "2018-04-17T05:50:08.461883: step 1393, loss 0.160809, acc 0.95\n",
      "2018-04-17T05:50:08.483280: step 1394, loss 0.211558, acc 0.93\n",
      "2018-04-17T05:50:08.502563: step 1395, loss 0.174023, acc 0.945205\n",
      "2018-04-17T05:50:08.525949: step 1396, loss 0.231933, acc 0.91\n",
      "2018-04-17T05:50:08.547439: step 1397, loss 0.167569, acc 0.94\n",
      "2018-04-17T05:50:08.570340: step 1398, loss 0.168569, acc 0.94\n",
      "2018-04-17T05:50:08.593928: step 1399, loss 0.181366, acc 0.94\n",
      "2018-04-17T05:50:08.617224: step 1400, loss 0.173706, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:08.720695: step 1400, loss 0.623216, acc 0.710029, rec 0.933865, pre 0.646409, f1 0.763993\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523944169/checkpoints/model-1400\n",
      "\n",
      "2018-04-17T05:50:08.788396: step 1401, loss 0.222199, acc 0.91\n",
      "2018-04-17T05:50:08.810135: step 1402, loss 0.222927, acc 0.88\n",
      "2018-04-17T05:50:08.831326: step 1403, loss 0.281722, acc 0.87\n",
      "2018-04-17T05:50:08.848394: step 1404, loss 0.151983, acc 0.958904\n",
      "2018-04-17T05:50:08.869522: step 1405, loss 0.125332, acc 0.96\n",
      "2018-04-17T05:50:08.891466: step 1406, loss 0.208871, acc 0.91\n",
      "2018-04-17T05:50:08.926756: step 1407, loss 0.211488, acc 0.92\n",
      "2018-04-17T05:50:08.949848: step 1408, loss 0.185607, acc 0.95\n",
      "2018-04-17T05:50:08.977439: step 1409, loss 0.175879, acc 0.93\n",
      "2018-04-17T05:50:09.004823: step 1410, loss 0.153959, acc 0.96\n",
      "2018-04-17T05:50:09.027778: step 1411, loss 0.179059, acc 0.96\n",
      "2018-04-17T05:50:09.052008: step 1412, loss 0.25865, acc 0.89\n",
      "2018-04-17T05:50:09.069616: step 1413, loss 0.278649, acc 0.876712\n",
      "2018-04-17T05:50:09.092473: step 1414, loss 0.181225, acc 0.95\n",
      "2018-04-17T05:50:09.113694: step 1415, loss 0.225354, acc 0.89\n",
      "2018-04-17T05:50:09.138335: step 1416, loss 0.285269, acc 0.89\n",
      "2018-04-17T05:50:09.160360: step 1417, loss 0.274941, acc 0.89\n",
      "2018-04-17T05:50:09.182970: step 1418, loss 0.215441, acc 0.92\n",
      "2018-04-17T05:50:09.205064: step 1419, loss 0.172448, acc 0.95\n",
      "2018-04-17T05:50:09.228312: step 1420, loss 0.221647, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:09.323752: step 1420, loss 0.472852, acc 0.804011, rec 0.769669, pre 0.828221, f1 0.797872\n",
      "\n",
      "2018-04-17T05:50:09.350093: step 1421, loss 0.160382, acc 0.92\n",
      "2018-04-17T05:50:09.367854: step 1422, loss 0.199167, acc 0.90411\n",
      "2018-04-17T05:50:09.389594: step 1423, loss 0.264183, acc 0.87\n",
      "2018-04-17T05:50:09.413373: step 1424, loss 0.202744, acc 0.9\n",
      "2018-04-17T05:50:09.436107: step 1425, loss 0.215135, acc 0.92\n",
      "2018-04-17T05:50:09.457833: step 1426, loss 0.207921, acc 0.93\n",
      "2018-04-17T05:50:09.480193: step 1427, loss 0.303528, acc 0.88\n",
      "2018-04-17T05:50:09.501908: step 1428, loss 0.175675, acc 0.92\n",
      "2018-04-17T05:50:09.525124: step 1429, loss 0.155519, acc 0.94\n",
      "2018-04-17T05:50:09.548093: step 1430, loss 0.138526, acc 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17T05:50:09.569155: step 1431, loss 0.238971, acc 0.876712\n",
      "2018-04-17T05:50:09.591232: step 1432, loss 0.184447, acc 0.95\n",
      "2018-04-17T05:50:09.613199: step 1433, loss 0.223413, acc 0.88\n",
      "2018-04-17T05:50:09.634770: step 1434, loss 0.197351, acc 0.93\n",
      "2018-04-17T05:50:09.656539: step 1435, loss 0.12728, acc 0.97\n",
      "2018-04-17T05:50:09.677802: step 1436, loss 0.202283, acc 0.9\n",
      "2018-04-17T05:50:09.699411: step 1437, loss 0.242934, acc 0.91\n",
      "2018-04-17T05:50:09.722951: step 1438, loss 0.236221, acc 0.9\n",
      "2018-04-17T05:50:09.744990: step 1439, loss 0.138781, acc 0.96\n",
      "2018-04-17T05:50:09.761961: step 1440, loss 0.27208, acc 0.90411\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:09.868980: step 1440, loss 0.515018, acc 0.77192, rec 0.63626, pre 0.875981, f1 0.73712\n",
      "\n",
      "2018-04-17T05:50:09.893826: step 1441, loss 0.298488, acc 0.88\n",
      "2018-04-17T05:50:09.917818: step 1442, loss 0.190196, acc 0.94\n",
      "2018-04-17T05:50:09.939743: step 1443, loss 0.294967, acc 0.87\n",
      "2018-04-17T05:50:09.961121: step 1444, loss 0.158625, acc 0.94\n",
      "2018-04-17T05:50:09.982291: step 1445, loss 0.159569, acc 0.94\n",
      "2018-04-17T05:50:10.004084: step 1446, loss 0.159677, acc 0.96\n",
      "2018-04-17T05:50:10.027102: step 1447, loss 0.118526, acc 0.96\n",
      "2018-04-17T05:50:10.052541: step 1448, loss 0.14337, acc 0.96\n",
      "2018-04-17T05:50:10.076428: step 1449, loss 0.389409, acc 0.849315\n",
      "2018-04-17T05:50:10.102418: step 1450, loss 0.374668, acc 0.85\n",
      "2018-04-17T05:50:10.126836: step 1451, loss 0.277138, acc 0.86\n",
      "2018-04-17T05:50:10.150128: step 1452, loss 0.136684, acc 0.96\n",
      "2018-04-17T05:50:10.173303: step 1453, loss 0.180672, acc 0.92\n",
      "2018-04-17T05:50:10.194249: step 1454, loss 0.137149, acc 0.94\n",
      "2018-04-17T05:50:10.216303: step 1455, loss 0.194406, acc 0.93\n",
      "2018-04-17T05:50:10.238172: step 1456, loss 0.183528, acc 0.95\n",
      "2018-04-17T05:50:10.260432: step 1457, loss 0.279298, acc 0.9\n",
      "2018-04-17T05:50:10.281194: step 1458, loss 0.222869, acc 0.917808\n",
      "2018-04-17T05:50:10.303548: step 1459, loss 0.218965, acc 0.89\n",
      "2018-04-17T05:50:10.326601: step 1460, loss 0.216004, acc 0.89\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:10.428847: step 1460, loss 0.537234, acc 0.761605, rec 0.906499, pre 0.704163, f1 0.792622\n",
      "\n",
      "2018-04-17T05:50:10.459999: step 1461, loss 0.259975, acc 0.89\n",
      "2018-04-17T05:50:10.496646: step 1462, loss 0.204224, acc 0.95\n",
      "2018-04-17T05:50:10.521516: step 1463, loss 0.154861, acc 0.95\n",
      "2018-04-17T05:50:10.544416: step 1464, loss 0.171906, acc 0.93\n",
      "2018-04-17T05:50:10.567088: step 1465, loss 0.150032, acc 0.96\n",
      "2018-04-17T05:50:10.589656: step 1466, loss 0.165734, acc 0.95\n",
      "2018-04-17T05:50:10.607081: step 1467, loss 0.190727, acc 0.945205\n",
      "2018-04-17T05:50:10.628909: step 1468, loss 0.170376, acc 0.94\n",
      "2018-04-17T05:50:10.650246: step 1469, loss 0.220928, acc 0.89\n",
      "2018-04-17T05:50:10.673643: step 1470, loss 0.190265, acc 0.92\n",
      "2018-04-17T05:50:10.695041: step 1471, loss 0.120108, acc 0.94\n",
      "2018-04-17T05:50:10.721937: step 1472, loss 0.12596, acc 0.96\n",
      "2018-04-17T05:50:10.744794: step 1473, loss 0.267743, acc 0.9\n",
      "2018-04-17T05:50:10.780717: step 1474, loss 0.226099, acc 0.9\n",
      "2018-04-17T05:50:10.819617: step 1475, loss 0.266468, acc 0.94\n",
      "2018-04-17T05:50:10.850466: step 1476, loss 0.25566, acc 0.849315\n",
      "2018-04-17T05:50:10.889474: step 1477, loss 0.193573, acc 0.95\n",
      "2018-04-17T05:50:10.931678: step 1478, loss 0.188918, acc 0.9\n",
      "2018-04-17T05:50:10.970516: step 1479, loss 0.186119, acc 0.95\n",
      "2018-04-17T05:50:11.009230: step 1480, loss 0.204987, acc 0.93\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:11.208525: step 1480, loss 0.554817, acc 0.760458, rec 0.903079, pre 0.704, f1 0.791209\n",
      "\n",
      "2018-04-17T05:50:11.249329: step 1481, loss 0.168657, acc 0.93\n",
      "2018-04-17T05:50:11.288568: step 1482, loss 0.15491, acc 0.95\n",
      "2018-04-17T05:50:11.326787: step 1483, loss 0.277887, acc 0.89\n",
      "2018-04-17T05:50:11.364977: step 1484, loss 0.184534, acc 0.95\n",
      "2018-04-17T05:50:11.390779: step 1485, loss 0.13564, acc 0.945205\n",
      "2018-04-17T05:50:11.417478: step 1486, loss 0.236753, acc 0.89\n",
      "2018-04-17T05:50:11.439729: step 1487, loss 0.242805, acc 0.89\n",
      "2018-04-17T05:50:11.462089: step 1488, loss 0.288857, acc 0.86\n",
      "2018-04-17T05:50:11.484171: step 1489, loss 0.351702, acc 0.84\n",
      "2018-04-17T05:50:11.506285: step 1490, loss 0.636776, acc 0.79\n",
      "2018-04-17T05:50:11.529050: step 1491, loss 0.228087, acc 0.9\n",
      "2018-04-17T05:50:11.552666: step 1492, loss 0.310397, acc 0.91\n",
      "2018-04-17T05:50:11.574961: step 1493, loss 0.152588, acc 0.93\n",
      "2018-04-17T05:50:11.592633: step 1494, loss 0.158257, acc 0.972603\n",
      "2018-04-17T05:50:11.615132: step 1495, loss 0.137197, acc 0.94\n",
      "2018-04-17T05:50:11.641079: step 1496, loss 0.215929, acc 0.88\n",
      "2018-04-17T05:50:11.663641: step 1497, loss 0.170614, acc 0.94\n",
      "2018-04-17T05:50:11.685597: step 1498, loss 0.207508, acc 0.92\n",
      "2018-04-17T05:50:11.708651: step 1499, loss 0.299301, acc 0.87\n",
      "2018-04-17T05:50:11.730846: step 1500, loss 0.100843, acc 0.98\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:11.834934: step 1500, loss 0.589177, acc 0.738109, rec 0.921323, pre 0.675585, f1 0.779547\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523944169/checkpoints/model-1500\n",
      "\n",
      "2018-04-17T05:50:11.910575: step 1501, loss 0.16071, acc 0.94\n",
      "2018-04-17T05:50:11.932005: step 1502, loss 0.170687, acc 0.91\n",
      "2018-04-17T05:50:11.949495: step 1503, loss 0.174826, acc 0.958904\n",
      "2018-04-17T05:50:11.971918: step 1504, loss 0.142632, acc 0.97\n",
      "2018-04-17T05:50:11.993805: step 1505, loss 0.144616, acc 0.96\n",
      "2018-04-17T05:50:12.015304: step 1506, loss 0.256051, acc 0.88\n",
      "2018-04-17T05:50:12.037542: step 1507, loss 0.153456, acc 0.94\n",
      "2018-04-17T05:50:12.059372: step 1508, loss 0.181685, acc 0.92\n",
      "2018-04-17T05:50:12.081286: step 1509, loss 0.147514, acc 0.95\n",
      "2018-04-17T05:50:12.106906: step 1510, loss 0.155986, acc 0.95\n",
      "2018-04-17T05:50:12.129375: step 1511, loss 0.203519, acc 0.91\n",
      "2018-04-17T05:50:12.147069: step 1512, loss 0.261883, acc 0.917808\n",
      "2018-04-17T05:50:12.169333: step 1513, loss 0.19033, acc 0.91\n",
      "2018-04-17T05:50:12.190968: step 1514, loss 0.250838, acc 0.85\n",
      "2018-04-17T05:50:12.212250: step 1515, loss 0.183019, acc 0.94\n",
      "2018-04-17T05:50:12.235208: step 1516, loss 0.225038, acc 0.89\n",
      "2018-04-17T05:50:12.256759: step 1517, loss 0.254418, acc 0.9\n",
      "2018-04-17T05:50:12.278504: step 1518, loss 0.251541, acc 0.9\n",
      "2018-04-17T05:50:12.300189: step 1519, loss 0.155208, acc 0.96\n",
      "2018-04-17T05:50:12.326250: step 1520, loss 0.157111, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:12.426833: step 1520, loss 0.547562, acc 0.762178, rec 0.895097, pre 0.708484, f1 0.790932\n",
      "\n",
      "2018-04-17T05:50:12.445818: step 1521, loss 0.159328, acc 0.945205\n",
      "2018-04-17T05:50:12.468840: step 1522, loss 0.269323, acc 0.87\n",
      "2018-04-17T05:50:12.491317: step 1523, loss 0.135956, acc 0.96\n",
      "2018-04-17T05:50:12.513195: step 1524, loss 0.152884, acc 0.96\n",
      "2018-04-17T05:50:12.538624: step 1525, loss 0.10583, acc 0.96\n",
      "2018-04-17T05:50:12.561331: step 1526, loss 0.126109, acc 0.96\n",
      "2018-04-17T05:50:12.582746: step 1527, loss 0.211825, acc 0.93\n",
      "2018-04-17T05:50:12.604448: step 1528, loss 0.273042, acc 0.86\n",
      "2018-04-17T05:50:12.626385: step 1529, loss 0.228409, acc 0.9\n",
      "2018-04-17T05:50:12.644264: step 1530, loss 0.189256, acc 0.931507\n",
      "2018-04-17T05:50:12.666505: step 1531, loss 0.208828, acc 0.91\n",
      "2018-04-17T05:50:12.688182: step 1532, loss 0.203517, acc 0.92\n",
      "2018-04-17T05:50:12.710242: step 1533, loss 0.161549, acc 0.94\n",
      "2018-04-17T05:50:12.731569: step 1534, loss 0.254238, acc 0.9\n",
      "2018-04-17T05:50:12.756610: step 1535, loss 0.133612, acc 0.95\n",
      "2018-04-17T05:50:12.778511: step 1536, loss 0.21939, acc 0.92\n",
      "2018-04-17T05:50:12.800250: step 1537, loss 0.170719, acc 0.93\n",
      "2018-04-17T05:50:12.822387: step 1538, loss 0.143359, acc 0.95\n",
      "2018-04-17T05:50:12.840114: step 1539, loss 0.207387, acc 0.917808\n",
      "2018-04-17T05:50:12.862289: step 1540, loss 0.252684, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:12.973016: step 1540, loss 0.890513, acc 0.62235, rec 0.961231, pre 0.574251, f1 0.718977\n",
      "\n",
      "2018-04-17T05:50:12.999950: step 1541, loss 0.145614, acc 0.93\n",
      "2018-04-17T05:50:13.022091: step 1542, loss 0.185166, acc 0.94\n",
      "2018-04-17T05:50:13.044196: step 1543, loss 0.179269, acc 0.92\n",
      "2018-04-17T05:50:13.066151: step 1544, loss 0.163873, acc 0.95\n",
      "2018-04-17T05:50:13.088873: step 1545, loss 0.160885, acc 0.94\n",
      "2018-04-17T05:50:13.111994: step 1546, loss 0.149231, acc 0.93\n",
      "2018-04-17T05:50:13.134645: step 1547, loss 0.130887, acc 0.96\n",
      "2018-04-17T05:50:13.152100: step 1548, loss 0.322614, acc 0.876712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17T05:50:13.174175: step 1549, loss 0.19604, acc 0.91\n",
      "2018-04-17T05:50:13.199008: step 1550, loss 0.141411, acc 0.93\n",
      "2018-04-17T05:50:13.220874: step 1551, loss 0.196886, acc 0.93\n",
      "2018-04-17T05:50:13.242261: step 1552, loss 0.204375, acc 0.91\n",
      "2018-04-17T05:50:13.264102: step 1553, loss 0.133804, acc 0.95\n",
      "2018-04-17T05:50:13.285087: step 1554, loss 0.247748, acc 0.94\n",
      "2018-04-17T05:50:13.306662: step 1555, loss 0.210772, acc 0.93\n",
      "2018-04-17T05:50:13.327705: step 1556, loss 0.169991, acc 0.92\n",
      "2018-04-17T05:50:13.345413: step 1557, loss 0.150141, acc 0.945205\n",
      "2018-04-17T05:50:13.367247: step 1558, loss 0.203011, acc 0.9\n",
      "2018-04-17T05:50:13.388401: step 1559, loss 0.158491, acc 0.95\n",
      "2018-04-17T05:50:13.414950: step 1560, loss 0.150619, acc 0.95\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:13.519108: step 1560, loss 0.830717, acc 0.640688, rec 0.95553, pre 0.587658, f1 0.727746\n",
      "\n",
      "2018-04-17T05:50:13.542612: step 1561, loss 0.226402, acc 0.92\n",
      "2018-04-17T05:50:13.564280: step 1562, loss 0.167374, acc 0.93\n",
      "2018-04-17T05:50:13.585995: step 1563, loss 0.137473, acc 0.95\n",
      "2018-04-17T05:50:13.607784: step 1564, loss 0.164528, acc 0.93\n",
      "2018-04-17T05:50:13.634838: step 1565, loss 0.192133, acc 0.91\n",
      "2018-04-17T05:50:13.652427: step 1566, loss 0.298164, acc 0.917808\n",
      "2018-04-17T05:50:13.677884: step 1567, loss 0.160604, acc 0.95\n",
      "2018-04-17T05:50:13.702625: step 1568, loss 0.185759, acc 0.93\n",
      "2018-04-17T05:50:13.728303: step 1569, loss 0.146261, acc 0.96\n",
      "2018-04-17T05:50:13.750880: step 1570, loss 0.188167, acc 0.92\n",
      "2018-04-17T05:50:13.775193: step 1571, loss 0.186822, acc 0.93\n",
      "2018-04-17T05:50:13.799127: step 1572, loss 0.259196, acc 0.89\n",
      "2018-04-17T05:50:13.820780: step 1573, loss 0.161776, acc 0.93\n",
      "2018-04-17T05:50:13.846512: step 1574, loss 0.158721, acc 0.93\n",
      "2018-04-17T05:50:13.863882: step 1575, loss 0.141333, acc 0.972603\n",
      "2018-04-17T05:50:13.885164: step 1576, loss 0.238269, acc 0.92\n",
      "2018-04-17T05:50:13.906390: step 1577, loss 0.0882598, acc 1\n",
      "2018-04-17T05:50:13.927815: step 1578, loss 0.159599, acc 0.95\n",
      "2018-04-17T05:50:13.949585: step 1579, loss 0.219831, acc 0.88\n",
      "2018-04-17T05:50:13.970949: step 1580, loss 0.174349, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:14.079243: step 1580, loss 0.520466, acc 0.773639, rec 0.877993, pre 0.727788, f1 0.795866\n",
      "\n",
      "2018-04-17T05:50:14.103381: step 1581, loss 0.232042, acc 0.92\n",
      "2018-04-17T05:50:14.125403: step 1582, loss 0.160617, acc 0.92\n",
      "2018-04-17T05:50:14.147409: step 1583, loss 0.124842, acc 0.95\n",
      "2018-04-17T05:50:14.164938: step 1584, loss 0.168303, acc 0.945205\n",
      "2018-04-17T05:50:14.187038: step 1585, loss 0.216557, acc 0.89\n",
      "2018-04-17T05:50:14.209541: step 1586, loss 0.219142, acc 0.88\n",
      "2018-04-17T05:50:14.231674: step 1587, loss 0.159852, acc 0.95\n",
      "2018-04-17T05:50:14.254227: step 1588, loss 0.136094, acc 0.93\n",
      "2018-04-17T05:50:14.276273: step 1589, loss 0.100816, acc 0.98\n",
      "2018-04-17T05:50:14.302182: step 1590, loss 0.179303, acc 0.94\n",
      "2018-04-17T05:50:14.324556: step 1591, loss 0.23526, acc 0.87\n",
      "2018-04-17T05:50:14.346834: step 1592, loss 0.220398, acc 0.9\n",
      "2018-04-17T05:50:14.365001: step 1593, loss 0.168694, acc 0.972603\n",
      "2018-04-17T05:50:14.386611: step 1594, loss 0.127561, acc 0.94\n",
      "2018-04-17T05:50:14.408521: step 1595, loss 0.231803, acc 0.93\n",
      "2018-04-17T05:50:14.430564: step 1596, loss 0.141856, acc 0.95\n",
      "2018-04-17T05:50:14.452315: step 1597, loss 0.163513, acc 0.96\n",
      "2018-04-17T05:50:14.474144: step 1598, loss 0.183821, acc 0.93\n",
      "2018-04-17T05:50:14.495971: step 1599, loss 0.12699, acc 0.97\n",
      "2018-04-17T05:50:14.521673: step 1600, loss 0.31431, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:14.636345: step 1600, loss 1.17463, acc 0.570201, rec 0.961231, pre 0.540731, f1 0.692118\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523944169/checkpoints/model-1600\n",
      "\n",
      "2018-04-17T05:50:14.709266: step 1601, loss 0.285138, acc 0.9\n",
      "2018-04-17T05:50:14.730702: step 1602, loss 0.204525, acc 0.90411\n",
      "2018-04-17T05:50:14.752754: step 1603, loss 0.220704, acc 0.9\n",
      "2018-04-17T05:50:14.775168: step 1604, loss 0.183813, acc 0.93\n",
      "2018-04-17T05:50:14.796999: step 1605, loss 0.194872, acc 0.94\n",
      "2018-04-17T05:50:14.819484: step 1606, loss 0.17724, acc 0.93\n",
      "2018-04-17T05:50:14.842064: step 1607, loss 0.211078, acc 0.94\n",
      "2018-04-17T05:50:14.864179: step 1608, loss 0.203318, acc 0.9\n",
      "2018-04-17T05:50:14.886012: step 1609, loss 0.217878, acc 0.92\n",
      "2018-04-17T05:50:14.908029: step 1610, loss 0.279907, acc 0.9\n",
      "2018-04-17T05:50:14.926874: step 1611, loss 0.184735, acc 0.931507\n",
      "2018-04-17T05:50:14.952536: step 1612, loss 0.247683, acc 0.9\n",
      "2018-04-17T05:50:14.973857: step 1613, loss 0.146074, acc 0.94\n",
      "2018-04-17T05:50:14.995820: step 1614, loss 0.138168, acc 0.95\n",
      "2018-04-17T05:50:15.017592: step 1615, loss 0.152772, acc 0.94\n",
      "2018-04-17T05:50:15.039967: step 1616, loss 0.17307, acc 0.94\n",
      "2018-04-17T05:50:15.061878: step 1617, loss 0.157013, acc 0.96\n",
      "2018-04-17T05:50:15.083605: step 1618, loss 0.163819, acc 0.94\n",
      "2018-04-17T05:50:15.104905: step 1619, loss 0.180253, acc 0.93\n",
      "2018-04-17T05:50:15.122363: step 1620, loss 0.170704, acc 0.958904\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:15.229686: step 1620, loss 0.480975, acc 0.794269, rec 0.793615, pre 0.796339, f1 0.794974\n",
      "\n",
      "2018-04-17T05:50:15.254124: step 1621, loss 0.214953, acc 0.93\n",
      "2018-04-17T05:50:15.278609: step 1622, loss 0.1676, acc 0.95\n",
      "2018-04-17T05:50:15.302456: step 1623, loss 0.195689, acc 0.92\n",
      "2018-04-17T05:50:15.330401: step 1624, loss 0.140788, acc 0.94\n",
      "2018-04-17T05:50:15.358288: step 1625, loss 0.123846, acc 0.96\n",
      "2018-04-17T05:50:15.384737: step 1626, loss 0.1811, acc 0.91\n",
      "2018-04-17T05:50:15.406474: step 1627, loss 0.152265, acc 0.94\n",
      "2018-04-17T05:50:15.427846: step 1628, loss 0.211219, acc 0.9\n",
      "2018-04-17T05:50:15.448811: step 1629, loss 0.193841, acc 0.931507\n",
      "2018-04-17T05:50:15.471405: step 1630, loss 0.156706, acc 0.93\n",
      "2018-04-17T05:50:15.492616: step 1631, loss 0.160815, acc 0.95\n",
      "2018-04-17T05:50:15.513891: step 1632, loss 0.262574, acc 0.88\n",
      "2018-04-17T05:50:15.534928: step 1633, loss 0.152148, acc 0.94\n",
      "2018-04-17T05:50:15.556763: step 1634, loss 0.150252, acc 0.94\n",
      "2018-04-17T05:50:15.577527: step 1635, loss 0.153515, acc 0.98\n",
      "2018-04-17T05:50:15.598173: step 1636, loss 0.101281, acc 0.97\n",
      "2018-04-17T05:50:15.618941: step 1637, loss 0.151117, acc 0.95\n",
      "2018-04-17T05:50:15.637666: step 1638, loss 0.224361, acc 0.890411\n",
      "2018-04-17T05:50:15.662488: step 1639, loss 0.158674, acc 0.95\n",
      "2018-04-17T05:50:15.683484: step 1640, loss 0.252973, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:15.785433: step 1640, loss 1.15789, acc 0.576504, rec 0.961231, pre 0.544574, f1 0.695258\n",
      "\n",
      "2018-04-17T05:50:15.808521: step 1641, loss 0.334333, acc 0.87\n",
      "2018-04-17T05:50:15.829509: step 1642, loss 0.195918, acc 0.92\n",
      "2018-04-17T05:50:15.850307: step 1643, loss 0.162639, acc 0.94\n",
      "2018-04-17T05:50:15.874362: step 1644, loss 0.157573, acc 0.96\n",
      "2018-04-17T05:50:15.895353: step 1645, loss 0.127885, acc 0.95\n",
      "2018-04-17T05:50:15.916073: step 1646, loss 0.205541, acc 0.93\n",
      "2018-04-17T05:50:15.932648: step 1647, loss 0.140359, acc 0.931507\n",
      "2018-04-17T05:50:15.953564: step 1648, loss 0.141235, acc 0.96\n",
      "2018-04-17T05:50:15.974110: step 1649, loss 0.126265, acc 0.96\n",
      "2018-04-17T05:50:15.994647: step 1650, loss 0.126971, acc 0.97\n",
      "2018-04-17T05:50:16.015240: step 1651, loss 0.182659, acc 0.92\n",
      "2018-04-17T05:50:16.036061: step 1652, loss 0.204161, acc 0.89\n",
      "2018-04-17T05:50:16.057132: step 1653, loss 0.169732, acc 0.92\n",
      "2018-04-17T05:50:16.081093: step 1654, loss 0.237694, acc 0.91\n",
      "2018-04-17T05:50:16.101915: step 1655, loss 0.298888, acc 0.87\n",
      "2018-04-17T05:50:16.118653: step 1656, loss 0.114687, acc 0.986301\n",
      "2018-04-17T05:50:16.139978: step 1657, loss 0.0919883, acc 0.96\n",
      "2018-04-17T05:50:16.160862: step 1658, loss 0.231972, acc 0.93\n",
      "2018-04-17T05:50:16.180930: step 1659, loss 0.257811, acc 0.89\n",
      "2018-04-17T05:50:16.200933: step 1660, loss 0.203642, acc 0.89\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:16.304053: step 1660, loss 0.831614, acc 0.649284, rec 0.947548, pre 0.594846, f1 0.730871\n",
      "\n",
      "2018-04-17T05:50:16.327660: step 1661, loss 0.103924, acc 0.98\n",
      "2018-04-17T05:50:16.349021: step 1662, loss 0.106836, acc 0.96\n",
      "2018-04-17T05:50:16.370244: step 1663, loss 0.205898, acc 0.95\n",
      "2018-04-17T05:50:16.391222: step 1664, loss 0.220368, acc 0.87\n",
      "2018-04-17T05:50:16.409174: step 1665, loss 0.238371, acc 0.945205\n",
      "2018-04-17T05:50:16.430550: step 1666, loss 0.131943, acc 0.94\n",
      "2018-04-17T05:50:16.451773: step 1667, loss 0.145588, acc 0.93\n",
      "2018-04-17T05:50:16.473117: step 1668, loss 0.166025, acc 0.94\n",
      "2018-04-17T05:50:16.494097: step 1669, loss 0.151184, acc 0.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17T05:50:16.518840: step 1670, loss 0.144749, acc 0.94\n",
      "2018-04-17T05:50:16.540047: step 1671, loss 0.180643, acc 0.94\n",
      "2018-04-17T05:50:16.561203: step 1672, loss 0.176647, acc 0.92\n",
      "2018-04-17T05:50:16.582256: step 1673, loss 0.161121, acc 0.91\n",
      "2018-04-17T05:50:16.598880: step 1674, loss 0.142885, acc 0.958904\n",
      "2018-04-17T05:50:16.620667: step 1675, loss 0.116732, acc 0.95\n",
      "2018-04-17T05:50:16.641784: step 1676, loss 0.140938, acc 0.97\n",
      "2018-04-17T05:50:16.662864: step 1677, loss 0.236758, acc 0.92\n",
      "2018-04-17T05:50:16.683789: step 1678, loss 0.177044, acc 0.94\n",
      "2018-04-17T05:50:16.704900: step 1679, loss 0.131467, acc 0.93\n",
      "2018-04-17T05:50:16.730048: step 1680, loss 0.160117, acc 0.93\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:16.830729: step 1680, loss 0.48709, acc 0.793696, rec 0.741163, pre 0.83014, f1 0.783133\n",
      "\n",
      "2018-04-17T05:50:16.853015: step 1681, loss 0.139979, acc 0.94\n",
      "2018-04-17T05:50:16.873223: step 1682, loss 0.196365, acc 0.94\n",
      "2018-04-17T05:50:16.889551: step 1683, loss 0.242206, acc 0.890411\n",
      "2018-04-17T05:50:16.910417: step 1684, loss 0.250884, acc 0.91\n",
      "2018-04-17T05:50:16.934332: step 1685, loss 0.114293, acc 0.97\n",
      "2018-04-17T05:50:16.954993: step 1686, loss 0.12241, acc 0.97\n",
      "2018-04-17T05:50:16.975718: step 1687, loss 0.0887833, acc 0.97\n",
      "2018-04-17T05:50:16.996578: step 1688, loss 0.183516, acc 0.95\n",
      "2018-04-17T05:50:17.017976: step 1689, loss 0.257254, acc 0.92\n",
      "2018-04-17T05:50:17.038680: step 1690, loss 0.224761, acc 0.91\n",
      "2018-04-17T05:50:17.059622: step 1691, loss 0.17527, acc 0.93\n",
      "2018-04-17T05:50:17.076583: step 1692, loss 0.154775, acc 0.931507\n",
      "2018-04-17T05:50:17.098171: step 1693, loss 0.204792, acc 0.95\n",
      "2018-04-17T05:50:17.119176: step 1694, loss 0.123323, acc 0.96\n",
      "2018-04-17T05:50:17.144054: step 1695, loss 0.146307, acc 0.94\n",
      "2018-04-17T05:50:17.165786: step 1696, loss 0.158386, acc 0.93\n",
      "2018-04-17T05:50:17.187083: step 1697, loss 0.101547, acc 0.97\n",
      "2018-04-17T05:50:17.208739: step 1698, loss 0.126428, acc 0.97\n",
      "2018-04-17T05:50:17.230052: step 1699, loss 0.236369, acc 0.9\n",
      "2018-04-17T05:50:17.251813: step 1700, loss 0.21507, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:17.357680: step 1700, loss 0.936508, acc 0.630372, rec 0.958951, pre 0.58, f1 0.722819\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523944169/checkpoints/model-1700\n",
      "\n",
      "2018-04-17T05:50:17.422117: step 1701, loss 0.168179, acc 0.958904\n",
      "2018-04-17T05:50:17.443112: step 1702, loss 0.0886763, acc 0.98\n",
      "2018-04-17T05:50:17.464100: step 1703, loss 0.236134, acc 0.94\n",
      "2018-04-17T05:50:17.485257: step 1704, loss 0.372499, acc 0.82\n",
      "2018-04-17T05:50:17.506562: step 1705, loss 0.344116, acc 0.89\n",
      "2018-04-17T05:50:17.527382: step 1706, loss 0.118701, acc 0.97\n",
      "2018-04-17T05:50:17.548152: step 1707, loss 0.130832, acc 0.95\n",
      "2018-04-17T05:50:17.573350: step 1708, loss 0.132134, acc 0.93\n",
      "2018-04-17T05:50:17.594370: step 1709, loss 0.207621, acc 0.94\n",
      "2018-04-17T05:50:17.611060: step 1710, loss 0.165806, acc 0.931507\n",
      "2018-04-17T05:50:17.633121: step 1711, loss 0.165706, acc 0.95\n",
      "2018-04-17T05:50:17.654104: step 1712, loss 0.174435, acc 0.92\n",
      "2018-04-17T05:50:17.675101: step 1713, loss 0.248721, acc 0.93\n",
      "2018-04-17T05:50:17.696497: step 1714, loss 0.224092, acc 0.94\n",
      "2018-04-17T05:50:17.717491: step 1715, loss 0.106082, acc 0.96\n",
      "2018-04-17T05:50:17.738227: step 1716, loss 0.146721, acc 0.95\n",
      "2018-04-17T05:50:17.758843: step 1717, loss 0.140819, acc 0.95\n",
      "2018-04-17T05:50:17.783712: step 1718, loss 0.207516, acc 0.94\n",
      "2018-04-17T05:50:17.800124: step 1719, loss 0.102735, acc 0.958904\n",
      "2018-04-17T05:50:17.821092: step 1720, loss 0.187395, acc 0.95\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:17.921176: step 1720, loss 0.556382, acc 0.763324, rec 0.887115, pre 0.712454, f1 0.790249\n",
      "\n",
      "2018-04-17T05:50:17.944206: step 1721, loss 0.135502, acc 0.94\n",
      "2018-04-17T05:50:17.965399: step 1722, loss 0.123653, acc 0.97\n",
      "2018-04-17T05:50:17.990479: step 1723, loss 0.178886, acc 0.95\n",
      "2018-04-17T05:50:18.011458: step 1724, loss 0.160688, acc 0.96\n",
      "2018-04-17T05:50:18.032654: step 1725, loss 0.217344, acc 0.91\n",
      "2018-04-17T05:50:18.053553: step 1726, loss 0.188158, acc 0.93\n",
      "2018-04-17T05:50:18.074413: step 1727, loss 0.152752, acc 0.95\n",
      "2018-04-17T05:50:18.091304: step 1728, loss 0.195471, acc 0.90411\n",
      "2018-04-17T05:50:18.112331: step 1729, loss 0.100924, acc 0.97\n",
      "2018-04-17T05:50:18.133198: step 1730, loss 0.180269, acc 0.96\n",
      "2018-04-17T05:50:18.154178: step 1731, loss 0.17877, acc 0.92\n",
      "2018-04-17T05:50:18.175093: step 1732, loss 0.122498, acc 0.97\n",
      "2018-04-17T05:50:18.200170: step 1733, loss 0.140014, acc 0.94\n",
      "2018-04-17T05:50:18.221289: step 1734, loss 0.165121, acc 0.92\n",
      "2018-04-17T05:50:18.242052: step 1735, loss 0.304517, acc 0.91\n",
      "2018-04-17T05:50:18.262839: step 1736, loss 0.168918, acc 0.94\n",
      "2018-04-17T05:50:18.279863: step 1737, loss 0.209343, acc 0.931507\n",
      "2018-04-17T05:50:18.301454: step 1738, loss 0.2041, acc 0.92\n",
      "2018-04-17T05:50:18.322783: step 1739, loss 0.092194, acc 0.98\n",
      "2018-04-17T05:50:18.343975: step 1740, loss 0.221339, acc 0.91\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:18.450696: step 1740, loss 0.605891, acc 0.744413, rec 0.915621, pre 0.683404, f1 0.782651\n",
      "\n",
      "2018-04-17T05:50:18.473639: step 1741, loss 0.116459, acc 0.96\n",
      "2018-04-17T05:50:18.494733: step 1742, loss 0.144171, acc 0.94\n",
      "2018-04-17T05:50:18.515861: step 1743, loss 0.203204, acc 0.91\n",
      "2018-04-17T05:50:18.537257: step 1744, loss 0.344635, acc 0.88\n",
      "2018-04-17T05:50:18.559326: step 1745, loss 0.213105, acc 0.9\n",
      "2018-04-17T05:50:18.576764: step 1746, loss 0.143453, acc 0.945205\n",
      "2018-04-17T05:50:18.598378: step 1747, loss 0.0921209, acc 0.98\n",
      "2018-04-17T05:50:18.619348: step 1748, loss 0.112907, acc 0.99\n",
      "2018-04-17T05:50:18.640567: step 1749, loss 0.145159, acc 0.98\n",
      "2018-04-17T05:50:18.666094: step 1750, loss 0.2125, acc 0.93\n",
      "2018-04-17T05:50:18.687615: step 1751, loss 0.138754, acc 0.93\n",
      "2018-04-17T05:50:18.709039: step 1752, loss 0.132953, acc 0.96\n",
      "2018-04-17T05:50:18.730131: step 1753, loss 0.110754, acc 0.94\n",
      "2018-04-17T05:50:18.751427: step 1754, loss 0.244944, acc 0.91\n",
      "2018-04-17T05:50:18.768803: step 1755, loss 0.137067, acc 0.945205\n",
      "2018-04-17T05:50:18.790243: step 1756, loss 0.204193, acc 0.93\n",
      "2018-04-17T05:50:18.811483: step 1757, loss 0.208637, acc 0.93\n",
      "2018-04-17T05:50:18.832974: step 1758, loss 0.127488, acc 0.97\n",
      "2018-04-17T05:50:18.854317: step 1759, loss 0.119016, acc 0.96\n",
      "2018-04-17T05:50:18.880274: step 1760, loss 0.0901111, acc 0.98\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:18.982576: step 1760, loss 0.556307, acc 0.763897, rec 0.891676, pre 0.711556, f1 0.791498\n",
      "\n",
      "2018-04-17T05:50:19.005980: step 1761, loss 0.113478, acc 0.97\n",
      "2018-04-17T05:50:19.026498: step 1762, loss 0.185475, acc 0.92\n",
      "2018-04-17T05:50:19.047252: step 1763, loss 0.2228, acc 0.89\n",
      "2018-04-17T05:50:19.064027: step 1764, loss 0.0633995, acc 1\n",
      "2018-04-17T05:50:19.088736: step 1765, loss 0.179467, acc 0.93\n",
      "2018-04-17T05:50:19.109661: step 1766, loss 0.130633, acc 0.97\n",
      "2018-04-17T05:50:19.130598: step 1767, loss 0.173927, acc 0.91\n",
      "2018-04-17T05:50:19.151382: step 1768, loss 0.118818, acc 0.96\n",
      "2018-04-17T05:50:19.171742: step 1769, loss 0.172156, acc 0.93\n",
      "2018-04-17T05:50:19.192233: step 1770, loss 0.306508, acc 0.89\n",
      "2018-04-17T05:50:19.213425: step 1771, loss 0.172863, acc 0.95\n",
      "2018-04-17T05:50:19.234080: step 1772, loss 0.116205, acc 0.95\n",
      "2018-04-17T05:50:19.251006: step 1773, loss 0.0902281, acc 0.972603\n",
      "2018-04-17T05:50:19.271999: step 1774, loss 0.198121, acc 0.93\n",
      "2018-04-17T05:50:19.296872: step 1775, loss 0.157287, acc 0.91\n",
      "2018-04-17T05:50:19.317695: step 1776, loss 0.132396, acc 0.94\n",
      "2018-04-17T05:50:19.339250: step 1777, loss 0.184715, acc 0.95\n",
      "2018-04-17T05:50:19.360364: step 1778, loss 0.156201, acc 0.93\n",
      "2018-04-17T05:50:19.381694: step 1779, loss 0.125524, acc 0.97\n",
      "2018-04-17T05:50:19.402638: step 1780, loss 0.156296, acc 0.93\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:19.507887: step 1780, loss 0.510339, acc 0.785673, rec 0.86203, pre 0.749257, f1 0.801697\n",
      "\n",
      "2018-04-17T05:50:19.530995: step 1781, loss 0.164181, acc 0.93\n",
      "2018-04-17T05:50:19.548282: step 1782, loss 0.141154, acc 0.945205\n",
      "2018-04-17T05:50:19.569428: step 1783, loss 0.145268, acc 0.95\n",
      "2018-04-17T05:50:19.590591: step 1784, loss 0.132928, acc 0.95\n",
      "2018-04-17T05:50:19.611923: step 1785, loss 0.160739, acc 0.95\n",
      "2018-04-17T05:50:19.633027: step 1786, loss 0.142628, acc 0.98\n",
      "2018-04-17T05:50:19.654220: step 1787, loss 0.200207, acc 0.92\n",
      "2018-04-17T05:50:19.676900: step 1788, loss 0.181834, acc 0.9\n",
      "2018-04-17T05:50:19.698361: step 1789, loss 0.1739, acc 0.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17T05:50:19.723138: step 1790, loss 0.149446, acc 0.96\n",
      "2018-04-17T05:50:19.740377: step 1791, loss 0.104057, acc 0.972603\n",
      "2018-04-17T05:50:19.761937: step 1792, loss 0.103804, acc 0.98\n",
      "2018-04-17T05:50:19.783580: step 1793, loss 0.18267, acc 0.95\n",
      "2018-04-17T05:50:19.805058: step 1794, loss 0.131327, acc 0.94\n",
      "2018-04-17T05:50:19.826063: step 1795, loss 0.150207, acc 0.96\n",
      "2018-04-17T05:50:19.847182: step 1796, loss 0.145351, acc 0.96\n",
      "2018-04-17T05:50:19.868757: step 1797, loss 0.184593, acc 0.91\n",
      "2018-04-17T05:50:19.890488: step 1798, loss 0.170001, acc 0.94\n",
      "2018-04-17T05:50:19.912361: step 1799, loss 0.132853, acc 0.98\n",
      "2018-04-17T05:50:19.935789: step 1800, loss 0.132393, acc 0.945205\n",
      "\n",
      "Evaluation:\n",
      "2018-04-17T05:50:20.038913: step 1800, loss 0.775161, acc 0.676791, rec 0.942987, pre 0.616704, f1 0.745717\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523944169/checkpoints/model-1800\n",
      "\n",
      "\n",
      "Test Set:\n",
      "2018-04-17T05:50:20.134301: step 1800, loss 0.703469, acc 0.684994, rec 0.94382, pre 0.626866, f1 0.753363\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", 0.75, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 50, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"1\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 50, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.4, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 100\n",
    "tf.flags.DEFINE_integer(\"batch_size\", batch_size, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 20, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=train_s.shape[1],\n",
    "            num_classes=2,\n",
    "            vocab_size=2500,\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "        \n",
    "         # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        #vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch1, x_batch2, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch1, x_batch2, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            #r \n",
    "            step, summaries, loss, accuracy, recall, precision, f1 = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy,cnn.recall, cnn.precision, cnn.f1_score],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, rec {:g}, pre {:g}, f1 {:g}\".format(time_str, step, loss, accuracy, recall, precision, f1))# recall, precision, f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(train_s, train_c,  expanded_train_labels)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch1, x_batch2, y_batch = zip(*batch)\n",
    "            train_step(x_batch1, x_batch1, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(validation_s, validation_c, expanded_validation_labels, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "print(\"\\nTest Set:\")\n",
    "dev_step(test_s, test_c, expanded_test_labels, writer=dev_summary_writer)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
