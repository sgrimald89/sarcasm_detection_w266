{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saulgrimaldo1/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from w266_common import utils, vocabulary\n",
    "import time\n",
    "import datetime\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "tokenizer = TweetTokenizer()\n",
    "x_data = []\n",
    "x_contexts = []\n",
    "labels = []\n",
    "sentences  = []\n",
    "contexts = []\n",
    "with open('dta/merged_data_v2.csv', 'r') as csvfile:\n",
    "    linereader = csv.reader(csvfile, delimiter = '|')\n",
    "    for i, row in enumerate(linereader):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        sentence, context, sarcasm = row\n",
    "        sentences.append(sentence)\n",
    "        contexts.append(context)\n",
    "        x_tokens = utils.canonicalize_words(tokenizer.tokenize(sentence), hashtags = True)\n",
    "        context_tokens = utils.canonicalize_words(tokenizer.tokenize(context), hashtags = True)\n",
    "        x_data.append(x_tokens)\n",
    "        x_contexts.append(context_tokens)\n",
    "        labels.append(int(sarcasm))\n",
    "\n",
    "\n",
    "#rng = np.random.RandomState(5)\n",
    "#rng.shuffle(x_data)  # in-place\n",
    "#train_split_idx = int(0.7 * len(labels))\n",
    "#test_split_idx  = int(0.9 * len(labels))\n",
    "\n",
    "shuffle_indices = np.random.permutation(np.arange(len(labels)))\n",
    "train_split_idx = int(0.7 * len(labels))\n",
    "test_split_idx  = int(0.9 * len(labels))\n",
    "\n",
    "train_indices = shuffle_indices[:train_split_idx]\n",
    "validation_indices = shuffle_indices[train_split_idx:test_split_idx]\n",
    "test_indices = shuffle_indices[test_split_idx:]\n",
    "\n",
    "\n",
    "train_sentences = np.array(x_data)[train_indices]\n",
    "train_contexts = np.array(x_contexts)[train_indices]\n",
    "train_labels= np.array(labels)[train_indices] \n",
    "validation_sentences = np.array(x_data)[validation_indices]\n",
    "validation_labels = np.array(labels)[validation_indices]\n",
    "validation_contexts = train_contexts = np.array(x_contexts)[validation_indices]\n",
    "test_sentences = np.array(x_data)[test_indices]  \n",
    "test_contexts = train_contexts = np.array(x_contexts)[test_indices]\n",
    "test_labels = np.array(labels)[test_indices]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 6, 2]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [2]*4\n",
    "a[2] = 6\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_labels(raw_label_set, size):\n",
    "    label_set = []\n",
    "    for label in raw_label_set:\n",
    "        labels = [0] * size\n",
    "        labels[label] = 1\n",
    "        label_set.append(labels)\n",
    "    return np.array(label_set)\n",
    "\n",
    "expanded_train_labels = transform_labels(train_labels, 2)\n",
    "expanded_validation_labels = transform_labels(validation_labels,2)\n",
    "expanded_test_labels = transform_labels(test_labels,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4, 5, 6, 7, 8, 8, 1, 5, 6, 7]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,3,4,5,6,7,8,8,1,5,6,7]\n",
    "a + [\"<PADDING>\"]*0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'angry',\n",
       " 'man',\n",
       " 'is',\n",
       " 'angry',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PaddingAndTruncating:\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def pad_or_truncate(self, sentence):\n",
    "        sen_len = len(sentence)\n",
    "        paddings = self.max_len - sen_len\n",
    "        if paddings >=0:\n",
    "            return sentence + [\"<PADDING>\"] * paddings\n",
    "        return sentence[0:paddings]\n",
    "        \n",
    "PadAndTrunc = PaddingAndTruncating(10)\n",
    "        \n",
    "        \n",
    "PadAndTrunc.pad_or_truncate([\"the\",\"angry\",\"man\",\"is\",\"angry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   4,    4,    4, ...,    3,    3,    3],\n",
       "       [  11,    5,    8, ...,    3,    3,    3],\n",
       "       [  11,    5,    8, ...,    2,   15,    3],\n",
       "       ..., \n",
       "       [  11,    5,    8, ...,    3,    3,    3],\n",
       "       [  11,    5,    8, ...,    2,   24, 1368],\n",
       "       [  11,    5,    8, ...,    3,    3,    3]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.contrib import learn\n",
    "\n",
    "#max_len = max([len(sent) for sent  in train_sentences])\n",
    "PadAndTrunc =PaddingAndTruncating(30)\n",
    "train_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, train_sentences))\n",
    "train_context_padded = list(map(PadAndTrunc.pad_or_truncate, train_contexts))\n",
    "validation_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, validation_sentences))\n",
    "validation_context_padded = list(map(PadAndTrunc.pad_or_truncate, validation_contexts))\n",
    "test_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, test_sentences))\n",
    "test_context_padded = list(map(PadAndTrunc.pad_or_truncate, test_contexts))\n",
    "\n",
    "vocab = vocabulary.Vocabulary(utils.flatten(list(train_sentences_padded) + list(train_context_padded)), 2500)\n",
    "train_s = np.array(list(map(vocab.words_to_ids, train_sentences_padded)))\n",
    "train_c = np.array(list(map(vocab.words_to_ids, train_context_padded)))\n",
    "validation_s = np.array(list(map(vocab.words_to_ids, validation_sentences_padded)))\n",
    "validation_c = np.array(list(map(vocab.words_to_ids, validation_context_padded)))\n",
    "test_s = np.array(list(map(vocab.words_to_ids, test_sentences_padded)))\n",
    "test_c = np.array(list(map(vocab.words_to_ids, test_context_padded)))\n",
    "train_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv-maxpool-3'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 3\n",
    "(\"conv-maxpool-%s\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size, \n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x1 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x1\")\n",
    "        self.input_x2 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x2\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars1 = tf.nn.embedding_lookup(self.W, self.input_x1)\n",
    "            self.embedded_chars_expanded1 = tf.expand_dims(self.embedded_chars1, -1)\n",
    "            self.embedded_chars2 = tf.nn.embedding_lookup(self.W, self.input_x2)\n",
    "            self.embedded_chars_expanded2 = tf.expand_dims(self.embedded_chars2, -1)\n",
    "\n",
    "        # Create a convolution + avgpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.variable_scope(\"conv-avgpool-%s\" % i) as scope:\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "              \n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded1,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h1 = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"tanh1\")\n",
    "               \n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.avg_pool(\n",
    "                    h1,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                \n",
    "                pooled_outputs.append(pooled)\n",
    "                #pooled_outputs.append(pooled)\n",
    "                scope.reuse_variables()\n",
    "                 \n",
    "                \n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded2,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h2 = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"tanh2\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled2 = tf.nn.avg_pool(\n",
    "                    h2,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled2)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "\n",
    "        num_filters_total = num_filters * len(filter_sizes) * 2\n",
    "\n",
    "        self.h_pool = tf.concat(pooled_outputs, 1)\n",
    "\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "        \n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "           \n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.labels = tf.argmax(self.input_y, 1)\n",
    "            TP = tf.count_nonzero(self.predictions * self.labels)\n",
    "            TN = tf.count_nonzero((self.predictions - 1) * (self.labels - 1))\n",
    "            FP = tf.count_nonzero(self.predictions * (self.labels - 1))\n",
    "            FN = tf.count_nonzero((self.predictions - 1) * self.labels)\n",
    "            correct_predictions = tf.equal(self.predictions, self.labels)\n",
    "            self.precision = TP / (TP + FP)\n",
    "            self.recall = TP / (TP + FN)\n",
    "            self.f1_score = 2 * self.precision * self.recall / (self.precision + self.recall)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=100\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.5\n",
      "DROPOUT_KEEP_PROB=0.25\n",
      "EMBEDDING_DIM=100\n",
      "EVALUATE_EVERY=20\n",
      "FILTER_SIZES=1\n",
      "L2_REG_LAMBDA=0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.neg\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=500\n",
      "NUM_FILTERS=100\n",
      "POSITIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.pos\n",
      "\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/W:0/grad/hist is illegal; using conv-avgpool-0/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/W:0/grad/sparsity is illegal; using conv-avgpool-0/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/b:0/grad/hist is illegal; using conv-avgpool-0/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/b:0/grad/sparsity is illegal; using conv-avgpool-0/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523839829\n",
      "\n",
      "2018-04-16T00:50:29.587372: step 1, loss 0.789505, acc 0.48\n",
      "2018-04-16T00:50:29.641381: step 2, loss 0.703087, acc 0.58\n",
      "2018-04-16T00:50:29.689719: step 3, loss 0.890191, acc 0.5\n",
      "2018-04-16T00:50:29.731367: step 4, loss 1.71006, acc 0.384615\n",
      "2018-04-16T00:50:29.780852: step 5, loss 0.804545, acc 0.55\n",
      "2018-04-16T00:50:29.837505: step 6, loss 0.650872, acc 0.66\n",
      "2018-04-16T00:50:29.885997: step 7, loss 0.692828, acc 0.58\n",
      "2018-04-16T00:50:29.927569: step 8, loss 0.639362, acc 0.679487\n",
      "2018-04-16T00:50:29.976870: step 9, loss 0.719817, acc 0.55\n",
      "2018-04-16T00:50:30.028430: step 10, loss 0.663568, acc 0.65\n",
      "2018-04-16T00:50:30.084097: step 11, loss 0.626178, acc 0.7\n",
      "2018-04-16T00:50:30.125827: step 12, loss 0.614052, acc 0.666667\n",
      "2018-04-16T00:50:30.174853: step 13, loss 0.631071, acc 0.7\n",
      "2018-04-16T00:50:30.227293: step 14, loss 0.589102, acc 0.75\n",
      "2018-04-16T00:50:30.280091: step 15, loss 0.640783, acc 0.7\n",
      "2018-04-16T00:50:30.325451: step 16, loss 0.686635, acc 0.628205\n",
      "2018-04-16T00:50:30.374718: step 17, loss 0.642431, acc 0.66\n",
      "2018-04-16T00:50:30.426501: step 18, loss 0.545162, acc 0.78\n",
      "2018-04-16T00:50:30.478412: step 19, loss 0.689936, acc 0.64\n",
      "2018-04-16T00:50:30.520030: step 20, loss 0.84307, acc 0.525641\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:50:30.651822: step 20, loss 0.699216, acc 0.513263, rec 0.0134409, pre 1, f1 0.0265252\n",
      "\n",
      "2018-04-16T00:50:30.699613: step 21, loss 0.719997, acc 0.55\n",
      "2018-04-16T00:50:30.747010: step 22, loss 0.758389, acc 0.56\n",
      "2018-04-16T00:50:30.796172: step 23, loss 0.684151, acc 0.65\n",
      "2018-04-16T00:50:30.835375: step 24, loss 0.682557, acc 0.653846\n",
      "2018-04-16T00:50:30.887321: step 25, loss 0.528573, acc 0.82\n",
      "2018-04-16T00:50:30.934545: step 26, loss 0.568748, acc 0.73\n",
      "2018-04-16T00:50:30.982235: step 27, loss 0.664393, acc 0.63\n",
      "2018-04-16T00:50:31.022038: step 28, loss 0.601818, acc 0.653846\n",
      "2018-04-16T00:50:31.069308: step 29, loss 0.549912, acc 0.74\n",
      "2018-04-16T00:50:31.120031: step 30, loss 0.626891, acc 0.65\n",
      "2018-04-16T00:50:31.167169: step 31, loss 0.687263, acc 0.64\n",
      "2018-04-16T00:50:31.206014: step 32, loss 0.627833, acc 0.615385\n",
      "2018-04-16T00:50:31.253995: step 33, loss 0.696438, acc 0.62\n",
      "2018-04-16T00:50:31.302516: step 34, loss 0.581093, acc 0.78\n",
      "2018-04-16T00:50:31.354437: step 35, loss 0.566582, acc 0.75\n",
      "2018-04-16T00:50:31.393109: step 36, loss 0.627064, acc 0.74359\n",
      "2018-04-16T00:50:31.440460: step 37, loss 0.67657, acc 0.65\n",
      "2018-04-16T00:50:31.488899: step 38, loss 0.678408, acc 0.65\n",
      "2018-04-16T00:50:31.536841: step 39, loss 0.675111, acc 0.65\n",
      "2018-04-16T00:50:31.579974: step 40, loss 0.54403, acc 0.730769\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:50:31.675585: step 40, loss 1.00787, acc 0.47878, rec 0.948925, pre 0.485557, f1 0.642402\n",
      "\n",
      "2018-04-16T00:50:31.723313: step 41, loss 0.657229, acc 0.65\n",
      "2018-04-16T00:50:31.771627: step 42, loss 0.605047, acc 0.71\n",
      "2018-04-16T00:50:31.824316: step 43, loss 0.549819, acc 0.75\n",
      "2018-04-16T00:50:31.865220: step 44, loss 0.498034, acc 0.820513\n",
      "2018-04-16T00:50:31.912915: step 45, loss 0.511666, acc 0.76\n",
      "2018-04-16T00:50:31.961101: step 46, loss 0.613745, acc 0.69\n",
      "2018-04-16T00:50:32.011516: step 47, loss 0.531932, acc 0.76\n",
      "2018-04-16T00:50:32.056354: step 48, loss 0.607057, acc 0.717949\n",
      "2018-04-16T00:50:32.105389: step 49, loss 0.517548, acc 0.8\n",
      "2018-04-16T00:50:32.153843: step 50, loss 0.482641, acc 0.78\n",
      "2018-04-16T00:50:32.202415: step 51, loss 0.657507, acc 0.69\n",
      "2018-04-16T00:50:32.242861: step 52, loss 0.938885, acc 0.615385\n",
      "2018-04-16T00:50:32.294784: step 53, loss 0.734987, acc 0.59\n",
      "2018-04-16T00:50:32.343275: step 54, loss 0.498975, acc 0.8\n",
      "2018-04-16T00:50:32.390686: step 55, loss 0.5845, acc 0.76\n",
      "2018-04-16T00:50:32.429114: step 56, loss 0.601072, acc 0.74359\n",
      "2018-04-16T00:50:32.476412: step 57, loss 0.626482, acc 0.67\n",
      "2018-04-16T00:50:32.527022: step 58, loss 0.758226, acc 0.6\n",
      "2018-04-16T00:50:32.574700: step 59, loss 0.585701, acc 0.64\n",
      "2018-04-16T00:50:32.612624: step 60, loss 0.638859, acc 0.705128\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:50:32.713484: step 60, loss 0.636234, acc 0.625995, rec 0.274194, pre 0.894737, f1 0.419753\n",
      "\n",
      "2018-04-16T00:50:32.770687: step 61, loss 0.541945, acc 0.72\n",
      "2018-04-16T00:50:32.823137: step 62, loss 0.602609, acc 0.74\n",
      "2018-04-16T00:50:32.875453: step 63, loss 0.573655, acc 0.71\n",
      "2018-04-16T00:50:32.919042: step 64, loss 0.560721, acc 0.717949\n",
      "2018-04-16T00:50:32.976754: step 65, loss 0.489795, acc 0.73\n",
      "2018-04-16T00:50:33.029676: step 66, loss 0.563441, acc 0.72\n",
      "2018-04-16T00:50:33.083084: step 67, loss 0.729992, acc 0.69\n",
      "2018-04-16T00:50:33.127054: step 68, loss 0.609442, acc 0.679487\n",
      "2018-04-16T00:50:33.184531: step 69, loss 0.671663, acc 0.66\n",
      "2018-04-16T00:50:33.236816: step 70, loss 0.55934, acc 0.7\n",
      "2018-04-16T00:50:33.290992: step 71, loss 0.515764, acc 0.81\n",
      "2018-04-16T00:50:33.337665: step 72, loss 0.496012, acc 0.794872\n",
      "2018-04-16T00:50:33.395570: step 73, loss 0.48512, acc 0.76\n",
      "2018-04-16T00:50:33.449933: step 74, loss 0.478459, acc 0.84\n",
      "2018-04-16T00:50:33.505941: step 75, loss 0.581237, acc 0.73\n",
      "2018-04-16T00:50:33.550016: step 76, loss 0.618366, acc 0.692308\n",
      "2018-04-16T00:50:33.607142: step 77, loss 0.692309, acc 0.73\n",
      "2018-04-16T00:50:33.660764: step 78, loss 0.685324, acc 0.62\n",
      "2018-04-16T00:50:33.717141: step 79, loss 0.545409, acc 0.76\n",
      "2018-04-16T00:50:33.759948: step 80, loss 0.447038, acc 0.820513\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:50:33.870131: step 80, loss 0.809001, acc 0.486737, rec 0.930108, pre 0.489392, f1 0.641335\n",
      "\n",
      "2018-04-16T00:50:33.924632: step 81, loss 0.691627, acc 0.78\n",
      "2018-04-16T00:50:33.978300: step 82, loss 0.463459, acc 0.74\n",
      "2018-04-16T00:50:34.030547: step 83, loss 0.45855, acc 0.79\n",
      "2018-04-16T00:50:34.079928: step 84, loss 0.521303, acc 0.769231\n",
      "2018-04-16T00:50:34.132829: step 85, loss 0.621409, acc 0.66\n",
      "2018-04-16T00:50:34.190558: step 86, loss 0.64454, acc 0.74\n",
      "2018-04-16T00:50:34.244798: step 87, loss 0.572815, acc 0.79\n",
      "2018-04-16T00:50:34.292502: step 88, loss 0.503343, acc 0.769231\n",
      "2018-04-16T00:50:34.344835: step 89, loss 0.48999, acc 0.75\n",
      "2018-04-16T00:50:34.396617: step 90, loss 0.471892, acc 0.83\n",
      "2018-04-16T00:50:34.451025: step 91, loss 0.50088, acc 0.74\n",
      "2018-04-16T00:50:34.499331: step 92, loss 0.782564, acc 0.666667\n",
      "2018-04-16T00:50:34.552365: step 93, loss 0.532128, acc 0.72\n",
      "2018-04-16T00:50:34.606432: step 94, loss 0.584533, acc 0.68\n",
      "2018-04-16T00:50:34.659868: step 95, loss 0.639295, acc 0.7\n",
      "2018-04-16T00:50:34.706492: step 96, loss 0.711484, acc 0.666667\n",
      "2018-04-16T00:50:34.759322: step 97, loss 0.769349, acc 0.6\n",
      "2018-04-16T00:50:34.811418: step 98, loss 0.637784, acc 0.71\n",
      "2018-04-16T00:50:34.866351: step 99, loss 0.45879, acc 0.81\n",
      "2018-04-16T00:50:34.913422: step 100, loss 0.521184, acc 0.782051\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:50:35.016993: step 100, loss 0.582244, acc 0.742706, rec 0.551075, pre 0.883621, f1 0.678808\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523839829/checkpoints/model-100\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:50:35.144075: step 101, loss 0.482973, acc 0.83\n",
      "2018-04-16T00:50:35.196162: step 102, loss 0.484458, acc 0.77\n",
      "2018-04-16T00:50:35.248579: step 103, loss 0.564229, acc 0.69\n",
      "2018-04-16T00:50:35.291047: step 104, loss 0.558755, acc 0.794872\n",
      "2018-04-16T00:50:35.343269: step 105, loss 0.623146, acc 0.69\n",
      "2018-04-16T00:50:35.400952: step 106, loss 0.584778, acc 0.73\n",
      "2018-04-16T00:50:35.453741: step 107, loss 0.448503, acc 0.81\n",
      "2018-04-16T00:50:35.497291: step 108, loss 0.457528, acc 0.75641\n",
      "2018-04-16T00:50:35.550409: step 109, loss 0.403306, acc 0.87\n",
      "2018-04-16T00:50:35.606481: step 110, loss 0.53684, acc 0.75\n",
      "2018-04-16T00:50:35.657819: step 111, loss 0.487241, acc 0.77\n",
      "2018-04-16T00:50:35.699986: step 112, loss 0.518402, acc 0.730769\n",
      "2018-04-16T00:50:35.754265: step 113, loss 0.587044, acc 0.75\n",
      "2018-04-16T00:50:35.806130: step 114, loss 0.484983, acc 0.8\n",
      "2018-04-16T00:50:35.863528: step 115, loss 0.367488, acc 0.89\n",
      "2018-04-16T00:50:35.908866: step 116, loss 0.541683, acc 0.807692\n",
      "2018-04-16T00:50:35.961726: step 117, loss 0.513288, acc 0.79\n",
      "2018-04-16T00:50:36.013810: step 118, loss 0.632686, acc 0.68\n",
      "2018-04-16T00:50:36.069957: step 119, loss 0.592465, acc 0.77\n",
      "2018-04-16T00:50:36.112912: step 120, loss 0.533291, acc 0.730769\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:50:36.213196: step 120, loss 0.598396, acc 0.645889, rec 0.301075, pre 0.941176, f1 0.456212\n",
      "\n",
      "2018-04-16T00:50:36.265354: step 121, loss 0.467705, acc 0.76\n",
      "2018-04-16T00:50:36.321789: step 122, loss 0.719557, acc 0.71\n",
      "2018-04-16T00:50:36.373836: step 123, loss 0.55588, acc 0.73\n",
      "2018-04-16T00:50:36.416379: step 124, loss 0.460327, acc 0.782051\n",
      "2018-04-16T00:50:36.468078: step 125, loss 0.44964, acc 0.79\n",
      "2018-04-16T00:50:36.520143: step 126, loss 0.49186, acc 0.79\n",
      "2018-04-16T00:50:36.576834: step 127, loss 0.49174, acc 0.78\n",
      "2018-04-16T00:50:36.619572: step 128, loss 0.726071, acc 0.692308\n",
      "2018-04-16T00:50:36.673309: step 129, loss 0.472602, acc 0.78\n",
      "2018-04-16T00:50:36.725955: step 130, loss 0.538576, acc 0.79\n",
      "2018-04-16T00:50:36.783145: step 131, loss 0.569046, acc 0.71\n",
      "2018-04-16T00:50:36.826683: step 132, loss 0.427217, acc 0.820513\n",
      "2018-04-16T00:50:36.880464: step 133, loss 0.417249, acc 0.83\n",
      "2018-04-16T00:50:36.933933: step 134, loss 0.53089, acc 0.77\n",
      "2018-04-16T00:50:36.989349: step 135, loss 0.38912, acc 0.8\n",
      "2018-04-16T00:50:37.032420: step 136, loss 0.479153, acc 0.75641\n",
      "2018-04-16T00:50:37.085414: step 137, loss 0.455622, acc 0.81\n",
      "2018-04-16T00:50:37.137995: step 138, loss 0.415491, acc 0.87\n",
      "2018-04-16T00:50:37.194536: step 139, loss 0.546861, acc 0.74\n",
      "2018-04-16T00:50:37.239111: step 140, loss 0.548031, acc 0.74359\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:50:37.343386: step 140, loss 0.556701, acc 0.722812, rec 0.494624, pre 0.897561, f1 0.637782\n",
      "\n",
      "2018-04-16T00:50:37.401415: step 141, loss 0.512758, acc 0.74\n",
      "2018-04-16T00:50:37.454342: step 142, loss 0.421214, acc 0.85\n",
      "2018-04-16T00:50:37.507965: step 143, loss 0.541238, acc 0.72\n",
      "2018-04-16T00:50:37.552460: step 144, loss 0.653208, acc 0.705128\n",
      "2018-04-16T00:50:37.610341: step 145, loss 0.470139, acc 0.82\n",
      "2018-04-16T00:50:37.662762: step 146, loss 0.537985, acc 0.76\n",
      "2018-04-16T00:50:37.717334: step 147, loss 0.592949, acc 0.68\n",
      "2018-04-16T00:50:37.760179: step 148, loss 0.452881, acc 0.820513\n",
      "2018-04-16T00:50:37.815958: step 149, loss 0.482195, acc 0.81\n",
      "2018-04-16T00:50:37.872078: step 150, loss 0.396811, acc 0.83\n",
      "2018-04-16T00:50:37.926064: step 151, loss 0.445125, acc 0.81\n",
      "2018-04-16T00:50:37.971932: step 152, loss 0.460563, acc 0.807692\n",
      "2018-04-16T00:50:38.029543: step 153, loss 0.430019, acc 0.83\n",
      "2018-04-16T00:50:38.083295: step 154, loss 0.442502, acc 0.78\n",
      "2018-04-16T00:50:38.136321: step 155, loss 0.478873, acc 0.81\n",
      "2018-04-16T00:50:38.178949: step 156, loss 0.409148, acc 0.807692\n",
      "2018-04-16T00:50:38.237259: step 157, loss 0.3946, acc 0.86\n",
      "2018-04-16T00:50:38.289700: step 158, loss 0.338865, acc 0.86\n",
      "2018-04-16T00:50:38.342103: step 159, loss 0.408192, acc 0.85\n",
      "2018-04-16T00:50:38.385480: step 160, loss 0.500166, acc 0.782051\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:50:38.489841: step 160, loss 0.685979, acc 0.587533, rec 0.174731, pre 0.942029, f1 0.294785\n",
      "\n",
      "2018-04-16T00:50:38.541887: step 161, loss 0.462912, acc 0.74\n",
      "2018-04-16T00:50:38.595739: step 162, loss 0.89364, acc 0.71\n",
      "2018-04-16T00:50:38.648297: step 163, loss 0.610098, acc 0.68\n",
      "2018-04-16T00:50:38.696053: step 164, loss 0.640269, acc 0.705128\n",
      "2018-04-16T00:50:38.749403: step 165, loss 0.450468, acc 0.81\n",
      "2018-04-16T00:50:38.802693: step 166, loss 0.426806, acc 0.79\n",
      "2018-04-16T00:50:38.854497: step 167, loss 0.419068, acc 0.84\n",
      "2018-04-16T00:50:38.903940: step 168, loss 0.411161, acc 0.807692\n",
      "2018-04-16T00:50:38.963461: step 169, loss 0.3635, acc 0.83\n",
      "2018-04-16T00:50:39.016788: step 170, loss 0.361729, acc 0.85\n",
      "2018-04-16T00:50:39.069402: step 171, loss 0.373356, acc 0.84\n",
      "2018-04-16T00:50:39.118729: step 172, loss 0.724622, acc 0.692308\n",
      "2018-04-16T00:50:39.172904: step 173, loss 0.59642, acc 0.75\n",
      "2018-04-16T00:50:39.224644: step 174, loss 0.485804, acc 0.81\n",
      "2018-04-16T00:50:39.277693: step 175, loss 0.53506, acc 0.76\n",
      "2018-04-16T00:50:39.323672: step 176, loss 0.490211, acc 0.74359\n",
      "2018-04-16T00:50:39.377132: step 177, loss 0.501131, acc 0.75\n",
      "2018-04-16T00:50:39.432894: step 178, loss 0.510007, acc 0.78\n",
      "2018-04-16T00:50:39.484941: step 179, loss 0.389089, acc 0.85\n",
      "2018-04-16T00:50:39.531342: step 180, loss 0.376707, acc 0.884615\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:50:39.633683: step 180, loss 0.633981, acc 0.623342, rec 0.25, pre 0.94898, f1 0.395745\n",
      "\n",
      "2018-04-16T00:50:39.687908: step 181, loss 0.617991, acc 0.72\n",
      "2018-04-16T00:50:39.745523: step 182, loss 0.456129, acc 0.83\n",
      "2018-04-16T00:50:39.797359: step 183, loss 0.425353, acc 0.85\n",
      "2018-04-16T00:50:39.840066: step 184, loss 0.435331, acc 0.820513\n",
      "2018-04-16T00:50:39.892414: step 185, loss 0.422805, acc 0.79\n",
      "2018-04-16T00:50:39.950563: step 186, loss 0.427649, acc 0.84\n",
      "2018-04-16T00:50:40.003698: step 187, loss 0.44768, acc 0.83\n",
      "2018-04-16T00:50:40.047265: step 188, loss 0.437033, acc 0.820513\n",
      "2018-04-16T00:50:40.100971: step 189, loss 0.346263, acc 0.84\n",
      "2018-04-16T00:50:40.158695: step 190, loss 0.616777, acc 0.75\n",
      "2018-04-16T00:50:40.211273: step 191, loss 0.710889, acc 0.67\n",
      "2018-04-16T00:50:40.256084: step 192, loss 0.561244, acc 0.820513\n",
      "2018-04-16T00:50:40.308653: step 193, loss 0.500747, acc 0.84\n",
      "2018-04-16T00:50:40.364512: step 194, loss 0.573798, acc 0.74\n",
      "2018-04-16T00:50:40.418279: step 195, loss 0.450103, acc 0.82\n",
      "2018-04-16T00:50:40.461650: step 196, loss 0.445762, acc 0.782051\n",
      "2018-04-16T00:50:40.515830: step 197, loss 0.383085, acc 0.84\n",
      "2018-04-16T00:50:40.572873: step 198, loss 0.337051, acc 0.9\n",
      "2018-04-16T00:50:40.625055: step 199, loss 0.399872, acc 0.84\n",
      "2018-04-16T00:50:40.668846: step 200, loss 0.502348, acc 0.769231\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:50:40.769174: step 200, loss 0.564666, acc 0.697613, rec 0.448925, pre 0.878947, f1 0.594306\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523839829/checkpoints/model-200\n",
      "\n",
      "2018-04-16T00:50:40.885812: step 201, loss 0.383517, acc 0.82\n",
      "2018-04-16T00:50:40.940168: step 202, loss 0.397943, acc 0.81\n",
      "2018-04-16T00:50:40.993627: step 203, loss 0.484931, acc 0.81\n",
      "2018-04-16T00:50:41.040955: step 204, loss 0.591141, acc 0.782051\n",
      "2018-04-16T00:50:41.093720: step 205, loss 0.454882, acc 0.8\n",
      "2018-04-16T00:50:41.146792: step 206, loss 0.419043, acc 0.85\n",
      "2018-04-16T00:50:41.199327: step 207, loss 0.399246, acc 0.82\n",
      "2018-04-16T00:50:41.246642: step 208, loss 0.346353, acc 0.871795\n",
      "2018-04-16T00:50:41.300905: step 209, loss 0.457093, acc 0.8\n",
      "2018-04-16T00:50:41.353047: step 210, loss 0.486118, acc 0.8\n",
      "2018-04-16T00:50:41.405103: step 211, loss 0.411027, acc 0.82\n",
      "2018-04-16T00:50:41.454533: step 212, loss 0.497346, acc 0.769231\n",
      "2018-04-16T00:50:41.507566: step 213, loss 0.620941, acc 0.75\n",
      "2018-04-16T00:50:41.561315: step 214, loss 0.537842, acc 0.76\n",
      "2018-04-16T00:50:41.612904: step 215, loss 0.525753, acc 0.77\n",
      "2018-04-16T00:50:41.659644: step 216, loss 0.408606, acc 0.794872\n",
      "2018-04-16T00:50:41.713233: step 217, loss 0.339588, acc 0.9\n",
      "2018-04-16T00:50:41.766433: step 218, loss 0.455116, acc 0.76\n",
      "2018-04-16T00:50:41.819239: step 219, loss 0.481361, acc 0.75\n",
      "2018-04-16T00:50:41.875452: step 220, loss 0.316656, acc 0.858974\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:50:41.980175: step 220, loss 0.664406, acc 0.612732, rec 0.895161, pre 0.568259, f1 0.695198\n",
      "\n",
      "2018-04-16T00:50:42.033551: step 221, loss 0.324851, acc 0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:50:42.092997: step 222, loss 0.428279, acc 0.82\n",
      "2018-04-16T00:50:42.145114: step 223, loss 0.417949, acc 0.85\n",
      "2018-04-16T00:50:42.187861: step 224, loss 0.329757, acc 0.833333\n",
      "2018-04-16T00:50:42.241702: step 225, loss 0.359861, acc 0.86\n",
      "2018-04-16T00:50:42.300588: step 226, loss 0.426634, acc 0.82\n",
      "2018-04-16T00:50:42.354433: step 227, loss 0.386732, acc 0.8\n",
      "2018-04-16T00:50:42.398018: step 228, loss 0.475828, acc 0.833333\n",
      "2018-04-16T00:50:42.454231: step 229, loss 0.476819, acc 0.83\n",
      "2018-04-16T00:50:42.523294: step 230, loss 0.367058, acc 0.83\n",
      "2018-04-16T00:50:42.583804: step 231, loss 0.30273, acc 0.87\n",
      "2018-04-16T00:50:42.642549: step 232, loss 0.425457, acc 0.807692\n",
      "2018-04-16T00:50:42.694466: step 233, loss 0.349922, acc 0.86\n",
      "2018-04-16T00:50:42.750228: step 234, loss 0.394163, acc 0.84\n",
      "2018-04-16T00:50:42.802954: step 235, loss 0.449114, acc 0.83\n",
      "2018-04-16T00:50:42.847630: step 236, loss 0.476021, acc 0.74359\n",
      "2018-04-16T00:50:42.901484: step 237, loss 0.465869, acc 0.81\n",
      "2018-04-16T00:50:42.961402: step 238, loss 0.369458, acc 0.83\n",
      "2018-04-16T00:50:43.014645: step 239, loss 0.549238, acc 0.79\n",
      "2018-04-16T00:50:43.057955: step 240, loss 0.540736, acc 0.75641\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:50:43.160149: step 240, loss 1.28337, acc 0.488064, rec 0.975806, pre 0.490541, f1 0.652878\n",
      "\n",
      "2018-04-16T00:50:43.218306: step 241, loss 0.472329, acc 0.87\n",
      "2018-04-16T00:50:43.270334: step 242, loss 0.327387, acc 0.86\n",
      "2018-04-16T00:50:43.323128: step 243, loss 0.409464, acc 0.83\n",
      "2018-04-16T00:50:43.366817: step 244, loss 0.418876, acc 0.833333\n",
      "2018-04-16T00:50:43.430406: step 245, loss 0.367191, acc 0.85\n",
      "2018-04-16T00:50:43.490930: step 246, loss 0.443084, acc 0.82\n",
      "2018-04-16T00:50:43.555011: step 247, loss 0.396795, acc 0.79\n",
      "2018-04-16T00:50:43.599347: step 248, loss 0.311367, acc 0.884615\n",
      "2018-04-16T00:50:43.656897: step 249, loss 0.421466, acc 0.81\n",
      "2018-04-16T00:50:43.713312: step 250, loss 0.359829, acc 0.8\n",
      "2018-04-16T00:50:43.765607: step 251, loss 0.341512, acc 0.9\n",
      "2018-04-16T00:50:43.809116: step 252, loss 0.41256, acc 0.858974\n",
      "2018-04-16T00:50:43.870026: step 253, loss 0.560255, acc 0.72\n",
      "2018-04-16T00:50:43.927638: step 254, loss 0.467361, acc 0.8\n",
      "2018-04-16T00:50:43.988330: step 255, loss 0.4621, acc 0.76\n",
      "2018-04-16T00:50:44.033025: step 256, loss 0.441923, acc 0.794872\n",
      "2018-04-16T00:50:44.091514: step 257, loss 0.343311, acc 0.83\n",
      "2018-04-16T00:50:44.148658: step 258, loss 0.384429, acc 0.83\n",
      "2018-04-16T00:50:44.202571: step 259, loss 0.518663, acc 0.82\n",
      "2018-04-16T00:50:44.246348: step 260, loss 0.471104, acc 0.782051\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:50:44.351542: step 260, loss 0.617465, acc 0.647215, rec 0.303763, pre 0.941667, f1 0.45935\n",
      "\n",
      "2018-04-16T00:50:44.403774: step 261, loss 0.277114, acc 0.87\n",
      "2018-04-16T00:50:44.456296: step 262, loss 0.319744, acc 0.84\n",
      "2018-04-16T00:50:44.508353: step 263, loss 0.415849, acc 0.82\n",
      "2018-04-16T00:50:44.558772: step 264, loss 0.481782, acc 0.74359\n",
      "2018-04-16T00:50:44.611264: step 265, loss 0.398898, acc 0.89\n",
      "2018-04-16T00:50:44.663331: step 266, loss 0.324354, acc 0.86\n",
      "2018-04-16T00:50:44.716051: step 267, loss 0.395177, acc 0.82\n",
      "2018-04-16T00:50:44.758783: step 268, loss 0.331742, acc 0.846154\n",
      "2018-04-16T00:50:44.815711: step 269, loss 0.29709, acc 0.89\n",
      "2018-04-16T00:50:44.868016: step 270, loss 0.351712, acc 0.83\n",
      "2018-04-16T00:50:44.920339: step 271, loss 0.377765, acc 0.83\n",
      "2018-04-16T00:50:44.963303: step 272, loss 0.307769, acc 0.871795\n",
      "2018-04-16T00:50:45.016644: step 273, loss 0.331668, acc 0.86\n",
      "2018-04-16T00:50:45.071504: step 274, loss 0.334637, acc 0.86\n",
      "2018-04-16T00:50:45.123370: step 275, loss 0.752882, acc 0.7\n",
      "2018-04-16T00:50:45.165573: step 276, loss 0.763775, acc 0.75641\n",
      "2018-04-16T00:50:45.216990: step 277, loss 0.329249, acc 0.9\n",
      "2018-04-16T00:50:45.269189: step 278, loss 0.349759, acc 0.86\n",
      "2018-04-16T00:50:45.324237: step 279, loss 0.351439, acc 0.85\n",
      "2018-04-16T00:50:45.367545: step 280, loss 0.435586, acc 0.782051\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:50:45.467404: step 280, loss 0.573874, acc 0.690981, rec 0.408602, pre 0.921212, f1 0.566108\n",
      "\n",
      "2018-04-16T00:50:45.518935: step 281, loss 0.407468, acc 0.79\n",
      "2018-04-16T00:50:45.575041: step 282, loss 0.299674, acc 0.9\n",
      "2018-04-16T00:50:45.627246: step 283, loss 0.514415, acc 0.79\n",
      "2018-04-16T00:50:45.670302: step 284, loss 0.2943, acc 0.858974\n",
      "2018-04-16T00:50:45.723649: step 285, loss 0.227412, acc 0.92\n",
      "2018-04-16T00:50:45.775435: step 286, loss 0.426412, acc 0.82\n",
      "2018-04-16T00:50:45.830217: step 287, loss 0.326711, acc 0.86\n",
      "2018-04-16T00:50:45.872759: step 288, loss 0.397549, acc 0.794872\n",
      "2018-04-16T00:50:45.924681: step 289, loss 0.480237, acc 0.83\n",
      "2018-04-16T00:50:45.976445: step 290, loss 0.507109, acc 0.76\n",
      "2018-04-16T00:50:46.028986: step 291, loss 0.329435, acc 0.83\n",
      "2018-04-16T00:50:46.074802: step 292, loss 0.22825, acc 0.910256\n",
      "2018-04-16T00:50:46.127211: step 293, loss 0.311065, acc 0.85\n",
      "2018-04-16T00:50:46.180439: step 294, loss 0.274844, acc 0.91\n",
      "2018-04-16T00:50:46.232581: step 295, loss 0.368474, acc 0.83\n",
      "2018-04-16T00:50:46.275255: step 296, loss 0.747671, acc 0.666667\n",
      "2018-04-16T00:50:46.331226: step 297, loss 0.79742, acc 0.75\n",
      "2018-04-16T00:50:46.383151: step 298, loss 0.425968, acc 0.85\n",
      "2018-04-16T00:50:46.435511: step 299, loss 0.360785, acc 0.83\n",
      "2018-04-16T00:50:46.478434: step 300, loss 0.267116, acc 0.910256\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:50:46.585023: step 300, loss 0.563352, acc 0.70557, rec 0.827957, pre 0.660944, f1 0.735084\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523839829/checkpoints/model-300\n",
      "\n",
      "2018-04-16T00:50:46.691461: step 301, loss 0.31654, acc 0.86\n",
      "2018-04-16T00:50:46.743261: step 302, loss 0.333852, acc 0.9\n",
      "2018-04-16T00:50:46.799919: step 303, loss 0.302395, acc 0.89\n",
      "2018-04-16T00:50:46.842821: step 304, loss 0.420058, acc 0.833333\n",
      "2018-04-16T00:50:46.894736: step 305, loss 0.312985, acc 0.85\n",
      "2018-04-16T00:50:46.946739: step 306, loss 0.283864, acc 0.91\n",
      "2018-04-16T00:50:46.999283: step 307, loss 0.379232, acc 0.84\n",
      "2018-04-16T00:50:47.046186: step 308, loss 0.333277, acc 0.884615\n",
      "2018-04-16T00:50:47.098641: step 309, loss 0.37455, acc 0.84\n",
      "2018-04-16T00:50:47.150792: step 310, loss 0.298491, acc 0.84\n",
      "2018-04-16T00:50:47.203210: step 311, loss 0.424335, acc 0.83\n",
      "2018-04-16T00:50:47.246223: step 312, loss 0.372947, acc 0.858974\n",
      "2018-04-16T00:50:47.302243: step 313, loss 0.304856, acc 0.9\n",
      "2018-04-16T00:50:47.355617: step 314, loss 0.502635, acc 0.73\n",
      "2018-04-16T00:50:47.407510: step 315, loss 0.61445, acc 0.76\n",
      "2018-04-16T00:50:47.450197: step 316, loss 0.344713, acc 0.884615\n",
      "2018-04-16T00:50:47.502003: step 317, loss 0.355458, acc 0.83\n",
      "2018-04-16T00:50:47.558337: step 318, loss 0.405687, acc 0.8\n",
      "2018-04-16T00:50:47.614795: step 319, loss 0.537163, acc 0.81\n",
      "2018-04-16T00:50:47.657873: step 320, loss 0.31619, acc 0.884615\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:50:47.758795: step 320, loss 0.627981, acc 0.66313, rec 0.905914, pre 0.606115, f1 0.726293\n",
      "\n",
      "2018-04-16T00:50:47.813809: step 321, loss 0.360588, acc 0.84\n",
      "2018-04-16T00:50:47.867110: step 322, loss 0.363967, acc 0.86\n",
      "2018-04-16T00:50:47.919408: step 323, loss 0.319806, acc 0.86\n",
      "2018-04-16T00:50:47.962097: step 324, loss 0.247706, acc 0.910256\n",
      "2018-04-16T00:50:48.019424: step 325, loss 0.258007, acc 0.89\n",
      "2018-04-16T00:50:48.071682: step 326, loss 0.430101, acc 0.84\n",
      "2018-04-16T00:50:48.123982: step 327, loss 0.469039, acc 0.77\n",
      "2018-04-16T00:50:48.167300: step 328, loss 0.573113, acc 0.730769\n",
      "2018-04-16T00:50:48.224141: step 329, loss 0.556667, acc 0.77\n",
      "2018-04-16T00:50:48.277326: step 330, loss 0.53275, acc 0.79\n",
      "2018-04-16T00:50:48.329559: step 331, loss 0.385414, acc 0.82\n",
      "2018-04-16T00:50:48.372973: step 332, loss 0.275291, acc 0.871795\n",
      "2018-04-16T00:50:48.430125: step 333, loss 0.277447, acc 0.89\n",
      "2018-04-16T00:50:48.482128: step 334, loss 0.395835, acc 0.81\n",
      "2018-04-16T00:50:48.533920: step 335, loss 0.280859, acc 0.88\n",
      "2018-04-16T00:50:48.576362: step 336, loss 0.269278, acc 0.910256\n",
      "2018-04-16T00:50:48.628472: step 337, loss 0.289278, acc 0.88\n",
      "2018-04-16T00:50:48.684562: step 338, loss 0.490556, acc 0.77\n",
      "2018-04-16T00:50:48.736961: step 339, loss 0.616353, acc 0.76\n",
      "2018-04-16T00:50:48.781224: step 340, loss 0.389777, acc 0.807692\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:50:48.882223: step 340, loss 1.10802, acc 0.485411, rec 0.951613, pre 0.48895, f1 0.645985\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:50:48.946166: step 341, loss 0.407904, acc 0.85\n",
      "2018-04-16T00:50:49.002058: step 342, loss 0.401701, acc 0.83\n",
      "2018-04-16T00:50:49.055263: step 343, loss 0.31163, acc 0.89\n",
      "2018-04-16T00:50:49.099032: step 344, loss 0.241884, acc 0.910256\n",
      "2018-04-16T00:50:49.157654: step 345, loss 0.291629, acc 0.88\n",
      "2018-04-16T00:50:49.209896: step 346, loss 0.375548, acc 0.86\n",
      "2018-04-16T00:50:49.261828: step 347, loss 0.29777, acc 0.83\n",
      "2018-04-16T00:50:49.304968: step 348, loss 0.408635, acc 0.858974\n",
      "2018-04-16T00:50:49.363832: step 349, loss 0.251233, acc 0.89\n",
      "2018-04-16T00:50:49.415776: step 350, loss 0.272566, acc 0.92\n",
      "2018-04-16T00:50:49.467519: step 351, loss 0.269021, acc 0.92\n",
      "2018-04-16T00:50:49.511158: step 352, loss 0.382392, acc 0.833333\n",
      "2018-04-16T00:50:49.563558: step 353, loss 0.333392, acc 0.85\n",
      "2018-04-16T00:50:49.622742: step 354, loss 0.294289, acc 0.89\n",
      "2018-04-16T00:50:49.675240: step 355, loss 0.306588, acc 0.87\n",
      "2018-04-16T00:50:49.718201: step 356, loss 0.386538, acc 0.897436\n",
      "2018-04-16T00:50:49.770258: step 357, loss 0.344218, acc 0.86\n",
      "2018-04-16T00:50:49.822430: step 358, loss 0.237045, acc 0.96\n",
      "2018-04-16T00:50:49.879166: step 359, loss 0.301362, acc 0.85\n",
      "2018-04-16T00:50:49.922725: step 360, loss 0.28101, acc 0.858974\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:50:50.024542: step 360, loss 0.843075, acc 0.559682, rec 0.951613, pre 0.52994, f1 0.680769\n",
      "\n",
      "2018-04-16T00:50:50.075949: step 361, loss 0.24222, acc 0.91\n",
      "2018-04-16T00:50:50.131022: step 362, loss 0.22798, acc 0.89\n",
      "2018-04-16T00:50:50.183247: step 363, loss 0.37186, acc 0.85\n",
      "2018-04-16T00:50:50.226000: step 364, loss 0.306091, acc 0.910256\n",
      "2018-04-16T00:50:50.278296: step 365, loss 0.251858, acc 0.88\n",
      "2018-04-16T00:50:50.330164: step 366, loss 0.249769, acc 0.91\n",
      "2018-04-16T00:50:50.387511: step 367, loss 0.266423, acc 0.89\n",
      "2018-04-16T00:50:50.430428: step 368, loss 0.608696, acc 0.717949\n",
      "2018-04-16T00:50:50.482924: step 369, loss 0.896045, acc 0.7\n",
      "2018-04-16T00:50:50.535084: step 370, loss 0.499117, acc 0.82\n",
      "2018-04-16T00:50:50.587257: step 371, loss 0.317155, acc 0.88\n",
      "2018-04-16T00:50:50.633377: step 372, loss 0.21933, acc 0.910256\n",
      "2018-04-16T00:50:50.685421: step 373, loss 0.268883, acc 0.9\n",
      "2018-04-16T00:50:50.737420: step 374, loss 0.352744, acc 0.82\n",
      "2018-04-16T00:50:50.789521: step 375, loss 0.242767, acc 0.92\n",
      "2018-04-16T00:50:50.832041: step 376, loss 0.246131, acc 0.871795\n",
      "2018-04-16T00:50:50.886963: step 377, loss 0.271854, acc 0.86\n",
      "2018-04-16T00:50:50.938981: step 378, loss 0.226848, acc 0.9\n",
      "2018-04-16T00:50:50.990858: step 379, loss 0.282645, acc 0.88\n",
      "2018-04-16T00:50:51.033438: step 380, loss 0.334312, acc 0.858974\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:50:51.139881: step 380, loss 0.787757, acc 0.572944, rec 0.927419, pre 0.539062, f1 0.681818\n",
      "\n",
      "2018-04-16T00:50:51.191519: step 381, loss 0.26276, acc 0.87\n",
      "2018-04-16T00:50:51.243724: step 382, loss 0.28983, acc 0.89\n",
      "2018-04-16T00:50:51.296561: step 383, loss 0.308408, acc 0.88\n",
      "2018-04-16T00:50:51.339583: step 384, loss 0.290705, acc 0.897436\n",
      "2018-04-16T00:50:51.396377: step 385, loss 0.429761, acc 0.85\n",
      "2018-04-16T00:50:51.449001: step 386, loss 0.358829, acc 0.84\n",
      "2018-04-16T00:50:51.501478: step 387, loss 0.385633, acc 0.85\n",
      "2018-04-16T00:50:51.544333: step 388, loss 0.248132, acc 0.858974\n",
      "2018-04-16T00:50:51.596271: step 389, loss 0.236097, acc 0.92\n",
      "2018-04-16T00:50:51.651249: step 390, loss 0.327291, acc 0.89\n",
      "2018-04-16T00:50:51.703394: step 391, loss 0.284104, acc 0.89\n",
      "2018-04-16T00:50:51.745861: step 392, loss 0.243047, acc 0.935897\n",
      "2018-04-16T00:50:51.797597: step 393, loss 0.200708, acc 0.92\n",
      "2018-04-16T00:50:51.849486: step 394, loss 0.375065, acc 0.84\n",
      "2018-04-16T00:50:51.905244: step 395, loss 0.270434, acc 0.86\n",
      "2018-04-16T00:50:51.948102: step 396, loss 0.285354, acc 0.897436\n",
      "2018-04-16T00:50:52.000371: step 397, loss 0.243458, acc 0.92\n",
      "2018-04-16T00:50:52.052653: step 398, loss 0.321844, acc 0.86\n",
      "2018-04-16T00:50:52.104817: step 399, loss 0.356024, acc 0.84\n",
      "2018-04-16T00:50:52.150967: step 400, loss 0.503307, acc 0.820513\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:50:52.251020: step 400, loss 0.600318, acc 0.689655, rec 0.435484, pre 0.870968, f1 0.580645\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523839829/checkpoints/model-400\n",
      "\n",
      "2018-04-16T00:50:52.356081: step 401, loss 0.452023, acc 0.84\n",
      "2018-04-16T00:50:52.408083: step 402, loss 0.262084, acc 0.9\n",
      "2018-04-16T00:50:52.463027: step 403, loss 0.201291, acc 0.93\n",
      "2018-04-16T00:50:52.506536: step 404, loss 0.292817, acc 0.884615\n",
      "2018-04-16T00:50:52.563644: step 405, loss 0.296125, acc 0.89\n",
      "2018-04-16T00:50:52.618886: step 406, loss 0.253954, acc 0.87\n",
      "2018-04-16T00:50:52.671050: step 407, loss 0.330321, acc 0.86\n",
      "2018-04-16T00:50:52.714766: step 408, loss 0.254804, acc 0.871795\n",
      "2018-04-16T00:50:52.771696: step 409, loss 0.194906, acc 0.95\n",
      "2018-04-16T00:50:52.824311: step 410, loss 0.319686, acc 0.91\n",
      "2018-04-16T00:50:52.876101: step 411, loss 0.362134, acc 0.85\n",
      "2018-04-16T00:50:52.918294: step 412, loss 0.210965, acc 0.948718\n",
      "2018-04-16T00:50:52.970663: step 413, loss 0.393088, acc 0.83\n",
      "2018-04-16T00:50:53.027919: step 414, loss 0.467967, acc 0.84\n",
      "2018-04-16T00:50:53.081223: step 415, loss 0.28021, acc 0.9\n",
      "2018-04-16T00:50:53.124313: step 416, loss 0.180391, acc 0.923077\n",
      "2018-04-16T00:50:53.176628: step 417, loss 0.196089, acc 0.91\n",
      "2018-04-16T00:50:53.228627: step 418, loss 0.285558, acc 0.88\n",
      "2018-04-16T00:50:53.284318: step 419, loss 0.280219, acc 0.88\n",
      "2018-04-16T00:50:53.326933: step 420, loss 0.364828, acc 0.820513\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:50:53.427564: step 420, loss 1.42807, acc 0.482759, rec 0.951613, pre 0.487603, f1 0.644809\n",
      "\n",
      "2018-04-16T00:50:53.479716: step 421, loss 0.255603, acc 0.89\n",
      "2018-04-16T00:50:53.536743: step 422, loss 0.199564, acc 0.93\n",
      "2018-04-16T00:50:53.589645: step 423, loss 0.265947, acc 0.89\n",
      "2018-04-16T00:50:53.635216: step 424, loss 0.341512, acc 0.858974\n",
      "2018-04-16T00:50:53.688078: step 425, loss 0.250374, acc 0.91\n",
      "2018-04-16T00:50:53.744935: step 426, loss 0.389088, acc 0.84\n",
      "2018-04-16T00:50:53.797688: step 427, loss 0.222933, acc 0.89\n",
      "2018-04-16T00:50:53.841335: step 428, loss 0.387534, acc 0.846154\n",
      "2018-04-16T00:50:53.894531: step 429, loss 0.441566, acc 0.84\n",
      "2018-04-16T00:50:53.951247: step 430, loss 0.325901, acc 0.86\n",
      "2018-04-16T00:50:54.005725: step 431, loss 0.30187, acc 0.88\n",
      "2018-04-16T00:50:54.049052: step 432, loss 0.316069, acc 0.884615\n",
      "2018-04-16T00:50:54.101418: step 433, loss 0.258223, acc 0.88\n",
      "2018-04-16T00:50:54.160670: step 434, loss 0.262371, acc 0.91\n",
      "2018-04-16T00:50:54.213062: step 435, loss 0.26822, acc 0.91\n",
      "2018-04-16T00:50:54.256386: step 436, loss 0.308304, acc 0.858974\n",
      "2018-04-16T00:50:54.309939: step 437, loss 0.231181, acc 0.95\n",
      "2018-04-16T00:50:54.366766: step 438, loss 0.26278, acc 0.87\n",
      "2018-04-16T00:50:54.419268: step 439, loss 0.278766, acc 0.9\n",
      "2018-04-16T00:50:54.462518: step 440, loss 0.251904, acc 0.897436\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:50:54.564304: step 440, loss 0.609215, acc 0.693634, rec 0.44086, pre 0.877005, f1 0.586762\n",
      "\n",
      "2018-04-16T00:50:54.621429: step 441, loss 0.241569, acc 0.88\n",
      "2018-04-16T00:50:54.674044: step 442, loss 0.215066, acc 0.9\n",
      "2018-04-16T00:50:54.727381: step 443, loss 0.246785, acc 0.88\n",
      "2018-04-16T00:50:54.771650: step 444, loss 0.292701, acc 0.923077\n",
      "2018-04-16T00:50:54.829718: step 445, loss 0.194881, acc 0.92\n",
      "2018-04-16T00:50:54.882185: step 446, loss 0.244541, acc 0.89\n",
      "2018-04-16T00:50:54.935222: step 447, loss 0.24595, acc 0.94\n",
      "2018-04-16T00:50:54.979489: step 448, loss 0.259116, acc 0.871795\n",
      "2018-04-16T00:50:55.035574: step 449, loss 0.37332, acc 0.87\n",
      "2018-04-16T00:50:55.088116: step 450, loss 0.415443, acc 0.79\n",
      "2018-04-16T00:50:55.141317: step 451, loss 0.450446, acc 0.84\n",
      "2018-04-16T00:50:55.184750: step 452, loss 0.174453, acc 0.948718\n",
      "2018-04-16T00:50:55.241908: step 453, loss 0.281341, acc 0.88\n",
      "2018-04-16T00:50:55.295019: step 454, loss 0.182485, acc 0.93\n",
      "2018-04-16T00:50:55.347740: step 455, loss 0.252793, acc 0.89\n",
      "2018-04-16T00:50:55.390319: step 456, loss 0.207254, acc 0.910256\n",
      "2018-04-16T00:50:55.442658: step 457, loss 0.214483, acc 0.93\n",
      "2018-04-16T00:50:55.499267: step 458, loss 0.223136, acc 0.9\n",
      "2018-04-16T00:50:55.552892: step 459, loss 0.322772, acc 0.86\n",
      "2018-04-16T00:50:55.596712: step 460, loss 0.237217, acc 0.897436\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:50:55.699606: step 460, loss 1.51418, acc 0.493369, rec 0.967742, pre 0.493151, f1 0.653358\n",
      "\n",
      "2018-04-16T00:50:55.755368: step 461, loss 0.207007, acc 0.93\n",
      "2018-04-16T00:50:55.808577: step 462, loss 0.268762, acc 0.91\n",
      "2018-04-16T00:50:55.861088: step 463, loss 0.309165, acc 0.86\n",
      "2018-04-16T00:50:55.904170: step 464, loss 0.211416, acc 0.897436\n",
      "2018-04-16T00:50:55.959811: step 465, loss 0.197248, acc 0.93\n",
      "2018-04-16T00:50:56.012452: step 466, loss 0.218443, acc 0.89\n",
      "2018-04-16T00:50:56.064686: step 467, loss 0.198687, acc 0.91\n",
      "2018-04-16T00:50:56.107831: step 468, loss 0.288241, acc 0.858974\n",
      "2018-04-16T00:50:56.160163: step 469, loss 0.194544, acc 0.92\n",
      "2018-04-16T00:50:56.215845: step 470, loss 0.267658, acc 0.91\n",
      "2018-04-16T00:50:56.268395: step 471, loss 0.236043, acc 0.91\n",
      "2018-04-16T00:50:56.312319: step 472, loss 0.197612, acc 0.910256\n",
      "2018-04-16T00:50:56.365941: step 473, loss 0.19857, acc 0.96\n",
      "2018-04-16T00:50:56.423476: step 474, loss 0.268318, acc 0.89\n",
      "2018-04-16T00:50:56.476473: step 475, loss 0.447162, acc 0.81\n",
      "2018-04-16T00:50:56.519759: step 476, loss 0.636268, acc 0.794872\n",
      "2018-04-16T00:50:56.572739: step 477, loss 0.316862, acc 0.85\n",
      "2018-04-16T00:50:56.630646: step 478, loss 0.286382, acc 0.86\n",
      "2018-04-16T00:50:56.684038: step 479, loss 0.259749, acc 0.93\n",
      "2018-04-16T00:50:56.727452: step 480, loss 0.23673, acc 0.923077\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:50:56.828186: step 480, loss 0.586734, acc 0.712202, rec 0.862903, pre 0.659138, f1 0.747381\n",
      "\n",
      "2018-04-16T00:50:56.884378: step 481, loss 0.203938, acc 0.9\n",
      "2018-04-16T00:50:56.936646: step 482, loss 0.271711, acc 0.92\n",
      "2018-04-16T00:50:56.988541: step 483, loss 0.306959, acc 0.85\n",
      "2018-04-16T00:50:57.032456: step 484, loss 0.413496, acc 0.782051\n",
      "2018-04-16T00:50:57.089514: step 485, loss 0.576978, acc 0.82\n",
      "2018-04-16T00:50:57.142004: step 486, loss 0.480321, acc 0.82\n",
      "2018-04-16T00:50:57.195812: step 487, loss 0.277471, acc 0.91\n",
      "2018-04-16T00:50:57.239215: step 488, loss 0.189706, acc 0.910256\n",
      "2018-04-16T00:50:57.296396: step 489, loss 0.231808, acc 0.92\n",
      "2018-04-16T00:50:57.350166: step 490, loss 0.213711, acc 0.92\n",
      "2018-04-16T00:50:57.403199: step 491, loss 0.165996, acc 0.95\n",
      "2018-04-16T00:50:57.446041: step 492, loss 0.268338, acc 0.910256\n",
      "2018-04-16T00:50:57.501948: step 493, loss 0.139012, acc 0.96\n",
      "2018-04-16T00:50:57.554437: step 494, loss 0.193262, acc 0.93\n",
      "2018-04-16T00:50:57.609391: step 495, loss 0.3086, acc 0.9\n",
      "2018-04-16T00:50:57.652015: step 496, loss 0.449445, acc 0.794872\n",
      "2018-04-16T00:50:57.708842: step 497, loss 0.585808, acc 0.83\n",
      "2018-04-16T00:50:57.761159: step 498, loss 0.285437, acc 0.86\n",
      "2018-04-16T00:50:57.815683: step 499, loss 0.28244, acc 0.89\n",
      "2018-04-16T00:50:57.859601: step 500, loss 0.191231, acc 0.948718\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:50:57.963461: step 500, loss 0.749482, acc 0.6313, rec 0.919355, pre 0.579661, f1 0.711019\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523839829/checkpoints/model-500\n",
      "\n",
      "2018-04-16T00:50:58.062196: step 501, loss 0.243691, acc 0.9\n",
      "2018-04-16T00:50:58.114176: step 502, loss 0.370577, acc 0.85\n",
      "2018-04-16T00:50:58.170034: step 503, loss 0.22811, acc 0.89\n",
      "2018-04-16T00:50:58.213225: step 504, loss 0.285783, acc 0.858974\n",
      "2018-04-16T00:50:58.265501: step 505, loss 0.294664, acc 0.91\n",
      "2018-04-16T00:50:58.317689: step 506, loss 0.238102, acc 0.91\n",
      "2018-04-16T00:50:58.369990: step 507, loss 0.225215, acc 0.93\n",
      "2018-04-16T00:50:58.415900: step 508, loss 0.274621, acc 0.923077\n",
      "2018-04-16T00:50:58.468477: step 509, loss 0.22052, acc 0.91\n",
      "2018-04-16T00:50:58.521357: step 510, loss 0.275501, acc 0.91\n",
      "2018-04-16T00:50:58.573717: step 511, loss 0.172672, acc 0.92\n",
      "2018-04-16T00:50:58.616713: step 512, loss 0.154906, acc 0.961538\n",
      "2018-04-16T00:50:58.672812: step 513, loss 0.235542, acc 0.91\n",
      "2018-04-16T00:50:58.725238: step 514, loss 0.24341, acc 0.88\n",
      "2018-04-16T00:50:58.778589: step 515, loss 0.211003, acc 0.92\n",
      "2018-04-16T00:50:58.822027: step 516, loss 0.126238, acc 0.974359\n",
      "2018-04-16T00:50:58.879230: step 517, loss 0.196328, acc 0.94\n",
      "2018-04-16T00:50:58.931739: step 518, loss 0.227829, acc 0.94\n",
      "2018-04-16T00:50:58.983799: step 519, loss 0.266332, acc 0.89\n",
      "2018-04-16T00:50:59.027150: step 520, loss 0.248272, acc 0.884615\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:50:59.131980: step 520, loss 0.536943, acc 0.757294, rec 0.650538, pre 0.820339, f1 0.725637\n",
      "\n",
      "2018-04-16T00:50:59.183805: step 521, loss 0.286441, acc 0.92\n",
      "2018-04-16T00:50:59.239113: step 522, loss 0.179922, acc 0.93\n",
      "2018-04-16T00:50:59.296486: step 523, loss 0.174395, acc 0.94\n",
      "2018-04-16T00:50:59.377975: step 524, loss 0.193062, acc 0.910256\n",
      "2018-04-16T00:50:59.432030: step 525, loss 0.281568, acc 0.9\n",
      "2018-04-16T00:50:59.489158: step 526, loss 0.136506, acc 0.96\n",
      "2018-04-16T00:50:59.541330: step 527, loss 0.217898, acc 0.92\n",
      "2018-04-16T00:50:59.589500: step 528, loss 0.173916, acc 0.935897\n",
      "2018-04-16T00:50:59.641764: step 529, loss 0.144063, acc 0.95\n",
      "2018-04-16T00:50:59.694343: step 530, loss 0.270803, acc 0.88\n",
      "2018-04-16T00:50:59.747311: step 531, loss 0.21222, acc 0.91\n",
      "2018-04-16T00:50:59.795006: step 532, loss 0.161324, acc 0.935897\n",
      "2018-04-16T00:50:59.848124: step 533, loss 0.169165, acc 0.94\n",
      "2018-04-16T00:50:59.904185: step 534, loss 0.155611, acc 0.94\n",
      "2018-04-16T00:50:59.959245: step 535, loss 0.193021, acc 0.96\n",
      "2018-04-16T00:51:00.007917: step 536, loss 0.260769, acc 0.897436\n",
      "2018-04-16T00:51:00.063603: step 537, loss 0.250258, acc 0.89\n",
      "2018-04-16T00:51:00.116716: step 538, loss 0.300085, acc 0.9\n",
      "2018-04-16T00:51:00.171069: step 539, loss 0.296388, acc 0.87\n",
      "2018-04-16T00:51:00.220100: step 540, loss 0.323401, acc 0.871795\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:00.322908: step 540, loss 0.669223, acc 0.680371, rec 0.403226, pre 0.887574, f1 0.554529\n",
      "\n",
      "2018-04-16T00:51:00.376123: step 541, loss 0.246794, acc 0.88\n",
      "2018-04-16T00:51:00.432938: step 542, loss 0.256887, acc 0.93\n",
      "2018-04-16T00:51:00.488513: step 543, loss 0.203956, acc 0.92\n",
      "2018-04-16T00:51:00.532494: step 544, loss 0.189559, acc 0.923077\n",
      "2018-04-16T00:51:00.585483: step 545, loss 0.238631, acc 0.91\n",
      "2018-04-16T00:51:00.642705: step 546, loss 0.184064, acc 0.95\n",
      "2018-04-16T00:51:00.695252: step 547, loss 0.162141, acc 0.95\n",
      "2018-04-16T00:51:00.740933: step 548, loss 0.163755, acc 0.961538\n",
      "2018-04-16T00:51:00.794036: step 549, loss 0.177965, acc 0.93\n",
      "2018-04-16T00:51:00.852738: step 550, loss 0.1897, acc 0.9\n",
      "2018-04-16T00:51:00.906095: step 551, loss 0.225115, acc 0.93\n",
      "2018-04-16T00:51:00.949121: step 552, loss 0.220067, acc 0.897436\n",
      "2018-04-16T00:51:01.002560: step 553, loss 0.226953, acc 0.92\n",
      "2018-04-16T00:51:01.059669: step 554, loss 0.162873, acc 0.93\n",
      "2018-04-16T00:51:01.113802: step 555, loss 0.238067, acc 0.92\n",
      "2018-04-16T00:51:01.158967: step 556, loss 0.173569, acc 0.935897\n",
      "2018-04-16T00:51:01.213669: step 557, loss 0.227057, acc 0.91\n",
      "2018-04-16T00:51:01.269379: step 558, loss 0.224181, acc 0.9\n",
      "2018-04-16T00:51:01.321454: step 559, loss 0.271007, acc 0.91\n",
      "2018-04-16T00:51:01.364896: step 560, loss 0.189159, acc 0.948718\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:01.467094: step 560, loss 1.16543, acc 0.547745, rec 0.946237, pre 0.523031, f1 0.673684\n",
      "\n",
      "2018-04-16T00:51:01.525156: step 561, loss 0.218366, acc 0.91\n",
      "2018-04-16T00:51:01.581134: step 562, loss 0.173346, acc 0.95\n",
      "2018-04-16T00:51:01.635478: step 563, loss 0.227461, acc 0.9\n",
      "2018-04-16T00:51:01.680626: step 564, loss 0.248762, acc 0.910256\n",
      "2018-04-16T00:51:01.741393: step 565, loss 0.366226, acc 0.84\n",
      "2018-04-16T00:51:01.796565: step 566, loss 0.472158, acc 0.83\n",
      "2018-04-16T00:51:01.851393: step 567, loss 0.427364, acc 0.86\n",
      "2018-04-16T00:51:01.898031: step 568, loss 0.18956, acc 0.935897\n",
      "2018-04-16T00:51:01.969552: step 569, loss 0.198788, acc 0.95\n",
      "2018-04-16T00:51:02.025123: step 570, loss 0.21102, acc 0.93\n",
      "2018-04-16T00:51:02.079559: step 571, loss 0.254683, acc 0.89\n",
      "2018-04-16T00:51:02.124758: step 572, loss 0.129774, acc 0.948718\n",
      "2018-04-16T00:51:02.180740: step 573, loss 0.212187, acc 0.91\n",
      "2018-04-16T00:51:02.234511: step 574, loss 0.221717, acc 0.9\n",
      "2018-04-16T00:51:02.288378: step 575, loss 0.268593, acc 0.92\n",
      "2018-04-16T00:51:02.332052: step 576, loss 0.189608, acc 0.910256\n",
      "2018-04-16T00:51:02.390061: step 577, loss 0.192772, acc 0.95\n",
      "2018-04-16T00:51:02.445412: step 578, loss 0.256905, acc 0.88\n",
      "2018-04-16T00:51:02.499844: step 579, loss 0.203285, acc 0.89\n",
      "2018-04-16T00:51:02.544577: step 580, loss 0.427229, acc 0.897436\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:51:02.653396: step 580, loss 0.812725, acc 0.6313, rec 0.935484, pre 0.578073, f1 0.714579\n",
      "\n",
      "2018-04-16T00:51:02.706446: step 581, loss 0.236428, acc 0.9\n",
      "2018-04-16T00:51:02.759544: step 582, loss 0.310779, acc 0.87\n",
      "2018-04-16T00:51:02.812155: step 583, loss 0.34012, acc 0.89\n",
      "2018-04-16T00:51:02.859169: step 584, loss 0.312566, acc 0.871795\n",
      "2018-04-16T00:51:02.911479: step 585, loss 0.169864, acc 0.91\n",
      "2018-04-16T00:51:02.964712: step 586, loss 0.198217, acc 0.93\n",
      "2018-04-16T00:51:03.018303: step 587, loss 0.194194, acc 0.94\n",
      "2018-04-16T00:51:03.064666: step 588, loss 0.269536, acc 0.910256\n",
      "2018-04-16T00:51:03.118823: step 589, loss 0.302044, acc 0.89\n",
      "2018-04-16T00:51:03.172589: step 590, loss 0.153864, acc 0.95\n",
      "2018-04-16T00:51:03.225227: step 591, loss 0.145196, acc 0.96\n",
      "2018-04-16T00:51:03.273518: step 592, loss 0.249397, acc 0.910256\n",
      "2018-04-16T00:51:03.326974: step 593, loss 0.276708, acc 0.9\n",
      "2018-04-16T00:51:03.380528: step 594, loss 0.102368, acc 0.98\n",
      "2018-04-16T00:51:03.433426: step 595, loss 0.221555, acc 0.9\n",
      "2018-04-16T00:51:03.482770: step 596, loss 0.222302, acc 0.935897\n",
      "2018-04-16T00:51:03.537254: step 597, loss 0.158833, acc 0.95\n",
      "2018-04-16T00:51:03.591297: step 598, loss 0.121558, acc 0.97\n",
      "2018-04-16T00:51:03.644137: step 599, loss 0.186375, acc 0.94\n",
      "2018-04-16T00:51:03.691839: step 600, loss 0.163356, acc 0.910256\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:03.796168: step 600, loss 0.55688, acc 0.745358, rec 0.83871, pre 0.702703, f1 0.764706\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523839829/checkpoints/model-600\n",
      "\n",
      "2018-04-16T00:51:03.906439: step 601, loss 0.160193, acc 0.93\n",
      "2018-04-16T00:51:03.961874: step 602, loss 0.166945, acc 0.95\n",
      "2018-04-16T00:51:04.017538: step 603, loss 0.199873, acc 0.92\n",
      "2018-04-16T00:51:04.060836: step 604, loss 0.11276, acc 0.948718\n",
      "2018-04-16T00:51:04.117923: step 605, loss 0.10998, acc 0.97\n",
      "2018-04-16T00:51:04.175172: step 606, loss 0.200279, acc 0.95\n",
      "2018-04-16T00:51:04.228854: step 607, loss 0.250277, acc 0.88\n",
      "2018-04-16T00:51:04.275764: step 608, loss 0.396608, acc 0.807692\n",
      "2018-04-16T00:51:04.332736: step 609, loss 0.309316, acc 0.85\n",
      "2018-04-16T00:51:04.387385: step 610, loss 0.280165, acc 0.92\n",
      "2018-04-16T00:51:04.446229: step 611, loss 0.241271, acc 0.92\n",
      "2018-04-16T00:51:04.490579: step 612, loss 0.139823, acc 0.948718\n",
      "2018-04-16T00:51:04.549499: step 613, loss 0.161713, acc 0.95\n",
      "2018-04-16T00:51:04.603096: step 614, loss 0.126247, acc 0.94\n",
      "2018-04-16T00:51:04.657021: step 615, loss 0.216883, acc 0.97\n",
      "2018-04-16T00:51:04.700498: step 616, loss 0.212132, acc 0.923077\n",
      "2018-04-16T00:51:04.758294: step 617, loss 0.118423, acc 0.95\n",
      "2018-04-16T00:51:04.812294: step 618, loss 0.17544, acc 0.94\n",
      "2018-04-16T00:51:04.866517: step 619, loss 0.155623, acc 0.96\n",
      "2018-04-16T00:51:04.910385: step 620, loss 0.13803, acc 0.935897\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:05.022704: step 620, loss 0.599564, acc 0.721485, rec 0.844086, pre 0.67382, f1 0.749403\n",
      "\n",
      "2018-04-16T00:51:05.076806: step 621, loss 0.165089, acc 0.95\n",
      "2018-04-16T00:51:05.130921: step 622, loss 0.136533, acc 0.94\n",
      "2018-04-16T00:51:05.185223: step 623, loss 0.159294, acc 0.95\n",
      "2018-04-16T00:51:05.232404: step 624, loss 0.152478, acc 0.961538\n",
      "2018-04-16T00:51:05.285089: step 625, loss 0.367841, acc 0.87\n",
      "2018-04-16T00:51:05.337531: step 626, loss 0.334247, acc 0.88\n",
      "2018-04-16T00:51:05.390112: step 627, loss 0.171364, acc 0.92\n",
      "2018-04-16T00:51:05.433227: step 628, loss 0.0823777, acc 0.974359\n",
      "2018-04-16T00:51:05.492201: step 629, loss 0.132252, acc 0.96\n",
      "2018-04-16T00:51:05.546345: step 630, loss 0.176486, acc 0.94\n",
      "2018-04-16T00:51:05.600039: step 631, loss 0.144079, acc 0.95\n",
      "2018-04-16T00:51:05.643177: step 632, loss 0.121439, acc 0.961538\n",
      "2018-04-16T00:51:05.700399: step 633, loss 0.169841, acc 0.94\n",
      "2018-04-16T00:51:05.781404: step 634, loss 0.116423, acc 0.97\n",
      "2018-04-16T00:51:05.861273: step 635, loss 0.259353, acc 0.92\n",
      "2018-04-16T00:51:05.932257: step 636, loss 0.355363, acc 0.833333\n",
      "2018-04-16T00:51:06.012445: step 637, loss 0.475188, acc 0.83\n",
      "2018-04-16T00:51:06.091565: step 638, loss 0.413087, acc 0.86\n",
      "2018-04-16T00:51:06.177093: step 639, loss 0.29266, acc 0.9\n",
      "2018-04-16T00:51:06.243098: step 640, loss 0.114726, acc 0.948718\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:06.407933: step 640, loss 0.792888, acc 0.659151, rec 0.932796, pre 0.599309, f1 0.729758\n",
      "\n",
      "2018-04-16T00:51:06.486847: step 641, loss 0.178151, acc 0.95\n",
      "2018-04-16T00:51:06.565212: step 642, loss 0.151347, acc 0.96\n",
      "2018-04-16T00:51:06.646576: step 643, loss 0.247685, acc 0.94\n",
      "2018-04-16T00:51:06.710012: step 644, loss 0.187103, acc 0.910256\n",
      "2018-04-16T00:51:06.790233: step 645, loss 0.165989, acc 0.95\n",
      "2018-04-16T00:51:06.874838: step 646, loss 0.171217, acc 0.94\n",
      "2018-04-16T00:51:06.955894: step 647, loss 0.140124, acc 0.94\n",
      "2018-04-16T00:51:07.021767: step 648, loss 0.368434, acc 0.871795\n",
      "2018-04-16T00:51:07.106941: step 649, loss 0.362268, acc 0.86\n",
      "2018-04-16T00:51:07.187466: step 650, loss 0.247418, acc 0.89\n",
      "2018-04-16T00:51:07.266000: step 651, loss 0.129219, acc 0.96\n",
      "2018-04-16T00:51:07.336507: step 652, loss 0.194404, acc 0.935897\n",
      "2018-04-16T00:51:07.416591: step 653, loss 0.168451, acc 0.96\n",
      "2018-04-16T00:51:07.495644: step 654, loss 0.13934, acc 0.94\n",
      "2018-04-16T00:51:07.575031: step 655, loss 0.21132, acc 0.91\n",
      "2018-04-16T00:51:07.620895: step 656, loss 0.252056, acc 0.897436\n",
      "2018-04-16T00:51:07.676466: step 657, loss 0.205288, acc 0.91\n",
      "2018-04-16T00:51:07.732861: step 658, loss 0.196354, acc 0.91\n",
      "2018-04-16T00:51:07.790395: step 659, loss 0.163401, acc 0.95\n",
      "2018-04-16T00:51:07.834998: step 660, loss 0.217103, acc 0.923077\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:07.940538: step 660, loss 0.647357, acc 0.692308, rec 0.887097, pre 0.634615, f1 0.73991\n",
      "\n",
      "2018-04-16T00:51:07.999375: step 661, loss 0.15187, acc 0.96\n",
      "2018-04-16T00:51:08.053980: step 662, loss 0.0854127, acc 0.98\n",
      "2018-04-16T00:51:08.109042: step 663, loss 0.190773, acc 0.93\n",
      "2018-04-16T00:51:08.153944: step 664, loss 0.166619, acc 0.935897\n",
      "2018-04-16T00:51:08.214625: step 665, loss 0.28415, acc 0.87\n",
      "2018-04-16T00:51:08.269157: step 666, loss 0.310393, acc 0.89\n",
      "2018-04-16T00:51:08.322591: step 667, loss 0.156322, acc 0.95\n",
      "2018-04-16T00:51:08.367517: step 668, loss 0.150183, acc 0.935897\n",
      "2018-04-16T00:51:08.425667: step 669, loss 0.199845, acc 0.92\n",
      "2018-04-16T00:51:08.479404: step 670, loss 0.0969267, acc 0.96\n",
      "2018-04-16T00:51:08.538149: step 671, loss 0.142422, acc 0.94\n",
      "2018-04-16T00:51:08.582159: step 672, loss 0.203941, acc 0.935897\n",
      "2018-04-16T00:51:08.638532: step 673, loss 0.132294, acc 0.96\n",
      "2018-04-16T00:51:08.691679: step 674, loss 0.212245, acc 0.89\n",
      "2018-04-16T00:51:08.753334: step 675, loss 0.0995499, acc 0.97\n",
      "2018-04-16T00:51:08.796317: step 676, loss 0.131146, acc 0.974359\n",
      "2018-04-16T00:51:08.852892: step 677, loss 0.199001, acc 0.93\n",
      "2018-04-16T00:51:08.906857: step 678, loss 0.176773, acc 0.91\n",
      "2018-04-16T00:51:08.959690: step 679, loss 0.163484, acc 0.95\n",
      "2018-04-16T00:51:09.011441: step 680, loss 0.18615, acc 0.923077\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:09.115049: step 680, loss 0.548747, acc 0.757294, rec 0.629032, pre 0.83871, f1 0.718894\n",
      "\n",
      "2018-04-16T00:51:09.167438: step 681, loss 0.124832, acc 0.97\n",
      "2018-04-16T00:51:09.219846: step 682, loss 0.114006, acc 0.98\n",
      "2018-04-16T00:51:09.283314: step 683, loss 0.118404, acc 0.97\n",
      "2018-04-16T00:51:09.340200: step 684, loss 0.107577, acc 0.961538\n",
      "2018-04-16T00:51:09.392909: step 685, loss 0.0752625, acc 1\n",
      "2018-04-16T00:51:09.445285: step 686, loss 0.201929, acc 0.96\n",
      "2018-04-16T00:51:09.501244: step 687, loss 0.215824, acc 0.93\n",
      "2018-04-16T00:51:09.549619: step 688, loss 0.265587, acc 0.897436\n",
      "2018-04-16T00:51:09.605110: step 689, loss 0.275965, acc 0.88\n",
      "2018-04-16T00:51:09.658466: step 690, loss 0.236967, acc 0.92\n",
      "2018-04-16T00:51:09.710806: step 691, loss 0.14828, acc 0.93\n",
      "2018-04-16T00:51:09.761435: step 692, loss 0.140547, acc 0.948718\n",
      "2018-04-16T00:51:09.817224: step 693, loss 0.255756, acc 0.93\n",
      "2018-04-16T00:51:09.870640: step 694, loss 0.175519, acc 0.93\n",
      "2018-04-16T00:51:09.923875: step 695, loss 0.0907665, acc 0.99\n",
      "2018-04-16T00:51:09.971746: step 696, loss 0.0724993, acc 0.974359\n",
      "2018-04-16T00:51:10.027148: step 697, loss 0.207176, acc 0.97\n",
      "2018-04-16T00:51:10.079859: step 698, loss 0.282373, acc 0.87\n",
      "2018-04-16T00:51:10.132914: step 699, loss 0.235056, acc 0.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:51:10.179832: step 700, loss 0.438295, acc 0.871795\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:10.280407: step 700, loss 0.816244, acc 0.66313, rec 0.341398, pre 0.933824, f1 0.5\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523839829/checkpoints/model-700\n",
      "\n",
      "2018-04-16T00:51:10.390381: step 701, loss 0.380165, acc 0.89\n",
      "2018-04-16T00:51:10.444597: step 702, loss 0.184754, acc 0.92\n",
      "2018-04-16T00:51:10.500761: step 703, loss 0.149226, acc 0.94\n",
      "2018-04-16T00:51:10.543940: step 704, loss 0.115548, acc 0.948718\n",
      "2018-04-16T00:51:10.603299: step 705, loss 0.146212, acc 0.96\n",
      "2018-04-16T00:51:10.657138: step 706, loss 0.107981, acc 0.99\n",
      "2018-04-16T00:51:10.709862: step 707, loss 0.182189, acc 0.92\n",
      "2018-04-16T00:51:10.753774: step 708, loss 0.0971739, acc 0.961538\n",
      "2018-04-16T00:51:10.810824: step 709, loss 0.108777, acc 0.97\n",
      "2018-04-16T00:51:10.863659: step 710, loss 0.146022, acc 0.96\n",
      "2018-04-16T00:51:10.916434: step 711, loss 0.122019, acc 0.96\n",
      "2018-04-16T00:51:10.960833: step 712, loss 0.182622, acc 0.923077\n",
      "2018-04-16T00:51:11.021427: step 713, loss 0.169006, acc 0.91\n",
      "2018-04-16T00:51:11.074445: step 714, loss 0.271214, acc 0.88\n",
      "2018-04-16T00:51:11.127258: step 715, loss 0.205448, acc 0.91\n",
      "2018-04-16T00:51:11.174558: step 716, loss 0.142142, acc 0.923077\n",
      "2018-04-16T00:51:11.234596: step 717, loss 0.132908, acc 0.92\n",
      "2018-04-16T00:51:11.288934: step 718, loss 0.133984, acc 0.95\n",
      "2018-04-16T00:51:11.343934: step 719, loss 0.0981775, acc 0.96\n",
      "2018-04-16T00:51:11.388779: step 720, loss 0.130497, acc 0.948718\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:11.496413: step 720, loss 0.982013, acc 0.622016, rec 0.943548, pre 0.570732, f1 0.711246\n",
      "\n",
      "2018-04-16T00:51:11.550052: step 721, loss 0.0818229, acc 0.98\n",
      "2018-04-16T00:51:11.603806: step 722, loss 0.106059, acc 0.95\n",
      "2018-04-16T00:51:11.657723: step 723, loss 0.235216, acc 0.94\n",
      "2018-04-16T00:51:11.704857: step 724, loss 0.150759, acc 0.923077\n",
      "2018-04-16T00:51:11.759499: step 725, loss 0.142173, acc 0.96\n",
      "2018-04-16T00:51:11.810946: step 726, loss 0.149486, acc 0.94\n",
      "2018-04-16T00:51:11.863650: step 727, loss 0.0952712, acc 0.98\n",
      "2018-04-16T00:51:11.909830: step 728, loss 0.118641, acc 0.948718\n",
      "2018-04-16T00:51:11.964252: step 729, loss 0.167516, acc 0.94\n",
      "2018-04-16T00:51:12.017565: step 730, loss 0.125414, acc 0.97\n",
      "2018-04-16T00:51:12.072183: step 731, loss 0.122522, acc 0.96\n",
      "2018-04-16T00:51:12.125903: step 732, loss 0.0886131, acc 0.974359\n",
      "2018-04-16T00:51:12.182422: step 733, loss 0.0811753, acc 0.98\n",
      "2018-04-16T00:51:12.237269: step 734, loss 0.167377, acc 0.94\n",
      "2018-04-16T00:51:12.292642: step 735, loss 0.118301, acc 0.95\n",
      "2018-04-16T00:51:12.339846: step 736, loss 0.0642867, acc 0.987179\n",
      "2018-04-16T00:51:12.395189: step 737, loss 0.0829909, acc 0.97\n",
      "2018-04-16T00:51:12.447373: step 738, loss 0.168704, acc 0.96\n",
      "2018-04-16T00:51:12.502399: step 739, loss 0.131219, acc 0.98\n",
      "2018-04-16T00:51:12.553921: step 740, loss 0.156941, acc 0.935897\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:12.666228: step 740, loss 0.569758, acc 0.759947, rec 0.647849, pre 0.828179, f1 0.726998\n",
      "\n",
      "2018-04-16T00:51:12.725684: step 741, loss 0.179179, acc 0.94\n",
      "2018-04-16T00:51:12.781841: step 742, loss 0.169157, acc 0.96\n",
      "2018-04-16T00:51:12.835667: step 743, loss 0.129231, acc 0.95\n",
      "2018-04-16T00:51:12.884121: step 744, loss 0.151106, acc 0.948718\n",
      "2018-04-16T00:51:12.944561: step 745, loss 0.240965, acc 0.91\n",
      "2018-04-16T00:51:13.003375: step 746, loss 0.240727, acc 0.93\n",
      "2018-04-16T00:51:13.057160: step 747, loss 0.127498, acc 0.96\n",
      "2018-04-16T00:51:13.101813: step 748, loss 0.287765, acc 0.910256\n",
      "2018-04-16T00:51:13.155700: step 749, loss 0.282511, acc 0.88\n",
      "2018-04-16T00:51:13.214596: step 750, loss 0.220765, acc 0.9\n",
      "2018-04-16T00:51:13.268137: step 751, loss 0.203134, acc 0.94\n",
      "2018-04-16T00:51:13.312603: step 752, loss 0.247759, acc 0.935897\n",
      "2018-04-16T00:51:13.368044: step 753, loss 0.231582, acc 0.9\n",
      "2018-04-16T00:51:13.424795: step 754, loss 0.147098, acc 0.96\n",
      "2018-04-16T00:51:13.477387: step 755, loss 0.127295, acc 0.97\n",
      "2018-04-16T00:51:13.521550: step 756, loss 0.108189, acc 0.961538\n",
      "2018-04-16T00:51:13.575025: step 757, loss 0.0776573, acc 0.98\n",
      "2018-04-16T00:51:13.632385: step 758, loss 0.143284, acc 0.96\n",
      "2018-04-16T00:51:13.689363: step 759, loss 0.0946654, acc 0.97\n",
      "2018-04-16T00:51:13.734285: step 760, loss 0.0915633, acc 0.974359\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:13.838284: step 760, loss 0.620021, acc 0.732095, rec 0.857527, pre 0.681624, f1 0.759524\n",
      "\n",
      "2018-04-16T00:51:13.891897: step 761, loss 0.104321, acc 0.97\n",
      "2018-04-16T00:51:13.944019: step 762, loss 0.115276, acc 0.98\n",
      "2018-04-16T00:51:14.000177: step 763, loss 0.0904023, acc 0.97\n",
      "2018-04-16T00:51:14.049765: step 764, loss 0.100274, acc 0.948718\n",
      "2018-04-16T00:51:14.102978: step 765, loss 0.113898, acc 0.97\n",
      "2018-04-16T00:51:14.155208: step 766, loss 0.138576, acc 0.94\n",
      "2018-04-16T00:51:14.207554: step 767, loss 0.115702, acc 0.95\n",
      "2018-04-16T00:51:14.250132: step 768, loss 0.145487, acc 0.923077\n",
      "2018-04-16T00:51:14.305332: step 769, loss 0.0989354, acc 0.97\n",
      "2018-04-16T00:51:14.360861: step 770, loss 0.107572, acc 0.95\n",
      "2018-04-16T00:51:14.414470: step 771, loss 0.160842, acc 0.97\n",
      "2018-04-16T00:51:14.458457: step 772, loss 0.0902918, acc 0.974359\n",
      "2018-04-16T00:51:14.516682: step 773, loss 0.122619, acc 0.95\n",
      "2018-04-16T00:51:14.576715: step 774, loss 0.0770727, acc 0.99\n",
      "2018-04-16T00:51:14.632461: step 775, loss 0.073549, acc 0.99\n",
      "2018-04-16T00:51:14.678962: step 776, loss 0.160405, acc 0.961538\n",
      "2018-04-16T00:51:14.740004: step 777, loss 0.0892737, acc 0.97\n",
      "2018-04-16T00:51:14.794473: step 778, loss 0.0933682, acc 0.96\n",
      "2018-04-16T00:51:14.848837: step 779, loss 0.124371, acc 0.96\n",
      "2018-04-16T00:51:14.894484: step 780, loss 0.190257, acc 0.935897\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:15.003398: step 780, loss 1.18712, acc 0.586207, rec 0.962366, pre 0.545732, f1 0.696498\n",
      "\n",
      "2018-04-16T00:51:15.057279: step 781, loss 0.132069, acc 0.96\n",
      "2018-04-16T00:51:15.111682: step 782, loss 0.0839162, acc 0.99\n",
      "2018-04-16T00:51:15.166046: step 783, loss 0.109681, acc 0.96\n",
      "2018-04-16T00:51:15.214214: step 784, loss 0.12878, acc 0.961538\n",
      "2018-04-16T00:51:15.268166: step 785, loss 0.117796, acc 0.95\n",
      "2018-04-16T00:51:15.321132: step 786, loss 0.302159, acc 0.91\n",
      "2018-04-16T00:51:15.376344: step 787, loss 0.0952909, acc 0.98\n",
      "2018-04-16T00:51:15.425991: step 788, loss 0.102519, acc 0.987179\n",
      "2018-04-16T00:51:15.480930: step 789, loss 0.160623, acc 0.94\n",
      "2018-04-16T00:51:15.535117: step 790, loss 0.188522, acc 0.93\n",
      "2018-04-16T00:51:15.589188: step 791, loss 0.0998286, acc 0.95\n",
      "2018-04-16T00:51:15.636539: step 792, loss 0.0837938, acc 0.987179\n",
      "2018-04-16T00:51:15.689530: step 793, loss 0.102768, acc 0.98\n",
      "2018-04-16T00:51:15.742715: step 794, loss 0.119913, acc 0.96\n",
      "2018-04-16T00:51:15.796503: step 795, loss 0.186033, acc 0.95\n",
      "2018-04-16T00:51:15.844617: step 796, loss 0.135221, acc 0.961538\n",
      "2018-04-16T00:51:15.898188: step 797, loss 0.139507, acc 0.94\n",
      "2018-04-16T00:51:15.951559: step 798, loss 0.168178, acc 0.94\n",
      "2018-04-16T00:51:16.004620: step 799, loss 0.0689905, acc 0.97\n",
      "2018-04-16T00:51:16.053751: step 800, loss 0.0648368, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:16.156157: step 800, loss 0.873399, acc 0.656499, rec 0.927419, pre 0.59792, f1 0.727081\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523839829/checkpoints/model-800\n",
      "\n",
      "2018-04-16T00:51:16.263917: step 801, loss 0.0307017, acc 1\n",
      "2018-04-16T00:51:16.317810: step 802, loss 0.152877, acc 0.96\n",
      "2018-04-16T00:51:16.370495: step 803, loss 0.121984, acc 0.93\n",
      "2018-04-16T00:51:16.413151: step 804, loss 0.160729, acc 0.935897\n",
      "2018-04-16T00:51:16.470071: step 805, loss 0.103237, acc 0.97\n",
      "2018-04-16T00:51:16.525931: step 806, loss 0.124473, acc 0.97\n",
      "2018-04-16T00:51:16.579585: step 807, loss 0.0917609, acc 0.99\n",
      "2018-04-16T00:51:16.623218: step 808, loss 0.19317, acc 0.923077\n",
      "2018-04-16T00:51:16.679657: step 809, loss 0.403422, acc 0.85\n",
      "2018-04-16T00:51:16.736285: step 810, loss 0.341017, acc 0.9\n",
      "2018-04-16T00:51:16.790899: step 811, loss 0.15243, acc 0.95\n",
      "2018-04-16T00:51:16.834844: step 812, loss 0.125732, acc 0.987179\n",
      "2018-04-16T00:51:16.892546: step 813, loss 0.136549, acc 0.91\n",
      "2018-04-16T00:51:16.947967: step 814, loss 0.173321, acc 0.94\n",
      "2018-04-16T00:51:17.002527: step 815, loss 0.0587855, acc 0.98\n",
      "2018-04-16T00:51:17.057068: step 816, loss 0.104304, acc 0.974359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:51:17.128784: step 817, loss 0.0809089, acc 0.97\n",
      "2018-04-16T00:51:17.182252: step 818, loss 0.107804, acc 0.95\n",
      "2018-04-16T00:51:17.235394: step 819, loss 0.181659, acc 0.95\n",
      "2018-04-16T00:51:17.279807: step 820, loss 0.184867, acc 0.897436\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:17.385728: step 820, loss 1.21708, acc 0.590186, rec 0.948925, pre 0.548989, f1 0.695567\n",
      "\n",
      "2018-04-16T00:51:17.438821: step 821, loss 0.299693, acc 0.93\n",
      "2018-04-16T00:51:17.491271: step 822, loss 0.344367, acc 0.86\n",
      "2018-04-16T00:51:17.547080: step 823, loss 0.273287, acc 0.89\n",
      "2018-04-16T00:51:17.595261: step 824, loss 0.133054, acc 0.961538\n",
      "2018-04-16T00:51:17.648417: step 825, loss 0.098414, acc 0.98\n",
      "2018-04-16T00:51:17.700883: step 826, loss 0.0785683, acc 0.98\n",
      "2018-04-16T00:51:17.754210: step 827, loss 0.0951982, acc 0.97\n",
      "2018-04-16T00:51:17.801191: step 828, loss 0.227702, acc 0.948718\n",
      "2018-04-16T00:51:17.855405: step 829, loss 0.143689, acc 0.95\n",
      "2018-04-16T00:51:17.913201: step 830, loss 0.122666, acc 0.97\n",
      "2018-04-16T00:51:17.968712: step 831, loss 0.177928, acc 0.91\n",
      "2018-04-16T00:51:18.017013: step 832, loss 0.196323, acc 0.935897\n",
      "2018-04-16T00:51:18.070081: step 833, loss 0.0807113, acc 0.96\n",
      "2018-04-16T00:51:18.123351: step 834, loss 0.121619, acc 0.95\n",
      "2018-04-16T00:51:18.176179: step 835, loss 0.140605, acc 0.97\n",
      "2018-04-16T00:51:18.223683: step 836, loss 0.0646512, acc 0.987179\n",
      "2018-04-16T00:51:18.280492: step 837, loss 0.100393, acc 0.97\n",
      "2018-04-16T00:51:18.333503: step 838, loss 0.114594, acc 0.96\n",
      "2018-04-16T00:51:18.385918: step 839, loss 0.168785, acc 0.94\n",
      "2018-04-16T00:51:18.432424: step 840, loss 0.106703, acc 0.961538\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:18.533714: step 840, loss 0.567592, acc 0.761273, rec 0.725806, pre 0.775862, f1 0.75\n",
      "\n",
      "2018-04-16T00:51:18.586618: step 841, loss 0.0947806, acc 0.97\n",
      "2018-04-16T00:51:18.643664: step 842, loss 0.0575893, acc 0.98\n",
      "2018-04-16T00:51:18.697472: step 843, loss 0.0822098, acc 0.98\n",
      "2018-04-16T00:51:18.742635: step 844, loss 0.174308, acc 0.948718\n",
      "2018-04-16T00:51:18.794852: step 845, loss 0.381072, acc 0.82\n",
      "2018-04-16T00:51:18.851960: step 846, loss 0.555527, acc 0.82\n",
      "2018-04-16T00:51:18.905319: step 847, loss 0.168784, acc 0.96\n",
      "2018-04-16T00:51:18.959682: step 848, loss 0.128447, acc 0.961538\n",
      "2018-04-16T00:51:19.013462: step 849, loss 0.123772, acc 0.96\n",
      "2018-04-16T00:51:19.070844: step 850, loss 0.134937, acc 0.97\n",
      "2018-04-16T00:51:19.123137: step 851, loss 0.147037, acc 0.94\n",
      "2018-04-16T00:51:19.166644: step 852, loss 0.0727966, acc 0.987179\n",
      "2018-04-16T00:51:19.220127: step 853, loss 0.0809499, acc 0.97\n",
      "2018-04-16T00:51:19.276943: step 854, loss 0.103328, acc 0.98\n",
      "2018-04-16T00:51:19.329852: step 855, loss 0.111858, acc 0.97\n",
      "2018-04-16T00:51:19.376394: step 856, loss 0.114574, acc 0.948718\n",
      "2018-04-16T00:51:19.430040: step 857, loss 0.165678, acc 0.96\n",
      "2018-04-16T00:51:19.487050: step 858, loss 0.15163, acc 0.95\n",
      "2018-04-16T00:51:19.540787: step 859, loss 0.137467, acc 0.93\n",
      "2018-04-16T00:51:19.586593: step 860, loss 0.0503414, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:19.707315: step 860, loss 0.58868, acc 0.750663, rec 0.795699, pre 0.72549, f1 0.758974\n",
      "\n",
      "2018-04-16T00:51:19.766771: step 861, loss 0.140331, acc 0.98\n",
      "2018-04-16T00:51:19.823335: step 862, loss 0.0981585, acc 0.96\n",
      "2018-04-16T00:51:19.881565: step 863, loss 0.0877449, acc 0.97\n",
      "2018-04-16T00:51:19.935877: step 864, loss 0.111397, acc 0.961538\n",
      "2018-04-16T00:51:20.005015: step 865, loss 0.113125, acc 0.96\n",
      "2018-04-16T00:51:20.061533: step 866, loss 0.101035, acc 0.97\n",
      "2018-04-16T00:51:20.117527: step 867, loss 0.0807767, acc 0.99\n",
      "2018-04-16T00:51:20.180192: step 868, loss 0.0396581, acc 1\n",
      "2018-04-16T00:51:20.252540: step 869, loss 0.0616589, acc 0.99\n",
      "2018-04-16T00:51:20.327960: step 870, loss 0.090694, acc 0.98\n",
      "2018-04-16T00:51:20.387758: step 871, loss 0.101159, acc 0.97\n",
      "2018-04-16T00:51:20.434087: step 872, loss 0.122028, acc 0.948718\n",
      "2018-04-16T00:51:20.490002: step 873, loss 0.127796, acc 0.95\n",
      "2018-04-16T00:51:20.549446: step 874, loss 0.0926422, acc 0.97\n",
      "2018-04-16T00:51:20.607091: step 875, loss 0.128674, acc 0.97\n",
      "2018-04-16T00:51:20.652515: step 876, loss 0.13339, acc 0.974359\n",
      "2018-04-16T00:51:20.709268: step 877, loss 0.0887917, acc 0.96\n",
      "2018-04-16T00:51:20.768352: step 878, loss 0.0641193, acc 0.99\n",
      "2018-04-16T00:51:20.826515: step 879, loss 0.141442, acc 0.97\n",
      "2018-04-16T00:51:20.871432: step 880, loss 0.0701009, acc 0.974359\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:20.973300: step 880, loss 0.671729, acc 0.722812, rec 0.870968, pre 0.668041, f1 0.756126\n",
      "\n",
      "2018-04-16T00:51:21.026339: step 881, loss 0.0852155, acc 0.98\n",
      "2018-04-16T00:51:21.085886: step 882, loss 0.0568475, acc 0.99\n",
      "2018-04-16T00:51:21.140326: step 883, loss 0.103575, acc 0.96\n",
      "2018-04-16T00:51:21.185693: step 884, loss 0.0990001, acc 0.987179\n",
      "2018-04-16T00:51:21.239082: step 885, loss 0.0590228, acc 0.99\n",
      "2018-04-16T00:51:21.300007: step 886, loss 0.112267, acc 0.96\n",
      "2018-04-16T00:51:21.354654: step 887, loss 0.12672, acc 0.94\n",
      "2018-04-16T00:51:21.399705: step 888, loss 0.147394, acc 0.923077\n",
      "2018-04-16T00:51:21.454154: step 889, loss 0.12784, acc 0.96\n",
      "2018-04-16T00:51:21.514567: step 890, loss 0.0932138, acc 0.96\n",
      "2018-04-16T00:51:21.568917: step 891, loss 0.0716368, acc 0.98\n",
      "2018-04-16T00:51:21.612621: step 892, loss 0.042512, acc 1\n",
      "2018-04-16T00:51:21.666937: step 893, loss 0.0844122, acc 0.96\n",
      "2018-04-16T00:51:21.724563: step 894, loss 0.122036, acc 0.98\n",
      "2018-04-16T00:51:21.780572: step 895, loss 0.115309, acc 0.95\n",
      "2018-04-16T00:51:21.824434: step 896, loss 0.108085, acc 0.948718\n",
      "2018-04-16T00:51:21.878381: step 897, loss 0.105091, acc 0.96\n",
      "2018-04-16T00:51:21.935024: step 898, loss 0.214812, acc 0.96\n",
      "2018-04-16T00:51:21.988192: step 899, loss 0.11682, acc 0.95\n",
      "2018-04-16T00:51:22.035029: step 900, loss 0.123991, acc 0.923077\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:22.148280: step 900, loss 0.831785, acc 0.677719, rec 0.392473, pre 0.895706, f1 0.545794\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523839829/checkpoints/model-900\n",
      "\n",
      "2018-04-16T00:51:22.268658: step 901, loss 0.148879, acc 0.94\n",
      "2018-04-16T00:51:22.322969: step 902, loss 0.183286, acc 0.94\n",
      "2018-04-16T00:51:22.380439: step 903, loss 0.248363, acc 0.92\n",
      "2018-04-16T00:51:22.424974: step 904, loss 0.23203, acc 0.884615\n",
      "2018-04-16T00:51:22.481557: step 905, loss 0.154103, acc 0.95\n",
      "2018-04-16T00:51:22.539329: step 906, loss 0.0775716, acc 0.96\n",
      "2018-04-16T00:51:22.595942: step 907, loss 0.110387, acc 0.96\n",
      "2018-04-16T00:51:22.639229: step 908, loss 0.190849, acc 0.923077\n",
      "2018-04-16T00:51:22.691596: step 909, loss 0.208718, acc 0.94\n",
      "2018-04-16T00:51:22.744246: step 910, loss 0.0853084, acc 0.98\n",
      "2018-04-16T00:51:22.801715: step 911, loss 0.0378127, acc 1\n",
      "2018-04-16T00:51:22.846014: step 912, loss 0.0832584, acc 0.987179\n",
      "2018-04-16T00:51:22.900300: step 913, loss 0.0606197, acc 0.99\n",
      "2018-04-16T00:51:22.954060: step 914, loss 0.103185, acc 0.97\n",
      "2018-04-16T00:51:23.011559: step 915, loss 0.176289, acc 0.92\n",
      "2018-04-16T00:51:23.056323: step 916, loss 0.163964, acc 0.923077\n",
      "2018-04-16T00:51:23.111813: step 917, loss 0.0550069, acc 0.98\n",
      "2018-04-16T00:51:23.164266: step 918, loss 0.0664086, acc 0.98\n",
      "2018-04-16T00:51:23.219499: step 919, loss 0.141787, acc 0.95\n",
      "2018-04-16T00:51:23.262926: step 920, loss 0.151292, acc 0.961538\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:23.372432: step 920, loss 1.01776, acc 0.637931, rec 0.943548, pre 0.58209, f1 0.72\n",
      "\n",
      "2018-04-16T00:51:23.436329: step 921, loss 0.130206, acc 0.95\n",
      "2018-04-16T00:51:23.488665: step 922, loss 0.136707, acc 0.93\n",
      "2018-04-16T00:51:23.543048: step 923, loss 0.0508487, acc 1\n",
      "2018-04-16T00:51:23.586573: step 924, loss 0.0756291, acc 0.987179\n",
      "2018-04-16T00:51:23.646540: step 925, loss 0.0605302, acc 0.99\n",
      "2018-04-16T00:51:23.701090: step 926, loss 0.113184, acc 0.97\n",
      "2018-04-16T00:51:23.754117: step 927, loss 0.0612656, acc 0.99\n",
      "2018-04-16T00:51:23.800334: step 928, loss 0.11476, acc 0.987179\n",
      "2018-04-16T00:51:23.856257: step 929, loss 0.0525938, acc 0.98\n",
      "2018-04-16T00:51:23.909797: step 930, loss 0.0705389, acc 0.99\n",
      "2018-04-16T00:51:23.963349: step 931, loss 0.117204, acc 0.96\n",
      "2018-04-16T00:51:24.006814: step 932, loss 0.0590594, acc 0.987179\n",
      "2018-04-16T00:51:24.063364: step 933, loss 0.0643267, acc 0.97\n",
      "2018-04-16T00:51:24.116310: step 934, loss 0.0768357, acc 0.98\n",
      "2018-04-16T00:51:24.171199: step 935, loss 0.142626, acc 0.96\n",
      "2018-04-16T00:51:24.215491: step 936, loss 0.0989186, acc 0.961538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:51:24.273673: step 937, loss 0.0527352, acc 0.98\n",
      "2018-04-16T00:51:24.327346: step 938, loss 0.140583, acc 0.96\n",
      "2018-04-16T00:51:24.382002: step 939, loss 0.0528963, acc 0.99\n",
      "2018-04-16T00:51:24.426455: step 940, loss 0.0316506, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:24.534404: step 940, loss 0.606045, acc 0.749337, rec 0.795699, pre 0.723716, f1 0.758003\n",
      "\n",
      "2018-04-16T00:51:24.593000: step 941, loss 0.0351204, acc 1\n",
      "2018-04-16T00:51:24.649879: step 942, loss 0.117533, acc 0.96\n",
      "2018-04-16T00:51:24.705319: step 943, loss 0.0813129, acc 0.99\n",
      "2018-04-16T00:51:24.755729: step 944, loss 0.196266, acc 0.948718\n",
      "2018-04-16T00:51:24.822843: step 945, loss 0.286117, acc 0.88\n",
      "2018-04-16T00:51:24.892420: step 946, loss 0.154852, acc 0.93\n",
      "2018-04-16T00:51:24.965618: step 947, loss 0.0638024, acc 0.98\n",
      "2018-04-16T00:51:25.009052: step 948, loss 0.0570662, acc 0.974359\n",
      "2018-04-16T00:51:25.063840: step 949, loss 0.0592902, acc 0.98\n",
      "2018-04-16T00:51:25.119089: step 950, loss 0.0296809, acc 1\n",
      "2018-04-16T00:51:25.179226: step 951, loss 0.0844366, acc 0.97\n",
      "2018-04-16T00:51:25.223808: step 952, loss 0.0718439, acc 0.974359\n",
      "2018-04-16T00:51:25.277408: step 953, loss 0.104314, acc 0.97\n",
      "2018-04-16T00:51:25.333861: step 954, loss 0.105809, acc 0.97\n",
      "2018-04-16T00:51:25.391695: step 955, loss 0.0561498, acc 0.97\n",
      "2018-04-16T00:51:25.437622: step 956, loss 0.101748, acc 0.948718\n",
      "2018-04-16T00:51:25.492892: step 957, loss 0.0659739, acc 0.98\n",
      "2018-04-16T00:51:25.548345: step 958, loss 0.0518194, acc 0.98\n",
      "2018-04-16T00:51:25.607905: step 959, loss 0.0403276, acc 0.99\n",
      "2018-04-16T00:51:25.652344: step 960, loss 0.108889, acc 0.961538\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:25.755093: step 960, loss 0.64228, acc 0.742706, rec 0.586022, pre 0.844961, f1 0.692063\n",
      "\n",
      "2018-04-16T00:51:25.812815: step 961, loss 0.0567593, acc 0.99\n",
      "2018-04-16T00:51:25.866806: step 962, loss 0.0496024, acc 0.99\n",
      "2018-04-16T00:51:25.950695: step 963, loss 0.110048, acc 0.97\n",
      "2018-04-16T00:51:26.000464: step 964, loss 0.0980011, acc 0.974359\n",
      "2018-04-16T00:51:26.061262: step 965, loss 0.0604789, acc 0.99\n",
      "2018-04-16T00:51:26.114657: step 966, loss 0.0997288, acc 0.97\n",
      "2018-04-16T00:51:26.169569: step 967, loss 0.129513, acc 0.95\n",
      "2018-04-16T00:51:26.215014: step 968, loss 0.0621243, acc 0.961538\n",
      "2018-04-16T00:51:26.285249: step 969, loss 0.0425834, acc 0.99\n",
      "2018-04-16T00:51:26.364479: step 970, loss 0.0804209, acc 0.97\n",
      "2018-04-16T00:51:26.442961: step 971, loss 0.132689, acc 0.97\n",
      "2018-04-16T00:51:26.513101: step 972, loss 0.0306094, acc 1\n",
      "2018-04-16T00:51:26.591261: step 973, loss 0.0640295, acc 0.98\n",
      "2018-04-16T00:51:26.658523: step 974, loss 0.0757896, acc 0.97\n",
      "2018-04-16T00:51:26.711806: step 975, loss 0.164717, acc 0.94\n",
      "2018-04-16T00:51:26.760197: step 976, loss 0.154457, acc 0.910256\n",
      "2018-04-16T00:51:26.814512: step 977, loss 0.10815, acc 0.96\n",
      "2018-04-16T00:51:26.880353: step 978, loss 0.0520762, acc 0.99\n",
      "2018-04-16T00:51:26.954001: step 979, loss 0.0884339, acc 0.97\n",
      "2018-04-16T00:51:27.002293: step 980, loss 0.096159, acc 0.961538\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:27.109319: step 980, loss 0.870509, acc 0.665783, rec 0.913978, pre 0.607143, f1 0.729614\n",
      "\n",
      "2018-04-16T00:51:27.164587: step 981, loss 0.166431, acc 0.95\n",
      "2018-04-16T00:51:27.222007: step 982, loss 0.139604, acc 0.94\n",
      "2018-04-16T00:51:27.278015: step 983, loss 0.0906454, acc 0.95\n",
      "2018-04-16T00:51:27.321808: step 984, loss 0.0538899, acc 0.987179\n",
      "2018-04-16T00:51:27.375909: step 985, loss 0.0656239, acc 0.98\n",
      "2018-04-16T00:51:27.432651: step 986, loss 0.0596937, acc 0.98\n",
      "2018-04-16T00:51:27.486260: step 987, loss 0.135895, acc 0.95\n",
      "2018-04-16T00:51:27.537485: step 988, loss 0.0554822, acc 0.987179\n",
      "2018-04-16T00:51:27.595580: step 989, loss 0.0797795, acc 0.96\n",
      "2018-04-16T00:51:27.673992: step 990, loss 0.153325, acc 0.92\n",
      "2018-04-16T00:51:27.731028: step 991, loss 0.0511555, acc 0.99\n",
      "2018-04-16T00:51:27.776240: step 992, loss 0.0783943, acc 0.961538\n",
      "2018-04-16T00:51:27.831889: step 993, loss 0.0608137, acc 0.97\n",
      "2018-04-16T00:51:27.894213: step 994, loss 0.0752021, acc 0.97\n",
      "2018-04-16T00:51:27.958243: step 995, loss 0.0691781, acc 0.98\n",
      "2018-04-16T00:51:28.005587: step 996, loss 0.103672, acc 0.974359\n",
      "2018-04-16T00:51:28.061896: step 997, loss 0.104394, acc 0.96\n",
      "2018-04-16T00:51:28.120871: step 998, loss 0.124748, acc 0.96\n",
      "2018-04-16T00:51:28.188678: step 999, loss 0.155662, acc 0.94\n",
      "2018-04-16T00:51:28.251600: step 1000, loss 0.135904, acc 0.948718\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:28.361205: step 1000, loss 0.679224, acc 0.737401, rec 0.642473, pre 0.786184, f1 0.707101\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523839829/checkpoints/model-1000\n",
      "\n",
      "2018-04-16T00:51:28.475637: step 1001, loss 0.0899653, acc 0.96\n",
      "2018-04-16T00:51:28.535202: step 1002, loss 0.0687454, acc 0.98\n",
      "2018-04-16T00:51:28.594939: step 1003, loss 0.0383091, acc 0.99\n",
      "2018-04-16T00:51:28.640923: step 1004, loss 0.0670272, acc 0.974359\n",
      "2018-04-16T00:51:28.698334: step 1005, loss 0.134708, acc 0.96\n",
      "2018-04-16T00:51:28.753320: step 1006, loss 0.0980612, acc 0.97\n",
      "2018-04-16T00:51:28.812321: step 1007, loss 0.0834864, acc 0.98\n",
      "2018-04-16T00:51:28.858194: step 1008, loss 0.0830636, acc 0.961538\n",
      "2018-04-16T00:51:28.912849: step 1009, loss 0.0632028, acc 0.98\n",
      "2018-04-16T00:51:28.969432: step 1010, loss 0.0919178, acc 0.99\n",
      "2018-04-16T00:51:29.036404: step 1011, loss 0.104197, acc 0.98\n",
      "2018-04-16T00:51:29.096437: step 1012, loss 0.0466546, acc 0.987179\n",
      "2018-04-16T00:51:29.150048: step 1013, loss 0.0503158, acc 0.98\n",
      "2018-04-16T00:51:29.203493: step 1014, loss 0.12003, acc 0.97\n",
      "2018-04-16T00:51:29.261138: step 1015, loss 0.113721, acc 0.97\n",
      "2018-04-16T00:51:29.305368: step 1016, loss 0.049718, acc 0.987179\n",
      "2018-04-16T00:51:29.358995: step 1017, loss 0.0363026, acc 1\n",
      "2018-04-16T00:51:29.411644: step 1018, loss 0.087979, acc 0.96\n",
      "2018-04-16T00:51:29.469018: step 1019, loss 0.0718807, acc 0.98\n",
      "2018-04-16T00:51:29.514107: step 1020, loss 0.089744, acc 0.948718\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:29.619517: step 1020, loss 0.623103, acc 0.753316, rec 0.793011, pre 0.730198, f1 0.760309\n",
      "\n",
      "2018-04-16T00:51:29.677176: step 1021, loss 0.0538095, acc 0.98\n",
      "2018-04-16T00:51:29.735276: step 1022, loss 0.0776358, acc 0.98\n",
      "2018-04-16T00:51:29.789119: step 1023, loss 0.0412957, acc 0.99\n",
      "2018-04-16T00:51:29.834258: step 1024, loss 0.0814218, acc 0.948718\n",
      "2018-04-16T00:51:29.895032: step 1025, loss 0.0678866, acc 0.99\n",
      "2018-04-16T00:51:29.948720: step 1026, loss 0.029613, acc 0.99\n",
      "2018-04-16T00:51:30.003853: step 1027, loss 0.0586859, acc 0.98\n",
      "2018-04-16T00:51:30.107309: step 1028, loss 0.0541701, acc 0.974359\n",
      "2018-04-16T00:51:30.162638: step 1029, loss 0.172625, acc 0.95\n",
      "2018-04-16T00:51:30.218197: step 1030, loss 0.069239, acc 0.98\n",
      "2018-04-16T00:51:30.273552: step 1031, loss 0.0569721, acc 0.97\n",
      "2018-04-16T00:51:30.321142: step 1032, loss 0.0207439, acc 1\n",
      "2018-04-16T00:51:30.377230: step 1033, loss 0.0798779, acc 0.98\n",
      "2018-04-16T00:51:30.431329: step 1034, loss 0.164024, acc 0.94\n",
      "2018-04-16T00:51:30.483734: step 1035, loss 0.0616791, acc 0.97\n",
      "2018-04-16T00:51:30.531018: step 1036, loss 0.0813586, acc 0.974359\n",
      "2018-04-16T00:51:30.585042: step 1037, loss 0.0968003, acc 0.96\n",
      "2018-04-16T00:51:30.638262: step 1038, loss 0.0787755, acc 0.98\n",
      "2018-04-16T00:51:30.690970: step 1039, loss 0.0381688, acc 0.98\n",
      "2018-04-16T00:51:30.738324: step 1040, loss 0.0463761, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:30.841246: step 1040, loss 0.652609, acc 0.744032, rec 0.825269, pre 0.705747, f1 0.760843\n",
      "\n",
      "2018-04-16T00:51:30.894137: step 1041, loss 0.0593702, acc 0.98\n",
      "2018-04-16T00:51:30.949411: step 1042, loss 0.0325947, acc 0.99\n",
      "2018-04-16T00:51:31.002319: step 1043, loss 0.0739138, acc 0.98\n",
      "2018-04-16T00:51:31.046446: step 1044, loss 0.0568574, acc 0.987179\n",
      "2018-04-16T00:51:31.099130: step 1045, loss 0.0814178, acc 0.98\n",
      "2018-04-16T00:51:31.156377: step 1046, loss 0.0375007, acc 0.99\n",
      "2018-04-16T00:51:31.209040: step 1047, loss 0.0746246, acc 0.98\n",
      "2018-04-16T00:51:31.253818: step 1048, loss 0.0766124, acc 0.974359\n",
      "2018-04-16T00:51:31.309447: step 1049, loss 0.0856055, acc 0.97\n",
      "2018-04-16T00:51:31.366559: step 1050, loss 0.117909, acc 0.97\n",
      "2018-04-16T00:51:31.419017: step 1051, loss 0.105175, acc 0.96\n",
      "2018-04-16T00:51:31.463830: step 1052, loss 0.0350432, acc 1\n",
      "2018-04-16T00:51:31.520649: step 1053, loss 0.0273156, acc 0.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:51:31.583495: step 1054, loss 0.057667, acc 0.99\n",
      "2018-04-16T00:51:31.639443: step 1055, loss 0.0463881, acc 0.99\n",
      "2018-04-16T00:51:31.684576: step 1056, loss 0.0483102, acc 0.987179\n",
      "2018-04-16T00:51:31.740727: step 1057, loss 0.0309599, acc 1\n",
      "2018-04-16T00:51:31.800196: step 1058, loss 0.0590896, acc 0.98\n",
      "2018-04-16T00:51:31.856048: step 1059, loss 0.0500045, acc 0.99\n",
      "2018-04-16T00:51:31.900880: step 1060, loss 0.0407734, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:32.007972: step 1060, loss 0.666287, acc 0.741379, rec 0.825269, pre 0.702517, f1 0.758962\n",
      "\n",
      "2018-04-16T00:51:32.063793: step 1061, loss 0.0846325, acc 0.96\n",
      "2018-04-16T00:51:32.118888: step 1062, loss 0.0880807, acc 0.99\n",
      "2018-04-16T00:51:32.174944: step 1063, loss 0.0432454, acc 0.99\n",
      "2018-04-16T00:51:32.244005: step 1064, loss 0.0341001, acc 1\n",
      "2018-04-16T00:51:32.300998: step 1065, loss 0.0538584, acc 0.99\n",
      "2018-04-16T00:51:32.354168: step 1066, loss 0.0746164, acc 0.98\n",
      "2018-04-16T00:51:32.407714: step 1067, loss 0.0891595, acc 0.98\n",
      "2018-04-16T00:51:32.457446: step 1068, loss 0.102654, acc 0.948718\n",
      "2018-04-16T00:51:32.511986: step 1069, loss 0.07348, acc 0.99\n",
      "2018-04-16T00:51:32.568777: step 1070, loss 0.0459761, acc 0.99\n",
      "2018-04-16T00:51:32.623921: step 1071, loss 0.0540155, acc 0.99\n",
      "2018-04-16T00:51:32.672142: step 1072, loss 0.0504131, acc 1\n",
      "2018-04-16T00:51:32.726540: step 1073, loss 0.0601474, acc 0.98\n",
      "2018-04-16T00:51:32.786363: step 1074, loss 0.0452872, acc 0.99\n",
      "2018-04-16T00:51:32.841330: step 1075, loss 0.045306, acc 0.99\n",
      "2018-04-16T00:51:32.894460: step 1076, loss 0.0511832, acc 0.987179\n",
      "2018-04-16T00:51:32.948295: step 1077, loss 0.10152, acc 0.96\n",
      "2018-04-16T00:51:33.001505: step 1078, loss 0.0555429, acc 0.98\n",
      "2018-04-16T00:51:33.054324: step 1079, loss 0.0460271, acc 0.98\n",
      "2018-04-16T00:51:33.100705: step 1080, loss 0.0332678, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:33.202531: step 1080, loss 0.65035, acc 0.745358, rec 0.80914, pre 0.71327, f1 0.758186\n",
      "\n",
      "2018-04-16T00:51:33.256061: step 1081, loss 0.0575775, acc 0.98\n",
      "2018-04-16T00:51:33.316868: step 1082, loss 0.0495545, acc 0.98\n",
      "2018-04-16T00:51:33.370844: step 1083, loss 0.0964839, acc 0.95\n",
      "2018-04-16T00:51:33.413593: step 1084, loss 0.124173, acc 0.948718\n",
      "2018-04-16T00:51:33.467024: step 1085, loss 0.0650623, acc 0.97\n",
      "2018-04-16T00:51:33.524355: step 1086, loss 0.0588571, acc 0.99\n",
      "2018-04-16T00:51:33.577650: step 1087, loss 0.031876, acc 0.98\n",
      "2018-04-16T00:51:33.620305: step 1088, loss 0.0711905, acc 0.974359\n",
      "2018-04-16T00:51:33.672912: step 1089, loss 0.0444757, acc 0.99\n",
      "2018-04-16T00:51:33.731061: step 1090, loss 0.0461982, acc 0.99\n",
      "2018-04-16T00:51:33.785905: step 1091, loss 0.0418865, acc 0.99\n",
      "2018-04-16T00:51:33.831011: step 1092, loss 0.0475659, acc 0.987179\n",
      "2018-04-16T00:51:33.886525: step 1093, loss 0.0394399, acc 0.98\n",
      "2018-04-16T00:51:33.942283: step 1094, loss 0.0531255, acc 0.99\n",
      "2018-04-16T00:51:33.994301: step 1095, loss 0.0620117, acc 0.97\n",
      "2018-04-16T00:51:34.038592: step 1096, loss 0.156462, acc 0.948718\n",
      "2018-04-16T00:51:34.091944: step 1097, loss 0.186453, acc 0.92\n",
      "2018-04-16T00:51:34.149309: step 1098, loss 0.365555, acc 0.9\n",
      "2018-04-16T00:51:34.202540: step 1099, loss 0.217788, acc 0.92\n",
      "2018-04-16T00:51:34.246103: step 1100, loss 0.36646, acc 0.897436\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:34.347731: step 1100, loss 2.68066, acc 0.567639, rec 0.129032, pre 0.96, f1 0.227488\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523839829/checkpoints/model-1100\n",
      "\n",
      "2018-04-16T00:51:34.455896: step 1101, loss 0.685302, acc 0.84\n",
      "2018-04-16T00:51:34.508581: step 1102, loss 0.344437, acc 0.9\n",
      "2018-04-16T00:51:34.561341: step 1103, loss 0.0426104, acc 0.98\n",
      "2018-04-16T00:51:34.608441: step 1104, loss 0.091267, acc 0.974359\n",
      "2018-04-16T00:51:34.661508: step 1105, loss 0.124116, acc 0.96\n",
      "2018-04-16T00:51:34.713823: step 1106, loss 0.0542839, acc 0.98\n",
      "2018-04-16T00:51:34.766972: step 1107, loss 0.0370036, acc 0.99\n",
      "2018-04-16T00:51:34.813264: step 1108, loss 0.0396571, acc 1\n",
      "2018-04-16T00:51:34.866860: step 1109, loss 0.0374315, acc 1\n",
      "2018-04-16T00:51:34.919819: step 1110, loss 0.0582823, acc 0.98\n",
      "2018-04-16T00:51:34.971787: step 1111, loss 0.0386676, acc 0.99\n",
      "2018-04-16T00:51:35.019024: step 1112, loss 0.0764339, acc 0.974359\n",
      "2018-04-16T00:51:35.073941: step 1113, loss 0.0159162, acc 1\n",
      "2018-04-16T00:51:35.126049: step 1114, loss 0.105795, acc 0.96\n",
      "2018-04-16T00:51:35.177897: step 1115, loss 0.0940998, acc 0.97\n",
      "2018-04-16T00:51:35.224256: step 1116, loss 0.0251628, acc 1\n",
      "2018-04-16T00:51:35.277233: step 1117, loss 0.0316333, acc 1\n",
      "2018-04-16T00:51:35.329959: step 1118, loss 0.0729646, acc 0.97\n",
      "2018-04-16T00:51:35.385821: step 1119, loss 0.111535, acc 0.97\n",
      "2018-04-16T00:51:35.433900: step 1120, loss 0.048193, acc 0.974359\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:35.536553: step 1120, loss 0.694221, acc 0.730769, rec 0.825269, pre 0.689888, f1 0.75153\n",
      "\n",
      "2018-04-16T00:51:35.590596: step 1121, loss 0.0347368, acc 1\n",
      "2018-04-16T00:51:35.646032: step 1122, loss 0.0422031, acc 0.99\n",
      "2018-04-16T00:51:35.698337: step 1123, loss 0.0599542, acc 0.98\n",
      "2018-04-16T00:51:35.744182: step 1124, loss 0.0391058, acc 1\n",
      "2018-04-16T00:51:35.796619: step 1125, loss 0.0544639, acc 0.99\n",
      "2018-04-16T00:51:35.852890: step 1126, loss 0.0976199, acc 0.98\n",
      "2018-04-16T00:51:35.905776: step 1127, loss 0.0436051, acc 0.99\n",
      "2018-04-16T00:51:35.949180: step 1128, loss 0.0266373, acc 1\n",
      "2018-04-16T00:51:36.001276: step 1129, loss 0.0515297, acc 0.99\n",
      "2018-04-16T00:51:36.059658: step 1130, loss 0.0993658, acc 0.97\n",
      "2018-04-16T00:51:36.111896: step 1131, loss 0.0511318, acc 0.99\n",
      "2018-04-16T00:51:36.154751: step 1132, loss 0.0172543, acc 1\n",
      "2018-04-16T00:51:36.206584: step 1133, loss 0.0650437, acc 0.98\n",
      "2018-04-16T00:51:36.258491: step 1134, loss 0.0138955, acc 1\n",
      "2018-04-16T00:51:36.315484: step 1135, loss 0.056787, acc 0.98\n",
      "2018-04-16T00:51:36.359507: step 1136, loss 0.0637486, acc 0.974359\n",
      "2018-04-16T00:51:36.413335: step 1137, loss 0.0337653, acc 1\n",
      "2018-04-16T00:51:36.465207: step 1138, loss 0.0453152, acc 0.98\n",
      "2018-04-16T00:51:36.524297: step 1139, loss 0.0539829, acc 0.98\n",
      "2018-04-16T00:51:36.569421: step 1140, loss 0.0759906, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:36.669616: step 1140, loss 0.721025, acc 0.728117, rec 0.841398, pre 0.681917, f1 0.753309\n",
      "\n",
      "2018-04-16T00:51:36.730472: step 1141, loss 0.0546178, acc 0.99\n",
      "2018-04-16T00:51:36.785130: step 1142, loss 0.0840011, acc 0.97\n",
      "2018-04-16T00:51:36.837582: step 1143, loss 0.0941007, acc 0.97\n",
      "2018-04-16T00:51:36.880180: step 1144, loss 0.0429387, acc 1\n",
      "2018-04-16T00:51:36.937539: step 1145, loss 0.0239869, acc 1\n",
      "2018-04-16T00:51:36.990647: step 1146, loss 0.0773166, acc 0.96\n",
      "2018-04-16T00:51:37.047225: step 1147, loss 0.0981809, acc 0.97\n",
      "2018-04-16T00:51:37.094247: step 1148, loss 0.0128572, acc 1\n",
      "2018-04-16T00:51:37.151622: step 1149, loss 0.0366405, acc 0.99\n",
      "2018-04-16T00:51:37.205104: step 1150, loss 0.0303809, acc 0.99\n",
      "2018-04-16T00:51:37.257159: step 1151, loss 0.0739485, acc 0.98\n",
      "2018-04-16T00:51:37.300512: step 1152, loss 0.120176, acc 0.961538\n",
      "2018-04-16T00:51:37.357054: step 1153, loss 0.0455771, acc 0.99\n",
      "2018-04-16T00:51:37.411255: step 1154, loss 0.0555327, acc 0.98\n",
      "2018-04-16T00:51:37.465032: step 1155, loss 0.0832879, acc 0.98\n",
      "2018-04-16T00:51:37.509745: step 1156, loss 0.0722587, acc 0.987179\n",
      "2018-04-16T00:51:37.566563: step 1157, loss 0.0530711, acc 0.97\n",
      "2018-04-16T00:51:37.619806: step 1158, loss 0.110567, acc 0.96\n",
      "2018-04-16T00:51:37.672956: step 1159, loss 0.0274717, acc 1\n",
      "2018-04-16T00:51:37.716382: step 1160, loss 0.0663049, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:37.823887: step 1160, loss 1.28835, acc 0.612732, rec 0.946237, pre 0.564103, f1 0.706827\n",
      "\n",
      "2018-04-16T00:51:37.879237: step 1161, loss 0.0433024, acc 0.98\n",
      "2018-04-16T00:51:37.932208: step 1162, loss 0.0571603, acc 0.99\n",
      "2018-04-16T00:51:37.985371: step 1163, loss 0.0398234, acc 0.99\n",
      "2018-04-16T00:51:38.034074: step 1164, loss 0.0666893, acc 0.974359\n",
      "2018-04-16T00:51:38.088896: step 1165, loss 0.0322651, acc 1\n",
      "2018-04-16T00:51:38.142259: step 1166, loss 0.0330507, acc 0.99\n",
      "2018-04-16T00:51:38.194674: step 1167, loss 0.0998019, acc 0.96\n",
      "2018-04-16T00:51:38.242325: step 1168, loss 0.075752, acc 0.987179\n",
      "2018-04-16T00:51:38.295016: step 1169, loss 0.0232107, acc 1\n",
      "2018-04-16T00:51:38.348135: step 1170, loss 0.0392011, acc 1\n",
      "2018-04-16T00:51:38.401006: step 1171, loss 0.0522396, acc 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:51:38.451215: step 1172, loss 0.0634526, acc 0.974359\n",
      "2018-04-16T00:51:38.504069: step 1173, loss 0.0723521, acc 0.96\n",
      "2018-04-16T00:51:38.558381: step 1174, loss 0.0824976, acc 0.96\n",
      "2018-04-16T00:51:38.610470: step 1175, loss 0.0422679, acc 0.98\n",
      "2018-04-16T00:51:38.657526: step 1176, loss 0.0489219, acc 0.987179\n",
      "2018-04-16T00:51:38.710273: step 1177, loss 0.117443, acc 0.97\n",
      "2018-04-16T00:51:38.765572: step 1178, loss 0.0401551, acc 0.98\n",
      "2018-04-16T00:51:38.819731: step 1179, loss 0.0445875, acc 0.98\n",
      "2018-04-16T00:51:38.868093: step 1180, loss 0.0991881, acc 0.961538\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:38.995573: step 1180, loss 0.716104, acc 0.744032, rec 0.591398, pre 0.842912, f1 0.695103\n",
      "\n",
      "2018-04-16T00:51:39.050770: step 1181, loss 0.0390841, acc 0.98\n",
      "2018-04-16T00:51:39.109021: step 1182, loss 0.0248614, acc 1\n",
      "2018-04-16T00:51:39.162707: step 1183, loss 0.103826, acc 0.96\n",
      "2018-04-16T00:51:39.206711: step 1184, loss 0.0153886, acc 1\n",
      "2018-04-16T00:51:39.260147: step 1185, loss 0.0243085, acc 1\n",
      "2018-04-16T00:51:39.328722: step 1186, loss 0.0758836, acc 0.99\n",
      "2018-04-16T00:51:39.381369: step 1187, loss 0.079641, acc 0.98\n",
      "2018-04-16T00:51:39.425180: step 1188, loss 0.0814569, acc 0.961538\n",
      "2018-04-16T00:51:39.478286: step 1189, loss 0.072239, acc 0.97\n",
      "2018-04-16T00:51:39.537654: step 1190, loss 0.0826388, acc 0.98\n",
      "2018-04-16T00:51:39.592552: step 1191, loss 0.0365171, acc 0.99\n",
      "2018-04-16T00:51:39.635930: step 1192, loss 0.0275134, acc 1\n",
      "2018-04-16T00:51:39.700003: step 1193, loss 0.0628664, acc 0.98\n",
      "2018-04-16T00:51:39.786576: step 1194, loss 0.0758888, acc 0.97\n",
      "2018-04-16T00:51:39.865841: step 1195, loss 0.0575339, acc 0.99\n",
      "2018-04-16T00:51:39.934227: step 1196, loss 0.0526179, acc 0.974359\n",
      "2018-04-16T00:51:40.022876: step 1197, loss 0.0223758, acc 0.99\n",
      "2018-04-16T00:51:40.106149: step 1198, loss 0.0713646, acc 0.99\n",
      "2018-04-16T00:51:40.188523: step 1199, loss 0.0497039, acc 0.98\n",
      "2018-04-16T00:51:40.263377: step 1200, loss 0.0288268, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:40.431430: step 1200, loss 0.869572, acc 0.689655, rec 0.884409, pre 0.632692, f1 0.737668\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523839829/checkpoints/model-1200\n",
      "\n",
      "2018-04-16T00:51:40.586048: step 1201, loss 0.0773535, acc 0.96\n",
      "2018-04-16T00:51:40.638374: step 1202, loss 0.102084, acc 0.97\n",
      "2018-04-16T00:51:40.690172: step 1203, loss 0.0822913, acc 0.98\n",
      "2018-04-16T00:51:40.736756: step 1204, loss 0.0399289, acc 0.974359\n",
      "2018-04-16T00:51:40.789663: step 1205, loss 0.0504582, acc 0.98\n",
      "2018-04-16T00:51:40.843523: step 1206, loss 0.0310676, acc 0.99\n",
      "2018-04-16T00:51:40.896915: step 1207, loss 0.040924, acc 0.99\n",
      "2018-04-16T00:51:40.946629: step 1208, loss 0.0896332, acc 0.987179\n",
      "2018-04-16T00:51:41.001009: step 1209, loss 0.0635334, acc 0.98\n",
      "2018-04-16T00:51:41.058431: step 1210, loss 0.0428776, acc 0.99\n",
      "2018-04-16T00:51:41.114230: step 1211, loss 0.0318137, acc 1\n",
      "2018-04-16T00:51:41.162329: step 1212, loss 0.0471009, acc 0.987179\n",
      "2018-04-16T00:51:41.217461: step 1213, loss 0.0383757, acc 0.98\n",
      "2018-04-16T00:51:41.269773: step 1214, loss 0.020717, acc 1\n",
      "2018-04-16T00:51:41.322424: step 1215, loss 0.0568696, acc 0.98\n",
      "2018-04-16T00:51:41.369427: step 1216, loss 0.0769642, acc 0.961538\n",
      "2018-04-16T00:51:41.424276: step 1217, loss 0.121126, acc 0.94\n",
      "2018-04-16T00:51:41.478201: step 1218, loss 0.101379, acc 0.98\n",
      "2018-04-16T00:51:41.534933: step 1219, loss 0.14195, acc 0.95\n",
      "2018-04-16T00:51:41.588384: step 1220, loss 0.09106, acc 0.974359\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:41.691149: step 1220, loss 1.28342, acc 0.628647, rec 0.94086, pre 0.575658, f1 0.714286\n",
      "\n",
      "2018-04-16T00:51:41.745847: step 1221, loss 0.027179, acc 0.99\n",
      "2018-04-16T00:51:41.801797: step 1222, loss 0.112364, acc 0.97\n",
      "2018-04-16T00:51:41.854188: step 1223, loss 0.0944664, acc 0.97\n",
      "2018-04-16T00:51:41.897854: step 1224, loss 0.124061, acc 0.974359\n",
      "2018-04-16T00:51:41.950974: step 1225, loss 0.0924981, acc 0.96\n",
      "2018-04-16T00:51:42.008079: step 1226, loss 0.0724736, acc 0.98\n",
      "2018-04-16T00:51:42.060523: step 1227, loss 0.0366216, acc 1\n",
      "2018-04-16T00:51:42.103892: step 1228, loss 0.0907127, acc 0.948718\n",
      "2018-04-16T00:51:42.156675: step 1229, loss 0.0531396, acc 0.99\n",
      "2018-04-16T00:51:42.217678: step 1230, loss 0.0292675, acc 0.99\n",
      "2018-04-16T00:51:42.281407: step 1231, loss 0.0535598, acc 0.99\n",
      "2018-04-16T00:51:42.328110: step 1232, loss 0.0378835, acc 0.987179\n",
      "2018-04-16T00:51:42.384064: step 1233, loss 0.0613308, acc 0.97\n",
      "2018-04-16T00:51:42.441659: step 1234, loss 0.0199067, acc 1\n",
      "2018-04-16T00:51:42.494384: step 1235, loss 0.0315933, acc 0.99\n",
      "2018-04-16T00:51:42.537982: step 1236, loss 0.0624549, acc 0.974359\n",
      "2018-04-16T00:51:42.592786: step 1237, loss 0.0747775, acc 0.98\n",
      "2018-04-16T00:51:42.653800: step 1238, loss 0.0458136, acc 0.98\n",
      "2018-04-16T00:51:42.708075: step 1239, loss 0.0257171, acc 1\n",
      "2018-04-16T00:51:42.754786: step 1240, loss 0.0342929, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:42.860803: step 1240, loss 0.676769, acc 0.751989, rec 0.715054, pre 0.766571, f1 0.739917\n",
      "\n",
      "2018-04-16T00:51:42.913787: step 1241, loss 0.0191083, acc 1\n",
      "2018-04-16T00:51:42.967654: step 1242, loss 0.0431238, acc 0.98\n",
      "2018-04-16T00:51:43.021253: step 1243, loss 0.0671648, acc 0.98\n",
      "2018-04-16T00:51:43.069993: step 1244, loss 0.0332591, acc 1\n",
      "2018-04-16T00:51:43.123372: step 1245, loss 0.0258996, acc 1\n",
      "2018-04-16T00:51:43.176868: step 1246, loss 0.023306, acc 0.99\n",
      "2018-04-16T00:51:43.230276: step 1247, loss 0.0456558, acc 0.98\n",
      "2018-04-16T00:51:43.278888: step 1248, loss 0.0981192, acc 0.961538\n",
      "2018-04-16T00:51:43.334873: step 1249, loss 0.110166, acc 0.97\n",
      "2018-04-16T00:51:43.391999: step 1250, loss 0.084522, acc 0.96\n",
      "2018-04-16T00:51:43.445508: step 1251, loss 0.0976807, acc 0.95\n",
      "2018-04-16T00:51:43.496045: step 1252, loss 0.0735099, acc 0.961538\n",
      "2018-04-16T00:51:43.550982: step 1253, loss 0.0697709, acc 0.98\n",
      "2018-04-16T00:51:43.605248: step 1254, loss 0.114425, acc 0.94\n",
      "2018-04-16T00:51:43.658164: step 1255, loss 0.0667061, acc 0.99\n",
      "2018-04-16T00:51:43.707065: step 1256, loss 0.0636367, acc 0.974359\n",
      "2018-04-16T00:51:43.762386: step 1257, loss 0.0217158, acc 1\n",
      "2018-04-16T00:51:43.817106: step 1258, loss 0.121306, acc 0.95\n",
      "2018-04-16T00:51:43.887605: step 1259, loss 0.0482298, acc 0.99\n",
      "2018-04-16T00:51:43.945741: step 1260, loss 0.0542645, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:44.048294: step 1260, loss 0.715304, acc 0.741379, rec 0.66129, pre 0.780952, f1 0.716157\n",
      "\n",
      "2018-04-16T00:51:44.100140: step 1261, loss 0.0327795, acc 1\n",
      "2018-04-16T00:51:44.161382: step 1262, loss 0.0925562, acc 0.96\n",
      "2018-04-16T00:51:44.215141: step 1263, loss 0.0532058, acc 0.99\n",
      "2018-04-16T00:51:44.260189: step 1264, loss 0.0154419, acc 1\n",
      "2018-04-16T00:51:44.313314: step 1265, loss 0.0271523, acc 0.99\n",
      "2018-04-16T00:51:44.369661: step 1266, loss 0.0343369, acc 0.99\n",
      "2018-04-16T00:51:44.422718: step 1267, loss 0.0712313, acc 0.97\n",
      "2018-04-16T00:51:44.466116: step 1268, loss 0.0623943, acc 0.974359\n",
      "2018-04-16T00:51:44.521611: step 1269, loss 0.0520202, acc 0.99\n",
      "2018-04-16T00:51:44.578098: step 1270, loss 0.0634228, acc 0.99\n",
      "2018-04-16T00:51:44.630453: step 1271, loss 0.0197008, acc 1\n",
      "2018-04-16T00:51:44.673943: step 1272, loss 0.0441602, acc 0.974359\n",
      "2018-04-16T00:51:44.726424: step 1273, loss 0.0184588, acc 1\n",
      "2018-04-16T00:51:44.778547: step 1274, loss 0.0512453, acc 0.98\n",
      "2018-04-16T00:51:44.833974: step 1275, loss 0.125121, acc 0.97\n",
      "2018-04-16T00:51:44.878242: step 1276, loss 0.215617, acc 0.923077\n",
      "2018-04-16T00:51:44.930968: step 1277, loss 0.380983, acc 0.85\n",
      "2018-04-16T00:51:44.983025: step 1278, loss 0.669593, acc 0.81\n",
      "2018-04-16T00:51:45.039957: step 1279, loss 0.597779, acc 0.81\n",
      "2018-04-16T00:51:45.085971: step 1280, loss 0.14462, acc 0.935897\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:45.187607: step 1280, loss 0.65984, acc 0.750663, rec 0.790323, pre 0.727723, f1 0.757732\n",
      "\n",
      "2018-04-16T00:51:45.245473: step 1281, loss 0.073519, acc 0.99\n",
      "2018-04-16T00:51:45.299467: step 1282, loss 0.0201378, acc 1\n",
      "2018-04-16T00:51:45.352799: step 1283, loss 0.0483774, acc 0.99\n",
      "2018-04-16T00:51:45.397093: step 1284, loss 0.0790305, acc 0.961538\n",
      "2018-04-16T00:51:45.457326: step 1285, loss 0.0329598, acc 0.99\n",
      "2018-04-16T00:51:45.513736: step 1286, loss 0.0557031, acc 0.99\n",
      "2018-04-16T00:51:45.568564: step 1287, loss 0.0413287, acc 0.98\n",
      "2018-04-16T00:51:45.612385: step 1288, loss 0.100562, acc 0.961538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:51:45.671617: step 1289, loss 0.0476357, acc 0.99\n",
      "2018-04-16T00:51:45.726398: step 1290, loss 0.0271949, acc 1\n",
      "2018-04-16T00:51:45.778843: step 1291, loss 0.0413664, acc 0.98\n",
      "2018-04-16T00:51:45.825124: step 1292, loss 0.0197045, acc 1\n",
      "2018-04-16T00:51:45.889644: step 1293, loss 0.0349218, acc 0.99\n",
      "2018-04-16T00:51:45.949388: step 1294, loss 0.0118237, acc 1\n",
      "2018-04-16T00:51:46.007704: step 1295, loss 0.0438627, acc 0.98\n",
      "2018-04-16T00:51:46.050355: step 1296, loss 0.0507547, acc 0.974359\n",
      "2018-04-16T00:51:46.108313: step 1297, loss 0.0467114, acc 0.99\n",
      "2018-04-16T00:51:46.160982: step 1298, loss 0.0927581, acc 0.98\n",
      "2018-04-16T00:51:46.213864: step 1299, loss 0.0542064, acc 0.98\n",
      "2018-04-16T00:51:46.256637: step 1300, loss 0.0290649, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:46.361148: step 1300, loss 0.694179, acc 0.745358, rec 0.677419, pre 0.777778, f1 0.724138\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523839829/checkpoints/model-1300\n",
      "\n",
      "2018-04-16T00:51:46.489722: step 1301, loss 0.021259, acc 1\n",
      "2018-04-16T00:51:46.549488: step 1302, loss 0.0551413, acc 0.98\n",
      "2018-04-16T00:51:46.609838: step 1303, loss 0.0367934, acc 0.99\n",
      "2018-04-16T00:51:46.655157: step 1304, loss 0.0365405, acc 0.987179\n",
      "2018-04-16T00:51:46.715522: step 1305, loss 0.026321, acc 1\n",
      "2018-04-16T00:51:46.769450: step 1306, loss 0.0369994, acc 0.98\n",
      "2018-04-16T00:51:46.826933: step 1307, loss 0.0253617, acc 0.99\n",
      "2018-04-16T00:51:46.872378: step 1308, loss 0.0352975, acc 0.987179\n",
      "2018-04-16T00:51:46.929596: step 1309, loss 0.0145103, acc 1\n",
      "2018-04-16T00:51:46.984053: step 1310, loss 0.0215668, acc 1\n",
      "2018-04-16T00:51:47.044111: step 1311, loss 0.0713538, acc 0.97\n",
      "2018-04-16T00:51:47.088803: step 1312, loss 0.0702801, acc 0.961538\n",
      "2018-04-16T00:51:47.143478: step 1313, loss 0.0215648, acc 1\n",
      "2018-04-16T00:51:47.198313: step 1314, loss 0.037927, acc 0.99\n",
      "2018-04-16T00:51:47.257578: step 1315, loss 0.041427, acc 0.99\n",
      "2018-04-16T00:51:47.303529: step 1316, loss 0.0539654, acc 0.974359\n",
      "2018-04-16T00:51:47.360341: step 1317, loss 0.0564626, acc 0.99\n",
      "2018-04-16T00:51:47.414460: step 1318, loss 0.0406392, acc 0.98\n",
      "2018-04-16T00:51:47.469529: step 1319, loss 0.0954261, acc 0.97\n",
      "2018-04-16T00:51:47.511130: step 1320, loss 0.0183948, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:47.612359: step 1320, loss 0.731523, acc 0.742706, rec 0.669355, pre 0.778125, f1 0.719653\n",
      "\n",
      "2018-04-16T00:51:47.666199: step 1321, loss 0.0172225, acc 1\n",
      "2018-04-16T00:51:47.726891: step 1322, loss 0.04123, acc 0.99\n",
      "2018-04-16T00:51:47.779750: step 1323, loss 0.0596116, acc 0.98\n",
      "2018-04-16T00:51:47.825495: step 1324, loss 0.0215901, acc 1\n",
      "2018-04-16T00:51:47.879982: step 1325, loss 0.0439975, acc 0.99\n",
      "2018-04-16T00:51:47.938721: step 1326, loss 0.0315014, acc 0.99\n",
      "2018-04-16T00:51:47.992262: step 1327, loss 0.0186694, acc 1\n",
      "2018-04-16T00:51:48.035740: step 1328, loss 0.0401824, acc 0.987179\n",
      "2018-04-16T00:51:48.089796: step 1329, loss 0.0308303, acc 0.99\n",
      "2018-04-16T00:51:48.148766: step 1330, loss 0.0545344, acc 0.98\n",
      "2018-04-16T00:51:48.202200: step 1331, loss 0.0636134, acc 0.98\n",
      "2018-04-16T00:51:48.244724: step 1332, loss 0.0646585, acc 0.987179\n",
      "2018-04-16T00:51:48.299718: step 1333, loss 0.0402692, acc 0.99\n",
      "2018-04-16T00:51:48.355930: step 1334, loss 0.0338239, acc 0.98\n",
      "2018-04-16T00:51:48.408373: step 1335, loss 0.0392396, acc 0.99\n",
      "2018-04-16T00:51:48.450900: step 1336, loss 0.0122035, acc 1\n",
      "2018-04-16T00:51:48.503151: step 1337, loss 0.0284567, acc 1\n",
      "2018-04-16T00:51:48.561503: step 1338, loss 0.0177422, acc 1\n",
      "2018-04-16T00:51:48.613561: step 1339, loss 0.0456611, acc 0.99\n",
      "2018-04-16T00:51:48.656239: step 1340, loss 0.0181721, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:48.755389: step 1340, loss 0.697954, acc 0.758621, rec 0.776882, pre 0.744845, f1 0.760526\n",
      "\n",
      "2018-04-16T00:51:48.810738: step 1341, loss 0.017075, acc 1\n",
      "2018-04-16T00:51:48.863616: step 1342, loss 0.0503471, acc 0.97\n",
      "2018-04-16T00:51:48.917541: step 1343, loss 0.0580075, acc 0.98\n",
      "2018-04-16T00:51:48.961896: step 1344, loss 0.0443449, acc 0.974359\n",
      "2018-04-16T00:51:49.021029: step 1345, loss 0.0370377, acc 0.99\n",
      "2018-04-16T00:51:49.074415: step 1346, loss 0.0175781, acc 1\n",
      "2018-04-16T00:51:49.133287: step 1347, loss 0.0972816, acc 0.96\n",
      "2018-04-16T00:51:49.178717: step 1348, loss 0.0244673, acc 1\n",
      "2018-04-16T00:51:49.235795: step 1349, loss 0.0327148, acc 0.99\n",
      "2018-04-16T00:51:49.290351: step 1350, loss 0.0292221, acc 0.99\n",
      "2018-04-16T00:51:49.343944: step 1351, loss 0.0543758, acc 0.97\n",
      "2018-04-16T00:51:49.388460: step 1352, loss 0.0375867, acc 0.987179\n",
      "2018-04-16T00:51:49.446414: step 1353, loss 0.0176077, acc 0.99\n",
      "2018-04-16T00:51:49.501318: step 1354, loss 0.0309166, acc 0.99\n",
      "2018-04-16T00:51:49.555758: step 1355, loss 0.0317759, acc 0.99\n",
      "2018-04-16T00:51:49.601665: step 1356, loss 0.0723286, acc 0.974359\n",
      "2018-04-16T00:51:49.659250: step 1357, loss 0.0787895, acc 0.96\n",
      "2018-04-16T00:51:49.712932: step 1358, loss 0.040475, acc 0.99\n",
      "2018-04-16T00:51:49.767531: step 1359, loss 0.0293075, acc 0.99\n",
      "2018-04-16T00:51:49.813072: step 1360, loss 0.0346572, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:49.935258: step 1360, loss 0.718114, acc 0.744032, rec 0.803763, pre 0.713604, f1 0.756005\n",
      "\n",
      "2018-04-16T00:51:49.997948: step 1361, loss 0.0347569, acc 0.98\n",
      "2018-04-16T00:51:50.053244: step 1362, loss 0.0525301, acc 0.98\n",
      "2018-04-16T00:51:50.108103: step 1363, loss 0.0453349, acc 0.99\n",
      "2018-04-16T00:51:50.155591: step 1364, loss 0.012122, acc 1\n",
      "2018-04-16T00:51:50.208583: step 1365, loss 0.0456122, acc 0.97\n",
      "2018-04-16T00:51:50.260535: step 1366, loss 0.0259737, acc 0.99\n",
      "2018-04-16T00:51:50.313081: step 1367, loss 0.0325433, acc 0.99\n",
      "2018-04-16T00:51:50.355657: step 1368, loss 0.018457, acc 1\n",
      "2018-04-16T00:51:50.411642: step 1369, loss 0.0492444, acc 0.97\n",
      "2018-04-16T00:51:50.466688: step 1370, loss 0.0466224, acc 0.99\n",
      "2018-04-16T00:51:50.522844: step 1371, loss 0.0417796, acc 0.98\n",
      "2018-04-16T00:51:50.567970: step 1372, loss 0.0189923, acc 1\n",
      "2018-04-16T00:51:50.625029: step 1373, loss 0.0223516, acc 1\n",
      "2018-04-16T00:51:50.679171: step 1374, loss 0.0074652, acc 1\n",
      "2018-04-16T00:51:50.731598: step 1375, loss 0.0511788, acc 0.98\n",
      "2018-04-16T00:51:50.775651: step 1376, loss 0.0706044, acc 0.974359\n",
      "2018-04-16T00:51:50.837109: step 1377, loss 0.0405742, acc 0.99\n",
      "2018-04-16T00:51:50.890436: step 1378, loss 0.0301203, acc 0.98\n",
      "2018-04-16T00:51:50.945339: step 1379, loss 0.0253726, acc 0.99\n",
      "2018-04-16T00:51:50.992070: step 1380, loss 0.0256913, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:51.099905: step 1380, loss 0.980249, acc 0.67374, rec 0.884409, pre 0.618421, f1 0.727876\n",
      "\n",
      "2018-04-16T00:51:51.152858: step 1381, loss 0.0581156, acc 0.97\n",
      "2018-04-16T00:51:51.207811: step 1382, loss 0.0239744, acc 1\n",
      "2018-04-16T00:51:51.261144: step 1383, loss 0.0304772, acc 0.99\n",
      "2018-04-16T00:51:51.310589: step 1384, loss 0.0170391, acc 0.987179\n",
      "2018-04-16T00:51:51.364661: step 1385, loss 0.0524068, acc 0.99\n",
      "2018-04-16T00:51:51.418726: step 1386, loss 0.01746, acc 1\n",
      "2018-04-16T00:51:51.471728: step 1387, loss 0.0236205, acc 1\n",
      "2018-04-16T00:51:51.521601: step 1388, loss 0.0279805, acc 0.987179\n",
      "2018-04-16T00:51:51.577123: step 1389, loss 0.0434575, acc 0.99\n",
      "2018-04-16T00:51:51.629572: step 1390, loss 0.0270207, acc 0.99\n",
      "2018-04-16T00:51:51.682399: step 1391, loss 0.018923, acc 1\n",
      "2018-04-16T00:51:51.731804: step 1392, loss 0.0451456, acc 0.987179\n",
      "2018-04-16T00:51:51.786232: step 1393, loss 0.0321384, acc 1\n",
      "2018-04-16T00:51:51.856312: step 1394, loss 0.0396686, acc 0.99\n",
      "2018-04-16T00:51:51.912057: step 1395, loss 0.016423, acc 1\n",
      "2018-04-16T00:51:51.961506: step 1396, loss 0.0211296, acc 0.987179\n",
      "2018-04-16T00:51:52.016536: step 1397, loss 0.0513458, acc 0.99\n",
      "2018-04-16T00:51:52.072376: step 1398, loss 0.0565164, acc 0.98\n",
      "2018-04-16T00:51:52.125008: step 1399, loss 0.0182568, acc 1\n",
      "2018-04-16T00:51:52.172915: step 1400, loss 0.0199798, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:52.276154: step 1400, loss 0.732099, acc 0.746684, rec 0.696237, pre 0.768546, f1 0.730606\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523839829/checkpoints/model-1400\n",
      "\n",
      "2018-04-16T00:51:52.386020: step 1401, loss 0.0406286, acc 0.99\n",
      "2018-04-16T00:51:52.438807: step 1402, loss 0.0207584, acc 1\n",
      "2018-04-16T00:51:52.491849: step 1403, loss 0.0256308, acc 1\n",
      "2018-04-16T00:51:52.537790: step 1404, loss 0.0149168, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:51:52.596710: step 1405, loss 0.0307073, acc 0.99\n",
      "2018-04-16T00:51:52.650689: step 1406, loss 0.0238998, acc 1\n",
      "2018-04-16T00:51:52.706476: step 1407, loss 0.0447922, acc 0.98\n",
      "2018-04-16T00:51:52.750384: step 1408, loss 0.0784983, acc 0.961538\n",
      "2018-04-16T00:51:52.807191: step 1409, loss 0.0844332, acc 0.97\n",
      "2018-04-16T00:51:52.866913: step 1410, loss 0.0746431, acc 0.98\n",
      "2018-04-16T00:51:52.947037: step 1411, loss 0.0487828, acc 0.97\n",
      "2018-04-16T00:51:53.018925: step 1412, loss 0.0398343, acc 0.987179\n",
      "2018-04-16T00:51:53.102271: step 1413, loss 0.0278387, acc 0.99\n",
      "2018-04-16T00:51:53.185138: step 1414, loss 0.025201, acc 0.99\n",
      "2018-04-16T00:51:53.272761: step 1415, loss 0.0546221, acc 0.99\n",
      "2018-04-16T00:51:53.341591: step 1416, loss 0.0191541, acc 1\n",
      "2018-04-16T00:51:53.425274: step 1417, loss 0.0333778, acc 0.99\n",
      "2018-04-16T00:51:53.515421: step 1418, loss 0.0452002, acc 0.97\n",
      "2018-04-16T00:51:53.599870: step 1419, loss 0.0302445, acc 0.99\n",
      "2018-04-16T00:51:53.668194: step 1420, loss 0.0308837, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:53.836410: step 1420, loss 0.717281, acc 0.748011, rec 0.77957, pre 0.728643, f1 0.753247\n",
      "\n",
      "2018-04-16T00:51:53.915992: step 1421, loss 0.0316863, acc 0.99\n",
      "2018-04-16T00:51:53.996692: step 1422, loss 0.0282004, acc 0.99\n",
      "2018-04-16T00:51:54.084330: step 1423, loss 0.0313753, acc 0.99\n",
      "2018-04-16T00:51:54.151102: step 1424, loss 0.0115684, acc 1\n",
      "2018-04-16T00:51:54.232417: step 1425, loss 0.0448236, acc 0.99\n",
      "2018-04-16T00:51:54.315846: step 1426, loss 0.0411986, acc 0.98\n",
      "2018-04-16T00:51:54.394991: step 1427, loss 0.0268675, acc 1\n",
      "2018-04-16T00:51:54.461139: step 1428, loss 0.0148447, acc 1\n",
      "2018-04-16T00:51:54.547519: step 1429, loss 0.00930172, acc 1\n",
      "2018-04-16T00:51:54.626841: step 1430, loss 0.0533331, acc 0.98\n",
      "2018-04-16T00:51:54.708302: step 1431, loss 0.0251078, acc 1\n",
      "2018-04-16T00:51:54.758658: step 1432, loss 0.0378627, acc 0.987179\n",
      "2018-04-16T00:51:54.831248: step 1433, loss 0.0335082, acc 0.99\n",
      "2018-04-16T00:51:54.904961: step 1434, loss 0.0175993, acc 1\n",
      "2018-04-16T00:51:54.986359: step 1435, loss 0.0160647, acc 1\n",
      "2018-04-16T00:51:55.046364: step 1436, loss 0.0768127, acc 0.974359\n",
      "2018-04-16T00:51:55.105358: step 1437, loss 0.0208653, acc 1\n",
      "2018-04-16T00:51:55.161088: step 1438, loss 0.0794289, acc 0.98\n",
      "2018-04-16T00:51:55.220656: step 1439, loss 0.0279966, acc 0.99\n",
      "2018-04-16T00:51:55.265756: step 1440, loss 0.0262702, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:55.369882: step 1440, loss 1.27095, acc 0.639257, rec 0.930108, pre 0.584459, f1 0.717842\n",
      "\n",
      "2018-04-16T00:51:55.432442: step 1441, loss 0.0632518, acc 0.99\n",
      "2018-04-16T00:51:55.485965: step 1442, loss 0.01732, acc 1\n",
      "2018-04-16T00:51:55.545215: step 1443, loss 0.0216915, acc 0.99\n",
      "2018-04-16T00:51:55.597554: step 1444, loss 0.0509979, acc 0.974359\n",
      "2018-04-16T00:51:55.667005: step 1445, loss 0.0137518, acc 1\n",
      "2018-04-16T00:51:55.730411: step 1446, loss 0.0340064, acc 0.99\n",
      "2018-04-16T00:51:55.784871: step 1447, loss 0.0341058, acc 0.99\n",
      "2018-04-16T00:51:55.830781: step 1448, loss 0.0189188, acc 1\n",
      "2018-04-16T00:51:55.890625: step 1449, loss 0.0372612, acc 0.99\n",
      "2018-04-16T00:51:55.946046: step 1450, loss 0.0263872, acc 0.99\n",
      "2018-04-16T00:51:55.999346: step 1451, loss 0.0437418, acc 0.98\n",
      "2018-04-16T00:51:56.043019: step 1452, loss 0.0958729, acc 0.974359\n",
      "2018-04-16T00:51:56.104820: step 1453, loss 0.0311707, acc 0.99\n",
      "2018-04-16T00:51:56.164891: step 1454, loss 0.0502822, acc 0.98\n",
      "2018-04-16T00:51:56.223167: step 1455, loss 0.04342, acc 0.98\n",
      "2018-04-16T00:51:56.268782: step 1456, loss 0.0501636, acc 0.961538\n",
      "2018-04-16T00:51:56.327692: step 1457, loss 0.015409, acc 1\n",
      "2018-04-16T00:51:56.380825: step 1458, loss 0.018363, acc 1\n",
      "2018-04-16T00:51:56.434067: step 1459, loss 0.0187721, acc 0.99\n",
      "2018-04-16T00:51:56.477741: step 1460, loss 0.0278825, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:56.586660: step 1460, loss 0.732956, acc 0.750663, rec 0.795699, pre 0.72549, f1 0.758974\n",
      "\n",
      "2018-04-16T00:51:56.641375: step 1461, loss 0.016624, acc 1\n",
      "2018-04-16T00:51:56.694986: step 1462, loss 0.0315246, acc 0.99\n",
      "2018-04-16T00:51:56.746936: step 1463, loss 0.0262206, acc 0.99\n",
      "2018-04-16T00:51:56.794514: step 1464, loss 0.0554111, acc 0.974359\n",
      "2018-04-16T00:51:56.849751: step 1465, loss 0.0188095, acc 1\n",
      "2018-04-16T00:51:56.906812: step 1466, loss 0.0161699, acc 1\n",
      "2018-04-16T00:51:56.960443: step 1467, loss 0.0395897, acc 0.98\n",
      "2018-04-16T00:51:57.007469: step 1468, loss 0.0177522, acc 1\n",
      "2018-04-16T00:51:57.061628: step 1469, loss 0.0186646, acc 1\n",
      "2018-04-16T00:51:57.115012: step 1470, loss 0.0526046, acc 0.98\n",
      "2018-04-16T00:51:57.172183: step 1471, loss 0.0269867, acc 0.99\n",
      "2018-04-16T00:51:57.221019: step 1472, loss 0.0390131, acc 1\n",
      "2018-04-16T00:51:57.275215: step 1473, loss 0.0279033, acc 0.99\n",
      "2018-04-16T00:51:57.329433: step 1474, loss 0.0169331, acc 1\n",
      "2018-04-16T00:51:57.397623: step 1475, loss 0.0603878, acc 0.99\n",
      "2018-04-16T00:51:57.460120: step 1476, loss 0.0113994, acc 1\n",
      "2018-04-16T00:51:57.516645: step 1477, loss 0.0305158, acc 1\n",
      "2018-04-16T00:51:57.574382: step 1478, loss 0.0225161, acc 0.99\n",
      "2018-04-16T00:51:57.635865: step 1479, loss 0.0166819, acc 1\n",
      "2018-04-16T00:51:57.687210: step 1480, loss 0.0236129, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:57.789658: step 1480, loss 0.812357, acc 0.734748, rec 0.599462, pre 0.813869, f1 0.690402\n",
      "\n",
      "2018-04-16T00:51:57.841273: step 1481, loss 0.0558074, acc 0.98\n",
      "2018-04-16T00:51:57.905476: step 1482, loss 0.0331124, acc 0.99\n",
      "2018-04-16T00:51:57.962205: step 1483, loss 0.0438116, acc 0.99\n",
      "2018-04-16T00:51:58.008138: step 1484, loss 0.0372812, acc 0.987179\n",
      "2018-04-16T00:51:58.063039: step 1485, loss 0.0238308, acc 1\n",
      "2018-04-16T00:51:58.121119: step 1486, loss 0.0400338, acc 0.99\n",
      "2018-04-16T00:51:58.176419: step 1487, loss 0.010449, acc 1\n",
      "2018-04-16T00:51:58.222011: step 1488, loss 0.00715801, acc 1\n",
      "2018-04-16T00:51:58.277792: step 1489, loss 0.0163692, acc 1\n",
      "2018-04-16T00:51:58.338044: step 1490, loss 0.0234324, acc 1\n",
      "2018-04-16T00:51:58.393899: step 1491, loss 0.0167831, acc 1\n",
      "2018-04-16T00:51:58.440610: step 1492, loss 0.0488361, acc 0.974359\n",
      "2018-04-16T00:51:58.494682: step 1493, loss 0.0198732, acc 1\n",
      "2018-04-16T00:51:58.557210: step 1494, loss 0.0160336, acc 1\n",
      "2018-04-16T00:51:58.612452: step 1495, loss 0.030643, acc 0.98\n",
      "2018-04-16T00:51:58.661323: step 1496, loss 0.0164293, acc 1\n",
      "2018-04-16T00:51:58.718959: step 1497, loss 0.0188863, acc 1\n",
      "2018-04-16T00:51:58.779650: step 1498, loss 0.045255, acc 0.99\n",
      "2018-04-16T00:51:58.834023: step 1499, loss 0.0194351, acc 1\n",
      "2018-04-16T00:51:58.879442: step 1500, loss 0.0216976, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:51:58.989415: step 1500, loss 0.777832, acc 0.732095, rec 0.747312, pre 0.720207, f1 0.733509\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523839829/checkpoints/model-1500\n",
      "\n",
      "2018-04-16T00:51:59.099884: step 1501, loss 0.0154022, acc 1\n",
      "2018-04-16T00:51:59.154960: step 1502, loss 0.0348706, acc 0.99\n",
      "2018-04-16T00:51:59.213026: step 1503, loss 0.0222228, acc 1\n",
      "2018-04-16T00:51:59.257445: step 1504, loss 0.0254912, acc 1\n",
      "2018-04-16T00:51:59.310627: step 1505, loss 0.00781993, acc 1\n",
      "2018-04-16T00:51:59.363889: step 1506, loss 0.0250833, acc 0.99\n",
      "2018-04-16T00:51:59.421018: step 1507, loss 0.0403892, acc 0.99\n",
      "2018-04-16T00:51:59.464984: step 1508, loss 0.0436987, acc 0.987179\n",
      "2018-04-16T00:51:59.517195: step 1509, loss 0.0196397, acc 1\n",
      "2018-04-16T00:51:59.570357: step 1510, loss 0.0300993, acc 1\n",
      "2018-04-16T00:51:59.627469: step 1511, loss 0.0155871, acc 1\n",
      "2018-04-16T00:51:59.672613: step 1512, loss 0.0349663, acc 1\n",
      "2018-04-16T00:51:59.727542: step 1513, loss 0.0756897, acc 0.97\n",
      "2018-04-16T00:51:59.780825: step 1514, loss 0.0405823, acc 0.99\n",
      "2018-04-16T00:51:59.839046: step 1515, loss 0.038423, acc 0.99\n",
      "2018-04-16T00:51:59.883579: step 1516, loss 0.0108885, acc 1\n",
      "2018-04-16T00:51:59.936160: step 1517, loss 0.0142278, acc 1\n",
      "2018-04-16T00:51:59.988936: step 1518, loss 0.0140779, acc 1\n",
      "2018-04-16T00:52:00.047798: step 1519, loss 0.027387, acc 0.99\n",
      "2018-04-16T00:52:00.096564: step 1520, loss 0.0104638, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:52:00.197273: step 1520, loss 0.759275, acc 0.740053, rec 0.741935, pre 0.734043, f1 0.737968\n",
      "\n",
      "2018-04-16T00:52:00.253712: step 1521, loss 0.017988, acc 0.99\n",
      "2018-04-16T00:52:00.306975: step 1522, loss 0.0372113, acc 0.99\n",
      "2018-04-16T00:52:00.361573: step 1523, loss 0.0139127, acc 1\n",
      "2018-04-16T00:52:00.408100: step 1524, loss 0.026611, acc 0.987179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:52:00.465987: step 1525, loss 0.0135093, acc 1\n",
      "2018-04-16T00:52:00.520532: step 1526, loss 0.0361836, acc 0.99\n",
      "2018-04-16T00:52:00.577804: step 1527, loss 0.0112239, acc 1\n",
      "2018-04-16T00:52:00.624010: step 1528, loss 0.0362256, acc 0.974359\n",
      "2018-04-16T00:52:00.682644: step 1529, loss 0.00887161, acc 1\n",
      "2018-04-16T00:52:00.739752: step 1530, loss 0.0245841, acc 0.99\n",
      "2018-04-16T00:52:00.794031: step 1531, loss 0.0188469, acc 1\n",
      "2018-04-16T00:52:00.839391: step 1532, loss 0.0141256, acc 1\n",
      "2018-04-16T00:52:00.904538: step 1533, loss 0.0378216, acc 0.99\n",
      "2018-04-16T00:52:00.968912: step 1534, loss 0.0141803, acc 1\n",
      "2018-04-16T00:52:01.022017: step 1535, loss 0.0551792, acc 0.98\n",
      "2018-04-16T00:52:01.065961: step 1536, loss 0.0948722, acc 0.974359\n",
      "2018-04-16T00:52:01.126263: step 1537, loss 0.046698, acc 0.98\n",
      "2018-04-16T00:52:01.183836: step 1538, loss 0.029813, acc 0.99\n",
      "2018-04-16T00:52:01.236246: step 1539, loss 0.0430203, acc 0.99\n",
      "2018-04-16T00:52:01.279222: step 1540, loss 0.0435081, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:52:01.392945: step 1540, loss 0.740292, acc 0.745358, rec 0.763441, pre 0.731959, f1 0.747368\n",
      "\n",
      "2018-04-16T00:52:01.450675: step 1541, loss 0.0107636, acc 1\n",
      "2018-04-16T00:52:01.506448: step 1542, loss 0.0142332, acc 1\n",
      "2018-04-16T00:52:01.561246: step 1543, loss 0.0293008, acc 0.99\n",
      "2018-04-16T00:52:01.611856: step 1544, loss 0.0538934, acc 0.974359\n",
      "2018-04-16T00:52:01.669337: step 1545, loss 0.0584907, acc 0.98\n",
      "2018-04-16T00:52:01.725860: step 1546, loss 0.0232751, acc 1\n",
      "2018-04-16T00:52:01.778431: step 1547, loss 0.0177202, acc 0.99\n",
      "2018-04-16T00:52:01.826059: step 1548, loss 0.0203344, acc 1\n",
      "2018-04-16T00:52:01.881100: step 1549, loss 0.0234719, acc 0.99\n",
      "2018-04-16T00:52:01.936310: step 1550, loss 0.0129267, acc 1\n",
      "2018-04-16T00:52:01.990182: step 1551, loss 0.0227257, acc 0.99\n",
      "2018-04-16T00:52:02.045275: step 1552, loss 0.0271389, acc 0.987179\n",
      "2018-04-16T00:52:02.103436: step 1553, loss 0.0283446, acc 0.99\n",
      "2018-04-16T00:52:02.158756: step 1554, loss 0.0170667, acc 1\n",
      "2018-04-16T00:52:02.215783: step 1555, loss 0.0265075, acc 0.99\n",
      "2018-04-16T00:52:02.266090: step 1556, loss 0.0187807, acc 1\n",
      "2018-04-16T00:52:02.324955: step 1557, loss 0.0188114, acc 1\n",
      "2018-04-16T00:52:02.379782: step 1558, loss 0.0230975, acc 0.99\n",
      "2018-04-16T00:52:02.436852: step 1559, loss 0.0347354, acc 0.98\n",
      "2018-04-16T00:52:02.486071: step 1560, loss 0.0897952, acc 0.974359\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:52:02.591745: step 1560, loss 2.05743, acc 0.564987, rec 0.948925, pre 0.533233, f1 0.682785\n",
      "\n",
      "2018-04-16T00:52:02.644169: step 1561, loss 0.282672, acc 0.89\n",
      "2018-04-16T00:52:02.701126: step 1562, loss 0.412668, acc 0.9\n",
      "2018-04-16T00:52:02.753731: step 1563, loss 0.341207, acc 0.89\n",
      "2018-04-16T00:52:02.797508: step 1564, loss 0.220493, acc 0.923077\n",
      "2018-04-16T00:52:02.854498: step 1565, loss 0.0326479, acc 0.99\n",
      "2018-04-16T00:52:02.926742: step 1566, loss 0.0365842, acc 0.99\n",
      "2018-04-16T00:52:02.979142: step 1567, loss 0.0351088, acc 0.99\n",
      "2018-04-16T00:52:03.021966: step 1568, loss 0.0510612, acc 0.987179\n",
      "2018-04-16T00:52:03.074291: step 1569, loss 0.0468904, acc 0.99\n",
      "2018-04-16T00:52:03.126513: step 1570, loss 0.0215232, acc 1\n",
      "2018-04-16T00:52:03.182801: step 1571, loss 0.0277304, acc 0.99\n",
      "2018-04-16T00:52:03.226123: step 1572, loss 0.0192501, acc 0.987179\n",
      "2018-04-16T00:52:03.280055: step 1573, loss 0.0222464, acc 0.99\n",
      "2018-04-16T00:52:03.332126: step 1574, loss 0.0189363, acc 1\n",
      "2018-04-16T00:52:03.388755: step 1575, loss 0.058373, acc 0.97\n",
      "2018-04-16T00:52:03.432654: step 1576, loss 0.0107641, acc 1\n",
      "2018-04-16T00:52:03.491893: step 1577, loss 0.0303653, acc 1\n",
      "2018-04-16T00:52:03.554534: step 1578, loss 0.0260182, acc 0.99\n",
      "2018-04-16T00:52:03.610633: step 1579, loss 0.0138587, acc 1\n",
      "2018-04-16T00:52:03.654539: step 1580, loss 0.0348894, acc 0.974359\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:52:03.757237: step 1580, loss 0.824318, acc 0.718833, rec 0.768817, pre 0.694175, f1 0.729592\n",
      "\n",
      "2018-04-16T00:52:03.809706: step 1581, loss 0.0372666, acc 0.98\n",
      "2018-04-16T00:52:03.869293: step 1582, loss 0.0120932, acc 1\n",
      "2018-04-16T00:52:03.924086: step 1583, loss 0.0237772, acc 1\n",
      "2018-04-16T00:52:03.968342: step 1584, loss 0.0280555, acc 0.987179\n",
      "2018-04-16T00:52:04.021167: step 1585, loss 0.023016, acc 0.99\n",
      "2018-04-16T00:52:04.079118: step 1586, loss 0.0214924, acc 0.99\n",
      "2018-04-16T00:52:04.133827: step 1587, loss 0.0317566, acc 0.99\n",
      "2018-04-16T00:52:04.177590: step 1588, loss 0.0340519, acc 0.987179\n",
      "2018-04-16T00:52:04.233847: step 1589, loss 0.00984028, acc 1\n",
      "2018-04-16T00:52:04.295470: step 1590, loss 0.0146613, acc 0.99\n",
      "2018-04-16T00:52:04.348939: step 1591, loss 0.0328709, acc 0.98\n",
      "2018-04-16T00:52:04.392013: step 1592, loss 0.00560434, acc 1\n",
      "2018-04-16T00:52:04.446137: step 1593, loss 0.01495, acc 1\n",
      "2018-04-16T00:52:04.503810: step 1594, loss 0.0413889, acc 0.98\n",
      "2018-04-16T00:52:04.559573: step 1595, loss 0.0519117, acc 0.99\n",
      "2018-04-16T00:52:04.603078: step 1596, loss 0.0279694, acc 1\n",
      "2018-04-16T00:52:04.661413: step 1597, loss 0.0135811, acc 1\n",
      "2018-04-16T00:52:04.723538: step 1598, loss 0.0325645, acc 0.98\n",
      "2018-04-16T00:52:04.776778: step 1599, loss 0.0590359, acc 0.98\n",
      "2018-04-16T00:52:04.820391: step 1600, loss 0.0533822, acc 0.974359\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:52:04.921449: step 1600, loss 1.21155, acc 0.659151, rec 0.903226, pre 0.603232, f1 0.723358\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523839829/checkpoints/model-1600\n",
      "\n",
      "2018-04-16T00:52:05.037920: step 1601, loss 0.0251943, acc 0.98\n",
      "2018-04-16T00:52:05.092104: step 1602, loss 0.0215288, acc 0.99\n",
      "2018-04-16T00:52:05.144745: step 1603, loss 0.0462048, acc 0.98\n",
      "2018-04-16T00:52:05.193773: step 1604, loss 0.027371, acc 1\n",
      "2018-04-16T00:52:05.247344: step 1605, loss 0.00831286, acc 1\n",
      "2018-04-16T00:52:05.300992: step 1606, loss 0.0413571, acc 0.99\n",
      "2018-04-16T00:52:05.355793: step 1607, loss 0.0475816, acc 0.98\n",
      "2018-04-16T00:52:05.405531: step 1608, loss 0.0389594, acc 0.987179\n",
      "2018-04-16T00:52:05.462866: step 1609, loss 0.0509473, acc 0.98\n",
      "2018-04-16T00:52:05.517324: step 1610, loss 0.0117921, acc 1\n",
      "2018-04-16T00:52:05.571339: step 1611, loss 0.00848323, acc 1\n",
      "2018-04-16T00:52:05.622972: step 1612, loss 0.0122902, acc 1\n",
      "2018-04-16T00:52:05.678145: step 1613, loss 0.0125617, acc 0.99\n",
      "2018-04-16T00:52:05.732354: step 1614, loss 0.00962893, acc 1\n",
      "2018-04-16T00:52:05.788003: step 1615, loss 0.0280034, acc 0.99\n",
      "2018-04-16T00:52:05.836295: step 1616, loss 0.0709232, acc 0.974359\n",
      "2018-04-16T00:52:05.889759: step 1617, loss 0.115114, acc 0.96\n",
      "2018-04-16T00:52:05.944145: step 1618, loss 0.0778755, acc 0.97\n",
      "2018-04-16T00:52:05.998707: step 1619, loss 0.0219619, acc 1\n",
      "2018-04-16T00:52:06.048034: step 1620, loss 0.012613, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:52:06.151914: step 1620, loss 0.884974, acc 0.712202, rec 0.763441, pre 0.687651, f1 0.723567\n",
      "\n",
      "2018-04-16T00:52:06.209664: step 1621, loss 0.0147245, acc 1\n",
      "2018-04-16T00:52:06.280099: step 1622, loss 0.0378448, acc 0.99\n",
      "2018-04-16T00:52:06.340074: step 1623, loss 0.0692436, acc 0.97\n",
      "2018-04-16T00:52:06.383291: step 1624, loss 0.00856875, acc 1\n",
      "2018-04-16T00:52:06.436839: step 1625, loss 0.0152739, acc 1\n",
      "2018-04-16T00:52:06.496090: step 1626, loss 0.0240812, acc 0.99\n",
      "2018-04-16T00:52:06.552793: step 1627, loss 0.0377138, acc 0.99\n",
      "2018-04-16T00:52:06.597902: step 1628, loss 0.0218915, acc 1\n",
      "2018-04-16T00:52:06.662498: step 1629, loss 0.023641, acc 0.99\n",
      "2018-04-16T00:52:06.728507: step 1630, loss 0.0140144, acc 1\n",
      "2018-04-16T00:52:06.782260: step 1631, loss 0.00971996, acc 1\n",
      "2018-04-16T00:52:06.825438: step 1632, loss 0.0110166, acc 1\n",
      "2018-04-16T00:52:06.878368: step 1633, loss 0.0283437, acc 0.99\n",
      "2018-04-16T00:52:06.936515: step 1634, loss 0.0298682, acc 0.99\n",
      "2018-04-16T00:52:06.989765: step 1635, loss 0.0228875, acc 1\n",
      "2018-04-16T00:52:07.034269: step 1636, loss 0.0078657, acc 1\n",
      "2018-04-16T00:52:07.088495: step 1637, loss 0.016668, acc 1\n",
      "2018-04-16T00:52:07.146442: step 1638, loss 0.0140617, acc 1\n",
      "2018-04-16T00:52:07.201414: step 1639, loss 0.0089495, acc 1\n",
      "2018-04-16T00:52:07.245894: step 1640, loss 0.0538334, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:52:07.347584: step 1640, loss 0.957962, acc 0.712202, rec 0.543011, pre 0.811245, f1 0.650564\n",
      "\n",
      "2018-04-16T00:52:07.403032: step 1641, loss 0.0275894, acc 0.99\n",
      "2018-04-16T00:52:07.457253: step 1642, loss 0.0240809, acc 0.99\n",
      "2018-04-16T00:52:07.512129: step 1643, loss 0.0235342, acc 1\n",
      "2018-04-16T00:52:07.559098: step 1644, loss 0.0138602, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:52:07.618104: step 1645, loss 0.0238601, acc 0.99\n",
      "2018-04-16T00:52:07.672670: step 1646, loss 0.0141624, acc 1\n",
      "2018-04-16T00:52:07.763933: step 1647, loss 0.0170964, acc 0.99\n",
      "2018-04-16T00:52:07.830200: step 1648, loss 0.0219594, acc 1\n",
      "2018-04-16T00:52:07.887212: step 1649, loss 0.0297371, acc 0.99\n",
      "2018-04-16T00:52:07.942196: step 1650, loss 0.0167779, acc 1\n",
      "2018-04-16T00:52:07.996708: step 1651, loss 0.0321615, acc 0.99\n",
      "2018-04-16T00:52:08.044823: step 1652, loss 0.035186, acc 0.987179\n",
      "2018-04-16T00:52:08.098075: step 1653, loss 0.0215082, acc 0.99\n",
      "2018-04-16T00:52:08.151638: step 1654, loss 0.00967932, acc 1\n",
      "2018-04-16T00:52:08.208848: step 1655, loss 0.0135795, acc 1\n",
      "2018-04-16T00:52:08.256044: step 1656, loss 0.0246696, acc 0.987179\n",
      "2018-04-16T00:52:08.308431: step 1657, loss 0.0241728, acc 1\n",
      "2018-04-16T00:52:08.361113: step 1658, loss 0.0183557, acc 1\n",
      "2018-04-16T00:52:08.413042: step 1659, loss 0.0363264, acc 0.99\n",
      "2018-04-16T00:52:08.455599: step 1660, loss 0.00705152, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:52:08.559283: step 1660, loss 0.797179, acc 0.741379, rec 0.704301, pre 0.755043, f1 0.72879\n",
      "\n",
      "2018-04-16T00:52:08.612869: step 1661, loss 0.018853, acc 1\n",
      "2018-04-16T00:52:08.671203: step 1662, loss 0.0102359, acc 1\n",
      "2018-04-16T00:52:08.726728: step 1663, loss 0.0336669, acc 0.99\n",
      "2018-04-16T00:52:08.770224: step 1664, loss 0.0147256, acc 1\n",
      "2018-04-16T00:52:08.823334: step 1665, loss 0.0141801, acc 1\n",
      "2018-04-16T00:52:08.880744: step 1666, loss 0.008503, acc 1\n",
      "2018-04-16T00:52:08.935464: step 1667, loss 0.0217666, acc 1\n",
      "2018-04-16T00:52:08.979697: step 1668, loss 0.0191017, acc 1\n",
      "2018-04-16T00:52:09.033039: step 1669, loss 0.00623374, acc 1\n",
      "2018-04-16T00:52:09.090797: step 1670, loss 0.0240753, acc 1\n",
      "2018-04-16T00:52:09.145820: step 1671, loss 0.0420388, acc 0.99\n",
      "2018-04-16T00:52:09.190085: step 1672, loss 0.078832, acc 0.974359\n",
      "2018-04-16T00:52:09.244134: step 1673, loss 0.0766405, acc 0.98\n",
      "2018-04-16T00:52:09.303209: step 1674, loss 0.0503059, acc 0.99\n",
      "2018-04-16T00:52:09.361960: step 1675, loss 0.0446688, acc 0.99\n",
      "2018-04-16T00:52:09.405695: step 1676, loss 0.00785797, acc 1\n",
      "2018-04-16T00:52:09.459845: step 1677, loss 0.00742264, acc 1\n",
      "2018-04-16T00:52:09.517385: step 1678, loss 0.0361531, acc 0.98\n",
      "2018-04-16T00:52:09.573447: step 1679, loss 0.0101346, acc 1\n",
      "2018-04-16T00:52:09.617957: step 1680, loss 0.0109596, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:52:09.722905: step 1680, loss 0.81182, acc 0.736074, rec 0.739247, pre 0.729443, f1 0.734312\n",
      "\n",
      "2018-04-16T00:52:09.776429: step 1681, loss 0.0211203, acc 0.99\n",
      "2018-04-16T00:52:09.829777: step 1682, loss 0.0146231, acc 1\n",
      "2018-04-16T00:52:09.882064: step 1683, loss 0.0149594, acc 0.99\n",
      "2018-04-16T00:52:09.928917: step 1684, loss 0.0279605, acc 0.974359\n",
      "2018-04-16T00:52:09.982828: step 1685, loss 0.0252028, acc 0.99\n",
      "2018-04-16T00:52:10.036327: step 1686, loss 0.0369748, acc 0.99\n",
      "2018-04-16T00:52:10.089882: step 1687, loss 0.0237866, acc 1\n",
      "2018-04-16T00:52:10.137296: step 1688, loss 0.0110179, acc 1\n",
      "2018-04-16T00:52:10.191801: step 1689, loss 0.0150236, acc 1\n",
      "2018-04-16T00:52:10.247271: step 1690, loss 0.0499073, acc 0.99\n",
      "2018-04-16T00:52:10.301590: step 1691, loss 0.0183689, acc 1\n",
      "2018-04-16T00:52:10.350062: step 1692, loss 0.0272953, acc 0.987179\n",
      "2018-04-16T00:52:10.403244: step 1693, loss 0.0220024, acc 0.99\n",
      "2018-04-16T00:52:10.458822: step 1694, loss 0.0197371, acc 0.99\n",
      "2018-04-16T00:52:10.511990: step 1695, loss 0.0256652, acc 0.99\n",
      "2018-04-16T00:52:10.559829: step 1696, loss 0.023168, acc 1\n",
      "2018-04-16T00:52:10.612059: step 1697, loss 0.0363582, acc 0.99\n",
      "2018-04-16T00:52:10.677679: step 1698, loss 0.0386592, acc 0.98\n",
      "2018-04-16T00:52:10.734095: step 1699, loss 0.0219728, acc 1\n",
      "2018-04-16T00:52:10.782068: step 1700, loss 0.0343473, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:52:10.886436: step 1700, loss 0.83268, acc 0.734748, rec 0.637097, pre 0.784768, f1 0.703264\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523839829/checkpoints/model-1700\n",
      "\n",
      "2018-04-16T00:52:10.995413: step 1701, loss 0.01444, acc 0.99\n",
      "2018-04-16T00:52:11.048274: step 1702, loss 0.0163055, acc 1\n",
      "2018-04-16T00:52:11.102496: step 1703, loss 0.0160096, acc 1\n",
      "2018-04-16T00:52:11.146128: step 1704, loss 0.0305108, acc 0.987179\n",
      "2018-04-16T00:52:11.203437: step 1705, loss 0.0126686, acc 1\n",
      "2018-04-16T00:52:11.256519: step 1706, loss 0.0420743, acc 0.99\n",
      "2018-04-16T00:52:11.309775: step 1707, loss 0.0104529, acc 1\n",
      "2018-04-16T00:52:11.352825: step 1708, loss 0.00791332, acc 1\n",
      "2018-04-16T00:52:11.409940: step 1709, loss 0.0234971, acc 0.99\n",
      "2018-04-16T00:52:11.462021: step 1710, loss 0.0157298, acc 1\n",
      "2018-04-16T00:52:11.514887: step 1711, loss 0.0297216, acc 0.99\n",
      "2018-04-16T00:52:11.557802: step 1712, loss 0.0106393, acc 1\n",
      "2018-04-16T00:52:11.610708: step 1713, loss 0.0311248, acc 0.99\n",
      "2018-04-16T00:52:11.667113: step 1714, loss 0.0371429, acc 0.98\n",
      "2018-04-16T00:52:11.722230: step 1715, loss 0.0112298, acc 1\n",
      "2018-04-16T00:52:11.766890: step 1716, loss 0.0342887, acc 0.974359\n",
      "2018-04-16T00:52:11.819661: step 1717, loss 0.0186899, acc 0.99\n",
      "2018-04-16T00:52:11.876102: step 1718, loss 0.0165077, acc 1\n",
      "2018-04-16T00:52:11.928650: step 1719, loss 0.00460472, acc 1\n",
      "2018-04-16T00:52:11.971165: step 1720, loss 0.0821158, acc 0.961538\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:52:12.072956: step 1720, loss 1.12904, acc 0.697613, rec 0.475806, pre 0.842857, f1 0.608247\n",
      "\n",
      "2018-04-16T00:52:12.129835: step 1721, loss 0.0487502, acc 0.98\n",
      "2018-04-16T00:52:12.183417: step 1722, loss 0.0144302, acc 1\n",
      "2018-04-16T00:52:12.237258: step 1723, loss 0.0201039, acc 1\n",
      "2018-04-16T00:52:12.283508: step 1724, loss 0.00944222, acc 1\n",
      "2018-04-16T00:52:12.340614: step 1725, loss 0.00502211, acc 1\n",
      "2018-04-16T00:52:12.394682: step 1726, loss 0.040744, acc 0.99\n",
      "2018-04-16T00:52:12.448389: step 1727, loss 0.0191676, acc 0.99\n",
      "2018-04-16T00:52:12.492664: step 1728, loss 0.0100005, acc 1\n",
      "2018-04-16T00:52:12.550629: step 1729, loss 0.0207057, acc 0.99\n",
      "2018-04-16T00:52:12.605133: step 1730, loss 0.0209662, acc 1\n",
      "2018-04-16T00:52:12.658898: step 1731, loss 0.0112423, acc 1\n",
      "2018-04-16T00:52:12.703524: step 1732, loss 0.0154031, acc 1\n",
      "2018-04-16T00:52:12.763257: step 1733, loss 0.0109021, acc 1\n",
      "2018-04-16T00:52:12.815617: step 1734, loss 0.0457222, acc 0.98\n",
      "2018-04-16T00:52:12.868684: step 1735, loss 0.0247862, acc 1\n",
      "2018-04-16T00:52:12.913934: step 1736, loss 0.0288, acc 0.987179\n",
      "2018-04-16T00:52:12.971243: step 1737, loss 0.00954215, acc 1\n",
      "2018-04-16T00:52:13.023565: step 1738, loss 0.016326, acc 1\n",
      "2018-04-16T00:52:13.078862: step 1739, loss 0.0162482, acc 1\n",
      "2018-04-16T00:52:13.124426: step 1740, loss 0.0339778, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:52:13.230783: step 1740, loss 0.975026, acc 0.70557, rec 0.817204, pre 0.663755, f1 0.73253\n",
      "\n",
      "2018-04-16T00:52:13.283884: step 1741, loss 0.042568, acc 0.98\n",
      "2018-04-16T00:52:13.337068: step 1742, loss 0.0286917, acc 0.99\n",
      "2018-04-16T00:52:13.389682: step 1743, loss 0.024866, acc 0.99\n",
      "2018-04-16T00:52:13.438022: step 1744, loss 0.0365601, acc 0.974359\n",
      "2018-04-16T00:52:13.490947: step 1745, loss 0.0107036, acc 1\n",
      "2018-04-16T00:52:13.546520: step 1746, loss 0.0518137, acc 0.99\n",
      "2018-04-16T00:52:13.598927: step 1747, loss 0.0139786, acc 1\n",
      "2018-04-16T00:52:13.645268: step 1748, loss 0.0120964, acc 1\n",
      "2018-04-16T00:52:13.701241: step 1749, loss 0.0267325, acc 0.99\n",
      "2018-04-16T00:52:13.753975: step 1750, loss 0.0210878, acc 1\n",
      "2018-04-16T00:52:13.806636: step 1751, loss 0.0124935, acc 1\n",
      "2018-04-16T00:52:13.853549: step 1752, loss 0.00977128, acc 1\n",
      "2018-04-16T00:52:13.906470: step 1753, loss 0.0136939, acc 1\n",
      "2018-04-16T00:52:13.961545: step 1754, loss 0.0334662, acc 0.99\n",
      "2018-04-16T00:52:14.014972: step 1755, loss 0.00965593, acc 1\n",
      "2018-04-16T00:52:14.061723: step 1756, loss 0.0263508, acc 1\n",
      "2018-04-16T00:52:14.117442: step 1757, loss 0.0320689, acc 0.99\n",
      "2018-04-16T00:52:14.174476: step 1758, loss 0.0288432, acc 0.99\n",
      "2018-04-16T00:52:14.228186: step 1759, loss 0.0176132, acc 1\n",
      "2018-04-16T00:52:14.274789: step 1760, loss 0.0183698, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:52:14.374995: step 1760, loss 0.884085, acc 0.72679, rec 0.596774, pre 0.798561, f1 0.683077\n",
      "\n",
      "2018-04-16T00:52:14.427036: step 1761, loss 0.00749541, acc 1\n",
      "2018-04-16T00:52:14.483274: step 1762, loss 0.0228224, acc 1\n",
      "2018-04-16T00:52:14.536610: step 1763, loss 0.0281759, acc 0.99\n",
      "2018-04-16T00:52:14.580676: step 1764, loss 0.0162208, acc 1\n",
      "2018-04-16T00:52:14.633982: step 1765, loss 0.0148809, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:52:14.691341: step 1766, loss 0.0153067, acc 1\n",
      "2018-04-16T00:52:14.748669: step 1767, loss 0.0163217, acc 1\n",
      "2018-04-16T00:52:14.792149: step 1768, loss 0.0195627, acc 0.987179\n",
      "2018-04-16T00:52:14.845009: step 1769, loss 0.0141296, acc 1\n",
      "2018-04-16T00:52:14.900801: step 1770, loss 0.0115355, acc 1\n",
      "2018-04-16T00:52:14.953464: step 1771, loss 0.0204043, acc 1\n",
      "2018-04-16T00:52:14.996389: step 1772, loss 0.0447402, acc 1\n",
      "2018-04-16T00:52:15.048602: step 1773, loss 0.0270395, acc 0.99\n",
      "2018-04-16T00:52:15.100289: step 1774, loss 0.0270511, acc 0.99\n",
      "2018-04-16T00:52:15.158905: step 1775, loss 0.011837, acc 1\n",
      "2018-04-16T00:52:15.203168: step 1776, loss 0.0150486, acc 1\n",
      "2018-04-16T00:52:15.260522: step 1777, loss 0.00839266, acc 1\n",
      "2018-04-16T00:52:15.316024: step 1778, loss 0.0264811, acc 0.98\n",
      "2018-04-16T00:52:15.372136: step 1779, loss 0.00891403, acc 1\n",
      "2018-04-16T00:52:15.414533: step 1780, loss 0.0223365, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:52:15.515550: step 1780, loss 0.83526, acc 0.744032, rec 0.72043, pre 0.7507, f1 0.735254\n",
      "\n",
      "2018-04-16T00:52:15.569271: step 1781, loss 0.00939285, acc 1\n",
      "2018-04-16T00:52:15.628563: step 1782, loss 0.0121824, acc 1\n",
      "2018-04-16T00:52:15.685121: step 1783, loss 0.0183874, acc 1\n",
      "2018-04-16T00:52:15.731717: step 1784, loss 0.0113986, acc 1\n",
      "2018-04-16T00:52:15.785777: step 1785, loss 0.00851744, acc 1\n",
      "2018-04-16T00:52:15.845605: step 1786, loss 0.0160237, acc 1\n",
      "2018-04-16T00:52:15.902580: step 1787, loss 0.0284882, acc 0.98\n",
      "2018-04-16T00:52:15.946752: step 1788, loss 0.00768987, acc 1\n",
      "2018-04-16T00:52:16.003878: step 1789, loss 0.0152187, acc 0.99\n",
      "2018-04-16T00:52:16.061393: step 1790, loss 0.00484782, acc 1\n",
      "2018-04-16T00:52:16.119747: step 1791, loss 0.0114378, acc 1\n",
      "2018-04-16T00:52:16.165867: step 1792, loss 0.0179978, acc 1\n",
      "2018-04-16T00:52:16.221090: step 1793, loss 0.017438, acc 0.99\n",
      "2018-04-16T00:52:16.277192: step 1794, loss 0.01927, acc 1\n",
      "2018-04-16T00:52:16.335003: step 1795, loss 0.0104942, acc 1\n",
      "2018-04-16T00:52:16.380388: step 1796, loss 0.00869157, acc 1\n",
      "2018-04-16T00:52:16.436686: step 1797, loss 0.00744955, acc 1\n",
      "2018-04-16T00:52:16.498811: step 1798, loss 0.00617139, acc 1\n",
      "2018-04-16T00:52:16.554357: step 1799, loss 0.0153893, acc 0.99\n",
      "2018-04-16T00:52:16.598250: step 1800, loss 0.0237283, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:52:16.698492: step 1800, loss 0.917469, acc 0.724138, rec 0.575269, pre 0.810606, f1 0.672956\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523839829/checkpoints/model-1800\n",
      "\n",
      "2018-04-16T00:52:16.809751: step 1801, loss 0.0153683, acc 1\n",
      "2018-04-16T00:52:16.861579: step 1802, loss 0.00582331, acc 1\n",
      "2018-04-16T00:52:16.913394: step 1803, loss 0.0114035, acc 1\n",
      "2018-04-16T00:52:16.960869: step 1804, loss 0.0244728, acc 0.987179\n",
      "2018-04-16T00:52:17.014137: step 1805, loss 0.012626, acc 1\n",
      "2018-04-16T00:52:17.065846: step 1806, loss 0.0173435, acc 0.99\n",
      "2018-04-16T00:52:17.125014: step 1807, loss 0.0165046, acc 1\n",
      "2018-04-16T00:52:17.171543: step 1808, loss 0.00706031, acc 1\n",
      "2018-04-16T00:52:17.223460: step 1809, loss 0.0103311, acc 1\n",
      "2018-04-16T00:52:17.274876: step 1810, loss 0.0124414, acc 1\n",
      "2018-04-16T00:52:17.326438: step 1811, loss 0.0144396, acc 1\n",
      "2018-04-16T00:52:17.368790: step 1812, loss 0.0123679, acc 1\n",
      "2018-04-16T00:52:17.423996: step 1813, loss 0.00929384, acc 1\n",
      "2018-04-16T00:52:17.475847: step 1814, loss 0.00521245, acc 1\n",
      "2018-04-16T00:52:17.527321: step 1815, loss 0.0175388, acc 1\n",
      "2018-04-16T00:52:17.572521: step 1816, loss 0.0133235, acc 1\n",
      "2018-04-16T00:52:17.629754: step 1817, loss 0.00873934, acc 1\n",
      "2018-04-16T00:52:17.682127: step 1818, loss 0.0285938, acc 0.99\n",
      "2018-04-16T00:52:17.734095: step 1819, loss 0.00928351, acc 1\n",
      "2018-04-16T00:52:17.777249: step 1820, loss 0.00979494, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:52:17.881160: step 1820, loss 0.870411, acc 0.728117, rec 0.725806, pre 0.723861, f1 0.724832\n",
      "\n",
      "2018-04-16T00:52:17.937100: step 1821, loss 0.0115367, acc 1\n",
      "2018-04-16T00:52:17.989728: step 1822, loss 0.0393294, acc 0.99\n",
      "2018-04-16T00:52:18.042186: step 1823, loss 0.0424011, acc 0.98\n",
      "2018-04-16T00:52:18.090486: step 1824, loss 0.00989471, acc 1\n",
      "2018-04-16T00:52:18.152022: step 1825, loss 0.00555044, acc 1\n",
      "2018-04-16T00:52:18.208704: step 1826, loss 0.00936805, acc 1\n",
      "2018-04-16T00:52:18.260529: step 1827, loss 0.0145791, acc 1\n",
      "2018-04-16T00:52:18.308195: step 1828, loss 0.0250708, acc 0.987179\n",
      "2018-04-16T00:52:18.360797: step 1829, loss 0.0398436, acc 0.99\n",
      "2018-04-16T00:52:18.412685: step 1830, loss 0.0167746, acc 1\n",
      "2018-04-16T00:52:18.466643: step 1831, loss 0.0148076, acc 1\n",
      "2018-04-16T00:52:18.513798: step 1832, loss 0.013323, acc 1\n",
      "2018-04-16T00:52:18.566722: step 1833, loss 0.00931321, acc 1\n",
      "2018-04-16T00:52:18.619891: step 1834, loss 0.0196876, acc 0.99\n",
      "2018-04-16T00:52:18.674004: step 1835, loss 0.0192468, acc 0.99\n",
      "2018-04-16T00:52:18.731147: step 1836, loss 0.013106, acc 0.987179\n",
      "2018-04-16T00:52:18.784163: step 1837, loss 0.0208058, acc 1\n",
      "2018-04-16T00:52:18.836775: step 1838, loss 0.0114861, acc 1\n",
      "2018-04-16T00:52:18.889611: step 1839, loss 0.0113698, acc 0.99\n",
      "2018-04-16T00:52:18.937155: step 1840, loss 0.025523, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:52:19.039273: step 1840, loss 0.95217, acc 0.720159, rec 0.567204, pre 0.808429, f1 0.666667\n",
      "\n",
      "2018-04-16T00:52:19.092976: step 1841, loss 0.00928113, acc 1\n",
      "2018-04-16T00:52:19.157189: step 1842, loss 0.0118903, acc 1\n",
      "2018-04-16T00:52:19.209585: step 1843, loss 0.00428296, acc 1\n",
      "2018-04-16T00:52:19.252053: step 1844, loss 0.0225205, acc 1\n",
      "2018-04-16T00:52:19.312748: step 1845, loss 0.0109813, acc 1\n",
      "2018-04-16T00:52:19.371866: step 1846, loss 0.0187129, acc 0.99\n",
      "2018-04-16T00:52:19.423908: step 1847, loss 0.01818, acc 0.99\n",
      "2018-04-16T00:52:19.466938: step 1848, loss 0.0214816, acc 1\n",
      "2018-04-16T00:52:19.519519: step 1849, loss 0.0180859, acc 1\n",
      "2018-04-16T00:52:19.572627: step 1850, loss 0.00976832, acc 1\n",
      "2018-04-16T00:52:19.630093: step 1851, loss 0.0400811, acc 0.99\n",
      "2018-04-16T00:52:19.672519: step 1852, loss 0.0138827, acc 1\n",
      "2018-04-16T00:52:19.724296: step 1853, loss 0.00704243, acc 1\n",
      "2018-04-16T00:52:19.777934: step 1854, loss 0.0126834, acc 1\n",
      "2018-04-16T00:52:19.837081: step 1855, loss 0.0285161, acc 0.99\n",
      "2018-04-16T00:52:19.884501: step 1856, loss 0.0197369, acc 1\n",
      "2018-04-16T00:52:19.938718: step 1857, loss 0.0112876, acc 1\n",
      "2018-04-16T00:52:19.996792: step 1858, loss 0.0253642, acc 0.99\n",
      "2018-04-16T00:52:20.053197: step 1859, loss 0.00718888, acc 1\n",
      "2018-04-16T00:52:20.097054: step 1860, loss 0.00608934, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:52:20.199545: step 1860, loss 0.879418, acc 0.722812, rec 0.75, pre 0.706329, f1 0.72751\n",
      "\n",
      "2018-04-16T00:52:20.251137: step 1861, loss 0.0101199, acc 1\n",
      "2018-04-16T00:52:20.309012: step 1862, loss 0.010572, acc 1\n",
      "2018-04-16T00:52:20.362648: step 1863, loss 0.0108375, acc 1\n",
      "2018-04-16T00:52:20.407428: step 1864, loss 0.00797901, acc 1\n",
      "2018-04-16T00:52:20.462362: step 1865, loss 0.00887501, acc 1\n",
      "2018-04-16T00:52:20.518844: step 1866, loss 0.0162901, acc 1\n",
      "2018-04-16T00:52:20.571199: step 1867, loss 0.00727587, acc 1\n",
      "2018-04-16T00:52:20.616707: step 1868, loss 0.00632464, acc 1\n",
      "2018-04-16T00:52:20.672202: step 1869, loss 0.0139578, acc 0.99\n",
      "2018-04-16T00:52:20.732564: step 1870, loss 0.0110228, acc 1\n",
      "2018-04-16T00:52:20.788265: step 1871, loss 0.0108042, acc 1\n",
      "2018-04-16T00:52:20.833792: step 1872, loss 0.0106011, acc 1\n",
      "2018-04-16T00:52:20.887174: step 1873, loss 0.010506, acc 1\n",
      "2018-04-16T00:52:20.946576: step 1874, loss 0.0064769, acc 1\n",
      "2018-04-16T00:52:21.001572: step 1875, loss 0.0325293, acc 0.99\n",
      "2018-04-16T00:52:21.044984: step 1876, loss 0.0209014, acc 0.987179\n",
      "2018-04-16T00:52:21.100336: step 1877, loss 0.00441156, acc 1\n",
      "2018-04-16T00:52:21.156176: step 1878, loss 0.0106372, acc 1\n",
      "2018-04-16T00:52:21.209220: step 1879, loss 0.0111422, acc 1\n",
      "2018-04-16T00:52:21.251713: step 1880, loss 0.0189157, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:52:21.361999: step 1880, loss 0.904749, acc 0.71618, rec 0.758065, pre 0.694581, f1 0.724936\n",
      "\n",
      "2018-04-16T00:52:21.414441: step 1881, loss 0.0273387, acc 0.98\n",
      "2018-04-16T00:52:21.467468: step 1882, loss 0.0271814, acc 0.99\n",
      "2018-04-16T00:52:21.521761: step 1883, loss 0.0335296, acc 0.99\n",
      "2018-04-16T00:52:21.569946: step 1884, loss 0.0203043, acc 1\n",
      "2018-04-16T00:52:21.623199: step 1885, loss 0.0128223, acc 0.99\n",
      "2018-04-16T00:52:21.675492: step 1886, loss 0.00582586, acc 1\n",
      "2018-04-16T00:52:21.731171: step 1887, loss 0.0264578, acc 0.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T00:52:21.778344: step 1888, loss 0.0294561, acc 0.987179\n",
      "2018-04-16T00:52:21.831312: step 1889, loss 0.0203735, acc 0.99\n",
      "2018-04-16T00:52:21.884962: step 1890, loss 0.0142894, acc 1\n",
      "2018-04-16T00:52:21.938567: step 1891, loss 0.00991929, acc 1\n",
      "2018-04-16T00:52:21.985534: step 1892, loss 0.00627188, acc 1\n",
      "2018-04-16T00:52:22.037110: step 1893, loss 0.00662401, acc 1\n",
      "2018-04-16T00:52:22.088403: step 1894, loss 0.010249, acc 1\n",
      "2018-04-16T00:52:22.139942: step 1895, loss 0.0160418, acc 0.99\n",
      "2018-04-16T00:52:22.182233: step 1896, loss 0.0249288, acc 0.987179\n",
      "2018-04-16T00:52:22.236978: step 1897, loss 0.0479809, acc 0.98\n",
      "2018-04-16T00:52:22.290033: step 1898, loss 0.0365667, acc 1\n",
      "2018-04-16T00:52:22.342418: step 1899, loss 0.0142174, acc 1\n",
      "2018-04-16T00:52:22.385249: step 1900, loss 0.00643265, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:52:22.489363: step 1900, loss 0.895297, acc 0.734748, rec 0.669355, pre 0.763804, f1 0.713467\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523839829/checkpoints/model-1900\n",
      "\n",
      "2018-04-16T00:52:22.598817: step 1901, loss 0.00945391, acc 1\n",
      "2018-04-16T00:52:22.650921: step 1902, loss 0.00792478, acc 1\n",
      "2018-04-16T00:52:22.707805: step 1903, loss 0.00633199, acc 1\n",
      "2018-04-16T00:52:22.750774: step 1904, loss 0.028007, acc 0.987179\n",
      "2018-04-16T00:52:22.803396: step 1905, loss 0.027799, acc 0.99\n",
      "2018-04-16T00:52:22.856443: step 1906, loss 0.0134801, acc 1\n",
      "2018-04-16T00:52:22.912849: step 1907, loss 0.0172818, acc 0.99\n",
      "2018-04-16T00:52:22.957199: step 1908, loss 0.00883502, acc 1\n",
      "2018-04-16T00:52:23.011421: step 1909, loss 0.0117396, acc 1\n",
      "2018-04-16T00:52:23.065542: step 1910, loss 0.0166371, acc 1\n",
      "2018-04-16T00:52:23.125325: step 1911, loss 0.0255653, acc 0.99\n",
      "2018-04-16T00:52:23.170874: step 1912, loss 0.024207, acc 1\n",
      "2018-04-16T00:52:23.223064: step 1913, loss 0.00765199, acc 1\n",
      "2018-04-16T00:52:23.274907: step 1914, loss 0.0072243, acc 1\n",
      "2018-04-16T00:52:23.333024: step 1915, loss 0.019835, acc 0.99\n",
      "2018-04-16T00:52:23.377322: step 1916, loss 0.00704651, acc 1\n",
      "2018-04-16T00:52:23.430055: step 1917, loss 0.0101918, acc 1\n",
      "2018-04-16T00:52:23.482454: step 1918, loss 0.0145072, acc 1\n",
      "2018-04-16T00:52:23.540404: step 1919, loss 0.0143993, acc 1\n",
      "2018-04-16T00:52:23.583099: step 1920, loss 0.00396985, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:52:23.682382: step 1920, loss 0.881549, acc 0.740053, rec 0.712366, pre 0.748588, f1 0.730028\n",
      "\n",
      "2018-04-16T00:52:23.737357: step 1921, loss 0.00452025, acc 1\n",
      "2018-04-16T00:52:23.793331: step 1922, loss 0.0405702, acc 0.98\n",
      "2018-04-16T00:52:23.845410: step 1923, loss 0.0162532, acc 1\n",
      "2018-04-16T00:52:23.888660: step 1924, loss 0.00656787, acc 1\n",
      "2018-04-16T00:52:23.940931: step 1925, loss 0.00982049, acc 1\n",
      "2018-04-16T00:52:23.997820: step 1926, loss 0.0101549, acc 1\n",
      "2018-04-16T00:52:24.050336: step 1927, loss 0.00889962, acc 1\n",
      "2018-04-16T00:52:24.094140: step 1928, loss 0.0161303, acc 0.987179\n",
      "2018-04-16T00:52:24.150734: step 1929, loss 0.0134518, acc 1\n",
      "2018-04-16T00:52:24.209150: step 1930, loss 0.0131314, acc 1\n",
      "2018-04-16T00:52:24.267471: step 1931, loss 0.0148771, acc 1\n",
      "2018-04-16T00:52:24.312843: step 1932, loss 0.00348523, acc 1\n",
      "2018-04-16T00:52:24.366839: step 1933, loss 0.0240386, acc 0.99\n",
      "2018-04-16T00:52:24.428442: step 1934, loss 0.00844698, acc 1\n",
      "2018-04-16T00:52:24.481790: step 1935, loss 0.00736449, acc 1\n",
      "2018-04-16T00:52:24.524525: step 1936, loss 0.0119639, acc 1\n",
      "2018-04-16T00:52:24.577273: step 1937, loss 0.00817319, acc 1\n",
      "2018-04-16T00:52:24.635300: step 1938, loss 0.00771595, acc 1\n",
      "2018-04-16T00:52:24.688776: step 1939, loss 0.0119479, acc 1\n",
      "2018-04-16T00:52:24.731307: step 1940, loss 0.00835753, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:52:24.830760: step 1940, loss 0.880982, acc 0.744032, rec 0.712366, pre 0.754986, f1 0.733057\n",
      "\n",
      "2018-04-16T00:52:24.887580: step 1941, loss 0.0101269, acc 1\n",
      "2018-04-16T00:52:24.941934: step 1942, loss 0.0207445, acc 1\n",
      "2018-04-16T00:52:25.010153: step 1943, loss 0.00886903, acc 1\n",
      "2018-04-16T00:52:25.055364: step 1944, loss 0.00460365, acc 1\n",
      "2018-04-16T00:52:25.114666: step 1945, loss 0.00449473, acc 1\n",
      "2018-04-16T00:52:25.172520: step 1946, loss 0.00499364, acc 1\n",
      "2018-04-16T00:52:25.225823: step 1947, loss 0.0182825, acc 1\n",
      "2018-04-16T00:52:25.268198: step 1948, loss 0.0411161, acc 0.987179\n",
      "2018-04-16T00:52:25.325555: step 1949, loss 0.0244595, acc 0.99\n",
      "2018-04-16T00:52:25.383220: step 1950, loss 0.0181705, acc 0.99\n",
      "2018-04-16T00:52:25.441127: step 1951, loss 0.0687083, acc 0.98\n",
      "2018-04-16T00:52:25.488971: step 1952, loss 0.0100756, acc 1\n",
      "2018-04-16T00:52:25.551136: step 1953, loss 0.0246604, acc 0.99\n",
      "2018-04-16T00:52:25.610341: step 1954, loss 0.0215992, acc 1\n",
      "2018-04-16T00:52:25.666893: step 1955, loss 0.0186801, acc 0.99\n",
      "2018-04-16T00:52:25.715613: step 1956, loss 0.00913585, acc 1\n",
      "2018-04-16T00:52:25.780029: step 1957, loss 0.0316012, acc 0.99\n",
      "2018-04-16T00:52:25.837484: step 1958, loss 0.0211964, acc 1\n",
      "2018-04-16T00:52:25.894173: step 1959, loss 0.0116376, acc 1\n",
      "2018-04-16T00:52:25.937302: step 1960, loss 0.00734057, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:52:26.041804: step 1960, loss 0.882085, acc 0.729443, rec 0.763441, pre 0.71, f1 0.735751\n",
      "\n",
      "2018-04-16T00:52:26.093966: step 1961, loss 0.002843, acc 1\n",
      "2018-04-16T00:52:26.148448: step 1962, loss 0.0147205, acc 1\n",
      "2018-04-16T00:52:26.200964: step 1963, loss 0.0082261, acc 1\n",
      "2018-04-16T00:52:26.247583: step 1964, loss 0.0113455, acc 1\n",
      "2018-04-16T00:52:26.301414: step 1965, loss 0.00638804, acc 1\n",
      "2018-04-16T00:52:26.354181: step 1966, loss 0.0142174, acc 0.99\n",
      "2018-04-16T00:52:26.407231: step 1967, loss 0.00898714, acc 1\n",
      "2018-04-16T00:52:26.454415: step 1968, loss 0.020671, acc 1\n",
      "2018-04-16T00:52:26.508051: step 1969, loss 0.0217014, acc 1\n",
      "2018-04-16T00:52:26.562444: step 1970, loss 0.0126576, acc 1\n",
      "2018-04-16T00:52:26.615725: step 1971, loss 0.00583188, acc 1\n",
      "2018-04-16T00:52:26.664461: step 1972, loss 0.0185596, acc 1\n",
      "2018-04-16T00:52:26.717711: step 1973, loss 0.0138727, acc 1\n",
      "2018-04-16T00:52:26.770789: step 1974, loss 0.00729536, acc 1\n",
      "2018-04-16T00:52:26.823294: step 1975, loss 0.00885646, acc 1\n",
      "2018-04-16T00:52:26.870290: step 1976, loss 0.00279518, acc 1\n",
      "2018-04-16T00:52:26.923328: step 1977, loss 0.00656379, acc 1\n",
      "2018-04-16T00:52:26.976603: step 1978, loss 0.00341009, acc 1\n",
      "2018-04-16T00:52:27.029218: step 1979, loss 0.00746872, acc 1\n",
      "2018-04-16T00:52:27.075898: step 1980, loss 0.0144278, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:52:27.178141: step 1980, loss 0.908766, acc 0.742706, rec 0.669355, pre 0.778125, f1 0.719653\n",
      "\n",
      "2018-04-16T00:52:27.232685: step 1981, loss 0.00711027, acc 1\n",
      "2018-04-16T00:52:27.289383: step 1982, loss 0.00658411, acc 1\n",
      "2018-04-16T00:52:27.342129: step 1983, loss 0.00927719, acc 1\n",
      "2018-04-16T00:52:27.385848: step 1984, loss 0.0269866, acc 0.987179\n",
      "2018-04-16T00:52:27.438715: step 1985, loss 0.0125848, acc 1\n",
      "2018-04-16T00:52:27.495402: step 1986, loss 0.0162398, acc 0.99\n",
      "2018-04-16T00:52:27.550176: step 1987, loss 0.045539, acc 0.98\n",
      "2018-04-16T00:52:27.594463: step 1988, loss 0.0409849, acc 0.987179\n",
      "2018-04-16T00:52:27.647937: step 1989, loss 0.0426012, acc 0.99\n",
      "2018-04-16T00:52:27.707931: step 1990, loss 0.0399463, acc 0.99\n",
      "2018-04-16T00:52:27.769828: step 1991, loss 0.0418385, acc 0.98\n",
      "2018-04-16T00:52:27.816972: step 1992, loss 0.0235915, acc 1\n",
      "2018-04-16T00:52:27.871315: step 1993, loss 0.0123012, acc 0.99\n",
      "2018-04-16T00:52:27.928437: step 1994, loss 0.0247316, acc 0.99\n",
      "2018-04-16T00:52:27.981983: step 1995, loss 0.0171562, acc 0.99\n",
      "2018-04-16T00:52:28.026536: step 1996, loss 0.0086171, acc 1\n",
      "2018-04-16T00:52:28.082682: step 1997, loss 0.005213, acc 1\n",
      "2018-04-16T00:52:28.142146: step 1998, loss 0.00777033, acc 1\n",
      "2018-04-16T00:52:28.196953: step 1999, loss 0.00726857, acc 1\n",
      "2018-04-16T00:52:28.240913: step 2000, loss 0.00434553, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T00:52:28.347576: step 2000, loss 0.897422, acc 0.724138, rec 0.741935, pre 0.71134, f1 0.726316\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1523839829/checkpoints/model-2000\n",
      "\n",
      "\n",
      "Test Set:\n",
      "2018-04-16T00:52:28.457364: step 2000, loss 0.946511, acc 0.772487, rec 0.747368, pre 0.788889, f1 0.767568\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.2\n",
    "\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", 0.5, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 100, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"1\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 100, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.25, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 100\n",
    "tf.flags.DEFINE_integer(\"batch_size\", batch_size, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 500, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 20, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=train_s.shape[1],\n",
    "            num_classes=2,\n",
    "            vocab_size=2500,\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "        \n",
    "         # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        #vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch1, x_batch2, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch1, x_batch2, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            #r \n",
    "            step, summaries, loss, accuracy, recall, precision, f1 = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy,cnn.recall, cnn.precision, cnn.f1_score],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, rec {:g}, pre {:g}, f1 {:g}\".format(time_str, step, loss, accuracy, recall, precision, f1))# recall, precision, f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(train_s, train_c,  expanded_train_labels)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch1, x_batch2, y_batch = zip(*batch)\n",
    "            train_step(x_batch1, x_batch1, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(validation_s, validation_c, expanded_validation_labels, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "print(\"\\nTest Set:\")\n",
    "dev_step(test_s, test_c, expanded_test_labels, writer=dev_summary_writer)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
