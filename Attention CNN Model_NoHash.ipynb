{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saulgrimaldo1/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from w266_common import utils, vocabulary\n",
    "import time\n",
    "import datetime\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "tokenizer = TweetTokenizer()\n",
    "x_data = []\n",
    "x_contexts = []\n",
    "labels = []\n",
    "sentences  = []\n",
    "contexts = []\n",
    "with open('merged_data_v4.csv', 'r') as csvfile:\n",
    "    linereader = csv.reader(csvfile, delimiter = '|')\n",
    "    for i, row in enumerate(linereader):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        sentence, context, sarcasm = row\n",
    "        sentences.append(sentence)\n",
    "        contexts.append(context)\n",
    "        x_tokens = utils.canonicalize_words(tokenizer.tokenize(sentence), hashtags = True)\n",
    "        context_tokens = utils.canonicalize_words(tokenizer.tokenize(context), hashtags = True)\n",
    "        x_data.append(x_tokens)\n",
    "        x_contexts.append(context_tokens)\n",
    "        labels.append(int(sarcasm))\n",
    "\n",
    "\n",
    "#rng = np.random.RandomState(5)\n",
    "#rng.shuffle(x_data)  # in-place\n",
    "#train_split_idx = int(0.7 * len(labels))\n",
    "#test_split_idx  = int(0.9 * len(labels))\n",
    "\n",
    "shuffle_indices = np.random.permutation(np.arange(len(labels)))\n",
    "train_split_idx = int(0.7 * len(labels))\n",
    "test_split_idx  = int(0.9 * len(labels))\n",
    "\n",
    "train_indices = shuffle_indices[:train_split_idx]\n",
    "validation_indices = shuffle_indices[train_split_idx:test_split_idx]\n",
    "test_indices = shuffle_indices[test_split_idx:]\n",
    "\n",
    "\n",
    "train_sentences = np.array(x_data)[train_indices]\n",
    "train_contexts = np.array(x_contexts)[train_indices]\n",
    "train_labels= np.array(labels)[train_indices] \n",
    "validation_sentences = np.array(x_data)[validation_indices]\n",
    "validation_labels = np.array(labels)[validation_indices]\n",
    "validation_contexts = train_contexts = np.array(x_contexts)[validation_indices]\n",
    "test_sentences = np.array(x_data)[test_indices]  \n",
    "test_contexts = train_contexts = np.array(x_contexts)[test_indices]\n",
    "test_labels = np.array(labels)[test_indices]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 6, 2]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [2]*4\n",
    "a[2] = 6\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_labels(raw_label_set, size):\n",
    "    label_set = []\n",
    "    for label in raw_label_set:\n",
    "        labels = [0] * size\n",
    "        labels[label] = 1\n",
    "        label_set.append(labels)\n",
    "    return np.array(label_set)\n",
    "\n",
    "expanded_train_labels = transform_labels(train_labels, 2)\n",
    "expanded_validation_labels = transform_labels(validation_labels,2)\n",
    "expanded_test_labels = transform_labels(test_labels,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,3,4,5,6,7,8,8,1,5,6,7]\n",
    "a + [\"<PADDING>\"]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'angry',\n",
       " 'man',\n",
       " 'is',\n",
       " 'angry',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PaddingAndTruncating:\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def pad_or_truncate(self, sentence):\n",
    "        sen_len = len(sentence)\n",
    "        paddings = self.max_len - sen_len\n",
    "        if paddings >=0:\n",
    "            return list(sentence) + [\"<PADDING>\"] * paddings\n",
    "        return sentence[0:paddings]\n",
    "        \n",
    "PadAndTrunc = PaddingAndTruncating(10)\n",
    "        \n",
    "        \n",
    "PadAndTrunc.pad_or_truncate([\"the\",\"angry\",\"man\",\"is\",\"angry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  12,    5,    9, ...,    3,    3,    3],\n",
       "       [  12,    5,    9, ...,    3,    3,    3],\n",
       "       [   8,    7,  482, ...,    3,    3,    3],\n",
       "       ..., \n",
       "       [   5,  481,  148, ..., 1656,   92, 3098],\n",
       "       [   8,  420,   18, ...,    3,    3,    3],\n",
       "       [ 189,   19,    2, ...,    3,    3,    3]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.contrib import learn\n",
    "vocab_size = 5000\n",
    "\n",
    "#max_len = max([len(sent) for sent  in train_sentences])\n",
    "PadAndTrunc =PaddingAndTruncating(50)\n",
    "train_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, train_sentences))\n",
    "train_context_padded = list(map(PadAndTrunc.pad_or_truncate, train_contexts))\n",
    "validation_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, validation_sentences))\n",
    "validation_context_padded = list(map(PadAndTrunc.pad_or_truncate, validation_contexts))\n",
    "test_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, test_sentences))\n",
    "test_context_padded = list(map(PadAndTrunc.pad_or_truncate, test_contexts))\n",
    "\n",
    "vocab = vocabulary.Vocabulary(utils.flatten(list(train_sentences_padded) + list(train_context_padded)), vocab_size)\n",
    "train_s = np.array(list(map(vocab.words_to_ids, train_sentences_padded)))\n",
    "train_c = np.array(list(map(vocab.words_to_ids, train_context_padded)))\n",
    "validation_s = np.array(list(map(vocab.words_to_ids, validation_sentences_padded)))\n",
    "validation_c = np.array(list(map(vocab.words_to_ids, validation_context_padded)))\n",
    "test_s = np.array(list(map(vocab.words_to_ids, test_sentences_padded)))\n",
    "test_c = np.array(list(map(vocab.words_to_ids, test_context_padded)))\n",
    "train_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv-maxpool-3'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 3\n",
    "(\"conv-maxpool-%s\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size, \n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x1 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x1\")\n",
    "        self.input_x2 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x2\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars1 = tf.nn.embedding_lookup(self.W, self.input_x1)\n",
    "            self.embedded_chars_expanded1 = tf.expand_dims(self.embedded_chars1, -1)\n",
    "            self.embedded_chars2 = tf.nn.embedding_lookup(self.W, self.input_x2)\n",
    "            self.embedded_chars_expanded2 = tf.expand_dims(self.embedded_chars2, -1)\n",
    "\n",
    "        # Create a convolution + avgpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.variable_scope(\"conv-avgpool-%s\" % i) as scope:\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "              \n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded1,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h1 = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"tanh1\")\n",
    "               \n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.avg_pool(\n",
    "                    h1,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                \n",
    "                pooled_outputs.append(pooled)\n",
    "                #pooled_outputs.append(pooled)\n",
    "                scope.reuse_variables()\n",
    "                 \n",
    "                \n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded2,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h2 = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"tanh2\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled2 = tf.nn.avg_pool(\n",
    "                    h2,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled2)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "\n",
    "        num_filters_total = num_filters * len(filter_sizes) * 2\n",
    "\n",
    "        self.h_pool = tf.concat(pooled_outputs, 1)\n",
    "\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "        \n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "           \n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.labels = tf.argmax(self.input_y, 1)\n",
    "            TP = tf.count_nonzero(self.predictions * self.labels)\n",
    "            TN = tf.count_nonzero((self.predictions - 1) * (self.labels - 1))\n",
    "            FP = tf.count_nonzero(self.predictions * (self.labels - 1))\n",
    "            FN = tf.count_nonzero((self.predictions - 1) * self.labels)\n",
    "            correct_predictions = tf.equal(self.predictions, self.labels)\n",
    "            self.precision = TP / (TP + FP)\n",
    "            self.recall = TP / (TP + FN)\n",
    "            self.f1_score = 2 * self.precision * self.recall / (self.precision + self.recall)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=50\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.75\n",
      "DROPOUT_KEEP_PROB=0.25\n",
      "EMBEDDING_DIM=100\n",
      "EVALUATE_EVERY=20\n",
      "FILTER_SIZES=1\n",
      "L2_REG_LAMBDA=0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.neg\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=250\n",
      "NUM_FILTERS=100\n",
      "POSITIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.pos\n",
      "\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/W:0/grad/hist is illegal; using conv-avgpool-0/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/W:0/grad/sparsity is illegal; using conv-avgpool-0/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/b:0/grad/hist is illegal; using conv-avgpool-0/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/b:0/grad/sparsity is illegal; using conv-avgpool-0/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489\n",
      "\n",
      "2018-04-21T23:48:10.023394: step 1, loss 0.973992, acc 0.38\n",
      "2018-04-21T23:48:10.073786: step 2, loss 1.18718, acc 0.56\n",
      "2018-04-21T23:48:10.119489: step 3, loss 1.12042, acc 0.48\n",
      "2018-04-21T23:48:10.164440: step 4, loss 0.94761, acc 0.54\n",
      "2018-04-21T23:48:10.209910: step 5, loss 0.952185, acc 0.4\n",
      "2018-04-21T23:48:10.260409: step 6, loss 0.779543, acc 0.48\n",
      "2018-04-21T23:48:10.306264: step 7, loss 0.76303, acc 0.48\n",
      "2018-04-21T23:48:10.350919: step 8, loss 0.714956, acc 0.52\n",
      "2018-04-21T23:48:10.395818: step 9, loss 0.750116, acc 0.64\n",
      "2018-04-21T23:48:10.440578: step 10, loss 0.781639, acc 0.52\n",
      "2018-04-21T23:48:10.489665: step 11, loss 0.702399, acc 0.56\n",
      "2018-04-21T23:48:10.535158: step 12, loss 0.7658, acc 0.52\n",
      "2018-04-21T23:48:10.580894: step 13, loss 0.609982, acc 0.66\n",
      "2018-04-21T23:48:10.626566: step 14, loss 0.721656, acc 0.56\n",
      "2018-04-21T23:48:10.672475: step 15, loss 0.771911, acc 0.62\n",
      "2018-04-21T23:48:10.723244: step 16, loss 0.685255, acc 0.54\n",
      "2018-04-21T23:48:10.769834: step 17, loss 0.636482, acc 0.66\n",
      "2018-04-21T23:48:10.816480: step 18, loss 0.705487, acc 0.54\n",
      "2018-04-21T23:48:10.862915: step 19, loss 0.650822, acc 0.56\n",
      "2018-04-21T23:48:10.908989: step 20, loss 0.699321, acc 0.62\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:48:11.620061: step 20, loss 0.815306, acc 0.498889, rec 0.999256, pre 0.49833, f1 0.665016\n",
      "\n",
      "2018-04-21T23:48:11.665778: step 21, loss 0.886747, acc 0.48\n",
      "2018-04-21T23:48:11.712207: step 22, loss 0.758421, acc 0.62\n",
      "2018-04-21T23:48:11.758639: step 23, loss 0.798492, acc 0.48\n",
      "2018-04-21T23:48:11.804256: step 24, loss 0.726046, acc 0.54\n",
      "2018-04-21T23:48:11.853167: step 25, loss 0.750144, acc 0.4\n",
      "2018-04-21T23:48:11.897888: step 26, loss 0.696715, acc 0.52\n",
      "2018-04-21T23:48:11.944057: step 27, loss 0.576962, acc 0.68\n",
      "2018-04-21T23:48:11.989623: step 28, loss 0.786356, acc 0.54\n",
      "2018-04-21T23:48:12.036154: step 29, loss 0.83674, acc 0.4\n",
      "2018-04-21T23:48:12.087260: step 30, loss 0.634784, acc 0.7\n",
      "2018-04-21T23:48:12.132768: step 31, loss 0.654288, acc 0.6\n",
      "2018-04-21T23:48:12.177710: step 32, loss 0.650115, acc 0.62\n",
      "2018-04-21T23:48:12.223238: step 33, loss 0.587444, acc 0.68\n",
      "2018-04-21T23:48:12.268938: step 34, loss 0.587213, acc 0.68\n",
      "2018-04-21T23:48:12.319071: step 35, loss 0.674959, acc 0.58\n",
      "2018-04-21T23:48:12.365775: step 36, loss 0.716625, acc 0.6\n",
      "2018-04-21T23:48:12.412356: step 37, loss 0.608016, acc 0.68\n",
      "2018-04-21T23:48:12.458190: step 38, loss 0.674492, acc 0.6\n",
      "2018-04-21T23:48:12.503655: step 39, loss 0.663506, acc 0.58\n",
      "2018-04-21T23:48:12.552570: step 40, loss 0.774279, acc 0.6\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:48:13.218719: step 40, loss 0.655148, acc 0.561111, rec 0.97247, pre 0.532383, f1 0.688076\n",
      "\n",
      "2018-04-21T23:48:13.263224: step 41, loss 0.618976, acc 0.66\n",
      "2018-04-21T23:48:13.307397: step 42, loss 0.707897, acc 0.58\n",
      "2018-04-21T23:48:13.353041: step 43, loss 0.684461, acc 0.66\n",
      "2018-04-21T23:48:13.397041: step 44, loss 0.628407, acc 0.7\n",
      "2018-04-21T23:48:13.443681: step 45, loss 0.636547, acc 0.68\n",
      "2018-04-21T23:48:13.487592: step 46, loss 0.610537, acc 0.72\n",
      "2018-04-21T23:48:13.531323: step 47, loss 0.647857, acc 0.58\n",
      "2018-04-21T23:48:13.575348: step 48, loss 0.551278, acc 0.76\n",
      "2018-04-21T23:48:13.618565: step 49, loss 0.712661, acc 0.56\n",
      "2018-04-21T23:48:13.665067: step 50, loss 0.713936, acc 0.5\n",
      "2018-04-21T23:48:13.709745: step 51, loss 0.855929, acc 0.44\n",
      "2018-04-21T23:48:13.753313: step 52, loss 0.55641, acc 0.76\n",
      "2018-04-21T23:48:13.797563: step 53, loss 0.565808, acc 0.74\n",
      "2018-04-21T23:48:13.841527: step 54, loss 0.659123, acc 0.64\n",
      "2018-04-21T23:48:13.889037: step 55, loss 0.703127, acc 0.56\n",
      "2018-04-21T23:48:13.933322: step 56, loss 0.762554, acc 0.54\n",
      "2018-04-21T23:48:13.977805: step 57, loss 0.788257, acc 0.5\n",
      "2018-04-21T23:48:14.021281: step 58, loss 0.764571, acc 0.56\n",
      "2018-04-21T23:48:14.065270: step 59, loss 0.705016, acc 0.52\n",
      "2018-04-21T23:48:14.112485: step 60, loss 0.669301, acc 0.56\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:48:14.756764: step 60, loss 0.66028, acc 0.503333, rec 0.00223214, pre 1, f1 0.00445434\n",
      "\n",
      "2018-04-21T23:48:14.803737: step 61, loss 0.541251, acc 0.78\n",
      "2018-04-21T23:48:14.850359: step 62, loss 0.692448, acc 0.5\n",
      "2018-04-21T23:48:14.897120: step 63, loss 0.616068, acc 0.68\n",
      "2018-04-21T23:48:14.943271: step 64, loss 0.577601, acc 0.72\n",
      "2018-04-21T23:48:14.993318: step 65, loss 0.561576, acc 0.82\n",
      "2018-04-21T23:48:15.041104: step 66, loss 0.648327, acc 0.68\n",
      "2018-04-21T23:48:15.087356: step 67, loss 0.753061, acc 0.54\n",
      "2018-04-21T23:48:15.134667: step 68, loss 0.762098, acc 0.56\n",
      "2018-04-21T23:48:15.184271: step 69, loss 0.622737, acc 0.66\n",
      "2018-04-21T23:48:15.233773: step 70, loss 0.602233, acc 0.62\n",
      "2018-04-21T23:48:15.280383: step 71, loss 0.750941, acc 0.6\n",
      "2018-04-21T23:48:15.330468: step 72, loss 1.04602, acc 0.44\n",
      "2018-04-21T23:48:15.379439: step 73, loss 0.689074, acc 0.58\n",
      "2018-04-21T23:48:15.424573: step 74, loss 0.619614, acc 0.72\n",
      "2018-04-21T23:48:15.473783: step 75, loss 0.604353, acc 0.66\n",
      "2018-04-21T23:48:15.520796: step 76, loss 0.577746, acc 0.8\n",
      "2018-04-21T23:48:15.566379: step 77, loss 0.605213, acc 0.74\n",
      "2018-04-21T23:48:15.612054: step 78, loss 0.550381, acc 0.74\n",
      "2018-04-21T23:48:15.659242: step 79, loss 0.631469, acc 0.7\n",
      "2018-04-21T23:48:15.710014: step 80, loss 0.682848, acc 0.72\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:48:16.384678: step 80, loss 0.68197, acc 0.516296, rec 0.979911, pre 0.507319, f1 0.668528\n",
      "\n",
      "2018-04-21T23:48:16.427745: step 81, loss 0.608974, acc 0.72\n",
      "2018-04-21T23:48:16.473302: step 82, loss 0.663528, acc 0.66\n",
      "2018-04-21T23:48:16.518519: step 83, loss 0.705702, acc 0.56\n",
      "2018-04-21T23:48:16.564233: step 84, loss 0.68744, acc 0.58\n",
      "2018-04-21T23:48:16.613576: step 85, loss 0.669682, acc 0.6\n",
      "2018-04-21T23:48:16.659054: step 86, loss 0.582859, acc 0.78\n",
      "2018-04-21T23:48:16.704439: step 87, loss 0.628018, acc 0.6\n",
      "2018-04-21T23:48:16.749534: step 88, loss 0.538392, acc 0.74\n",
      "2018-04-21T23:48:16.793186: step 89, loss 0.562801, acc 0.78\n",
      "2018-04-21T23:48:16.841126: step 90, loss 0.532329, acc 0.76\n",
      "2018-04-21T23:48:16.886276: step 91, loss 0.651876, acc 0.6\n",
      "2018-04-21T23:48:16.931416: step 92, loss 0.607524, acc 0.68\n",
      "2018-04-21T23:48:16.975869: step 93, loss 0.541272, acc 0.8\n",
      "2018-04-21T23:48:17.020420: step 94, loss 0.626333, acc 0.72\n",
      "2018-04-21T23:48:17.070430: step 95, loss 0.615431, acc 0.64\n",
      "2018-04-21T23:48:17.115575: step 96, loss 0.531943, acc 0.7\n",
      "2018-04-21T23:48:17.160679: step 97, loss 0.733529, acc 0.5\n",
      "2018-04-21T23:48:17.205516: step 98, loss 0.63723, acc 0.66\n",
      "2018-04-21T23:48:17.251155: step 99, loss 0.787144, acc 0.5\n",
      "2018-04-21T23:48:17.299726: step 100, loss 0.883085, acc 0.5\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:48:18.324493: step 100, loss 0.714833, acc 0.507778, rec 0.985119, pre 0.502848, f1 0.665829\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-100\n",
      "\n",
      "2018-04-21T23:48:18.504950: step 101, loss 0.684695, acc 0.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:48:18.604498: step 102, loss 0.685514, acc 0.64\n",
      "2018-04-21T23:48:18.696483: step 103, loss 0.608003, acc 0.64\n",
      "2018-04-21T23:48:18.792499: step 104, loss 0.617203, acc 0.78\n",
      "2018-04-21T23:48:18.888490: step 105, loss 0.618681, acc 0.62\n",
      "2018-04-21T23:48:18.983062: step 106, loss 0.607676, acc 0.6\n",
      "2018-04-21T23:48:19.080491: step 107, loss 0.507033, acc 0.82\n",
      "2018-04-21T23:48:19.177124: step 108, loss 0.583077, acc 0.72\n",
      "2018-04-21T23:48:19.276535: step 109, loss 0.651711, acc 0.66\n",
      "2018-04-21T23:48:19.372531: step 110, loss 0.637115, acc 0.6\n",
      "2018-04-21T23:48:19.468728: step 111, loss 0.605654, acc 0.76\n",
      "2018-04-21T23:48:19.568520: step 112, loss 0.728229, acc 0.72\n",
      "2018-04-21T23:48:19.661396: step 113, loss 0.552143, acc 0.78\n",
      "2018-04-21T23:48:19.760500: step 114, loss 0.490597, acc 0.8\n",
      "2018-04-21T23:48:19.852746: step 115, loss 0.621159, acc 0.62\n",
      "2018-04-21T23:48:19.944498: step 116, loss 0.518417, acc 0.7\n",
      "2018-04-21T23:48:20.044485: step 117, loss 0.6742, acc 0.64\n",
      "2018-04-21T23:48:20.136494: step 118, loss 0.670909, acc 0.56\n",
      "2018-04-21T23:48:20.229527: step 119, loss 0.648365, acc 0.6\n",
      "2018-04-21T23:48:20.327528: step 120, loss 0.851209, acc 0.46\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:48:21.689211: step 120, loss 0.797252, acc 0.502963, rec 0.0014881, pre 1, f1 0.00297177\n",
      "\n",
      "2018-04-21T23:48:21.780905: step 121, loss 0.70271, acc 0.62\n",
      "2018-04-21T23:48:21.873734: step 122, loss 0.614162, acc 0.62\n",
      "2018-04-21T23:48:21.970098: step 123, loss 0.541544, acc 0.74\n",
      "2018-04-21T23:48:22.064679: step 124, loss 0.51214, acc 0.74\n",
      "2018-04-21T23:48:22.156483: step 125, loss 0.644331, acc 0.66\n",
      "2018-04-21T23:48:22.253871: step 126, loss 0.647122, acc 0.62\n",
      "2018-04-21T23:48:22.348527: step 127, loss 0.584768, acc 0.64\n",
      "2018-04-21T23:48:22.440839: step 128, loss 0.565288, acc 0.72\n",
      "2018-04-21T23:48:22.538075: step 129, loss 0.5021, acc 0.78\n",
      "2018-04-21T23:48:22.629377: step 130, loss 0.75123, acc 0.48\n",
      "2018-04-21T23:48:22.725387: step 131, loss 0.873925, acc 0.48\n",
      "2018-04-21T23:48:22.821480: step 132, loss 0.672018, acc 0.6\n",
      "2018-04-21T23:48:22.914948: step 133, loss 0.703531, acc 0.64\n",
      "2018-04-21T23:48:23.012528: step 134, loss 0.579154, acc 0.64\n",
      "2018-04-21T23:48:23.112494: step 135, loss 0.557815, acc 0.72\n",
      "2018-04-21T23:48:23.200489: step 136, loss 0.565603, acc 0.76\n",
      "2018-04-21T23:48:23.288933: step 137, loss 0.642501, acc 0.68\n",
      "2018-04-21T23:48:23.388486: step 138, loss 0.705103, acc 0.6\n",
      "2018-04-21T23:48:23.475833: step 139, loss 0.639007, acc 0.62\n",
      "2018-04-21T23:48:23.572518: step 140, loss 0.624148, acc 0.62\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:48:24.933044: step 140, loss 0.649804, acc 0.551481, rec 0.97619, pre 0.526696, f1 0.684224\n",
      "\n",
      "2018-04-21T23:48:25.028921: step 141, loss 0.529429, acc 0.72\n",
      "2018-04-21T23:48:25.121219: step 142, loss 0.506598, acc 0.8\n",
      "2018-04-21T23:48:25.218112: step 143, loss 0.523759, acc 0.8\n",
      "2018-04-21T23:48:25.312468: step 144, loss 0.627102, acc 0.64\n",
      "2018-04-21T23:48:25.404499: step 145, loss 0.701652, acc 0.74\n",
      "2018-04-21T23:48:25.495092: step 146, loss 0.557974, acc 0.64\n",
      "2018-04-21T23:48:25.587466: step 147, loss 0.641926, acc 0.82\n",
      "2018-04-21T23:48:25.677788: step 148, loss 0.588997, acc 0.74\n",
      "2018-04-21T23:48:25.773554: step 149, loss 0.447856, acc 0.9\n",
      "2018-04-21T23:48:25.864476: step 150, loss 0.541166, acc 0.68\n",
      "2018-04-21T23:48:25.956486: step 151, loss 0.573889, acc 0.72\n",
      "2018-04-21T23:48:26.052524: step 152, loss 0.772842, acc 0.7\n",
      "2018-04-21T23:48:26.144588: step 153, loss 0.579052, acc 0.64\n",
      "2018-04-21T23:48:26.236495: step 154, loss 0.607919, acc 0.6\n",
      "2018-04-21T23:48:26.332731: step 155, loss 0.624854, acc 0.62\n",
      "2018-04-21T23:48:26.428494: step 156, loss 0.558365, acc 0.84\n",
      "2018-04-21T23:48:26.520479: step 157, loss 0.527671, acc 0.76\n",
      "2018-04-21T23:48:26.612645: step 158, loss 0.482918, acc 0.84\n",
      "2018-04-21T23:48:26.708498: step 159, loss 0.598259, acc 0.66\n",
      "2018-04-21T23:48:26.800481: step 160, loss 0.562637, acc 0.7\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:48:28.181499: step 160, loss 0.774631, acc 0.507407, rec 0.985863, pre 0.502656, f1 0.665829\n",
      "\n",
      "2018-04-21T23:48:28.277000: step 161, loss 0.620976, acc 0.64\n",
      "2018-04-21T23:48:28.376501: step 162, loss 0.598137, acc 0.74\n",
      "2018-04-21T23:48:28.476527: step 163, loss 0.693516, acc 0.64\n",
      "2018-04-21T23:48:28.576293: step 164, loss 0.464925, acc 0.82\n",
      "2018-04-21T23:48:28.666162: step 165, loss 0.526281, acc 0.72\n",
      "2018-04-21T23:48:28.763082: step 166, loss 0.511883, acc 0.8\n",
      "2018-04-21T23:48:28.858782: step 167, loss 0.52674, acc 0.84\n",
      "2018-04-21T23:48:28.949751: step 168, loss 0.68684, acc 0.76\n",
      "2018-04-21T23:48:29.045487: step 169, loss 0.513476, acc 0.74\n",
      "2018-04-21T23:48:29.141165: step 170, loss 0.526112, acc 0.7\n",
      "2018-04-21T23:48:29.232478: step 171, loss 0.555581, acc 0.72\n",
      "2018-04-21T23:48:29.328478: step 172, loss 0.594845, acc 0.7\n",
      "2018-04-21T23:48:29.420471: step 173, loss 0.591879, acc 0.66\n",
      "2018-04-21T23:48:29.512464: step 174, loss 0.53441, acc 0.66\n",
      "2018-04-21T23:48:29.601751: step 175, loss 0.621479, acc 0.6\n",
      "2018-04-21T23:48:29.696475: step 176, loss 0.758552, acc 0.52\n",
      "2018-04-21T23:48:29.788441: step 177, loss 0.73054, acc 0.6\n",
      "2018-04-21T23:48:29.873279: step 178, loss 0.586006, acc 0.68\n",
      "2018-04-21T23:48:29.962054: step 179, loss 0.564387, acc 0.72\n",
      "2018-04-21T23:48:30.050750: step 180, loss 0.508581, acc 0.74\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:48:30.912114: step 180, loss 0.896859, acc 0.504444, rec 0.990327, pre 0.50113, f1 0.6655\n",
      "\n",
      "2018-04-21T23:48:30.958345: step 181, loss 0.716344, acc 0.56\n",
      "2018-04-21T23:48:31.003675: step 182, loss 0.702259, acc 0.68\n",
      "2018-04-21T23:48:31.049343: step 183, loss 0.703532, acc 0.66\n",
      "2018-04-21T23:48:31.094073: step 184, loss 0.569336, acc 0.66\n",
      "2018-04-21T23:48:31.143694: step 185, loss 0.599265, acc 0.74\n",
      "2018-04-21T23:48:31.188913: step 186, loss 0.637961, acc 0.72\n",
      "2018-04-21T23:48:31.233336: step 187, loss 0.581181, acc 0.7\n",
      "2018-04-21T23:48:31.277995: step 188, loss 0.510907, acc 0.74\n",
      "2018-04-21T23:48:31.322109: step 189, loss 0.501349, acc 0.78\n",
      "2018-04-21T23:48:31.372809: step 190, loss 0.546158, acc 0.7\n",
      "2018-04-21T23:48:31.418549: step 191, loss 0.582516, acc 0.88\n",
      "2018-04-21T23:48:31.463667: step 192, loss 0.530489, acc 0.7\n",
      "2018-04-21T23:48:31.510015: step 193, loss 0.411994, acc 0.82\n",
      "2018-04-21T23:48:31.554077: step 194, loss 0.738726, acc 0.6\n",
      "2018-04-21T23:48:31.603239: step 195, loss 0.582491, acc 0.68\n",
      "2018-04-21T23:48:31.648239: step 196, loss 0.593538, acc 0.76\n",
      "2018-04-21T23:48:31.692285: step 197, loss 0.57964, acc 0.78\n",
      "2018-04-21T23:48:31.736878: step 198, loss 0.533104, acc 0.74\n",
      "2018-04-21T23:48:31.781008: step 199, loss 0.521603, acc 0.72\n",
      "2018-04-21T23:48:31.828082: step 200, loss 0.506598, acc 0.82\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:48:32.476613: step 200, loss 0.639779, acc 0.578519, rec 0.15625, pre 0.981308, f1 0.269576\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-200\n",
      "\n",
      "2018-04-21T23:48:32.560369: step 201, loss 0.591606, acc 0.72\n",
      "2018-04-21T23:48:32.604173: step 202, loss 0.538877, acc 0.7\n",
      "2018-04-21T23:48:32.647743: step 203, loss 0.988424, acc 0.48\n",
      "2018-04-21T23:48:32.695377: step 204, loss 0.933911, acc 0.58\n",
      "2018-04-21T23:48:32.739806: step 205, loss 0.49921, acc 0.88\n",
      "2018-04-21T23:48:32.785257: step 206, loss 0.697152, acc 0.66\n",
      "2018-04-21T23:48:32.828871: step 207, loss 0.475446, acc 0.84\n",
      "2018-04-21T23:48:32.872745: step 208, loss 0.469719, acc 0.84\n",
      "2018-04-21T23:48:32.919274: step 209, loss 0.512361, acc 0.78\n",
      "2018-04-21T23:48:32.962775: step 210, loss 0.484731, acc 0.78\n",
      "2018-04-21T23:48:33.007134: step 211, loss 0.780132, acc 0.74\n",
      "2018-04-21T23:48:33.051252: step 212, loss 0.615574, acc 0.64\n",
      "2018-04-21T23:48:33.095465: step 213, loss 0.515614, acc 0.8\n",
      "2018-04-21T23:48:33.143154: step 214, loss 0.531537, acc 0.76\n",
      "2018-04-21T23:48:33.187434: step 215, loss 0.541894, acc 0.78\n",
      "2018-04-21T23:48:33.231791: step 216, loss 0.531027, acc 0.72\n",
      "2018-04-21T23:48:33.275787: step 217, loss 0.691365, acc 0.58\n",
      "2018-04-21T23:48:33.319816: step 218, loss 0.729515, acc 0.6\n",
      "2018-04-21T23:48:33.366494: step 219, loss 0.534309, acc 0.72\n",
      "2018-04-21T23:48:33.410555: step 220, loss 0.500066, acc 0.72\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:48:34.043367: step 220, loss 0.666917, acc 0.542593, rec 0.977679, pre 0.521636, f1 0.6803\n",
      "\n",
      "2018-04-21T23:48:34.086967: step 221, loss 0.525448, acc 0.72\n",
      "2018-04-21T23:48:34.132172: step 222, loss 0.489983, acc 0.82\n",
      "2018-04-21T23:48:34.175358: step 223, loss 0.470175, acc 0.82\n",
      "2018-04-21T23:48:34.218403: step 224, loss 0.538005, acc 0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:48:34.264628: step 225, loss 0.537436, acc 0.7\n",
      "2018-04-21T23:48:34.308091: step 226, loss 0.639389, acc 0.62\n",
      "2018-04-21T23:48:34.351804: step 227, loss 0.704875, acc 0.54\n",
      "2018-04-21T23:48:34.395246: step 228, loss 0.675314, acc 0.58\n",
      "2018-04-21T23:48:34.438404: step 229, loss 0.512865, acc 0.88\n",
      "2018-04-21T23:48:34.484588: step 230, loss 0.562706, acc 0.74\n",
      "2018-04-21T23:48:34.527868: step 231, loss 0.61729, acc 0.64\n",
      "2018-04-21T23:48:34.571901: step 232, loss 0.477181, acc 0.78\n",
      "2018-04-21T23:48:34.616148: step 233, loss 0.567754, acc 0.74\n",
      "2018-04-21T23:48:34.659279: step 234, loss 0.573118, acc 0.7\n",
      "2018-04-21T23:48:34.705866: step 235, loss 0.602841, acc 0.72\n",
      "2018-04-21T23:48:34.751577: step 236, loss 0.538314, acc 0.72\n",
      "2018-04-21T23:48:34.795378: step 237, loss 0.516621, acc 0.72\n",
      "2018-04-21T23:48:34.839829: step 238, loss 0.476352, acc 0.82\n",
      "2018-04-21T23:48:34.883124: step 239, loss 0.508763, acc 0.74\n",
      "2018-04-21T23:48:34.929844: step 240, loss 0.861112, acc 0.7\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:48:35.561924: step 240, loss 0.727473, acc 0.522222, rec 0.982143, pre 0.510441, f1 0.671756\n",
      "\n",
      "2018-04-21T23:48:35.605252: step 241, loss 0.487569, acc 0.78\n",
      "2018-04-21T23:48:35.649767: step 242, loss 0.437441, acc 0.86\n",
      "2018-04-21T23:48:35.693911: step 243, loss 0.562534, acc 0.62\n",
      "2018-04-21T23:48:35.738088: step 244, loss 0.652838, acc 0.62\n",
      "2018-04-21T23:48:35.784514: step 245, loss 0.550991, acc 0.7\n",
      "2018-04-21T23:48:35.827136: step 246, loss 0.79746, acc 0.62\n",
      "2018-04-21T23:48:35.870040: step 247, loss 0.463652, acc 0.8\n",
      "2018-04-21T23:48:35.912894: step 248, loss 0.443251, acc 0.86\n",
      "2018-04-21T23:48:35.956397: step 249, loss 0.438403, acc 0.8\n",
      "2018-04-21T23:48:36.002430: step 250, loss 0.609086, acc 0.8\n",
      "2018-04-21T23:48:36.046830: step 251, loss 0.573844, acc 0.68\n",
      "2018-04-21T23:48:36.090585: step 252, loss 0.589452, acc 0.72\n",
      "2018-04-21T23:48:36.134162: step 253, loss 0.681217, acc 0.66\n",
      "2018-04-21T23:48:36.177581: step 254, loss 0.558771, acc 0.88\n",
      "2018-04-21T23:48:36.225364: step 255, loss 0.493033, acc 0.84\n",
      "2018-04-21T23:48:36.269211: step 256, loss 0.654671, acc 0.58\n",
      "2018-04-21T23:48:36.313001: step 257, loss 0.948652, acc 0.6\n",
      "2018-04-21T23:48:36.356909: step 258, loss 0.505306, acc 0.78\n",
      "2018-04-21T23:48:36.400404: step 259, loss 0.578058, acc 0.76\n",
      "2018-04-21T23:48:36.448085: step 260, loss 0.700788, acc 0.64\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:48:37.081437: step 260, loss 0.574462, acc 0.707407, rec 0.927827, pre 0.642784, f1 0.75944\n",
      "\n",
      "2018-04-21T23:48:37.125182: step 261, loss 0.470155, acc 0.86\n",
      "2018-04-21T23:48:37.169603: step 262, loss 0.578165, acc 0.68\n",
      "2018-04-21T23:48:37.214411: step 263, loss 0.689594, acc 0.54\n",
      "2018-04-21T23:48:37.259451: step 264, loss 0.774619, acc 0.54\n",
      "2018-04-21T23:48:37.306262: step 265, loss 0.504996, acc 0.7\n",
      "2018-04-21T23:48:37.351844: step 266, loss 0.500559, acc 0.72\n",
      "2018-04-21T23:48:37.396122: step 267, loss 0.468798, acc 0.8\n",
      "2018-04-21T23:48:37.439923: step 268, loss 0.509865, acc 0.78\n",
      "2018-04-21T23:48:37.483565: step 269, loss 0.547655, acc 0.76\n",
      "2018-04-21T23:48:37.530670: step 270, loss 0.65768, acc 0.76\n",
      "2018-04-21T23:48:37.575153: step 271, loss 0.607483, acc 0.66\n",
      "2018-04-21T23:48:37.618689: step 272, loss 0.812732, acc 0.58\n",
      "2018-04-21T23:48:37.662498: step 273, loss 0.566129, acc 0.64\n",
      "2018-04-21T23:48:37.706674: step 274, loss 0.500366, acc 0.78\n",
      "2018-04-21T23:48:37.754552: step 275, loss 0.512824, acc 0.82\n",
      "2018-04-21T23:48:37.801197: step 276, loss 0.403673, acc 0.92\n",
      "2018-04-21T23:48:37.845915: step 277, loss 0.455652, acc 0.88\n",
      "2018-04-21T23:48:37.891190: step 278, loss 0.707008, acc 0.64\n",
      "2018-04-21T23:48:37.936144: step 279, loss 0.772353, acc 0.56\n",
      "2018-04-21T23:48:37.985201: step 280, loss 0.773138, acc 0.48\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:48:38.662933: step 280, loss 0.61407, acc 0.574444, rec 0.146577, pre 0.98995, f1 0.255347\n",
      "\n",
      "2018-04-21T23:48:38.706629: step 281, loss 0.697304, acc 0.74\n",
      "2018-04-21T23:48:38.752000: step 282, loss 0.550989, acc 0.72\n",
      "2018-04-21T23:48:38.796396: step 283, loss 0.475219, acc 0.84\n",
      "2018-04-21T23:48:38.841204: step 284, loss 0.486569, acc 0.76\n",
      "2018-04-21T23:48:38.890645: step 285, loss 0.590703, acc 0.66\n",
      "2018-04-21T23:48:38.937473: step 286, loss 0.529589, acc 0.76\n",
      "2018-04-21T23:48:38.982858: step 287, loss 0.460884, acc 0.74\n",
      "2018-04-21T23:48:39.028035: step 288, loss 0.548636, acc 0.64\n",
      "2018-04-21T23:48:39.073298: step 289, loss 0.52897, acc 0.72\n",
      "2018-04-21T23:48:39.122225: step 290, loss 0.462032, acc 0.8\n",
      "2018-04-21T23:48:39.168406: step 291, loss 0.550389, acc 0.72\n",
      "2018-04-21T23:48:39.212906: step 292, loss 0.430469, acc 0.8\n",
      "2018-04-21T23:48:39.257022: step 293, loss 0.440307, acc 0.8\n",
      "2018-04-21T23:48:39.300420: step 294, loss 0.715715, acc 0.8\n",
      "2018-04-21T23:48:39.350222: step 295, loss 0.468292, acc 0.76\n",
      "2018-04-21T23:48:39.395392: step 296, loss 0.434861, acc 0.84\n",
      "2018-04-21T23:48:39.440093: step 297, loss 0.522289, acc 0.76\n",
      "2018-04-21T23:48:39.486008: step 298, loss 0.563418, acc 0.64\n",
      "2018-04-21T23:48:39.532010: step 299, loss 0.608692, acc 0.68\n",
      "2018-04-21T23:48:39.581218: step 300, loss 0.56548, acc 0.7\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:48:40.259392: step 300, loss 0.661256, acc 0.555556, rec 0.977679, pre 0.528986, f1 0.68652\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-300\n",
      "\n",
      "2018-04-21T23:48:40.345527: step 301, loss 0.531116, acc 0.7\n",
      "2018-04-21T23:48:40.389860: step 302, loss 0.567546, acc 0.7\n",
      "2018-04-21T23:48:40.435319: step 303, loss 0.692366, acc 0.66\n",
      "2018-04-21T23:48:40.483539: step 304, loss 0.476251, acc 0.8\n",
      "2018-04-21T23:48:40.527920: step 305, loss 0.44325, acc 0.84\n",
      "2018-04-21T23:48:40.572103: step 306, loss 0.46768, acc 0.78\n",
      "2018-04-21T23:48:40.616848: step 307, loss 0.591786, acc 0.72\n",
      "2018-04-21T23:48:40.662828: step 308, loss 0.531477, acc 0.7\n",
      "2018-04-21T23:48:40.713987: step 309, loss 0.668998, acc 0.64\n",
      "2018-04-21T23:48:40.759830: step 310, loss 0.820347, acc 0.68\n",
      "2018-04-21T23:48:40.804935: step 311, loss 0.698735, acc 0.68\n",
      "2018-04-21T23:48:40.849692: step 312, loss 0.511101, acc 0.74\n",
      "2018-04-21T23:48:40.900122: step 313, loss 0.603873, acc 0.7\n",
      "2018-04-21T23:48:40.948620: step 314, loss 0.619467, acc 0.66\n",
      "2018-04-21T23:48:40.993145: step 315, loss 0.366051, acc 0.96\n",
      "2018-04-21T23:48:41.039042: step 316, loss 0.570151, acc 0.7\n",
      "2018-04-21T23:48:41.083557: step 317, loss 0.396551, acc 0.88\n",
      "2018-04-21T23:48:41.127742: step 318, loss 0.400924, acc 0.88\n",
      "2018-04-21T23:48:41.176878: step 319, loss 0.511883, acc 0.76\n",
      "2018-04-21T23:48:41.221348: step 320, loss 0.525979, acc 0.78\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:48:41.866330: step 320, loss 0.543914, acc 0.783704, rec 0.90253, pre 0.728091, f1 0.80598\n",
      "\n",
      "2018-04-21T23:48:41.910212: step 321, loss 0.720436, acc 0.78\n",
      "2018-04-21T23:48:41.953693: step 322, loss 0.48511, acc 0.82\n",
      "2018-04-21T23:48:41.998022: step 323, loss 0.489709, acc 0.76\n",
      "2018-04-21T23:48:42.042685: step 324, loss 0.450797, acc 0.84\n",
      "2018-04-21T23:48:42.091064: step 325, loss 0.469623, acc 0.86\n",
      "2018-04-21T23:48:42.134615: step 326, loss 0.657621, acc 0.82\n",
      "2018-04-21T23:48:42.178418: step 327, loss 0.549766, acc 0.76\n",
      "2018-04-21T23:48:42.221759: step 328, loss 0.560543, acc 0.78\n",
      "2018-04-21T23:48:42.265134: step 329, loss 0.443833, acc 0.84\n",
      "2018-04-21T23:48:42.311879: step 330, loss 0.455414, acc 0.82\n",
      "2018-04-21T23:48:42.355136: step 331, loss 0.714921, acc 0.66\n",
      "2018-04-21T23:48:42.399506: step 332, loss 0.599682, acc 0.72\n",
      "2018-04-21T23:48:42.443206: step 333, loss 0.550847, acc 0.66\n",
      "2018-04-21T23:48:42.487151: step 334, loss 0.370797, acc 0.84\n",
      "2018-04-21T23:48:42.536552: step 335, loss 0.384111, acc 0.86\n",
      "2018-04-21T23:48:42.581284: step 336, loss 0.466143, acc 0.94\n",
      "2018-04-21T23:48:42.625615: step 337, loss 0.410817, acc 0.84\n",
      "2018-04-21T23:48:42.670389: step 338, loss 0.449295, acc 0.86\n",
      "2018-04-21T23:48:42.714967: step 339, loss 0.503192, acc 0.72\n",
      "2018-04-21T23:48:42.762932: step 340, loss 0.429469, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:48:43.396005: step 340, loss 0.534901, acc 0.781482, rec 0.904018, pre 0.72494, f1 0.804636\n",
      "\n",
      "2018-04-21T23:48:43.439145: step 341, loss 0.425568, acc 0.84\n",
      "2018-04-21T23:48:43.483850: step 342, loss 0.485497, acc 0.74\n",
      "2018-04-21T23:48:43.527846: step 343, loss 0.802618, acc 0.72\n",
      "2018-04-21T23:48:43.572347: step 344, loss 0.559071, acc 0.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:48:43.619508: step 345, loss 0.573276, acc 0.7\n",
      "2018-04-21T23:48:43.663066: step 346, loss 0.515986, acc 0.7\n",
      "2018-04-21T23:48:43.707110: step 347, loss 0.540903, acc 0.76\n",
      "2018-04-21T23:48:43.750805: step 348, loss 0.545166, acc 0.82\n",
      "2018-04-21T23:48:43.797066: step 349, loss 0.688068, acc 0.64\n",
      "2018-04-21T23:48:43.845431: step 350, loss 0.569397, acc 0.66\n",
      "2018-04-21T23:48:43.894866: step 351, loss 0.482328, acc 0.72\n",
      "2018-04-21T23:48:43.939313: step 352, loss 0.497944, acc 0.78\n",
      "2018-04-21T23:48:43.984392: step 353, loss 0.97218, acc 0.68\n",
      "2018-04-21T23:48:44.029844: step 354, loss 0.481709, acc 0.84\n",
      "2018-04-21T23:48:44.076510: step 355, loss 0.414028, acc 0.86\n",
      "2018-04-21T23:48:44.120410: step 356, loss 0.413092, acc 0.88\n",
      "2018-04-21T23:48:44.165968: step 357, loss 0.673572, acc 0.52\n",
      "2018-04-21T23:48:44.209461: step 358, loss 0.682361, acc 0.6\n",
      "2018-04-21T23:48:44.253402: step 359, loss 0.61085, acc 0.58\n",
      "2018-04-21T23:48:44.299959: step 360, loss 0.563155, acc 0.68\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:48:44.931765: step 360, loss 0.546274, acc 0.843333, rec 0.78125, pre 0.890585, f1 0.832342\n",
      "\n",
      "2018-04-21T23:48:44.974822: step 361, loss 0.497897, acc 0.82\n",
      "2018-04-21T23:48:45.018147: step 362, loss 0.456907, acc 0.8\n",
      "2018-04-21T23:48:45.062589: step 363, loss 0.394346, acc 0.86\n",
      "2018-04-21T23:48:45.105670: step 364, loss 0.464624, acc 0.8\n",
      "2018-04-21T23:48:45.152794: step 365, loss 0.605682, acc 0.6\n",
      "2018-04-21T23:48:45.196742: step 366, loss 0.986083, acc 0.54\n",
      "2018-04-21T23:48:45.241018: step 367, loss 0.416599, acc 0.74\n",
      "2018-04-21T23:48:45.285041: step 368, loss 0.474575, acc 0.76\n",
      "2018-04-21T23:48:45.329009: step 369, loss 0.440268, acc 0.74\n",
      "2018-04-21T23:48:45.376512: step 370, loss 0.659544, acc 0.72\n",
      "2018-04-21T23:48:45.420268: step 371, loss 0.577906, acc 0.7\n",
      "2018-04-21T23:48:45.464281: step 372, loss 0.635082, acc 0.64\n",
      "2018-04-21T23:48:45.508262: step 373, loss 0.525136, acc 0.76\n",
      "2018-04-21T23:48:45.552357: step 374, loss 0.492688, acc 0.82\n",
      "2018-04-21T23:48:45.599206: step 375, loss 0.655439, acc 0.74\n",
      "2018-04-21T23:48:45.642788: step 376, loss 0.485759, acc 0.78\n",
      "2018-04-21T23:48:45.687759: step 377, loss 0.448823, acc 0.86\n",
      "2018-04-21T23:48:45.731514: step 378, loss 0.463984, acc 0.8\n",
      "2018-04-21T23:48:45.778169: step 379, loss 0.602024, acc 0.66\n",
      "2018-04-21T23:48:45.828252: step 380, loss 0.661937, acc 0.82\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:48:46.513223: step 380, loss 0.639805, acc 0.566296, rec 0.97619, pre 0.535292, f1 0.691436\n",
      "\n",
      "2018-04-21T23:48:46.557704: step 381, loss 0.474973, acc 0.78\n",
      "2018-04-21T23:48:46.601184: step 382, loss 0.540236, acc 0.84\n",
      "2018-04-21T23:48:46.645771: step 383, loss 0.534719, acc 0.78\n",
      "2018-04-21T23:48:46.690687: step 384, loss 0.557547, acc 0.68\n",
      "2018-04-21T23:48:46.741018: step 385, loss 0.452475, acc 0.8\n",
      "2018-04-21T23:48:46.786435: step 386, loss 0.46537, acc 0.82\n",
      "2018-04-21T23:48:46.831344: step 387, loss 0.500764, acc 0.8\n",
      "2018-04-21T23:48:46.875101: step 388, loss 0.382824, acc 0.84\n",
      "2018-04-21T23:48:46.919291: step 389, loss 0.366508, acc 0.86\n",
      "2018-04-21T23:48:46.968225: step 390, loss 0.754749, acc 0.62\n",
      "2018-04-21T23:48:47.051900: step 391, loss 0.45187, acc 0.76\n",
      "2018-04-21T23:48:47.096832: step 392, loss 0.401457, acc 0.84\n",
      "2018-04-21T23:48:47.141713: step 393, loss 0.465255, acc 0.8\n",
      "2018-04-21T23:48:47.196330: step 394, loss 0.823609, acc 0.56\n",
      "2018-04-21T23:48:47.241568: step 395, loss 0.501655, acc 0.66\n",
      "2018-04-21T23:48:47.287755: step 396, loss 0.393537, acc 0.88\n",
      "2018-04-21T23:48:47.334100: step 397, loss 0.451048, acc 0.84\n",
      "2018-04-21T23:48:47.396508: step 398, loss 0.548808, acc 0.82\n",
      "2018-04-21T23:48:47.444412: step 399, loss 0.594389, acc 0.82\n",
      "2018-04-21T23:48:47.489903: step 400, loss 0.577575, acc 0.76\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:48:48.167330: step 400, loss 0.530186, acc 0.847778, rec 0.813988, pre 0.871713, f1 0.841862\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-400\n",
      "\n",
      "2018-04-21T23:48:48.253037: step 401, loss 0.514559, acc 0.82\n",
      "2018-04-21T23:48:48.297112: step 402, loss 0.507698, acc 0.72\n",
      "2018-04-21T23:48:48.342177: step 403, loss 0.47631, acc 0.8\n",
      "2018-04-21T23:48:48.392082: step 404, loss 0.479862, acc 0.9\n",
      "2018-04-21T23:48:48.436747: step 405, loss 0.532258, acc 0.78\n",
      "2018-04-21T23:48:48.482052: step 406, loss 0.500548, acc 0.7\n",
      "2018-04-21T23:48:48.527827: step 407, loss 0.397017, acc 0.84\n",
      "2018-04-21T23:48:48.572483: step 408, loss 0.396906, acc 0.8\n",
      "2018-04-21T23:48:48.623382: step 409, loss 0.491047, acc 0.76\n",
      "2018-04-21T23:48:48.668855: step 410, loss 0.621824, acc 0.64\n",
      "2018-04-21T23:48:48.714064: step 411, loss 0.707312, acc 0.64\n",
      "2018-04-21T23:48:48.759309: step 412, loss 0.987279, acc 0.54\n",
      "2018-04-21T23:48:48.803658: step 413, loss 0.597289, acc 0.6\n",
      "2018-04-21T23:48:48.851535: step 414, loss 0.506786, acc 0.84\n",
      "2018-04-21T23:48:48.896861: step 415, loss 0.504789, acc 0.74\n",
      "2018-04-21T23:48:48.941230: step 416, loss 0.487104, acc 0.86\n",
      "2018-04-21T23:48:49.004776: step 417, loss 0.468298, acc 0.82\n",
      "2018-04-21T23:48:49.107578: step 418, loss 0.638947, acc 0.78\n",
      "2018-04-21T23:48:49.197994: step 419, loss 0.629268, acc 0.68\n",
      "2018-04-21T23:48:49.292645: step 420, loss 0.437536, acc 0.7\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:48:50.665112: step 420, loss 0.617712, acc 0.591852, rec 0.974702, pre 0.550883, f1 0.703923\n",
      "\n",
      "2018-04-21T23:48:50.757085: step 421, loss 0.550632, acc 0.8\n",
      "2018-04-21T23:48:50.852504: step 422, loss 0.531655, acc 0.7\n",
      "2018-04-21T23:48:50.948502: step 423, loss 0.436853, acc 0.76\n",
      "2018-04-21T23:48:51.040516: step 424, loss 0.465645, acc 0.84\n",
      "2018-04-21T23:48:51.136512: step 425, loss 0.737862, acc 0.7\n",
      "2018-04-21T23:48:51.228417: step 426, loss 0.552034, acc 0.7\n",
      "2018-04-21T23:48:51.325034: step 427, loss 0.451944, acc 0.86\n",
      "2018-04-21T23:48:51.417353: step 428, loss 0.445373, acc 0.8\n",
      "2018-04-21T23:48:51.513589: step 429, loss 0.500009, acc 0.86\n",
      "2018-04-21T23:48:51.610241: step 430, loss 0.513584, acc 0.88\n",
      "2018-04-21T23:48:51.708513: step 431, loss 0.55387, acc 0.76\n",
      "2018-04-21T23:48:51.802759: step 432, loss 0.525605, acc 0.76\n",
      "2018-04-21T23:48:51.900479: step 433, loss 0.482302, acc 0.82\n",
      "2018-04-21T23:48:51.992486: step 434, loss 0.474356, acc 0.76\n",
      "2018-04-21T23:48:52.088511: step 435, loss 0.53578, acc 0.78\n",
      "2018-04-21T23:48:52.184490: step 436, loss 0.428494, acc 0.78\n",
      "2018-04-21T23:48:52.276487: step 437, loss 0.504225, acc 0.74\n",
      "2018-04-21T23:48:52.372493: step 438, loss 0.629593, acc 0.6\n",
      "2018-04-21T23:48:52.464518: step 439, loss 0.452027, acc 0.76\n",
      "2018-04-21T23:48:52.556568: step 440, loss 0.52414, acc 0.76\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:48:53.934682: step 440, loss 0.522776, acc 0.854074, rec 0.833333, pre 0.868217, f1 0.850418\n",
      "\n",
      "2018-04-21T23:48:54.028496: step 441, loss 0.78014, acc 0.86\n",
      "2018-04-21T23:48:54.124467: step 442, loss 0.396496, acc 0.92\n",
      "2018-04-21T23:48:54.220994: step 443, loss 0.49029, acc 0.84\n",
      "2018-04-21T23:48:54.316725: step 444, loss 0.444828, acc 0.84\n",
      "2018-04-21T23:48:54.405885: step 445, loss 0.491285, acc 0.8\n",
      "2018-04-21T23:48:54.508478: step 446, loss 0.451311, acc 0.86\n",
      "2018-04-21T23:48:54.600478: step 447, loss 0.418021, acc 0.82\n",
      "2018-04-21T23:48:54.692479: step 448, loss 0.609059, acc 0.82\n",
      "2018-04-21T23:48:54.788479: step 449, loss 0.450964, acc 0.76\n",
      "2018-04-21T23:48:54.880481: step 450, loss 0.57167, acc 0.72\n",
      "2018-04-21T23:48:54.972480: step 451, loss 0.55325, acc 0.74\n",
      "2018-04-21T23:48:55.068475: step 452, loss 0.45925, acc 0.74\n",
      "2018-04-21T23:48:55.164497: step 453, loss 0.384794, acc 0.88\n",
      "2018-04-21T23:48:55.252505: step 454, loss 0.480276, acc 0.8\n",
      "2018-04-21T23:48:55.352495: step 455, loss 0.587168, acc 0.78\n",
      "2018-04-21T23:48:55.444474: step 456, loss 0.421011, acc 0.82\n",
      "2018-04-21T23:48:55.536738: step 457, loss 0.384721, acc 0.9\n",
      "2018-04-21T23:48:55.632489: step 458, loss 0.5208, acc 0.82\n",
      "2018-04-21T23:48:55.724027: step 459, loss 0.626521, acc 0.8\n",
      "2018-04-21T23:48:55.820789: step 460, loss 0.344948, acc 0.82\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:48:56.669103: step 460, loss 0.5269, acc 0.751111, rec 0.921875, pre 0.686047, f1 0.786667\n",
      "\n",
      "2018-04-21T23:48:56.714539: step 461, loss 0.369763, acc 0.9\n",
      "2018-04-21T23:48:56.759687: step 462, loss 0.359222, acc 0.84\n",
      "2018-04-21T23:48:56.805698: step 463, loss 0.553712, acc 0.74\n",
      "2018-04-21T23:48:56.852542: step 464, loss 0.494767, acc 0.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:48:56.902444: step 465, loss 0.517894, acc 0.64\n",
      "2018-04-21T23:48:56.947951: step 466, loss 0.632382, acc 0.62\n",
      "2018-04-21T23:48:56.994560: step 467, loss 0.551274, acc 0.66\n",
      "2018-04-21T23:48:57.040160: step 468, loss 0.474823, acc 0.76\n",
      "2018-04-21T23:48:57.085025: step 469, loss 0.381833, acc 0.82\n",
      "2018-04-21T23:48:57.133890: step 470, loss 0.356279, acc 0.88\n",
      "2018-04-21T23:48:57.179304: step 471, loss 0.380205, acc 0.86\n",
      "2018-04-21T23:48:57.225901: step 472, loss 0.58153, acc 0.76\n",
      "2018-04-21T23:48:57.271331: step 473, loss 0.393064, acc 0.82\n",
      "2018-04-21T23:48:57.317606: step 474, loss 0.347937, acc 0.88\n",
      "2018-04-21T23:48:57.369565: step 475, loss 0.511648, acc 0.74\n",
      "2018-04-21T23:48:57.415430: step 476, loss 0.627592, acc 0.78\n",
      "2018-04-21T23:48:57.460049: step 477, loss 0.547655, acc 0.8\n",
      "2018-04-21T23:48:57.505000: step 478, loss 0.459316, acc 0.86\n",
      "2018-04-21T23:48:57.551695: step 479, loss 0.862966, acc 0.76\n",
      "2018-04-21T23:48:57.601414: step 480, loss 0.683533, acc 0.66\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:48:58.241859: step 480, loss 0.598477, acc 0.614444, rec 0.230655, pre 0.977918, f1 0.373269\n",
      "\n",
      "2018-04-21T23:48:58.285411: step 481, loss 0.708765, acc 0.78\n",
      "2018-04-21T23:48:58.328339: step 482, loss 0.565727, acc 0.7\n",
      "2018-04-21T23:48:58.371710: step 483, loss 0.704794, acc 0.64\n",
      "2018-04-21T23:48:58.415319: step 484, loss 0.598127, acc 0.68\n",
      "2018-04-21T23:48:58.462103: step 485, loss 0.467427, acc 0.88\n",
      "2018-04-21T23:48:58.505097: step 486, loss 0.405094, acc 0.84\n",
      "2018-04-21T23:48:58.548689: step 487, loss 0.35826, acc 0.9\n",
      "2018-04-21T23:48:58.592996: step 488, loss 0.428781, acc 0.84\n",
      "2018-04-21T23:48:58.637009: step 489, loss 0.376812, acc 0.84\n",
      "2018-04-21T23:48:58.682584: step 490, loss 0.474175, acc 0.76\n",
      "2018-04-21T23:48:58.726935: step 491, loss 0.355224, acc 0.88\n",
      "2018-04-21T23:48:58.772981: step 492, loss 0.555113, acc 0.82\n",
      "2018-04-21T23:48:58.819471: step 493, loss 0.448163, acc 0.8\n",
      "2018-04-21T23:48:58.865705: step 494, loss 0.703067, acc 0.68\n",
      "2018-04-21T23:48:58.917476: step 495, loss 0.380006, acc 0.84\n",
      "2018-04-21T23:48:58.964942: step 496, loss 0.495049, acc 0.92\n",
      "2018-04-21T23:48:59.012826: step 497, loss 0.453182, acc 0.78\n",
      "2018-04-21T23:48:59.059056: step 498, loss 0.753323, acc 0.76\n",
      "2018-04-21T23:48:59.105806: step 499, loss 0.567529, acc 0.66\n",
      "2018-04-21T23:48:59.156495: step 500, loss 0.612145, acc 0.76\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:48:59.851511: step 500, loss 0.618428, acc 0.593333, rec 0.974702, pre 0.551811, f1 0.70468\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-500\n",
      "\n",
      "2018-04-21T23:48:59.940968: step 501, loss 0.389199, acc 0.78\n",
      "2018-04-21T23:48:59.987263: step 502, loss 0.510171, acc 0.84\n",
      "2018-04-21T23:49:00.034144: step 503, loss 0.541498, acc 0.72\n",
      "2018-04-21T23:49:00.083683: step 504, loss 0.426506, acc 0.78\n",
      "2018-04-21T23:49:00.129657: step 505, loss 0.442609, acc 0.78\n",
      "2018-04-21T23:49:00.175720: step 506, loss 0.320785, acc 0.9\n",
      "2018-04-21T23:49:00.220981: step 507, loss 0.504528, acc 0.76\n",
      "2018-04-21T23:49:00.265945: step 508, loss 0.679144, acc 0.76\n",
      "2018-04-21T23:49:00.362489: step 509, loss 0.568214, acc 0.84\n",
      "2018-04-21T23:49:00.456500: step 510, loss 0.520591, acc 0.78\n",
      "2018-04-21T23:49:00.549322: step 511, loss 0.376974, acc 0.92\n",
      "2018-04-21T23:49:00.645693: step 512, loss 0.42614, acc 0.78\n",
      "2018-04-21T23:49:00.740512: step 513, loss 0.426349, acc 0.86\n",
      "2018-04-21T23:49:00.836656: step 514, loss 0.421293, acc 0.82\n",
      "2018-04-21T23:49:00.929285: step 515, loss 0.386927, acc 0.76\n",
      "2018-04-21T23:49:01.020479: step 516, loss 0.478344, acc 0.88\n",
      "2018-04-21T23:49:01.112578: step 517, loss 0.513519, acc 0.82\n",
      "2018-04-21T23:49:01.212593: step 518, loss 0.504194, acc 0.82\n",
      "2018-04-21T23:49:01.304515: step 519, loss 0.51027, acc 0.78\n",
      "2018-04-21T23:49:01.395881: step 520, loss 0.546736, acc 0.82\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:02.105603: step 520, loss 0.617881, acc 0.596296, rec 0.974702, pre 0.553677, f1 0.706199\n",
      "\n",
      "2018-04-21T23:49:02.150661: step 521, loss 0.502517, acc 0.82\n",
      "2018-04-21T23:49:02.196185: step 522, loss 0.433105, acc 0.82\n",
      "2018-04-21T23:49:02.241709: step 523, loss 0.577393, acc 0.78\n",
      "2018-04-21T23:49:02.286181: step 524, loss 0.387047, acc 0.88\n",
      "2018-04-21T23:49:02.336032: step 525, loss 0.618017, acc 0.66\n",
      "2018-04-21T23:49:02.380853: step 526, loss 0.394854, acc 0.7\n",
      "2018-04-21T23:49:02.424361: step 527, loss 0.513643, acc 0.88\n",
      "2018-04-21T23:49:02.467897: step 528, loss 0.41995, acc 0.76\n",
      "2018-04-21T23:49:02.512997: step 529, loss 0.405461, acc 0.78\n",
      "2018-04-21T23:49:02.561184: step 530, loss 0.524031, acc 0.76\n",
      "2018-04-21T23:49:02.605396: step 531, loss 0.428312, acc 0.84\n",
      "2018-04-21T23:49:02.650245: step 532, loss 0.405291, acc 0.88\n",
      "2018-04-21T23:49:02.695759: step 533, loss 0.527639, acc 0.82\n",
      "2018-04-21T23:49:02.740452: step 534, loss 0.514555, acc 0.76\n",
      "2018-04-21T23:49:02.789177: step 535, loss 0.580516, acc 0.82\n",
      "2018-04-21T23:49:02.833296: step 536, loss 0.527151, acc 0.72\n",
      "2018-04-21T23:49:02.877909: step 537, loss 0.616981, acc 0.64\n",
      "2018-04-21T23:49:02.922409: step 538, loss 0.544891, acc 0.8\n",
      "2018-04-21T23:49:02.966629: step 539, loss 0.549963, acc 0.7\n",
      "2018-04-21T23:49:03.015338: step 540, loss 0.421726, acc 0.8\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:03.698193: step 540, loss 0.569776, acc 0.637037, rec 0.956845, pre 0.582428, f1 0.724099\n",
      "\n",
      "2018-04-21T23:49:03.742431: step 541, loss 0.434528, acc 0.84\n",
      "2018-04-21T23:49:03.786569: step 542, loss 0.514096, acc 0.76\n",
      "2018-04-21T23:49:03.830635: step 543, loss 0.508374, acc 0.76\n",
      "2018-04-21T23:49:03.874982: step 544, loss 0.48686, acc 0.84\n",
      "2018-04-21T23:49:03.924971: step 545, loss 0.568548, acc 0.74\n",
      "2018-04-21T23:49:03.972045: step 546, loss 0.926874, acc 0.62\n",
      "2018-04-21T23:49:04.016608: step 547, loss 0.557859, acc 0.66\n",
      "2018-04-21T23:49:04.061893: step 548, loss 0.629395, acc 0.64\n",
      "2018-04-21T23:49:04.106790: step 549, loss 0.486343, acc 0.72\n",
      "2018-04-21T23:49:04.155884: step 550, loss 0.507784, acc 0.76\n",
      "2018-04-21T23:49:04.201720: step 551, loss 0.635945, acc 0.66\n",
      "2018-04-21T23:49:04.247275: step 552, loss 0.347352, acc 0.92\n",
      "2018-04-21T23:49:04.293109: step 553, loss 0.413656, acc 0.78\n",
      "2018-04-21T23:49:04.339319: step 554, loss 0.63695, acc 0.74\n",
      "2018-04-21T23:49:04.388132: step 555, loss 0.522958, acc 0.68\n",
      "2018-04-21T23:49:04.434499: step 556, loss 0.383331, acc 0.8\n",
      "2018-04-21T23:49:04.480730: step 557, loss 0.441586, acc 0.82\n",
      "2018-04-21T23:49:04.525732: step 558, loss 0.407819, acc 0.8\n",
      "2018-04-21T23:49:04.570558: step 559, loss 0.353876, acc 0.88\n",
      "2018-04-21T23:49:04.620190: step 560, loss 0.379119, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:05.297691: step 560, loss 0.520401, acc 0.824815, rec 0.692708, pre 0.939455, f1 0.79743\n",
      "\n",
      "2018-04-21T23:49:05.343183: step 561, loss 0.498192, acc 0.78\n",
      "2018-04-21T23:49:05.387316: step 562, loss 0.41582, acc 0.86\n",
      "2018-04-21T23:49:05.431466: step 563, loss 0.460569, acc 0.88\n",
      "2018-04-21T23:49:05.477445: step 564, loss 0.446504, acc 0.8\n",
      "2018-04-21T23:49:05.527158: step 565, loss 0.394443, acc 0.82\n",
      "2018-04-21T23:49:05.570968: step 566, loss 0.599489, acc 0.66\n",
      "2018-04-21T23:49:05.616148: step 567, loss 0.797269, acc 0.76\n",
      "2018-04-21T23:49:05.662312: step 568, loss 0.460314, acc 0.84\n",
      "2018-04-21T23:49:05.707774: step 569, loss 0.469342, acc 0.72\n",
      "2018-04-21T23:49:05.757137: step 570, loss 0.792762, acc 0.48\n",
      "2018-04-21T23:49:05.803419: step 571, loss 0.610457, acc 0.62\n",
      "2018-04-21T23:49:05.849386: step 572, loss 0.810837, acc 0.68\n",
      "2018-04-21T23:49:05.894868: step 573, loss 0.358834, acc 0.88\n",
      "2018-04-21T23:49:05.939099: step 574, loss 0.40331, acc 0.84\n",
      "2018-04-21T23:49:06.024490: step 575, loss 0.585838, acc 0.84\n",
      "2018-04-21T23:49:06.118897: step 576, loss 0.492144, acc 0.88\n",
      "2018-04-21T23:49:06.210395: step 577, loss 0.441586, acc 0.78\n",
      "2018-04-21T23:49:06.305685: step 578, loss 0.335789, acc 0.92\n",
      "2018-04-21T23:49:06.399300: step 579, loss 0.358393, acc 0.86\n",
      "2018-04-21T23:49:06.492061: step 580, loss 0.460163, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:07.282520: step 580, loss 0.512524, acc 0.786667, rec 0.917411, pre 0.726148, f1 0.810651\n",
      "\n",
      "2018-04-21T23:49:07.327432: step 581, loss 0.47184, acc 0.84\n",
      "2018-04-21T23:49:07.373604: step 582, loss 0.419194, acc 0.8\n",
      "2018-04-21T23:49:07.420549: step 583, loss 0.326248, acc 0.9\n",
      "2018-04-21T23:49:07.466592: step 584, loss 0.646274, acc 0.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:49:07.516497: step 585, loss 0.398122, acc 0.86\n",
      "2018-04-21T23:49:07.562641: step 586, loss 0.416688, acc 0.82\n",
      "2018-04-21T23:49:07.608521: step 587, loss 0.389666, acc 0.88\n",
      "2018-04-21T23:49:07.670245: step 588, loss 0.462598, acc 0.78\n",
      "2018-04-21T23:49:07.715656: step 589, loss 0.408812, acc 0.82\n",
      "2018-04-21T23:49:07.765698: step 590, loss 0.972801, acc 0.76\n",
      "2018-04-21T23:49:07.810490: step 591, loss 0.454576, acc 0.84\n",
      "2018-04-21T23:49:07.856179: step 592, loss 0.357197, acc 0.9\n",
      "2018-04-21T23:49:07.901234: step 593, loss 0.40065, acc 0.88\n",
      "2018-04-21T23:49:07.946814: step 594, loss 0.688405, acc 0.76\n",
      "2018-04-21T23:49:07.996246: step 595, loss 0.436502, acc 0.82\n",
      "2018-04-21T23:49:08.041440: step 596, loss 0.324858, acc 0.9\n",
      "2018-04-21T23:49:08.087507: step 597, loss 0.525295, acc 0.82\n",
      "2018-04-21T23:49:08.132691: step 598, loss 0.366759, acc 0.84\n",
      "2018-04-21T23:49:08.176915: step 599, loss 0.338746, acc 0.86\n",
      "2018-04-21T23:49:08.225528: step 600, loss 0.394643, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:08.929892: step 600, loss 0.498704, acc 0.808889, rec 0.90625, pre 0.757463, f1 0.825203\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-600\n",
      "\n",
      "2018-04-21T23:49:09.021188: step 601, loss 0.556476, acc 0.76\n",
      "2018-04-21T23:49:09.066921: step 602, loss 0.714113, acc 0.72\n",
      "2018-04-21T23:49:09.111977: step 603, loss 0.609025, acc 0.7\n",
      "2018-04-21T23:49:09.161839: step 604, loss 0.56066, acc 0.86\n",
      "2018-04-21T23:49:09.206876: step 605, loss 0.566055, acc 0.8\n",
      "2018-04-21T23:49:09.251415: step 606, loss 0.66866, acc 0.88\n",
      "2018-04-21T23:49:09.295730: step 607, loss 0.39759, acc 0.74\n",
      "2018-04-21T23:49:09.341068: step 608, loss 0.452969, acc 0.84\n",
      "2018-04-21T23:49:09.389932: step 609, loss 0.650653, acc 0.72\n",
      "2018-04-21T23:49:09.434933: step 610, loss 0.532028, acc 0.88\n",
      "2018-04-21T23:49:09.481049: step 611, loss 0.3432, acc 0.88\n",
      "2018-04-21T23:49:09.526173: step 612, loss 0.544827, acc 0.8\n",
      "2018-04-21T23:49:09.572357: step 613, loss 0.414983, acc 0.92\n",
      "2018-04-21T23:49:09.622008: step 614, loss 0.351549, acc 0.9\n",
      "2018-04-21T23:49:09.668254: step 615, loss 0.400482, acc 0.84\n",
      "2018-04-21T23:49:09.715968: step 616, loss 0.434152, acc 0.78\n",
      "2018-04-21T23:49:09.764290: step 617, loss 0.531587, acc 0.76\n",
      "2018-04-21T23:49:09.811671: step 618, loss 0.45397, acc 0.84\n",
      "2018-04-21T23:49:09.863344: step 619, loss 0.291208, acc 0.92\n",
      "2018-04-21T23:49:09.908721: step 620, loss 0.420328, acc 0.78\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:10.835014: step 620, loss 0.503768, acc 0.83963, rec 0.729167, pre 0.934223, f1 0.819056\n",
      "\n",
      "2018-04-21T23:49:10.879727: step 621, loss 0.406614, acc 0.84\n",
      "2018-04-21T23:49:10.925938: step 622, loss 0.317255, acc 0.9\n",
      "2018-04-21T23:49:10.971241: step 623, loss 0.395557, acc 0.8\n",
      "2018-04-21T23:49:11.016664: step 624, loss 0.575227, acc 0.88\n",
      "2018-04-21T23:49:11.066618: step 625, loss 0.415451, acc 0.86\n",
      "2018-04-21T23:49:11.112986: step 626, loss 0.809836, acc 0.72\n",
      "2018-04-21T23:49:11.159204: step 627, loss 0.562733, acc 0.64\n",
      "2018-04-21T23:49:11.204765: step 628, loss 0.590173, acc 0.7\n",
      "2018-04-21T23:49:11.249875: step 629, loss 0.522183, acc 0.78\n",
      "2018-04-21T23:49:11.298872: step 630, loss 0.61515, acc 0.74\n",
      "2018-04-21T23:49:11.345233: step 631, loss 0.498787, acc 0.74\n",
      "2018-04-21T23:49:11.391097: step 632, loss 0.435392, acc 0.7\n",
      "2018-04-21T23:49:11.437148: step 633, loss 0.402176, acc 0.88\n",
      "2018-04-21T23:49:11.485740: step 634, loss 0.418241, acc 0.82\n",
      "2018-04-21T23:49:11.534553: step 635, loss 0.663982, acc 0.76\n",
      "2018-04-21T23:49:11.580124: step 636, loss 0.337794, acc 0.84\n",
      "2018-04-21T23:49:11.625138: step 637, loss 0.457915, acc 0.72\n",
      "2018-04-21T23:49:11.670756: step 638, loss 0.407939, acc 0.88\n",
      "2018-04-21T23:49:11.715314: step 639, loss 0.526892, acc 0.7\n",
      "2018-04-21T23:49:11.763651: step 640, loss 0.351673, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:12.474685: step 640, loss 0.564131, acc 0.641481, rec 0.956845, pre 0.58561, f1 0.726554\n",
      "\n",
      "2018-04-21T23:49:12.519239: step 641, loss 0.389949, acc 0.8\n",
      "2018-04-21T23:49:12.564476: step 642, loss 0.362916, acc 0.88\n",
      "2018-04-21T23:49:12.609764: step 643, loss 0.363373, acc 0.88\n",
      "2018-04-21T23:49:12.655350: step 644, loss 0.528646, acc 0.8\n",
      "2018-04-21T23:49:12.705823: step 645, loss 0.40693, acc 0.88\n",
      "2018-04-21T23:49:12.750834: step 646, loss 0.333979, acc 0.92\n",
      "2018-04-21T23:49:12.795801: step 647, loss 0.587729, acc 0.76\n",
      "2018-04-21T23:49:12.840521: step 648, loss 0.475448, acc 0.86\n",
      "2018-04-21T23:49:12.886194: step 649, loss 0.451419, acc 0.78\n",
      "2018-04-21T23:49:12.935227: step 650, loss 0.473186, acc 0.78\n",
      "2018-04-21T23:49:12.981788: step 651, loss 0.399568, acc 0.8\n",
      "2018-04-21T23:49:13.027211: step 652, loss 0.456613, acc 0.76\n",
      "2018-04-21T23:49:13.072894: step 653, loss 0.558288, acc 0.88\n",
      "2018-04-21T23:49:13.117964: step 654, loss 0.351277, acc 0.86\n",
      "2018-04-21T23:49:13.166678: step 655, loss 0.395655, acc 0.8\n",
      "2018-04-21T23:49:13.211778: step 656, loss 0.374055, acc 0.82\n",
      "2018-04-21T23:49:13.257192: step 657, loss 0.570573, acc 0.78\n",
      "2018-04-21T23:49:13.303521: step 658, loss 0.494488, acc 0.78\n",
      "2018-04-21T23:49:13.347996: step 659, loss 0.600469, acc 0.68\n",
      "2018-04-21T23:49:13.396650: step 660, loss 0.632287, acc 0.66\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:14.076450: step 660, loss 0.787884, acc 0.541111, rec 0.0811012, pre 0.964602, f1 0.149623\n",
      "\n",
      "2018-04-21T23:49:14.120602: step 661, loss 0.611748, acc 0.7\n",
      "2018-04-21T23:49:14.165790: step 662, loss 0.571011, acc 0.64\n",
      "2018-04-21T23:49:14.214874: step 663, loss 0.450829, acc 0.8\n",
      "2018-04-21T23:49:14.320438: step 664, loss 0.466153, acc 0.76\n",
      "2018-04-21T23:49:14.366828: step 665, loss 0.650023, acc 0.72\n",
      "2018-04-21T23:49:14.412114: step 666, loss 0.374223, acc 0.88\n",
      "2018-04-21T23:49:14.457644: step 667, loss 0.496073, acc 0.76\n",
      "2018-04-21T23:49:14.504300: step 668, loss 0.602136, acc 0.82\n",
      "2018-04-21T23:49:14.554481: step 669, loss 0.467654, acc 0.8\n",
      "2018-04-21T23:49:14.600764: step 670, loss 0.299966, acc 0.96\n",
      "2018-04-21T23:49:14.647083: step 671, loss 0.467449, acc 0.86\n",
      "2018-04-21T23:49:14.693297: step 672, loss 0.360982, acc 0.92\n",
      "2018-04-21T23:49:14.739583: step 673, loss 0.716479, acc 0.78\n",
      "2018-04-21T23:49:14.789810: step 674, loss 0.349166, acc 0.9\n",
      "2018-04-21T23:49:14.836174: step 675, loss 0.464816, acc 0.76\n",
      "2018-04-21T23:49:14.881795: step 676, loss 0.576228, acc 0.66\n",
      "2018-04-21T23:49:14.928483: step 677, loss 0.485249, acc 0.76\n",
      "2018-04-21T23:49:14.974358: step 678, loss 0.395326, acc 0.78\n",
      "2018-04-21T23:49:15.025866: step 679, loss 0.441599, acc 0.82\n",
      "2018-04-21T23:49:15.072897: step 680, loss 0.541549, acc 0.74\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:15.755424: step 680, loss 0.844378, acc 0.519259, rec 0.985119, pre 0.508839, f1 0.671059\n",
      "\n",
      "2018-04-21T23:49:15.799584: step 681, loss 0.798659, acc 0.68\n",
      "2018-04-21T23:49:15.844387: step 682, loss 0.444291, acc 0.76\n",
      "2018-04-21T23:49:15.889311: step 683, loss 0.441514, acc 0.76\n",
      "2018-04-21T23:49:15.934527: step 684, loss 0.379369, acc 0.86\n",
      "2018-04-21T23:49:15.983052: step 685, loss 0.458493, acc 0.94\n",
      "2018-04-21T23:49:16.028584: step 686, loss 0.449317, acc 0.82\n",
      "2018-04-21T23:49:16.073833: step 687, loss 0.583532, acc 0.82\n",
      "2018-04-21T23:49:16.119563: step 688, loss 0.486183, acc 0.76\n",
      "2018-04-21T23:49:16.165718: step 689, loss 0.588635, acc 0.76\n",
      "2018-04-21T23:49:16.214332: step 690, loss 0.320517, acc 0.92\n",
      "2018-04-21T23:49:16.260080: step 691, loss 0.29277, acc 0.94\n",
      "2018-04-21T23:49:16.304980: step 692, loss 0.37548, acc 0.88\n",
      "2018-04-21T23:49:16.349463: step 693, loss 0.404985, acc 0.84\n",
      "2018-04-21T23:49:16.393994: step 694, loss 0.4221, acc 0.72\n",
      "2018-04-21T23:49:16.443044: step 695, loss 0.416609, acc 0.82\n",
      "2018-04-21T23:49:16.489595: step 696, loss 0.591159, acc 0.86\n",
      "2018-04-21T23:49:16.534343: step 697, loss 0.527638, acc 0.84\n",
      "2018-04-21T23:49:16.579244: step 698, loss 0.401702, acc 0.86\n",
      "2018-04-21T23:49:16.624074: step 699, loss 0.581825, acc 0.78\n",
      "2018-04-21T23:49:16.675900: step 700, loss 0.331602, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:17.356977: step 700, loss 0.511287, acc 0.725926, rec 0.93378, pre 0.658447, f1 0.772308\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-700\n",
      "\n",
      "2018-04-21T23:49:17.449110: step 701, loss 0.533448, acc 0.8\n",
      "2018-04-21T23:49:17.497064: step 702, loss 0.466807, acc 0.86\n",
      "2018-04-21T23:49:17.542672: step 703, loss 0.390686, acc 0.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:49:17.591692: step 704, loss 0.386107, acc 0.8\n",
      "2018-04-21T23:49:17.636685: step 705, loss 0.610321, acc 0.86\n",
      "2018-04-21T23:49:17.682951: step 706, loss 0.327213, acc 0.84\n",
      "2018-04-21T23:49:17.728121: step 707, loss 0.427435, acc 0.8\n",
      "2018-04-21T23:49:17.773186: step 708, loss 0.472918, acc 0.78\n",
      "2018-04-21T23:49:17.822098: step 709, loss 0.684402, acc 0.76\n",
      "2018-04-21T23:49:17.866903: step 710, loss 0.398907, acc 0.86\n",
      "2018-04-21T23:49:17.911288: step 711, loss 0.33096, acc 0.9\n",
      "2018-04-21T23:49:17.955973: step 712, loss 0.383482, acc 0.9\n",
      "2018-04-21T23:49:18.000911: step 713, loss 0.708727, acc 0.78\n",
      "2018-04-21T23:49:18.050381: step 714, loss 0.292303, acc 0.9\n",
      "2018-04-21T23:49:18.095722: step 715, loss 0.525218, acc 0.8\n",
      "2018-04-21T23:49:18.140566: step 716, loss 0.445814, acc 0.76\n",
      "2018-04-21T23:49:18.184503: step 717, loss 0.553541, acc 0.7\n",
      "2018-04-21T23:49:18.229016: step 718, loss 0.440618, acc 0.82\n",
      "2018-04-21T23:49:18.277789: step 719, loss 0.395121, acc 0.82\n",
      "2018-04-21T23:49:18.322801: step 720, loss 0.45743, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:19.003975: step 720, loss 0.79182, acc 0.528518, rec 0.984375, pre 0.513786, f1 0.675172\n",
      "\n",
      "2018-04-21T23:49:19.048500: step 721, loss 0.444689, acc 0.86\n",
      "2018-04-21T23:49:19.093230: step 722, loss 0.43977, acc 0.78\n",
      "2018-04-21T23:49:19.138902: step 723, loss 0.544887, acc 0.76\n",
      "2018-04-21T23:49:19.182984: step 724, loss 0.466699, acc 0.82\n",
      "2018-04-21T23:49:19.230322: step 725, loss 0.562585, acc 0.86\n",
      "2018-04-21T23:49:19.274641: step 726, loss 0.474223, acc 0.74\n",
      "2018-04-21T23:49:19.320972: step 727, loss 0.444046, acc 0.86\n",
      "2018-04-21T23:49:19.366088: step 728, loss 0.306327, acc 0.92\n",
      "2018-04-21T23:49:19.411029: step 729, loss 0.457214, acc 0.74\n",
      "2018-04-21T23:49:19.459578: step 730, loss 0.39862, acc 0.84\n",
      "2018-04-21T23:49:19.505236: step 731, loss 0.389258, acc 0.82\n",
      "2018-04-21T23:49:19.550228: step 732, loss 0.504591, acc 0.86\n",
      "2018-04-21T23:49:19.595245: step 733, loss 0.712021, acc 0.84\n",
      "2018-04-21T23:49:19.640949: step 734, loss 0.372663, acc 0.8\n",
      "2018-04-21T23:49:19.691339: step 735, loss 0.399028, acc 0.84\n",
      "2018-04-21T23:49:19.753761: step 736, loss 0.691224, acc 0.86\n",
      "2018-04-21T23:49:19.813479: step 737, loss 0.475253, acc 0.7\n",
      "2018-04-21T23:49:19.857203: step 738, loss 0.493584, acc 0.74\n",
      "2018-04-21T23:49:19.903107: step 739, loss 0.386729, acc 0.82\n",
      "2018-04-21T23:49:19.946337: step 740, loss 0.45154, acc 0.8\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:20.583026: step 740, loss 0.519482, acc 0.702963, rec 0.943452, pre 0.635908, f1 0.759736\n",
      "\n",
      "2018-04-21T23:49:20.627123: step 741, loss 0.385397, acc 0.86\n",
      "2018-04-21T23:49:20.670672: step 742, loss 0.620067, acc 0.76\n",
      "2018-04-21T23:49:20.714512: step 743, loss 0.313767, acc 0.92\n",
      "2018-04-21T23:49:20.758409: step 744, loss 0.415836, acc 0.84\n",
      "2018-04-21T23:49:20.818821: step 745, loss 0.317393, acc 0.86\n",
      "2018-04-21T23:49:20.861987: step 746, loss 0.38181, acc 0.84\n",
      "2018-04-21T23:49:20.905761: step 747, loss 0.3106, acc 0.9\n",
      "2018-04-21T23:49:20.951817: step 748, loss 0.556717, acc 0.8\n",
      "2018-04-21T23:49:20.995936: step 749, loss 0.350577, acc 0.84\n",
      "2018-04-21T23:49:21.042991: step 750, loss 0.458724, acc 0.82\n",
      "2018-04-21T23:49:21.086639: step 751, loss 0.679751, acc 0.84\n",
      "2018-04-21T23:49:21.130465: step 752, loss 0.386941, acc 0.86\n",
      "2018-04-21T23:49:21.174343: step 753, loss 0.661548, acc 0.64\n",
      "2018-04-21T23:49:21.218471: step 754, loss 0.493352, acc 0.74\n",
      "2018-04-21T23:49:21.265554: step 755, loss 0.640186, acc 0.7\n",
      "2018-04-21T23:49:21.308660: step 756, loss 0.463681, acc 0.78\n",
      "2018-04-21T23:49:21.352791: step 757, loss 0.664884, acc 0.66\n",
      "2018-04-21T23:49:21.396257: step 758, loss 0.405675, acc 0.82\n",
      "2018-04-21T23:49:21.440296: step 759, loss 0.35081, acc 0.86\n",
      "2018-04-21T23:49:21.486697: step 760, loss 0.639758, acc 0.74\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:22.183910: step 760, loss 0.543523, acc 0.667037, rec 0.957589, pre 0.604509, f1 0.741146\n",
      "\n",
      "2018-04-21T23:49:22.227604: step 761, loss 0.534471, acc 0.86\n",
      "2018-04-21T23:49:22.271316: step 762, loss 0.398897, acc 0.86\n",
      "2018-04-21T23:49:22.315243: step 763, loss 0.376106, acc 0.86\n",
      "2018-04-21T23:49:22.362071: step 764, loss 0.319016, acc 0.9\n",
      "2018-04-21T23:49:22.440364: step 765, loss 0.321319, acc 0.9\n",
      "2018-04-21T23:49:22.514468: step 766, loss 0.355943, acc 0.86\n",
      "2018-04-21T23:49:22.586647: step 767, loss 0.380236, acc 0.92\n",
      "2018-04-21T23:49:22.663468: step 768, loss 0.404814, acc 0.82\n",
      "2018-04-21T23:49:22.713907: step 769, loss 0.770735, acc 0.74\n",
      "2018-04-21T23:49:22.757424: step 770, loss 0.464752, acc 0.74\n",
      "2018-04-21T23:49:22.802950: step 771, loss 0.410989, acc 0.84\n",
      "2018-04-21T23:49:22.846696: step 772, loss 0.48887, acc 0.88\n",
      "2018-04-21T23:49:22.893057: step 773, loss 0.414827, acc 0.88\n",
      "2018-04-21T23:49:22.936510: step 774, loss 0.289265, acc 0.92\n",
      "2018-04-21T23:49:22.979692: step 775, loss 0.348246, acc 0.92\n",
      "2018-04-21T23:49:23.023339: step 776, loss 0.360161, acc 0.94\n",
      "2018-04-21T23:49:23.066670: step 777, loss 0.553222, acc 0.82\n",
      "2018-04-21T23:49:23.113023: step 778, loss 0.386008, acc 0.84\n",
      "2018-04-21T23:49:23.156450: step 779, loss 0.395939, acc 0.86\n",
      "2018-04-21T23:49:23.200388: step 780, loss 0.535273, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:23.832689: step 780, loss 0.774752, acc 0.532222, rec 0.983631, pre 0.515802, f1 0.676734\n",
      "\n",
      "2018-04-21T23:49:23.876233: step 781, loss 0.344869, acc 0.86\n",
      "2018-04-21T23:49:23.920009: step 782, loss 0.501243, acc 0.84\n",
      "2018-04-21T23:49:23.963305: step 783, loss 0.352921, acc 0.84\n",
      "2018-04-21T23:49:24.006812: step 784, loss 0.555669, acc 0.88\n",
      "2018-04-21T23:49:24.053391: step 785, loss 0.313072, acc 0.88\n",
      "2018-04-21T23:49:24.097677: step 786, loss 0.339082, acc 0.86\n",
      "2018-04-21T23:49:24.141952: step 787, loss 0.404711, acc 0.82\n",
      "2018-04-21T23:49:24.185714: step 788, loss 0.505807, acc 0.8\n",
      "2018-04-21T23:49:24.229696: step 789, loss 0.775963, acc 0.7\n",
      "2018-04-21T23:49:24.277452: step 790, loss 0.460212, acc 0.76\n",
      "2018-04-21T23:49:24.321594: step 791, loss 0.521453, acc 0.72\n",
      "2018-04-21T23:49:24.365791: step 792, loss 0.365606, acc 0.8\n",
      "2018-04-21T23:49:24.410066: step 793, loss 0.439724, acc 0.78\n",
      "2018-04-21T23:49:24.456414: step 794, loss 0.400412, acc 0.8\n",
      "2018-04-21T23:49:24.503035: step 795, loss 0.600867, acc 0.84\n",
      "2018-04-21T23:49:24.546803: step 796, loss 0.412046, acc 0.8\n",
      "2018-04-21T23:49:24.590934: step 797, loss 0.376695, acc 0.88\n",
      "2018-04-21T23:49:24.635267: step 798, loss 0.618318, acc 0.76\n",
      "2018-04-21T23:49:24.687346: step 799, loss 0.386972, acc 0.86\n",
      "2018-04-21T23:49:24.739038: step 800, loss 0.44727, acc 0.78\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:25.510321: step 800, loss 0.638689, acc 0.592963, rec 0.186012, pre 0.980392, f1 0.312695\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-800\n",
      "\n",
      "2018-04-21T23:49:25.596495: step 801, loss 0.727674, acc 0.66\n",
      "2018-04-21T23:49:25.668777: step 802, loss 0.407867, acc 0.76\n",
      "2018-04-21T23:49:25.739434: step 803, loss 0.288022, acc 0.9\n",
      "2018-04-21T23:49:25.811499: step 804, loss 0.440624, acc 0.8\n",
      "2018-04-21T23:49:25.883920: step 805, loss 0.500686, acc 0.84\n",
      "2018-04-21T23:49:25.956395: step 806, loss 0.510445, acc 0.74\n",
      "2018-04-21T23:49:26.024602: step 807, loss 0.390646, acc 0.78\n",
      "2018-04-21T23:49:26.101547: step 808, loss 0.667269, acc 0.74\n",
      "2018-04-21T23:49:26.180189: step 809, loss 0.312397, acc 0.84\n",
      "2018-04-21T23:49:26.255250: step 810, loss 0.341484, acc 0.88\n",
      "2018-04-21T23:49:26.331910: step 811, loss 0.283177, acc 0.94\n",
      "2018-04-21T23:49:26.411222: step 812, loss 0.435137, acc 0.82\n",
      "2018-04-21T23:49:26.487450: step 813, loss 0.313715, acc 0.9\n",
      "2018-04-21T23:49:26.562720: step 814, loss 0.312276, acc 0.9\n",
      "2018-04-21T23:49:26.632871: step 815, loss 0.399811, acc 0.78\n",
      "2018-04-21T23:49:26.680930: step 816, loss 0.413451, acc 0.9\n",
      "2018-04-21T23:49:26.726841: step 817, loss 0.587535, acc 0.9\n",
      "2018-04-21T23:49:26.771859: step 818, loss 0.706234, acc 0.68\n",
      "2018-04-21T23:49:26.816587: step 819, loss 0.517125, acc 0.76\n",
      "2018-04-21T23:49:26.865861: step 820, loss 0.52494, acc 0.7\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:27.547581: step 820, loss 0.475032, acc 0.851111, rec 0.879464, pre 0.831224, f1 0.854664\n",
      "\n",
      "2018-04-21T23:49:27.591043: step 821, loss 0.461875, acc 0.84\n",
      "2018-04-21T23:49:27.634746: step 822, loss 0.809556, acc 0.84\n",
      "2018-04-21T23:49:27.680260: step 823, loss 0.340907, acc 0.88\n",
      "2018-04-21T23:49:27.724738: step 824, loss 0.593263, acc 0.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:49:27.772973: step 825, loss 0.283729, acc 0.86\n",
      "2018-04-21T23:49:27.818915: step 826, loss 0.470675, acc 0.84\n",
      "2018-04-21T23:49:27.863756: step 827, loss 0.429601, acc 0.96\n",
      "2018-04-21T23:49:27.908246: step 828, loss 0.425635, acc 0.8\n",
      "2018-04-21T23:49:27.953953: step 829, loss 0.336971, acc 0.9\n",
      "2018-04-21T23:49:28.002539: step 830, loss 0.378607, acc 0.84\n",
      "2018-04-21T23:49:28.047540: step 831, loss 0.575851, acc 0.84\n",
      "2018-04-21T23:49:28.091862: step 832, loss 0.403779, acc 0.88\n",
      "2018-04-21T23:49:28.137147: step 833, loss 0.337965, acc 0.9\n",
      "2018-04-21T23:49:28.181690: step 834, loss 0.393872, acc 0.82\n",
      "2018-04-21T23:49:28.229982: step 835, loss 0.516342, acc 0.76\n",
      "2018-04-21T23:49:28.274400: step 836, loss 0.638897, acc 0.64\n",
      "2018-04-21T23:49:28.318531: step 837, loss 0.40948, acc 0.82\n",
      "2018-04-21T23:49:28.362676: step 838, loss 0.23271, acc 0.96\n",
      "2018-04-21T23:49:28.406696: step 839, loss 0.398093, acc 0.84\n",
      "2018-04-21T23:49:28.455282: step 840, loss 0.340672, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:29.136557: step 840, loss 0.470138, acc 0.852963, rec 0.879464, pre 0.834157, f1 0.856212\n",
      "\n",
      "2018-04-21T23:49:29.180913: step 841, loss 0.463887, acc 0.86\n",
      "2018-04-21T23:49:29.225286: step 842, loss 0.479586, acc 0.8\n",
      "2018-04-21T23:49:29.268738: step 843, loss 0.401443, acc 0.86\n",
      "2018-04-21T23:49:29.313803: step 844, loss 0.523112, acc 0.86\n",
      "2018-04-21T23:49:29.361805: step 845, loss 0.469249, acc 0.78\n",
      "2018-04-21T23:49:29.405956: step 846, loss 0.61581, acc 0.7\n",
      "2018-04-21T23:49:29.449963: step 847, loss 0.666068, acc 0.68\n",
      "2018-04-21T23:49:29.495721: step 848, loss 0.497115, acc 0.72\n",
      "2018-04-21T23:49:29.541405: step 849, loss 0.398501, acc 0.8\n",
      "2018-04-21T23:49:29.589538: step 850, loss 0.322097, acc 0.92\n",
      "2018-04-21T23:49:29.635173: step 851, loss 0.642745, acc 0.8\n",
      "2018-04-21T23:49:29.680568: step 852, loss 0.537259, acc 0.86\n",
      "2018-04-21T23:49:29.726266: step 853, loss 0.356495, acc 0.88\n",
      "2018-04-21T23:49:29.771348: step 854, loss 0.42071, acc 0.8\n",
      "2018-04-21T23:49:29.820054: step 855, loss 0.638379, acc 0.82\n",
      "2018-04-21T23:49:29.864656: step 856, loss 0.331059, acc 0.92\n",
      "2018-04-21T23:49:29.909329: step 857, loss 0.308116, acc 0.94\n",
      "2018-04-21T23:49:29.954244: step 858, loss 0.354202, acc 0.84\n",
      "2018-04-21T23:49:29.998198: step 859, loss 0.653881, acc 0.76\n",
      "2018-04-21T23:49:30.047905: step 860, loss 0.540644, acc 0.7\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:30.733186: step 860, loss 0.684078, acc 0.568148, rec 0.979911, pre 0.536238, f1 0.693158\n",
      "\n",
      "2018-04-21T23:49:30.777510: step 861, loss 0.289775, acc 0.86\n",
      "2018-04-21T23:49:30.821453: step 862, loss 0.37973, acc 0.86\n",
      "2018-04-21T23:49:30.865344: step 863, loss 0.538958, acc 0.86\n",
      "2018-04-21T23:49:30.909604: step 864, loss 0.4907, acc 0.88\n",
      "2018-04-21T23:49:30.957960: step 865, loss 0.407932, acc 0.86\n",
      "2018-04-21T23:49:31.002405: step 866, loss 0.395168, acc 0.86\n",
      "2018-04-21T23:49:31.047539: step 867, loss 0.309442, acc 0.86\n",
      "2018-04-21T23:49:31.091741: step 868, loss 0.431249, acc 0.76\n",
      "2018-04-21T23:49:31.136109: step 869, loss 0.637648, acc 0.82\n",
      "2018-04-21T23:49:31.191877: step 870, loss 0.311604, acc 0.88\n",
      "2018-04-21T23:49:31.252369: step 871, loss 0.535407, acc 0.82\n",
      "2018-04-21T23:49:31.312013: step 872, loss 0.39444, acc 0.84\n",
      "2018-04-21T23:49:31.369667: step 873, loss 0.393726, acc 0.84\n",
      "2018-04-21T23:49:31.416860: step 874, loss 0.511574, acc 0.74\n",
      "2018-04-21T23:49:31.461606: step 875, loss 0.314513, acc 0.92\n",
      "2018-04-21T23:49:31.506233: step 876, loss 0.680197, acc 0.88\n",
      "2018-04-21T23:49:31.550325: step 877, loss 0.384158, acc 0.8\n",
      "2018-04-21T23:49:31.595392: step 878, loss 0.37209, acc 0.84\n",
      "2018-04-21T23:49:31.644074: step 879, loss 0.332024, acc 0.88\n",
      "2018-04-21T23:49:31.689952: step 880, loss 0.330258, acc 0.8\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:32.372856: step 880, loss 0.475205, acc 0.822593, rec 0.90997, pre 0.773561, f1 0.836239\n",
      "\n",
      "2018-04-21T23:49:32.416288: step 881, loss 0.311327, acc 0.96\n",
      "2018-04-21T23:49:32.460185: step 882, loss 0.376229, acc 0.84\n",
      "2018-04-21T23:49:32.506237: step 883, loss 0.395663, acc 0.78\n",
      "2018-04-21T23:49:32.551658: step 884, loss 0.294235, acc 0.88\n",
      "2018-04-21T23:49:32.600867: step 885, loss 0.628451, acc 0.84\n",
      "2018-04-21T23:49:32.645897: step 886, loss 0.352695, acc 0.84\n",
      "2018-04-21T23:49:32.692061: step 887, loss 0.395127, acc 0.8\n",
      "2018-04-21T23:49:32.736019: step 888, loss 0.439766, acc 0.84\n",
      "2018-04-21T23:49:32.781449: step 889, loss 0.429431, acc 0.86\n",
      "2018-04-21T23:49:32.829957: step 890, loss 0.472023, acc 0.72\n",
      "2018-04-21T23:49:32.876005: step 891, loss 0.489273, acc 0.88\n",
      "2018-04-21T23:49:32.921744: step 892, loss 0.36168, acc 0.82\n",
      "2018-04-21T23:49:32.965560: step 893, loss 0.426341, acc 0.86\n",
      "2018-04-21T23:49:33.009314: step 894, loss 0.501255, acc 0.82\n",
      "2018-04-21T23:49:33.059058: step 895, loss 0.35226, acc 0.9\n",
      "2018-04-21T23:49:33.104828: step 896, loss 0.338961, acc 0.88\n",
      "2018-04-21T23:49:33.150169: step 897, loss 0.770117, acc 0.82\n",
      "2018-04-21T23:49:33.194093: step 898, loss 0.462598, acc 0.8\n",
      "2018-04-21T23:49:33.238913: step 899, loss 0.407541, acc 0.78\n",
      "2018-04-21T23:49:33.286585: step 900, loss 0.345923, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:33.968640: step 900, loss 0.605012, acc 0.611481, rec 0.973214, pre 0.56355, f1 0.713779\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-900\n",
      "\n",
      "2018-04-21T23:49:34.063512: step 901, loss 0.341137, acc 0.82\n",
      "2018-04-21T23:49:34.109169: step 902, loss 0.317198, acc 0.88\n",
      "2018-04-21T23:49:34.154168: step 903, loss 0.468448, acc 0.82\n",
      "2018-04-21T23:49:34.203449: step 904, loss 0.381461, acc 0.84\n",
      "2018-04-21T23:49:34.248257: step 905, loss 0.493804, acc 0.82\n",
      "2018-04-21T23:49:34.292648: step 906, loss 0.35562, acc 0.84\n",
      "2018-04-21T23:49:34.336741: step 907, loss 0.308688, acc 0.9\n",
      "2018-04-21T23:49:34.381781: step 908, loss 0.422733, acc 0.86\n",
      "2018-04-21T23:49:34.430816: step 909, loss 0.774633, acc 0.66\n",
      "2018-04-21T23:49:34.476401: step 910, loss 0.726289, acc 0.78\n",
      "2018-04-21T23:49:34.523369: step 911, loss 0.514995, acc 0.7\n",
      "2018-04-21T23:49:34.568920: step 912, loss 0.648426, acc 0.8\n",
      "2018-04-21T23:49:34.614499: step 913, loss 0.498654, acc 0.76\n",
      "2018-04-21T23:49:34.664034: step 914, loss 0.563149, acc 0.7\n",
      "2018-04-21T23:49:34.712428: step 915, loss 0.481257, acc 0.82\n",
      "2018-04-21T23:49:34.759494: step 916, loss 0.383347, acc 0.82\n",
      "2018-04-21T23:49:34.805345: step 917, loss 0.347739, acc 0.9\n",
      "2018-04-21T23:49:34.853237: step 918, loss 0.481709, acc 0.82\n",
      "2018-04-21T23:49:34.902441: step 919, loss 0.427699, acc 0.82\n",
      "2018-04-21T23:49:34.947080: step 920, loss 0.522574, acc 0.66\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:35.598971: step 920, loss 1.11281, acc 0.503333, rec 0.990327, pre 0.500564, f1 0.665001\n",
      "\n",
      "2018-04-21T23:49:35.644930: step 921, loss 0.572335, acc 0.74\n",
      "2018-04-21T23:49:35.691013: step 922, loss 0.565851, acc 0.86\n",
      "2018-04-21T23:49:35.736738: step 923, loss 0.398344, acc 0.84\n",
      "2018-04-21T23:49:35.782606: step 924, loss 0.664408, acc 0.8\n",
      "2018-04-21T23:49:35.832745: step 925, loss 0.532626, acc 0.8\n",
      "2018-04-21T23:49:35.879511: step 926, loss 0.309558, acc 0.86\n",
      "2018-04-21T23:49:35.924474: step 927, loss 0.462291, acc 0.86\n",
      "2018-04-21T23:49:35.968628: step 928, loss 0.41026, acc 0.8\n",
      "2018-04-21T23:49:36.013741: step 929, loss 0.371441, acc 0.84\n",
      "2018-04-21T23:49:36.065225: step 930, loss 0.357965, acc 0.9\n",
      "2018-04-21T23:49:36.111078: step 931, loss 0.431334, acc 0.84\n",
      "2018-04-21T23:49:36.156903: step 932, loss 0.419543, acc 0.76\n",
      "2018-04-21T23:49:36.202429: step 933, loss 0.31347, acc 0.84\n",
      "2018-04-21T23:49:36.247784: step 934, loss 0.417325, acc 0.78\n",
      "2018-04-21T23:49:36.297860: step 935, loss 0.399358, acc 0.84\n",
      "2018-04-21T23:49:36.343787: step 936, loss 0.364487, acc 0.84\n",
      "2018-04-21T23:49:36.388527: step 937, loss 0.331016, acc 0.86\n",
      "2018-04-21T23:49:36.432430: step 938, loss 0.391923, acc 0.94\n",
      "2018-04-21T23:49:36.476530: step 939, loss 0.361098, acc 0.84\n",
      "2018-04-21T23:49:36.525771: step 940, loss 0.562959, acc 0.7\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:37.212086: step 940, loss 0.464002, acc 0.85963, rec 0.87128, pre 0.850399, f1 0.860713\n",
      "\n",
      "2018-04-21T23:49:37.256230: step 941, loss 0.37691, acc 0.82\n",
      "2018-04-21T23:49:37.302132: step 942, loss 0.322785, acc 0.86\n",
      "2018-04-21T23:49:37.347884: step 943, loss 0.578099, acc 0.78\n",
      "2018-04-21T23:49:37.393498: step 944, loss 0.623937, acc 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:49:37.443766: step 945, loss 0.325229, acc 0.9\n",
      "2018-04-21T23:49:37.490347: step 946, loss 0.2586, acc 0.92\n",
      "2018-04-21T23:49:37.536023: step 947, loss 0.896796, acc 0.82\n",
      "2018-04-21T23:49:37.582453: step 948, loss 0.22079, acc 0.96\n",
      "2018-04-21T23:49:37.628656: step 949, loss 0.433009, acc 0.88\n",
      "2018-04-21T23:49:37.680149: step 950, loss 0.303449, acc 0.9\n",
      "2018-04-21T23:49:37.728247: step 951, loss 0.488111, acc 0.78\n",
      "2018-04-21T23:49:37.776649: step 952, loss 0.430112, acc 0.8\n",
      "2018-04-21T23:49:37.830962: step 953, loss 0.414334, acc 0.86\n",
      "2018-04-21T23:49:37.877254: step 954, loss 0.469119, acc 0.74\n",
      "2018-04-21T23:49:37.927502: step 955, loss 0.545972, acc 0.76\n",
      "2018-04-21T23:49:37.973276: step 956, loss 0.453006, acc 0.84\n",
      "2018-04-21T23:49:38.018266: step 957, loss 0.302606, acc 0.94\n",
      "2018-04-21T23:49:38.064555: step 958, loss 0.567201, acc 0.86\n",
      "2018-04-21T23:49:38.113992: step 959, loss 0.612822, acc 0.8\n",
      "2018-04-21T23:49:38.164165: step 960, loss 0.329629, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:38.889710: step 960, loss 0.66336, acc 0.57, rec 0.982887, pre 0.53721, f1 0.694715\n",
      "\n",
      "2018-04-21T23:49:38.934867: step 961, loss 0.374546, acc 0.8\n",
      "2018-04-21T23:49:38.981324: step 962, loss 0.450078, acc 0.76\n",
      "2018-04-21T23:49:39.029064: step 963, loss 0.566758, acc 0.72\n",
      "2018-04-21T23:49:39.075491: step 964, loss 0.336392, acc 0.88\n",
      "2018-04-21T23:49:39.125600: step 965, loss 0.364964, acc 0.86\n",
      "2018-04-21T23:49:39.171040: step 966, loss 0.564923, acc 0.8\n",
      "2018-04-21T23:49:39.216202: step 967, loss 0.373677, acc 0.88\n",
      "2018-04-21T23:49:39.261247: step 968, loss 0.299019, acc 0.94\n",
      "2018-04-21T23:49:39.306370: step 969, loss 0.386609, acc 0.86\n",
      "2018-04-21T23:49:39.356225: step 970, loss 0.312762, acc 0.88\n",
      "2018-04-21T23:49:39.401705: step 971, loss 0.342589, acc 0.84\n",
      "2018-04-21T23:49:39.447226: step 972, loss 0.651481, acc 0.8\n",
      "2018-04-21T23:49:39.496007: step 973, loss 0.324942, acc 0.88\n",
      "2018-04-21T23:49:39.542427: step 974, loss 0.309124, acc 0.9\n",
      "2018-04-21T23:49:39.593977: step 975, loss 0.403717, acc 0.92\n",
      "2018-04-21T23:49:39.640439: step 976, loss 0.465441, acc 0.84\n",
      "2018-04-21T23:49:39.689329: step 977, loss 0.423252, acc 0.82\n",
      "2018-04-21T23:49:39.736421: step 978, loss 0.351431, acc 0.86\n",
      "2018-04-21T23:49:39.783539: step 979, loss 0.377826, acc 0.82\n",
      "2018-04-21T23:49:39.833784: step 980, loss 0.237132, acc 0.96\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:40.520622: step 980, loss 0.510393, acc 0.706667, rec 0.950149, pre 0.637862, f1 0.763299\n",
      "\n",
      "2018-04-21T23:49:40.566120: step 981, loss 0.679008, acc 0.74\n",
      "2018-04-21T23:49:40.613061: step 982, loss 0.27341, acc 0.86\n",
      "2018-04-21T23:49:40.660683: step 983, loss 0.314672, acc 0.88\n",
      "2018-04-21T23:49:40.707464: step 984, loss 0.469355, acc 0.88\n",
      "2018-04-21T23:49:40.757883: step 985, loss 0.797903, acc 0.82\n",
      "2018-04-21T23:49:40.805109: step 986, loss 0.744162, acc 0.76\n",
      "2018-04-21T23:49:40.851651: step 987, loss 0.337145, acc 0.88\n",
      "2018-04-21T23:49:40.897908: step 988, loss 0.412319, acc 0.76\n",
      "2018-04-21T23:49:40.944627: step 989, loss 0.605683, acc 0.88\n",
      "2018-04-21T23:49:40.994633: step 990, loss 0.463057, acc 0.82\n",
      "2018-04-21T23:49:41.041504: step 991, loss 0.383909, acc 0.84\n",
      "2018-04-21T23:49:41.087848: step 992, loss 0.325272, acc 0.86\n",
      "2018-04-21T23:49:41.134264: step 993, loss 0.416919, acc 0.84\n",
      "2018-04-21T23:49:41.180473: step 994, loss 0.248625, acc 0.9\n",
      "2018-04-21T23:49:41.229862: step 995, loss 0.314301, acc 0.84\n",
      "2018-04-21T23:49:41.275125: step 996, loss 0.360326, acc 0.88\n",
      "2018-04-21T23:49:41.320616: step 997, loss 0.373307, acc 0.84\n",
      "2018-04-21T23:49:41.366794: step 998, loss 0.409107, acc 0.86\n",
      "2018-04-21T23:49:41.412176: step 999, loss 0.449374, acc 0.86\n",
      "2018-04-21T23:49:41.462441: step 1000, loss 0.427317, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:42.152175: step 1000, loss 0.598044, acc 0.614815, rec 0.973214, pre 0.565744, f1 0.715536\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-1000\n",
      "\n",
      "2018-04-21T23:49:42.243303: step 1001, loss 0.501193, acc 0.86\n",
      "2018-04-21T23:49:42.288102: step 1002, loss 0.440812, acc 0.88\n",
      "2018-04-21T23:49:42.333982: step 1003, loss 0.647348, acc 0.88\n",
      "2018-04-21T23:49:42.383217: step 1004, loss 0.42682, acc 0.84\n",
      "2018-04-21T23:49:42.429750: step 1005, loss 0.363495, acc 0.82\n",
      "2018-04-21T23:49:42.474582: step 1006, loss 0.378342, acc 0.88\n",
      "2018-04-21T23:49:42.520627: step 1007, loss 0.312617, acc 0.98\n",
      "2018-04-21T23:49:42.565111: step 1008, loss 0.360751, acc 0.88\n",
      "2018-04-21T23:49:42.613474: step 1009, loss 0.242579, acc 0.9\n",
      "2018-04-21T23:49:42.658933: step 1010, loss 0.419877, acc 0.86\n",
      "2018-04-21T23:49:42.704283: step 1011, loss 0.762389, acc 0.8\n",
      "2018-04-21T23:49:42.748263: step 1012, loss 0.290555, acc 0.96\n",
      "2018-04-21T23:49:42.793297: step 1013, loss 0.345286, acc 0.84\n",
      "2018-04-21T23:49:42.841625: step 1014, loss 0.377241, acc 0.78\n",
      "2018-04-21T23:49:42.887375: step 1015, loss 0.441124, acc 0.8\n",
      "2018-04-21T23:49:42.933754: step 1016, loss 0.347807, acc 0.9\n",
      "2018-04-21T23:49:42.979882: step 1017, loss 0.421982, acc 0.82\n",
      "2018-04-21T23:49:43.025978: step 1018, loss 0.512223, acc 0.76\n",
      "2018-04-21T23:49:43.075574: step 1019, loss 0.347366, acc 0.86\n",
      "2018-04-21T23:49:43.120833: step 1020, loss 0.343168, acc 0.78\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:43.802308: step 1020, loss 0.483234, acc 0.76037, rec 0.936756, pre 0.691378, f1 0.795577\n",
      "\n",
      "2018-04-21T23:49:43.847321: step 1021, loss 0.403196, acc 0.86\n",
      "2018-04-21T23:49:43.896035: step 1022, loss 0.484929, acc 0.8\n",
      "2018-04-21T23:49:43.941249: step 1023, loss 0.46975, acc 0.82\n",
      "2018-04-21T23:49:43.986886: step 1024, loss 0.395205, acc 0.8\n",
      "2018-04-21T23:49:44.037976: step 1025, loss 0.384084, acc 0.86\n",
      "2018-04-21T23:49:44.083716: step 1026, loss 0.5035, acc 0.76\n",
      "2018-04-21T23:49:44.129176: step 1027, loss 0.421961, acc 0.82\n",
      "2018-04-21T23:49:44.173599: step 1028, loss 0.498226, acc 0.72\n",
      "2018-04-21T23:49:44.217751: step 1029, loss 0.429958, acc 0.76\n",
      "2018-04-21T23:49:44.265865: step 1030, loss 0.363729, acc 0.86\n",
      "2018-04-21T23:49:44.311297: step 1031, loss 0.28862, acc 0.86\n",
      "2018-04-21T23:49:44.356301: step 1032, loss 0.444357, acc 0.74\n",
      "2018-04-21T23:49:44.401058: step 1033, loss 0.595736, acc 0.72\n",
      "2018-04-21T23:49:44.446625: step 1034, loss 0.714459, acc 0.72\n",
      "2018-04-21T23:49:44.496808: step 1035, loss 0.400426, acc 0.86\n",
      "2018-04-21T23:49:44.543151: step 1036, loss 0.539993, acc 0.74\n",
      "2018-04-21T23:49:44.589199: step 1037, loss 0.595798, acc 0.76\n",
      "2018-04-21T23:49:44.633889: step 1038, loss 0.390475, acc 0.82\n",
      "2018-04-21T23:49:44.681280: step 1039, loss 0.425476, acc 0.82\n",
      "2018-04-21T23:49:44.732340: step 1040, loss 0.445073, acc 0.82\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:45.419313: step 1040, loss 0.81992, acc 0.52037, rec 0.986607, pre 0.509412, f1 0.671903\n",
      "\n",
      "2018-04-21T23:49:45.463607: step 1041, loss 0.288923, acc 0.88\n",
      "2018-04-21T23:49:45.510328: step 1042, loss 0.276264, acc 0.92\n",
      "2018-04-21T23:49:45.555291: step 1043, loss 0.305955, acc 0.9\n",
      "2018-04-21T23:49:45.600176: step 1044, loss 0.367584, acc 0.86\n",
      "2018-04-21T23:49:45.652313: step 1045, loss 0.375036, acc 0.82\n",
      "2018-04-21T23:49:45.697118: step 1046, loss 0.616047, acc 0.84\n",
      "2018-04-21T23:49:45.780441: step 1047, loss 0.382041, acc 0.88\n",
      "2018-04-21T23:49:45.870450: step 1048, loss 0.339372, acc 0.96\n",
      "2018-04-21T23:49:45.960442: step 1049, loss 0.341775, acc 0.84\n",
      "2018-04-21T23:49:46.047448: step 1050, loss 0.456591, acc 0.88\n",
      "2018-04-21T23:49:46.144447: step 1051, loss 0.365396, acc 0.86\n",
      "2018-04-21T23:49:46.232440: step 1052, loss 0.449172, acc 0.82\n",
      "2018-04-21T23:49:46.317892: step 1053, loss 0.649794, acc 0.84\n",
      "2018-04-21T23:49:46.412435: step 1054, loss 0.432917, acc 0.76\n",
      "2018-04-21T23:49:46.504433: step 1055, loss 0.413676, acc 0.8\n",
      "2018-04-21T23:49:46.590287: step 1056, loss 0.678481, acc 0.8\n",
      "2018-04-21T23:49:46.682783: step 1057, loss 0.262623, acc 0.96\n",
      "2018-04-21T23:49:46.772706: step 1058, loss 0.603168, acc 0.88\n",
      "2018-04-21T23:49:46.868612: step 1059, loss 0.357127, acc 0.86\n",
      "2018-04-21T23:49:46.965495: step 1060, loss 0.392983, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:48.333540: step 1060, loss 0.539276, acc 0.664074, rec 0.960565, pre 0.601865, f1 0.74004\n",
      "\n",
      "2018-04-21T23:49:48.422694: step 1061, loss 0.326564, acc 0.84\n",
      "2018-04-21T23:49:48.520502: step 1062, loss 0.469267, acc 0.88\n",
      "2018-04-21T23:49:48.613377: step 1063, loss 0.40098, acc 0.86\n",
      "2018-04-21T23:49:48.709118: step 1064, loss 0.426947, acc 0.86\n",
      "2018-04-21T23:49:48.801479: step 1065, loss 0.241214, acc 0.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:49:48.896484: step 1066, loss 0.272417, acc 0.94\n",
      "2018-04-21T23:49:48.992493: step 1067, loss 0.428679, acc 0.74\n",
      "2018-04-21T23:49:49.084490: step 1068, loss 0.652092, acc 0.68\n",
      "2018-04-21T23:49:49.180479: step 1069, loss 0.677601, acc 0.72\n",
      "2018-04-21T23:49:49.272523: step 1070, loss 0.367513, acc 0.86\n",
      "2018-04-21T23:49:49.368494: step 1071, loss 0.28109, acc 0.9\n",
      "2018-04-21T23:49:49.462504: step 1072, loss 0.291109, acc 0.9\n",
      "2018-04-21T23:49:49.560495: step 1073, loss 0.405097, acc 0.84\n",
      "2018-04-21T23:49:49.652633: step 1074, loss 0.296041, acc 0.9\n",
      "2018-04-21T23:49:49.752500: step 1075, loss 0.453205, acc 0.78\n",
      "2018-04-21T23:49:49.844969: step 1076, loss 0.421619, acc 0.76\n",
      "2018-04-21T23:49:49.940511: step 1077, loss 0.450339, acc 0.84\n",
      "2018-04-21T23:49:50.040128: step 1078, loss 0.34522, acc 0.86\n",
      "2018-04-21T23:49:50.140491: step 1079, loss 0.379569, acc 0.8\n",
      "2018-04-21T23:49:50.232488: step 1080, loss 0.545449, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:51.552601: step 1080, loss 0.461446, acc 0.851481, rec 0.752232, pre 0.936979, f1 0.834503\n",
      "\n",
      "2018-04-21T23:49:51.641332: step 1081, loss 0.461958, acc 0.78\n",
      "2018-04-21T23:49:51.731951: step 1082, loss 0.592648, acc 0.68\n",
      "2018-04-21T23:49:51.832451: step 1083, loss 0.504737, acc 0.7\n",
      "2018-04-21T23:49:51.918727: step 1084, loss 0.56242, acc 0.72\n",
      "2018-04-21T23:49:52.012441: step 1085, loss 0.419066, acc 0.76\n",
      "2018-04-21T23:49:52.105253: step 1086, loss 0.383158, acc 0.84\n",
      "2018-04-21T23:49:52.192441: step 1087, loss 0.34736, acc 0.84\n",
      "2018-04-21T23:49:52.280460: step 1088, loss 0.373567, acc 0.86\n",
      "2018-04-21T23:49:52.376470: step 1089, loss 0.33365, acc 0.9\n",
      "2018-04-21T23:49:52.461202: step 1090, loss 0.338958, acc 0.9\n",
      "2018-04-21T23:49:52.552482: step 1091, loss 0.659248, acc 0.84\n",
      "2018-04-21T23:49:52.643856: step 1092, loss 0.544523, acc 0.76\n",
      "2018-04-21T23:49:52.730146: step 1093, loss 0.337966, acc 0.88\n",
      "2018-04-21T23:49:52.824429: step 1094, loss 0.346455, acc 0.8\n",
      "2018-04-21T23:49:52.920439: step 1095, loss 0.327769, acc 0.94\n",
      "2018-04-21T23:49:53.012454: step 1096, loss 0.424046, acc 0.8\n",
      "2018-04-21T23:49:53.098015: step 1097, loss 0.301973, acc 0.88\n",
      "2018-04-21T23:49:53.196450: step 1098, loss 0.383089, acc 0.86\n",
      "2018-04-21T23:49:53.281528: step 1099, loss 0.319911, acc 0.86\n",
      "2018-04-21T23:49:53.372431: step 1100, loss 0.6636, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:54.549789: step 1100, loss 0.459741, acc 0.824074, rec 0.909226, pre 0.775873, f1 0.837273\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-1100\n",
      "\n",
      "2018-04-21T23:49:54.637889: step 1101, loss 0.384292, acc 0.84\n",
      "2018-04-21T23:49:54.682840: step 1102, loss 0.30775, acc 0.84\n",
      "2018-04-21T23:49:54.727038: step 1103, loss 0.223539, acc 0.98\n",
      "2018-04-21T23:49:54.775494: step 1104, loss 0.396426, acc 0.9\n",
      "2018-04-21T23:49:54.819366: step 1105, loss 0.572739, acc 0.88\n",
      "2018-04-21T23:49:54.863376: step 1106, loss 0.341224, acc 0.82\n",
      "2018-04-21T23:49:54.907398: step 1107, loss 0.454027, acc 0.84\n",
      "2018-04-21T23:49:54.952378: step 1108, loss 0.481216, acc 0.9\n",
      "2018-04-21T23:49:54.999078: step 1109, loss 0.375051, acc 0.8\n",
      "2018-04-21T23:49:55.043709: step 1110, loss 0.734237, acc 0.78\n",
      "2018-04-21T23:49:55.087556: step 1111, loss 0.404354, acc 0.88\n",
      "2018-04-21T23:49:55.131895: step 1112, loss 0.399641, acc 0.88\n",
      "2018-04-21T23:49:55.176725: step 1113, loss 0.378829, acc 0.84\n",
      "2018-04-21T23:49:55.223976: step 1114, loss 0.396388, acc 0.84\n",
      "2018-04-21T23:49:55.268141: step 1115, loss 0.435457, acc 0.84\n",
      "2018-04-21T23:49:55.311928: step 1116, loss 0.363506, acc 0.82\n",
      "2018-04-21T23:49:55.355267: step 1117, loss 0.292507, acc 0.92\n",
      "2018-04-21T23:49:55.399386: step 1118, loss 0.33967, acc 0.88\n",
      "2018-04-21T23:49:55.446424: step 1119, loss 0.257422, acc 0.88\n",
      "2018-04-21T23:49:55.489829: step 1120, loss 0.213117, acc 0.96\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:56.122919: step 1120, loss 0.448378, acc 0.849259, rec 0.888393, pre 0.822881, f1 0.854383\n",
      "\n",
      "2018-04-21T23:49:56.166553: step 1121, loss 0.686122, acc 0.8\n",
      "2018-04-21T23:49:56.210658: step 1122, loss 0.241664, acc 0.94\n",
      "2018-04-21T23:49:56.254226: step 1123, loss 0.345326, acc 0.88\n",
      "2018-04-21T23:49:56.297959: step 1124, loss 0.261267, acc 0.92\n",
      "2018-04-21T23:49:56.344962: step 1125, loss 0.507658, acc 0.72\n",
      "2018-04-21T23:49:56.388758: step 1126, loss 0.458505, acc 0.8\n",
      "2018-04-21T23:49:56.432162: step 1127, loss 0.443218, acc 0.82\n",
      "2018-04-21T23:49:56.476628: step 1128, loss 0.396025, acc 0.8\n",
      "2018-04-21T23:49:56.521745: step 1129, loss 0.523488, acc 0.86\n",
      "2018-04-21T23:49:56.569915: step 1130, loss 0.273371, acc 0.92\n",
      "2018-04-21T23:49:56.613450: step 1131, loss 0.268215, acc 0.96\n",
      "2018-04-21T23:49:56.656613: step 1132, loss 0.380167, acc 0.84\n",
      "2018-04-21T23:49:56.700454: step 1133, loss 0.361257, acc 0.84\n",
      "2018-04-21T23:49:56.745353: step 1134, loss 0.488346, acc 0.8\n",
      "2018-04-21T23:49:56.792635: step 1135, loss 0.351635, acc 0.84\n",
      "2018-04-21T23:49:56.836381: step 1136, loss 0.53406, acc 0.74\n",
      "2018-04-21T23:49:56.880409: step 1137, loss 1.18975, acc 0.76\n",
      "2018-04-21T23:49:56.924142: step 1138, loss 0.363219, acc 0.84\n",
      "2018-04-21T23:49:56.966961: step 1139, loss 0.341896, acc 0.9\n",
      "2018-04-21T23:49:57.014283: step 1140, loss 0.298928, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:57.646192: step 1140, loss 0.46513, acc 0.839259, rec 0.713542, pre 0.951389, f1 0.815476\n",
      "\n",
      "2018-04-21T23:49:57.690139: step 1141, loss 0.345881, acc 0.9\n",
      "2018-04-21T23:49:57.733574: step 1142, loss 0.323396, acc 0.9\n",
      "2018-04-21T23:49:57.777392: step 1143, loss 0.233467, acc 0.94\n",
      "2018-04-21T23:49:57.821326: step 1144, loss 0.394218, acc 0.88\n",
      "2018-04-21T23:49:57.867793: step 1145, loss 0.445074, acc 0.86\n",
      "2018-04-21T23:49:57.911833: step 1146, loss 0.354059, acc 0.82\n",
      "2018-04-21T23:49:57.955763: step 1147, loss 0.339456, acc 0.86\n",
      "2018-04-21T23:49:57.999131: step 1148, loss 0.526214, acc 0.74\n",
      "2018-04-21T23:49:58.042798: step 1149, loss 0.556862, acc 0.66\n",
      "2018-04-21T23:49:58.088554: step 1150, loss 0.305593, acc 0.92\n",
      "2018-04-21T23:49:58.132374: step 1151, loss 0.573196, acc 0.8\n",
      "2018-04-21T23:49:58.176120: step 1152, loss 0.331102, acc 0.88\n",
      "2018-04-21T23:49:58.219602: step 1153, loss 0.427231, acc 0.78\n",
      "2018-04-21T23:49:58.263385: step 1154, loss 0.379984, acc 0.84\n",
      "2018-04-21T23:49:58.310321: step 1155, loss 0.36623, acc 0.82\n",
      "2018-04-21T23:49:58.353982: step 1156, loss 0.245573, acc 0.94\n",
      "2018-04-21T23:49:58.397381: step 1157, loss 0.273297, acc 0.92\n",
      "2018-04-21T23:49:58.440784: step 1158, loss 0.272004, acc 0.88\n",
      "2018-04-21T23:49:58.484013: step 1159, loss 0.346297, acc 0.88\n",
      "2018-04-21T23:49:58.530504: step 1160, loss 0.394622, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:49:59.165140: step 1160, loss 0.53381, acc 0.67037, rec 0.960565, pre 0.606673, f1 0.743664\n",
      "\n",
      "2018-04-21T23:49:59.208160: step 1161, loss 0.264876, acc 0.92\n",
      "2018-04-21T23:49:59.253015: step 1162, loss 0.265936, acc 0.94\n",
      "2018-04-21T23:49:59.296348: step 1163, loss 0.369516, acc 0.84\n",
      "2018-04-21T23:49:59.340816: step 1164, loss 0.410218, acc 0.86\n",
      "2018-04-21T23:49:59.388821: step 1165, loss 0.320955, acc 0.9\n",
      "2018-04-21T23:49:59.433275: step 1166, loss 0.493794, acc 0.8\n",
      "2018-04-21T23:49:59.477219: step 1167, loss 0.553827, acc 0.78\n",
      "2018-04-21T23:49:59.521920: step 1168, loss 0.417384, acc 0.86\n",
      "2018-04-21T23:49:59.565747: step 1169, loss 0.434183, acc 0.84\n",
      "2018-04-21T23:49:59.612268: step 1170, loss 0.471983, acc 0.82\n",
      "2018-04-21T23:49:59.656472: step 1171, loss 0.748157, acc 0.84\n",
      "2018-04-21T23:49:59.700642: step 1172, loss 0.401858, acc 0.84\n",
      "2018-04-21T23:49:59.745289: step 1173, loss 0.355367, acc 0.86\n",
      "2018-04-21T23:49:59.790962: step 1174, loss 0.391233, acc 0.8\n",
      "2018-04-21T23:49:59.837069: step 1175, loss 0.536107, acc 0.88\n",
      "2018-04-21T23:49:59.881945: step 1176, loss 0.369969, acc 0.8\n",
      "2018-04-21T23:49:59.925627: step 1177, loss 0.267153, acc 0.92\n",
      "2018-04-21T23:49:59.968745: step 1178, loss 0.406711, acc 0.86\n",
      "2018-04-21T23:50:00.011836: step 1179, loss 0.273187, acc 0.94\n",
      "2018-04-21T23:50:00.058059: step 1180, loss 0.315328, acc 0.8\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:00.896574: step 1180, loss 0.450722, acc 0.850741, rec 0.75, pre 0.937674, f1 0.833402\n",
      "\n",
      "2018-04-21T23:50:00.978609: step 1181, loss 0.344125, acc 0.88\n",
      "2018-04-21T23:50:01.023479: step 1182, loss 0.462865, acc 0.88\n",
      "2018-04-21T23:50:01.067537: step 1183, loss 0.316604, acc 0.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:50:01.115013: step 1184, loss 0.334227, acc 0.84\n",
      "2018-04-21T23:50:01.159842: step 1185, loss 0.319335, acc 0.88\n",
      "2018-04-21T23:50:01.203815: step 1186, loss 0.385811, acc 0.88\n",
      "2018-04-21T23:50:01.248544: step 1187, loss 0.533586, acc 0.78\n",
      "2018-04-21T23:50:01.291812: step 1188, loss 0.352825, acc 0.9\n",
      "2018-04-21T23:50:01.338964: step 1189, loss 0.412821, acc 0.8\n",
      "2018-04-21T23:50:01.383757: step 1190, loss 0.375561, acc 0.82\n",
      "2018-04-21T23:50:01.427831: step 1191, loss 0.310344, acc 0.9\n",
      "2018-04-21T23:50:01.472236: step 1192, loss 0.292079, acc 0.9\n",
      "2018-04-21T23:50:01.516993: step 1193, loss 0.671387, acc 0.72\n",
      "2018-04-21T23:50:01.565317: step 1194, loss 0.996066, acc 0.7\n",
      "2018-04-21T23:50:01.609983: step 1195, loss 0.379985, acc 0.8\n",
      "2018-04-21T23:50:01.654658: step 1196, loss 0.539982, acc 0.66\n",
      "2018-04-21T23:50:01.699096: step 1197, loss 0.559253, acc 0.78\n",
      "2018-04-21T23:50:01.762058: step 1198, loss 0.355006, acc 0.86\n",
      "2018-04-21T23:50:01.810173: step 1199, loss 0.548491, acc 0.74\n",
      "2018-04-21T23:50:01.859367: step 1200, loss 0.381483, acc 0.8\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:02.554968: step 1200, loss 0.443487, acc 0.848889, rec 0.886905, pre 0.823204, f1 0.853868\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-1200\n",
      "\n",
      "2018-04-21T23:50:02.647795: step 1201, loss 0.599845, acc 0.84\n",
      "2018-04-21T23:50:02.693791: step 1202, loss 0.362106, acc 0.86\n",
      "2018-04-21T23:50:02.738847: step 1203, loss 0.39819, acc 0.82\n",
      "2018-04-21T23:50:02.789852: step 1204, loss 0.409903, acc 0.82\n",
      "2018-04-21T23:50:02.834886: step 1205, loss 0.296599, acc 0.92\n",
      "2018-04-21T23:50:02.919990: step 1206, loss 0.233724, acc 0.92\n",
      "2018-04-21T23:50:02.971842: step 1207, loss 0.324664, acc 0.88\n",
      "2018-04-21T23:50:03.019172: step 1208, loss 0.246326, acc 0.94\n",
      "2018-04-21T23:50:03.064165: step 1209, loss 0.343825, acc 0.88\n",
      "2018-04-21T23:50:03.108972: step 1210, loss 0.54022, acc 0.74\n",
      "2018-04-21T23:50:03.153459: step 1211, loss 0.569923, acc 0.72\n",
      "2018-04-21T23:50:03.197834: step 1212, loss 0.487238, acc 0.82\n",
      "2018-04-21T23:50:03.245114: step 1213, loss 0.398879, acc 0.9\n",
      "2018-04-21T23:50:03.289383: step 1214, loss 0.448771, acc 0.94\n",
      "2018-04-21T23:50:03.333543: step 1215, loss 0.34261, acc 0.8\n",
      "2018-04-21T23:50:03.378381: step 1216, loss 0.474381, acc 0.88\n",
      "2018-04-21T23:50:03.422781: step 1217, loss 0.282191, acc 0.9\n",
      "2018-04-21T23:50:03.469967: step 1218, loss 0.405722, acc 0.76\n",
      "2018-04-21T23:50:03.516080: step 1219, loss 0.412828, acc 0.86\n",
      "2018-04-21T23:50:03.581147: step 1220, loss 0.292058, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:04.490972: step 1220, loss 0.438945, acc 0.85037, rec 0.880952, pre 0.829132, f1 0.854257\n",
      "\n",
      "2018-04-21T23:50:04.534303: step 1221, loss 0.239957, acc 0.92\n",
      "2018-04-21T23:50:04.578621: step 1222, loss 0.24997, acc 0.92\n",
      "2018-04-21T23:50:04.623275: step 1223, loss 0.21149, acc 0.94\n",
      "2018-04-21T23:50:04.668207: step 1224, loss 0.34491, acc 0.92\n",
      "2018-04-21T23:50:04.715982: step 1225, loss 0.350497, acc 0.86\n",
      "2018-04-21T23:50:04.760851: step 1226, loss 0.395976, acc 0.8\n",
      "2018-04-21T23:50:04.804493: step 1227, loss 0.501853, acc 0.72\n",
      "2018-04-21T23:50:04.848325: step 1228, loss 0.610408, acc 0.8\n",
      "2018-04-21T23:50:04.892380: step 1229, loss 0.457524, acc 0.84\n",
      "2018-04-21T23:50:04.940027: step 1230, loss 0.393736, acc 0.86\n",
      "2018-04-21T23:50:04.984188: step 1231, loss 0.463779, acc 0.82\n",
      "2018-04-21T23:50:05.028693: step 1232, loss 0.341711, acc 0.86\n",
      "2018-04-21T23:50:05.072747: step 1233, loss 0.660208, acc 0.82\n",
      "2018-04-21T23:50:05.118766: step 1234, loss 0.476131, acc 0.88\n",
      "2018-04-21T23:50:05.166493: step 1235, loss 0.348189, acc 0.86\n",
      "2018-04-21T23:50:05.210825: step 1236, loss 0.356674, acc 0.9\n",
      "2018-04-21T23:50:05.254983: step 1237, loss 0.438906, acc 0.76\n",
      "2018-04-21T23:50:05.299811: step 1238, loss 0.653104, acc 0.8\n",
      "2018-04-21T23:50:05.364332: step 1239, loss 0.296432, acc 0.9\n",
      "2018-04-21T23:50:05.424701: step 1240, loss 0.389388, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:06.067454: step 1240, loss 0.585387, acc 0.625926, rec 0.977679, pre 0.572799, f1 0.722375\n",
      "\n",
      "2018-04-21T23:50:06.110989: step 1241, loss 0.315696, acc 0.82\n",
      "2018-04-21T23:50:06.154828: step 1242, loss 0.393958, acc 0.78\n",
      "2018-04-21T23:50:06.199109: step 1243, loss 0.398675, acc 0.8\n",
      "2018-04-21T23:50:06.243245: step 1244, loss 0.506265, acc 0.84\n",
      "2018-04-21T23:50:06.290453: step 1245, loss 0.319411, acc 0.84\n",
      "2018-04-21T23:50:06.334339: step 1246, loss 0.490062, acc 0.84\n",
      "2018-04-21T23:50:06.378493: step 1247, loss 0.286083, acc 0.92\n",
      "2018-04-21T23:50:06.422199: step 1248, loss 0.285, acc 0.92\n",
      "2018-04-21T23:50:06.465808: step 1249, loss 0.349455, acc 0.84\n",
      "2018-04-21T23:50:06.513185: step 1250, loss 0.383484, acc 0.84\n",
      "2018-04-21T23:50:06.556904: step 1251, loss 0.327087, acc 0.84\n",
      "2018-04-21T23:50:06.602555: step 1252, loss 0.282604, acc 0.92\n",
      "2018-04-21T23:50:06.648062: step 1253, loss 0.739788, acc 0.76\n",
      "2018-04-21T23:50:06.694560: step 1254, loss 0.326387, acc 0.86\n",
      "2018-04-21T23:50:06.743724: step 1255, loss 0.33435, acc 0.86\n",
      "2018-04-21T23:50:06.789084: step 1256, loss 0.531953, acc 0.88\n",
      "2018-04-21T23:50:06.835373: step 1257, loss 0.327656, acc 0.84\n",
      "2018-04-21T23:50:06.879728: step 1258, loss 0.329155, acc 0.86\n",
      "2018-04-21T23:50:06.924637: step 1259, loss 0.523649, acc 0.74\n",
      "2018-04-21T23:50:06.972987: step 1260, loss 0.264182, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:07.635775: step 1260, loss 0.837241, acc 0.525926, rec 0.986607, pre 0.512365, f1 0.674466\n",
      "\n",
      "2018-04-21T23:50:07.680324: step 1261, loss 0.358449, acc 0.82\n",
      "2018-04-21T23:50:07.728446: step 1262, loss 0.555458, acc 0.88\n",
      "2018-04-21T23:50:07.775941: step 1263, loss 0.361775, acc 0.86\n",
      "2018-04-21T23:50:07.824638: step 1264, loss 0.381177, acc 0.86\n",
      "2018-04-21T23:50:07.875381: step 1265, loss 0.36563, acc 0.88\n",
      "2018-04-21T23:50:07.923175: step 1266, loss 0.293525, acc 0.92\n",
      "2018-04-21T23:50:07.970256: step 1267, loss 0.465636, acc 0.8\n",
      "2018-04-21T23:50:08.017159: step 1268, loss 0.373464, acc 0.94\n",
      "2018-04-21T23:50:08.064131: step 1269, loss 0.363958, acc 0.78\n",
      "2018-04-21T23:50:08.116768: step 1270, loss 0.32786, acc 0.94\n",
      "2018-04-21T23:50:08.162613: step 1271, loss 0.444511, acc 0.86\n",
      "2018-04-21T23:50:08.209558: step 1272, loss 0.344556, acc 0.78\n",
      "2018-04-21T23:50:08.280963: step 1273, loss 0.46379, acc 0.76\n",
      "2018-04-21T23:50:08.332511: step 1274, loss 0.261774, acc 0.9\n",
      "2018-04-21T23:50:08.378676: step 1275, loss 0.328952, acc 0.88\n",
      "2018-04-21T23:50:08.426319: step 1276, loss 0.425265, acc 0.76\n",
      "2018-04-21T23:50:08.473844: step 1277, loss 0.229141, acc 0.96\n",
      "2018-04-21T23:50:08.522205: step 1278, loss 0.449, acc 0.86\n",
      "2018-04-21T23:50:08.571969: step 1279, loss 0.302605, acc 0.86\n",
      "2018-04-21T23:50:08.617871: step 1280, loss 0.245063, acc 0.96\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:09.304006: step 1280, loss 0.430679, acc 0.869259, rec 0.838542, pre 0.89232, f1 0.864595\n",
      "\n",
      "2018-04-21T23:50:09.350487: step 1281, loss 0.34919, acc 0.9\n",
      "2018-04-21T23:50:09.397355: step 1282, loss 0.460191, acc 0.84\n",
      "2018-04-21T23:50:09.443821: step 1283, loss 0.334932, acc 0.9\n",
      "2018-04-21T23:50:09.490189: step 1284, loss 0.259781, acc 0.92\n",
      "2018-04-21T23:50:09.540556: step 1285, loss 0.311842, acc 0.88\n",
      "2018-04-21T23:50:09.587527: step 1286, loss 0.404692, acc 0.84\n",
      "2018-04-21T23:50:09.633576: step 1287, loss 0.673865, acc 0.6\n",
      "2018-04-21T23:50:09.680511: step 1288, loss 0.735593, acc 0.58\n",
      "2018-04-21T23:50:09.727194: step 1289, loss 0.425281, acc 0.78\n",
      "2018-04-21T23:50:09.777485: step 1290, loss 0.222212, acc 0.96\n",
      "2018-04-21T23:50:09.824177: step 1291, loss 0.625002, acc 0.84\n",
      "2018-04-21T23:50:09.870321: step 1292, loss 0.825779, acc 0.86\n",
      "2018-04-21T23:50:09.916269: step 1293, loss 0.395253, acc 0.8\n",
      "2018-04-21T23:50:09.962771: step 1294, loss 0.30359, acc 0.92\n",
      "2018-04-21T23:50:10.013255: step 1295, loss 0.439371, acc 0.84\n",
      "2018-04-21T23:50:10.059112: step 1296, loss 0.390975, acc 0.8\n",
      "2018-04-21T23:50:10.105746: step 1297, loss 0.381866, acc 0.8\n",
      "2018-04-21T23:50:10.152126: step 1298, loss 0.265873, acc 0.92\n",
      "2018-04-21T23:50:10.197655: step 1299, loss 0.366782, acc 0.86\n",
      "2018-04-21T23:50:10.247326: step 1300, loss 0.389357, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:10.929221: step 1300, loss 0.451463, acc 0.81, rec 0.924107, pre 0.751361, f1 0.828829\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-1300\n",
      "\n",
      "2018-04-21T23:50:11.021048: step 1301, loss 0.344237, acc 0.86\n",
      "2018-04-21T23:50:11.067539: step 1302, loss 0.342988, acc 0.9\n",
      "2018-04-21T23:50:11.113479: step 1303, loss 0.412813, acc 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:50:11.163116: step 1304, loss 0.443056, acc 0.82\n",
      "2018-04-21T23:50:11.208837: step 1305, loss 0.363205, acc 0.82\n",
      "2018-04-21T23:50:11.254073: step 1306, loss 0.387302, acc 0.82\n",
      "2018-04-21T23:50:11.299623: step 1307, loss 0.442939, acc 0.8\n",
      "2018-04-21T23:50:11.348235: step 1308, loss 0.304917, acc 0.92\n",
      "2018-04-21T23:50:11.399485: step 1309, loss 0.321164, acc 0.92\n",
      "2018-04-21T23:50:11.445432: step 1310, loss 0.314022, acc 0.88\n",
      "2018-04-21T23:50:11.491588: step 1311, loss 0.420108, acc 0.86\n",
      "2018-04-21T23:50:11.537559: step 1312, loss 0.327943, acc 0.88\n",
      "2018-04-21T23:50:11.583591: step 1313, loss 0.532361, acc 0.82\n",
      "2018-04-21T23:50:11.634054: step 1314, loss 0.333474, acc 0.88\n",
      "2018-04-21T23:50:11.681366: step 1315, loss 0.328079, acc 0.86\n",
      "2018-04-21T23:50:11.727747: step 1316, loss 0.309172, acc 0.86\n",
      "2018-04-21T23:50:11.774284: step 1317, loss 0.455892, acc 0.9\n",
      "2018-04-21T23:50:11.820700: step 1318, loss 0.662841, acc 0.76\n",
      "2018-04-21T23:50:11.871181: step 1319, loss 0.754171, acc 0.74\n",
      "2018-04-21T23:50:11.917658: step 1320, loss 0.680329, acc 0.66\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:12.603253: step 1320, loss 1.1384, acc 0.502963, rec 0.99628, pre 0.500374, f1 0.666169\n",
      "\n",
      "2018-04-21T23:50:12.649544: step 1321, loss 0.784795, acc 0.62\n",
      "2018-04-21T23:50:12.698137: step 1322, loss 0.34467, acc 0.82\n",
      "2018-04-21T23:50:12.746404: step 1323, loss 0.290964, acc 0.9\n",
      "2018-04-21T23:50:12.791851: step 1324, loss 0.467439, acc 0.78\n",
      "2018-04-21T23:50:12.839953: step 1325, loss 0.44855, acc 0.9\n",
      "2018-04-21T23:50:12.885210: step 1326, loss 0.404774, acc 0.78\n",
      "2018-04-21T23:50:12.931307: step 1327, loss 0.34601, acc 0.8\n",
      "2018-04-21T23:50:12.977104: step 1328, loss 0.483019, acc 0.8\n",
      "2018-04-21T23:50:13.021209: step 1329, loss 0.344464, acc 0.82\n",
      "2018-04-21T23:50:13.068828: step 1330, loss 0.295827, acc 0.86\n",
      "2018-04-21T23:50:13.114221: step 1331, loss 0.405531, acc 0.88\n",
      "2018-04-21T23:50:13.159167: step 1332, loss 0.27663, acc 0.9\n",
      "2018-04-21T23:50:13.204910: step 1333, loss 0.423936, acc 0.8\n",
      "2018-04-21T23:50:13.249586: step 1334, loss 0.186633, acc 1\n",
      "2018-04-21T23:50:13.297057: step 1335, loss 0.282183, acc 0.86\n",
      "2018-04-21T23:50:13.343075: step 1336, loss 0.265251, acc 0.9\n",
      "2018-04-21T23:50:13.387773: step 1337, loss 0.386318, acc 0.78\n",
      "2018-04-21T23:50:13.432602: step 1338, loss 0.295887, acc 0.9\n",
      "2018-04-21T23:50:13.477704: step 1339, loss 0.312149, acc 0.86\n",
      "2018-04-21T23:50:13.526679: step 1340, loss 0.329597, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:14.170429: step 1340, loss 0.435477, acc 0.846296, rec 0.898065, pre 0.812795, f1 0.853305\n",
      "\n",
      "2018-04-21T23:50:14.215647: step 1341, loss 0.326761, acc 0.9\n",
      "2018-04-21T23:50:14.260582: step 1342, loss 0.387884, acc 0.86\n",
      "2018-04-21T23:50:14.304785: step 1343, loss 0.477434, acc 0.88\n",
      "2018-04-21T23:50:14.349638: step 1344, loss 0.470419, acc 0.74\n",
      "2018-04-21T23:50:14.397153: step 1345, loss 0.500137, acc 0.8\n",
      "2018-04-21T23:50:14.441739: step 1346, loss 0.363703, acc 0.86\n",
      "2018-04-21T23:50:14.487343: step 1347, loss 0.272145, acc 0.92\n",
      "2018-04-21T23:50:14.533110: step 1348, loss 0.196873, acc 0.98\n",
      "2018-04-21T23:50:14.580067: step 1349, loss 0.705025, acc 0.88\n",
      "2018-04-21T23:50:14.628373: step 1350, loss 0.56106, acc 0.94\n",
      "2018-04-21T23:50:14.672780: step 1351, loss 0.474656, acc 0.8\n",
      "2018-04-21T23:50:14.717070: step 1352, loss 0.5107, acc 0.84\n",
      "2018-04-21T23:50:14.762935: step 1353, loss 0.257078, acc 0.92\n",
      "2018-04-21T23:50:14.807449: step 1354, loss 0.613119, acc 0.88\n",
      "2018-04-21T23:50:14.855486: step 1355, loss 0.350125, acc 0.88\n",
      "2018-04-21T23:50:14.899787: step 1356, loss 0.273218, acc 0.88\n",
      "2018-04-21T23:50:14.944900: step 1357, loss 0.307106, acc 0.86\n",
      "2018-04-21T23:50:14.989144: step 1358, loss 0.317259, acc 0.86\n",
      "2018-04-21T23:50:15.033824: step 1359, loss 0.293462, acc 0.88\n",
      "2018-04-21T23:50:15.081972: step 1360, loss 0.307893, acc 0.82\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:15.720845: step 1360, loss 0.471012, acc 0.766667, rec 0.944196, pre 0.695724, f1 0.801136\n",
      "\n",
      "2018-04-21T23:50:15.765544: step 1361, loss 0.636069, acc 0.84\n",
      "2018-04-21T23:50:15.817154: step 1362, loss 0.319062, acc 0.9\n",
      "2018-04-21T23:50:15.872443: step 1363, loss 0.363629, acc 0.88\n",
      "2018-04-21T23:50:15.929031: step 1364, loss 0.322862, acc 0.9\n",
      "2018-04-21T23:50:15.986405: step 1365, loss 0.338018, acc 0.86\n",
      "2018-04-21T23:50:16.041299: step 1366, loss 0.339021, acc 0.86\n",
      "2018-04-21T23:50:16.096771: step 1367, loss 0.523792, acc 0.78\n",
      "2018-04-21T23:50:16.151050: step 1368, loss 0.562707, acc 0.7\n",
      "2018-04-21T23:50:16.210818: step 1369, loss 0.836832, acc 0.76\n",
      "2018-04-21T23:50:16.263048: step 1370, loss 0.382496, acc 0.82\n",
      "2018-04-21T23:50:16.310038: step 1371, loss 0.393879, acc 0.84\n",
      "2018-04-21T23:50:16.382036: step 1372, loss 0.257952, acc 0.9\n",
      "2018-04-21T23:50:16.438799: step 1373, loss 0.331666, acc 0.84\n",
      "2018-04-21T23:50:16.492110: step 1374, loss 0.430191, acc 0.86\n",
      "2018-04-21T23:50:16.555724: step 1375, loss 0.225039, acc 1\n",
      "2018-04-21T23:50:16.615186: step 1376, loss 0.314117, acc 0.88\n",
      "2018-04-21T23:50:16.659207: step 1377, loss 0.286453, acc 0.94\n",
      "2018-04-21T23:50:16.703699: step 1378, loss 0.254883, acc 0.88\n",
      "2018-04-21T23:50:16.747709: step 1379, loss 0.344101, acc 0.9\n",
      "2018-04-21T23:50:16.792777: step 1380, loss 0.547391, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:17.428630: step 1380, loss 0.453113, acc 0.801111, rec 0.927083, pre 0.739466, f1 0.822714\n",
      "\n",
      "2018-04-21T23:50:17.472238: step 1381, loss 0.323887, acc 0.92\n",
      "2018-04-21T23:50:17.516974: step 1382, loss 0.391713, acc 0.86\n",
      "2018-04-21T23:50:17.560915: step 1383, loss 0.331428, acc 0.88\n",
      "2018-04-21T23:50:17.604476: step 1384, loss 0.347003, acc 0.9\n",
      "2018-04-21T23:50:17.651983: step 1385, loss 0.508648, acc 0.76\n",
      "2018-04-21T23:50:17.696449: step 1386, loss 0.27882, acc 0.92\n",
      "2018-04-21T23:50:17.740024: step 1387, loss 0.343329, acc 0.86\n",
      "2018-04-21T23:50:17.787387: step 1388, loss 0.225886, acc 0.98\n",
      "2018-04-21T23:50:17.833151: step 1389, loss 0.474738, acc 0.9\n",
      "2018-04-21T23:50:17.881401: step 1390, loss 0.247796, acc 0.9\n",
      "2018-04-21T23:50:17.926312: step 1391, loss 0.346961, acc 0.92\n",
      "2018-04-21T23:50:17.971441: step 1392, loss 0.284019, acc 0.9\n",
      "2018-04-21T23:50:18.016875: step 1393, loss 0.408129, acc 0.86\n",
      "2018-04-21T23:50:18.062613: step 1394, loss 0.343255, acc 0.84\n",
      "2018-04-21T23:50:18.112980: step 1395, loss 0.271525, acc 0.9\n",
      "2018-04-21T23:50:18.158981: step 1396, loss 0.364153, acc 0.84\n",
      "2018-04-21T23:50:18.203805: step 1397, loss 0.37327, acc 0.76\n",
      "2018-04-21T23:50:18.249004: step 1398, loss 0.31959, acc 0.9\n",
      "2018-04-21T23:50:18.293552: step 1399, loss 0.299786, acc 0.88\n",
      "2018-04-21T23:50:18.341858: step 1400, loss 0.636439, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:19.030170: step 1400, loss 0.468223, acc 0.761111, rec 0.942708, pre 0.690463, f1 0.797106\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-1400\n",
      "\n",
      "2018-04-21T23:50:19.122265: step 1401, loss 0.345423, acc 0.84\n",
      "2018-04-21T23:50:19.167465: step 1402, loss 0.473202, acc 0.82\n",
      "2018-04-21T23:50:19.212901: step 1403, loss 0.356886, acc 0.84\n",
      "2018-04-21T23:50:19.261729: step 1404, loss 0.463982, acc 0.74\n",
      "2018-04-21T23:50:19.307084: step 1405, loss 0.317739, acc 0.8\n",
      "2018-04-21T23:50:19.352447: step 1406, loss 0.410808, acc 0.84\n",
      "2018-04-21T23:50:19.398908: step 1407, loss 0.290161, acc 0.9\n",
      "2018-04-21T23:50:19.444390: step 1408, loss 0.545309, acc 0.86\n",
      "2018-04-21T23:50:19.494188: step 1409, loss 0.2976, acc 0.86\n",
      "2018-04-21T23:50:19.540067: step 1410, loss 0.448775, acc 0.86\n",
      "2018-04-21T23:50:19.585244: step 1411, loss 0.33798, acc 0.84\n",
      "2018-04-21T23:50:19.631972: step 1412, loss 0.309168, acc 0.92\n",
      "2018-04-21T23:50:19.678052: step 1413, loss 0.263724, acc 0.88\n",
      "2018-04-21T23:50:19.727549: step 1414, loss 0.203575, acc 0.96\n",
      "2018-04-21T23:50:19.773927: step 1415, loss 0.378956, acc 0.88\n",
      "2018-04-21T23:50:19.819755: step 1416, loss 0.241397, acc 0.96\n",
      "2018-04-21T23:50:19.866328: step 1417, loss 0.334977, acc 0.88\n",
      "2018-04-21T23:50:19.912889: step 1418, loss 0.544321, acc 0.78\n",
      "2018-04-21T23:50:19.962715: step 1419, loss 0.807851, acc 0.76\n",
      "2018-04-21T23:50:20.008803: step 1420, loss 0.47367, acc 0.82\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:20.692595: step 1420, loss 0.488675, acc 0.732222, rec 0.956101, pre 0.659312, f1 0.780443\n",
      "\n",
      "2018-04-21T23:50:20.738479: step 1421, loss 0.209945, acc 0.94\n",
      "2018-04-21T23:50:20.784863: step 1422, loss 0.423917, acc 0.78\n",
      "2018-04-21T23:50:20.831128: step 1423, loss 0.453599, acc 0.88\n",
      "2018-04-21T23:50:20.876399: step 1424, loss 0.265501, acc 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:50:20.925985: step 1425, loss 0.335875, acc 0.86\n",
      "2018-04-21T23:50:20.971509: step 1426, loss 0.360236, acc 0.88\n",
      "2018-04-21T23:50:21.017228: step 1427, loss 0.373891, acc 0.92\n",
      "2018-04-21T23:50:21.063681: step 1428, loss 0.222626, acc 0.92\n",
      "2018-04-21T23:50:21.111702: step 1429, loss 0.380803, acc 0.9\n",
      "2018-04-21T23:50:21.161971: step 1430, loss 0.576511, acc 0.86\n",
      "2018-04-21T23:50:21.207750: step 1431, loss 0.384007, acc 0.9\n",
      "2018-04-21T23:50:21.253735: step 1432, loss 0.287803, acc 0.9\n",
      "2018-04-21T23:50:21.298432: step 1433, loss 0.401415, acc 0.86\n",
      "2018-04-21T23:50:21.344185: step 1434, loss 0.28126, acc 0.86\n",
      "2018-04-21T23:50:21.394634: step 1435, loss 0.342278, acc 0.86\n",
      "2018-04-21T23:50:21.440054: step 1436, loss 0.507776, acc 0.82\n",
      "2018-04-21T23:50:21.485788: step 1437, loss 0.464891, acc 0.76\n",
      "2018-04-21T23:50:21.530065: step 1438, loss 0.508025, acc 0.84\n",
      "2018-04-21T23:50:21.576031: step 1439, loss 0.433523, acc 0.78\n",
      "2018-04-21T23:50:21.625137: step 1440, loss 0.285362, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:22.307347: step 1440, loss 0.523345, acc 0.682593, rec 0.971726, pre 0.614588, f1 0.752955\n",
      "\n",
      "2018-04-21T23:50:22.351748: step 1441, loss 0.243735, acc 0.94\n",
      "2018-04-21T23:50:22.396797: step 1442, loss 0.206449, acc 0.94\n",
      "2018-04-21T23:50:22.441630: step 1443, loss 0.341236, acc 0.86\n",
      "2018-04-21T23:50:22.486798: step 1444, loss 0.565982, acc 0.8\n",
      "2018-04-21T23:50:22.538040: step 1445, loss 0.334569, acc 0.86\n",
      "2018-04-21T23:50:22.586961: step 1446, loss 0.283682, acc 0.92\n",
      "2018-04-21T23:50:22.632663: step 1447, loss 0.337938, acc 0.92\n",
      "2018-04-21T23:50:22.679184: step 1448, loss 0.200975, acc 0.94\n",
      "2018-04-21T23:50:22.724845: step 1449, loss 0.355627, acc 0.88\n",
      "2018-04-21T23:50:22.775335: step 1450, loss 0.473973, acc 0.8\n",
      "2018-04-21T23:50:22.821974: step 1451, loss 0.791713, acc 0.74\n",
      "2018-04-21T23:50:22.868045: step 1452, loss 0.412602, acc 0.84\n",
      "2018-04-21T23:50:22.913893: step 1453, loss 0.309305, acc 0.92\n",
      "2018-04-21T23:50:22.959870: step 1454, loss 0.25941, acc 0.9\n",
      "2018-04-21T23:50:23.008612: step 1455, loss 0.33751, acc 0.88\n",
      "2018-04-21T23:50:23.054915: step 1456, loss 0.447319, acc 0.86\n",
      "2018-04-21T23:50:23.100116: step 1457, loss 0.304893, acc 0.84\n",
      "2018-04-21T23:50:23.145247: step 1458, loss 0.342718, acc 0.82\n",
      "2018-04-21T23:50:23.190670: step 1459, loss 0.361631, acc 0.86\n",
      "2018-04-21T23:50:23.240758: step 1460, loss 0.394592, acc 0.78\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:23.923252: step 1460, loss 0.440967, acc 0.838148, rec 0.712054, pre 0.950348, f1 0.814122\n",
      "\n",
      "2018-04-21T23:50:23.968475: step 1461, loss 0.277009, acc 0.86\n",
      "2018-04-21T23:50:24.013471: step 1462, loss 0.482701, acc 0.84\n",
      "2018-04-21T23:50:24.059148: step 1463, loss 0.399651, acc 0.86\n",
      "2018-04-21T23:50:24.104817: step 1464, loss 0.77913, acc 0.82\n",
      "2018-04-21T23:50:24.154883: step 1465, loss 0.524635, acc 0.76\n",
      "2018-04-21T23:50:24.200994: step 1466, loss 0.360527, acc 0.84\n",
      "2018-04-21T23:50:24.247514: step 1467, loss 0.330938, acc 0.86\n",
      "2018-04-21T23:50:24.293148: step 1468, loss 0.238238, acc 0.94\n",
      "2018-04-21T23:50:24.339733: step 1469, loss 0.319362, acc 0.92\n",
      "2018-04-21T23:50:24.390126: step 1470, loss 0.322183, acc 0.9\n",
      "2018-04-21T23:50:24.436875: step 1471, loss 0.307525, acc 0.9\n",
      "2018-04-21T23:50:24.483464: step 1472, loss 0.477396, acc 0.78\n",
      "2018-04-21T23:50:24.529566: step 1473, loss 0.320766, acc 0.88\n",
      "2018-04-21T23:50:24.575666: step 1474, loss 0.27976, acc 0.86\n",
      "2018-04-21T23:50:24.625389: step 1475, loss 0.457559, acc 0.86\n",
      "2018-04-21T23:50:24.672400: step 1476, loss 0.725362, acc 0.8\n",
      "2018-04-21T23:50:24.718464: step 1477, loss 0.37155, acc 0.82\n",
      "2018-04-21T23:50:24.764112: step 1478, loss 0.292935, acc 0.86\n",
      "2018-04-21T23:50:24.809826: step 1479, loss 0.407338, acc 0.9\n",
      "2018-04-21T23:50:24.860344: step 1480, loss 0.2621, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:25.543005: step 1480, loss 0.468939, acc 0.759259, rec 0.94494, pre 0.687974, f1 0.796238\n",
      "\n",
      "2018-04-21T23:50:25.587152: step 1481, loss 0.212523, acc 0.94\n",
      "2018-04-21T23:50:25.631939: step 1482, loss 0.26568, acc 0.94\n",
      "2018-04-21T23:50:25.678099: step 1483, loss 0.25598, acc 0.9\n",
      "2018-04-21T23:50:25.724104: step 1484, loss 0.546195, acc 0.8\n",
      "2018-04-21T23:50:25.773954: step 1485, loss 0.35816, acc 0.88\n",
      "2018-04-21T23:50:25.820325: step 1486, loss 0.266138, acc 0.86\n",
      "2018-04-21T23:50:25.867150: step 1487, loss 0.288753, acc 0.84\n",
      "2018-04-21T23:50:25.915076: step 1488, loss 0.259392, acc 0.9\n",
      "2018-04-21T23:50:25.960628: step 1489, loss 0.366582, acc 0.84\n",
      "2018-04-21T23:50:26.009971: step 1490, loss 0.289645, acc 0.88\n",
      "2018-04-21T23:50:26.056316: step 1491, loss 0.32648, acc 0.84\n",
      "2018-04-21T23:50:26.101336: step 1492, loss 0.68079, acc 0.76\n",
      "2018-04-21T23:50:26.147406: step 1493, loss 0.342228, acc 0.88\n",
      "2018-04-21T23:50:26.193057: step 1494, loss 0.333377, acc 0.88\n",
      "2018-04-21T23:50:26.243190: step 1495, loss 0.567574, acc 0.76\n",
      "2018-04-21T23:50:26.289932: step 1496, loss 0.241624, acc 0.92\n",
      "2018-04-21T23:50:26.336505: step 1497, loss 0.436927, acc 0.88\n",
      "2018-04-21T23:50:26.382064: step 1498, loss 0.56422, acc 0.8\n",
      "2018-04-21T23:50:26.428180: step 1499, loss 0.243146, acc 0.9\n",
      "2018-04-21T23:50:26.478855: step 1500, loss 0.483602, acc 0.76\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:27.159265: step 1500, loss 0.619475, acc 0.607407, rec 0.979167, pre 0.560477, f1 0.712893\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-1500\n",
      "\n",
      "2018-04-21T23:50:27.251099: step 1501, loss 0.273089, acc 0.9\n",
      "2018-04-21T23:50:27.296475: step 1502, loss 0.288401, acc 0.86\n",
      "2018-04-21T23:50:27.341587: step 1503, loss 0.290686, acc 0.88\n",
      "2018-04-21T23:50:27.390838: step 1504, loss 0.269443, acc 0.94\n",
      "2018-04-21T23:50:27.436692: step 1505, loss 0.197591, acc 0.94\n",
      "2018-04-21T23:50:27.482483: step 1506, loss 0.394043, acc 0.84\n",
      "2018-04-21T23:50:27.529072: step 1507, loss 0.344377, acc 0.86\n",
      "2018-04-21T23:50:27.575419: step 1508, loss 0.230145, acc 0.9\n",
      "2018-04-21T23:50:27.625241: step 1509, loss 0.360523, acc 0.92\n",
      "2018-04-21T23:50:27.673103: step 1510, loss 0.339823, acc 0.8\n",
      "2018-04-21T23:50:27.720266: step 1511, loss 0.670255, acc 0.88\n",
      "2018-04-21T23:50:27.766426: step 1512, loss 0.505228, acc 0.82\n",
      "2018-04-21T23:50:27.813557: step 1513, loss 0.237815, acc 0.92\n",
      "2018-04-21T23:50:27.863949: step 1514, loss 0.348687, acc 0.9\n",
      "2018-04-21T23:50:27.908993: step 1515, loss 0.198371, acc 0.98\n",
      "2018-04-21T23:50:27.954810: step 1516, loss 0.307449, acc 0.92\n",
      "2018-04-21T23:50:27.998998: step 1517, loss 0.312318, acc 0.92\n",
      "2018-04-21T23:50:28.045268: step 1518, loss 0.315752, acc 0.88\n",
      "2018-04-21T23:50:28.095454: step 1519, loss 0.301274, acc 0.86\n",
      "2018-04-21T23:50:28.140309: step 1520, loss 0.774229, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:28.824338: step 1520, loss 0.540406, acc 0.669259, rec 0.97619, pre 0.603774, f1 0.74609\n",
      "\n",
      "2018-04-21T23:50:28.868814: step 1521, loss 0.306036, acc 0.92\n",
      "2018-04-21T23:50:28.916089: step 1522, loss 0.250411, acc 0.9\n",
      "2018-04-21T23:50:28.961299: step 1523, loss 0.469288, acc 0.7\n",
      "2018-04-21T23:50:29.006913: step 1524, loss 0.827235, acc 0.68\n",
      "2018-04-21T23:50:29.061143: step 1525, loss 0.560102, acc 0.68\n",
      "2018-04-21T23:50:29.107229: step 1526, loss 0.551945, acc 0.8\n",
      "2018-04-21T23:50:29.152435: step 1527, loss 0.326225, acc 0.82\n",
      "2018-04-21T23:50:29.198213: step 1528, loss 0.172621, acc 0.96\n",
      "2018-04-21T23:50:29.244343: step 1529, loss 0.270369, acc 0.96\n",
      "2018-04-21T23:50:29.294398: step 1530, loss 0.453087, acc 0.82\n",
      "2018-04-21T23:50:29.341189: step 1531, loss 0.253798, acc 0.96\n",
      "2018-04-21T23:50:29.386762: step 1532, loss 0.292937, acc 0.92\n",
      "2018-04-21T23:50:29.432635: step 1533, loss 0.355122, acc 0.84\n",
      "2018-04-21T23:50:29.478876: step 1534, loss 0.279671, acc 0.92\n",
      "2018-04-21T23:50:29.529350: step 1535, loss 0.208918, acc 0.96\n",
      "2018-04-21T23:50:29.575440: step 1536, loss 0.401649, acc 0.82\n",
      "2018-04-21T23:50:29.623246: step 1537, loss 0.418472, acc 0.76\n",
      "2018-04-21T23:50:29.669007: step 1538, loss 0.586966, acc 0.86\n",
      "2018-04-21T23:50:29.714455: step 1539, loss 0.370201, acc 0.84\n",
      "2018-04-21T23:50:29.764053: step 1540, loss 0.310602, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:30.443556: step 1540, loss 0.441797, acc 0.832963, rec 0.698661, pre 0.953299, f1 0.806355\n",
      "\n",
      "2018-04-21T23:50:30.489652: step 1541, loss 0.324625, acc 0.88\n",
      "2018-04-21T23:50:30.534598: step 1542, loss 0.318372, acc 0.84\n",
      "2018-04-21T23:50:30.579736: step 1543, loss 0.365465, acc 0.84\n",
      "2018-04-21T23:50:30.625620: step 1544, loss 0.356784, acc 0.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:50:30.676768: step 1545, loss 0.423128, acc 0.86\n",
      "2018-04-21T23:50:30.722549: step 1546, loss 0.312977, acc 0.86\n",
      "2018-04-21T23:50:30.768394: step 1547, loss 0.263414, acc 0.9\n",
      "2018-04-21T23:50:30.813707: step 1548, loss 0.425401, acc 0.82\n",
      "2018-04-21T23:50:30.858664: step 1549, loss 0.415361, acc 0.8\n",
      "2018-04-21T23:50:30.907713: step 1550, loss 0.47652, acc 0.76\n",
      "2018-04-21T23:50:30.953623: step 1551, loss 0.441155, acc 0.78\n",
      "2018-04-21T23:50:30.998880: step 1552, loss 0.611567, acc 0.68\n",
      "2018-04-21T23:50:31.044299: step 1553, loss 0.629113, acc 0.66\n",
      "2018-04-21T23:50:31.090137: step 1554, loss 0.341137, acc 0.84\n",
      "2018-04-21T23:50:31.138948: step 1555, loss 0.298659, acc 0.94\n",
      "2018-04-21T23:50:31.184567: step 1556, loss 0.606483, acc 0.84\n",
      "2018-04-21T23:50:31.231001: step 1557, loss 0.321286, acc 0.86\n",
      "2018-04-21T23:50:31.277017: step 1558, loss 0.281029, acc 0.88\n",
      "2018-04-21T23:50:31.322560: step 1559, loss 0.207159, acc 0.96\n",
      "2018-04-21T23:50:31.372508: step 1560, loss 0.498245, acc 0.78\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:32.030162: step 1560, loss 0.42619, acc 0.842593, rec 0.899554, pre 0.806538, f1 0.85051\n",
      "\n",
      "2018-04-21T23:50:32.075374: step 1561, loss 0.692057, acc 0.82\n",
      "2018-04-21T23:50:32.118766: step 1562, loss 0.481672, acc 0.88\n",
      "2018-04-21T23:50:32.162674: step 1563, loss 0.228757, acc 0.92\n",
      "2018-04-21T23:50:32.206613: step 1564, loss 0.470722, acc 0.82\n",
      "2018-04-21T23:50:32.253761: step 1565, loss 0.423721, acc 0.8\n",
      "2018-04-21T23:50:32.297907: step 1566, loss 0.185188, acc 0.98\n",
      "2018-04-21T23:50:32.342271: step 1567, loss 0.261548, acc 0.96\n",
      "2018-04-21T23:50:32.386234: step 1568, loss 0.298315, acc 0.9\n",
      "2018-04-21T23:50:32.429474: step 1569, loss 0.416846, acc 0.8\n",
      "2018-04-21T23:50:32.475653: step 1570, loss 0.316152, acc 0.86\n",
      "2018-04-21T23:50:32.520059: step 1571, loss 0.309955, acc 0.9\n",
      "2018-04-21T23:50:32.565606: step 1572, loss 0.477549, acc 0.88\n",
      "2018-04-21T23:50:32.610216: step 1573, loss 0.259424, acc 0.94\n",
      "2018-04-21T23:50:32.653939: step 1574, loss 0.514486, acc 0.9\n",
      "2018-04-21T23:50:32.701382: step 1575, loss 0.30786, acc 0.86\n",
      "2018-04-21T23:50:32.746209: step 1576, loss 0.38936, acc 0.82\n",
      "2018-04-21T23:50:32.791438: step 1577, loss 0.304038, acc 0.84\n",
      "2018-04-21T23:50:32.836076: step 1578, loss 0.256797, acc 0.92\n",
      "2018-04-21T23:50:32.880706: step 1579, loss 0.497171, acc 0.82\n",
      "2018-04-21T23:50:32.929162: step 1580, loss 0.299383, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:33.611797: step 1580, loss 0.444334, acc 0.8, rec 0.931548, pre 0.736471, f1 0.822602\n",
      "\n",
      "2018-04-21T23:50:33.657401: step 1581, loss 0.636149, acc 0.88\n",
      "2018-04-21T23:50:33.703740: step 1582, loss 0.248015, acc 0.9\n",
      "2018-04-21T23:50:33.747939: step 1583, loss 0.220478, acc 0.94\n",
      "2018-04-21T23:50:33.792047: step 1584, loss 0.321576, acc 0.84\n",
      "2018-04-21T23:50:33.840010: step 1585, loss 0.278262, acc 0.84\n",
      "2018-04-21T23:50:33.885355: step 1586, loss 0.323943, acc 0.92\n",
      "2018-04-21T23:50:33.930712: step 1587, loss 0.407181, acc 0.8\n",
      "2018-04-21T23:50:33.976077: step 1588, loss 0.381301, acc 0.84\n",
      "2018-04-21T23:50:34.020994: step 1589, loss 0.256575, acc 0.9\n",
      "2018-04-21T23:50:34.071029: step 1590, loss 0.30509, acc 0.88\n",
      "2018-04-21T23:50:34.115837: step 1591, loss 0.308667, acc 0.9\n",
      "2018-04-21T23:50:34.160804: step 1592, loss 0.422802, acc 0.86\n",
      "2018-04-21T23:50:34.204113: step 1593, loss 0.462976, acc 0.82\n",
      "2018-04-21T23:50:34.249358: step 1594, loss 0.301716, acc 0.84\n",
      "2018-04-21T23:50:34.298782: step 1595, loss 0.342593, acc 0.88\n",
      "2018-04-21T23:50:34.344127: step 1596, loss 0.529395, acc 0.82\n",
      "2018-04-21T23:50:34.389408: step 1597, loss 0.357998, acc 0.84\n",
      "2018-04-21T23:50:34.434876: step 1598, loss 0.352408, acc 0.82\n",
      "2018-04-21T23:50:34.480817: step 1599, loss 0.39251, acc 0.82\n",
      "2018-04-21T23:50:34.530980: step 1600, loss 0.308965, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:35.207622: step 1600, loss 0.427174, acc 0.82963, rec 0.904762, pre 0.78553, f1 0.840941\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-1600\n",
      "\n",
      "2018-04-21T23:50:35.298739: step 1601, loss 0.461157, acc 0.9\n",
      "2018-04-21T23:50:35.345948: step 1602, loss 0.266955, acc 0.92\n",
      "2018-04-21T23:50:35.391702: step 1603, loss 0.319819, acc 0.86\n",
      "2018-04-21T23:50:35.441527: step 1604, loss 0.309845, acc 0.82\n",
      "2018-04-21T23:50:35.488033: step 1605, loss 0.295709, acc 0.9\n",
      "2018-04-21T23:50:35.534084: step 1606, loss 0.47365, acc 0.84\n",
      "2018-04-21T23:50:35.578980: step 1607, loss 0.420331, acc 0.82\n",
      "2018-04-21T23:50:35.623663: step 1608, loss 0.418198, acc 0.84\n",
      "2018-04-21T23:50:35.673395: step 1609, loss 0.325778, acc 0.88\n",
      "2018-04-21T23:50:35.719469: step 1610, loss 0.344749, acc 0.86\n",
      "2018-04-21T23:50:35.764526: step 1611, loss 0.299947, acc 0.86\n",
      "2018-04-21T23:50:35.808440: step 1612, loss 0.314183, acc 0.88\n",
      "2018-04-21T23:50:35.853365: step 1613, loss 0.303869, acc 0.88\n",
      "2018-04-21T23:50:35.901440: step 1614, loss 0.254824, acc 0.94\n",
      "2018-04-21T23:50:35.947031: step 1615, loss 0.338227, acc 0.86\n",
      "2018-04-21T23:50:35.992609: step 1616, loss 0.42057, acc 0.86\n",
      "2018-04-21T23:50:36.037699: step 1617, loss 0.35714, acc 0.88\n",
      "2018-04-21T23:50:36.082631: step 1618, loss 0.555395, acc 0.86\n",
      "2018-04-21T23:50:36.133462: step 1619, loss 0.364883, acc 0.88\n",
      "2018-04-21T23:50:36.178237: step 1620, loss 0.340841, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:36.855452: step 1620, loss 0.470081, acc 0.756667, rec 0.953869, pre 0.683005, f1 0.796026\n",
      "\n",
      "2018-04-21T23:50:36.901578: step 1621, loss 0.384213, acc 0.88\n",
      "2018-04-21T23:50:36.947395: step 1622, loss 0.324609, acc 0.9\n",
      "2018-04-21T23:50:36.994001: step 1623, loss 0.26326, acc 0.94\n",
      "2018-04-21T23:50:37.039893: step 1624, loss 0.200277, acc 0.94\n",
      "2018-04-21T23:50:37.090125: step 1625, loss 0.252097, acc 0.92\n",
      "2018-04-21T23:50:37.135622: step 1626, loss 0.293103, acc 0.9\n",
      "2018-04-21T23:50:37.181030: step 1627, loss 0.206812, acc 0.94\n",
      "2018-04-21T23:50:37.226792: step 1628, loss 0.406193, acc 0.8\n",
      "2018-04-21T23:50:37.272354: step 1629, loss 0.253881, acc 0.92\n",
      "2018-04-21T23:50:37.322073: step 1630, loss 0.246127, acc 0.92\n",
      "2018-04-21T23:50:37.367180: step 1631, loss 0.409044, acc 0.84\n",
      "2018-04-21T23:50:37.411012: step 1632, loss 0.325585, acc 0.84\n",
      "2018-04-21T23:50:37.455328: step 1633, loss 0.58226, acc 0.84\n",
      "2018-04-21T23:50:37.500496: step 1634, loss 0.296318, acc 0.88\n",
      "2018-04-21T23:50:37.550616: step 1635, loss 0.395862, acc 0.82\n",
      "2018-04-21T23:50:37.596189: step 1636, loss 0.439725, acc 0.88\n",
      "2018-04-21T23:50:37.640992: step 1637, loss 0.413919, acc 0.86\n",
      "2018-04-21T23:50:37.690058: step 1638, loss 0.490257, acc 0.78\n",
      "2018-04-21T23:50:37.736976: step 1639, loss 0.364744, acc 0.84\n",
      "2018-04-21T23:50:37.786400: step 1640, loss 0.271015, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:38.466396: step 1640, loss 0.757153, acc 0.551481, rec 0.985119, pre 0.526441, f1 0.686188\n",
      "\n",
      "2018-04-21T23:50:38.511741: step 1641, loss 0.52196, acc 0.72\n",
      "2018-04-21T23:50:38.558189: step 1642, loss 0.63335, acc 0.74\n",
      "2018-04-21T23:50:38.604947: step 1643, loss 0.436175, acc 0.74\n",
      "2018-04-21T23:50:38.650163: step 1644, loss 0.745264, acc 0.76\n",
      "2018-04-21T23:50:38.700936: step 1645, loss 0.371518, acc 0.92\n",
      "2018-04-21T23:50:38.747941: step 1646, loss 0.203723, acc 0.94\n",
      "2018-04-21T23:50:38.792384: step 1647, loss 0.325229, acc 0.86\n",
      "2018-04-21T23:50:38.837000: step 1648, loss 0.361682, acc 0.86\n",
      "2018-04-21T23:50:38.881533: step 1649, loss 0.586925, acc 0.82\n",
      "2018-04-21T23:50:38.929877: step 1650, loss 0.502274, acc 0.78\n",
      "2018-04-21T23:50:38.975529: step 1651, loss 0.390146, acc 0.8\n",
      "2018-04-21T23:50:39.020248: step 1652, loss 0.321725, acc 0.84\n",
      "2018-04-21T23:50:39.067190: step 1653, loss 0.487422, acc 0.78\n",
      "2018-04-21T23:50:39.111893: step 1654, loss 0.436896, acc 0.72\n",
      "2018-04-21T23:50:39.160396: step 1655, loss 0.442738, acc 0.9\n",
      "2018-04-21T23:50:39.205582: step 1656, loss 0.347039, acc 0.86\n",
      "2018-04-21T23:50:39.250736: step 1657, loss 0.249499, acc 0.9\n",
      "2018-04-21T23:50:39.295516: step 1658, loss 0.273954, acc 0.9\n",
      "2018-04-21T23:50:39.339919: step 1659, loss 0.244314, acc 0.96\n",
      "2018-04-21T23:50:39.388806: step 1660, loss 0.227917, acc 0.96\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:40.029073: step 1660, loss 0.553433, acc 0.661111, rec 0.976935, pre 0.597633, f1 0.741598\n",
      "\n",
      "2018-04-21T23:50:40.072694: step 1661, loss 0.41324, acc 0.88\n",
      "2018-04-21T23:50:40.117170: step 1662, loss 0.220836, acc 0.96\n",
      "2018-04-21T23:50:40.161115: step 1663, loss 0.401312, acc 0.92\n",
      "2018-04-21T23:50:40.204965: step 1664, loss 0.521519, acc 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:50:40.251782: step 1665, loss 0.32932, acc 0.88\n",
      "2018-04-21T23:50:40.295762: step 1666, loss 0.51103, acc 0.82\n",
      "2018-04-21T23:50:40.339165: step 1667, loss 0.268647, acc 0.9\n",
      "2018-04-21T23:50:40.382338: step 1668, loss 0.235302, acc 0.92\n",
      "2018-04-21T23:50:40.425651: step 1669, loss 0.398798, acc 0.84\n",
      "2018-04-21T23:50:40.472165: step 1670, loss 0.311116, acc 0.88\n",
      "2018-04-21T23:50:40.516230: step 1671, loss 0.282659, acc 0.9\n",
      "2018-04-21T23:50:40.561339: step 1672, loss 0.291957, acc 0.92\n",
      "2018-04-21T23:50:40.604887: step 1673, loss 0.559073, acc 0.78\n",
      "2018-04-21T23:50:40.648800: step 1674, loss 0.348417, acc 0.8\n",
      "2018-04-21T23:50:40.696431: step 1675, loss 0.191579, acc 0.98\n",
      "2018-04-21T23:50:40.742931: step 1676, loss 0.319594, acc 0.88\n",
      "2018-04-21T23:50:40.786573: step 1677, loss 0.573993, acc 0.76\n",
      "2018-04-21T23:50:40.829869: step 1678, loss 0.352958, acc 0.88\n",
      "2018-04-21T23:50:40.873268: step 1679, loss 0.672876, acc 0.84\n",
      "2018-04-21T23:50:40.919717: step 1680, loss 0.429816, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:41.553852: step 1680, loss 0.467506, acc 0.763333, rec 0.953869, pre 0.689618, f1 0.8005\n",
      "\n",
      "2018-04-21T23:50:41.597401: step 1681, loss 0.408448, acc 0.94\n",
      "2018-04-21T23:50:41.640950: step 1682, loss 0.293491, acc 0.88\n",
      "2018-04-21T23:50:41.685250: step 1683, loss 0.295502, acc 0.9\n",
      "2018-04-21T23:50:41.729049: step 1684, loss 0.309254, acc 0.9\n",
      "2018-04-21T23:50:41.777165: step 1685, loss 0.390113, acc 0.82\n",
      "2018-04-21T23:50:41.820520: step 1686, loss 0.534106, acc 0.86\n",
      "2018-04-21T23:50:41.863554: step 1687, loss 0.200767, acc 0.92\n",
      "2018-04-21T23:50:41.906657: step 1688, loss 0.345838, acc 0.9\n",
      "2018-04-21T23:50:41.950166: step 1689, loss 0.269732, acc 0.9\n",
      "2018-04-21T23:50:41.997739: step 1690, loss 0.268933, acc 0.86\n",
      "2018-04-21T23:50:42.041020: step 1691, loss 0.418971, acc 0.8\n",
      "2018-04-21T23:50:42.085309: step 1692, loss 0.332782, acc 0.82\n",
      "2018-04-21T23:50:42.129595: step 1693, loss 0.205495, acc 0.96\n",
      "2018-04-21T23:50:42.173415: step 1694, loss 0.338406, acc 0.84\n",
      "2018-04-21T23:50:42.220400: step 1695, loss 0.306607, acc 0.9\n",
      "2018-04-21T23:50:42.264066: step 1696, loss 0.21675, acc 0.94\n",
      "2018-04-21T23:50:42.308160: step 1697, loss 0.379499, acc 0.8\n",
      "2018-04-21T23:50:42.352003: step 1698, loss 0.2629, acc 0.86\n",
      "2018-04-21T23:50:42.395843: step 1699, loss 0.339089, acc 0.82\n",
      "2018-04-21T23:50:42.442306: step 1700, loss 0.239418, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:43.074092: step 1700, loss 0.469412, acc 0.754815, rec 0.953125, pre 0.681383, f1 0.794665\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-1700\n",
      "\n",
      "2018-04-21T23:50:43.158782: step 1701, loss 0.453807, acc 0.84\n",
      "2018-04-21T23:50:43.201979: step 1702, loss 0.434992, acc 0.82\n",
      "2018-04-21T23:50:43.244926: step 1703, loss 0.343199, acc 0.82\n",
      "2018-04-21T23:50:43.296000: step 1704, loss 0.270643, acc 0.92\n",
      "2018-04-21T23:50:43.339684: step 1705, loss 0.287438, acc 0.84\n",
      "2018-04-21T23:50:43.383411: step 1706, loss 0.482399, acc 0.8\n",
      "2018-04-21T23:50:43.426745: step 1707, loss 0.365292, acc 0.92\n",
      "2018-04-21T23:50:43.470026: step 1708, loss 0.235347, acc 0.94\n",
      "2018-04-21T23:50:43.515851: step 1709, loss 0.408663, acc 0.82\n",
      "2018-04-21T23:50:43.559245: step 1710, loss 0.286942, acc 0.86\n",
      "2018-04-21T23:50:43.604017: step 1711, loss 0.217603, acc 0.94\n",
      "2018-04-21T23:50:43.647657: step 1712, loss 0.204273, acc 0.9\n",
      "2018-04-21T23:50:43.691026: step 1713, loss 0.180956, acc 0.98\n",
      "2018-04-21T23:50:43.739633: step 1714, loss 0.316194, acc 0.9\n",
      "2018-04-21T23:50:43.783846: step 1715, loss 0.52921, acc 0.9\n",
      "2018-04-21T23:50:43.827361: step 1716, loss 0.441089, acc 0.9\n",
      "2018-04-21T23:50:43.871368: step 1717, loss 0.322407, acc 0.92\n",
      "2018-04-21T23:50:43.915104: step 1718, loss 0.295752, acc 0.88\n",
      "2018-04-21T23:50:43.962145: step 1719, loss 0.22833, acc 0.92\n",
      "2018-04-21T23:50:44.005753: step 1720, loss 0.223045, acc 0.96\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:44.636717: step 1720, loss 0.497947, acc 0.719259, rec 0.973214, pre 0.644335, f1 0.775341\n",
      "\n",
      "2018-04-21T23:50:44.679316: step 1721, loss 0.583629, acc 0.82\n",
      "2018-04-21T23:50:44.723116: step 1722, loss 0.286573, acc 0.88\n",
      "2018-04-21T23:50:44.767996: step 1723, loss 0.240169, acc 0.92\n",
      "2018-04-21T23:50:44.811819: step 1724, loss 0.619038, acc 0.78\n",
      "2018-04-21T23:50:44.859593: step 1725, loss 0.330932, acc 0.84\n",
      "2018-04-21T23:50:44.903906: step 1726, loss 0.521496, acc 0.72\n",
      "2018-04-21T23:50:44.947630: step 1727, loss 0.733496, acc 0.66\n",
      "2018-04-21T23:50:44.990934: step 1728, loss 0.716826, acc 0.66\n",
      "2018-04-21T23:50:45.034409: step 1729, loss 0.337563, acc 0.86\n",
      "2018-04-21T23:50:45.080330: step 1730, loss 0.314549, acc 0.88\n",
      "2018-04-21T23:50:45.123565: step 1731, loss 0.356733, acc 0.84\n",
      "2018-04-21T23:50:45.168368: step 1732, loss 0.50611, acc 0.82\n",
      "2018-04-21T23:50:45.216152: step 1733, loss 0.527868, acc 0.78\n",
      "2018-04-21T23:50:45.264336: step 1734, loss 0.257186, acc 0.88\n",
      "2018-04-21T23:50:45.316526: step 1735, loss 0.29313, acc 0.92\n",
      "2018-04-21T23:50:45.363960: step 1736, loss 0.332394, acc 0.86\n",
      "2018-04-21T23:50:45.411003: step 1737, loss 0.22303, acc 0.94\n",
      "2018-04-21T23:50:45.458420: step 1738, loss 0.238376, acc 0.88\n",
      "2018-04-21T23:50:45.502298: step 1739, loss 0.493329, acc 0.92\n",
      "2018-04-21T23:50:45.552489: step 1740, loss 0.277438, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:46.184177: step 1740, loss 0.415121, acc 0.865556, rec 0.802827, pre 0.916737, f1 0.85601\n",
      "\n",
      "2018-04-21T23:50:46.227447: step 1741, loss 0.417598, acc 0.88\n",
      "2018-04-21T23:50:46.271417: step 1742, loss 0.379822, acc 0.82\n",
      "2018-04-21T23:50:46.315766: step 1743, loss 0.480175, acc 0.76\n",
      "2018-04-21T23:50:46.360304: step 1744, loss 0.411294, acc 0.82\n",
      "2018-04-21T23:50:46.411448: step 1745, loss 0.190516, acc 0.96\n",
      "2018-04-21T23:50:46.455940: step 1746, loss 0.349861, acc 0.82\n",
      "2018-04-21T23:50:46.499702: step 1747, loss 0.387321, acc 0.82\n",
      "2018-04-21T23:50:46.543196: step 1748, loss 0.988481, acc 0.8\n",
      "2018-04-21T23:50:46.587508: step 1749, loss 0.251578, acc 0.9\n",
      "2018-04-21T23:50:46.634959: step 1750, loss 0.226757, acc 0.9\n",
      "2018-04-21T23:50:46.678618: step 1751, loss 0.406615, acc 0.86\n",
      "2018-04-21T23:50:46.722282: step 1752, loss 0.194074, acc 1\n",
      "2018-04-21T23:50:46.767545: step 1753, loss 0.361042, acc 0.82\n",
      "2018-04-21T23:50:46.812939: step 1754, loss 0.378296, acc 0.84\n",
      "2018-04-21T23:50:46.862510: step 1755, loss 0.468369, acc 0.82\n",
      "2018-04-21T23:50:46.907773: step 1756, loss 0.360782, acc 0.84\n",
      "2018-04-21T23:50:46.951787: step 1757, loss 0.493448, acc 0.92\n",
      "2018-04-21T23:50:46.996140: step 1758, loss 0.263997, acc 0.94\n",
      "2018-04-21T23:50:47.041658: step 1759, loss 0.280805, acc 0.9\n",
      "2018-04-21T23:50:47.091212: step 1760, loss 0.341017, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:47.770109: step 1760, loss 0.48845, acc 0.733704, rec 0.971726, pre 0.657272, f1 0.784149\n",
      "\n",
      "2018-04-21T23:50:47.814287: step 1761, loss 0.29177, acc 0.88\n",
      "2018-04-21T23:50:47.858396: step 1762, loss 0.37596, acc 0.9\n",
      "2018-04-21T23:50:47.902135: step 1763, loss 0.571064, acc 0.88\n",
      "2018-04-21T23:50:47.945856: step 1764, loss 0.329424, acc 0.88\n",
      "2018-04-21T23:50:47.993570: step 1765, loss 0.333414, acc 0.84\n",
      "2018-04-21T23:50:48.037572: step 1766, loss 0.373843, acc 0.94\n",
      "2018-04-21T23:50:48.081002: step 1767, loss 0.356182, acc 0.78\n",
      "2018-04-21T23:50:48.124936: step 1768, loss 0.282305, acc 0.88\n",
      "2018-04-21T23:50:48.179978: step 1769, loss 0.292073, acc 0.84\n",
      "2018-04-21T23:50:48.227026: step 1770, loss 0.423119, acc 0.9\n",
      "2018-04-21T23:50:48.270930: step 1771, loss 0.23645, acc 0.94\n",
      "2018-04-21T23:50:48.314785: step 1772, loss 0.477093, acc 0.78\n",
      "2018-04-21T23:50:48.359004: step 1773, loss 0.244204, acc 0.9\n",
      "2018-04-21T23:50:48.402680: step 1774, loss 0.245646, acc 0.92\n",
      "2018-04-21T23:50:48.449008: step 1775, loss 0.356276, acc 0.86\n",
      "2018-04-21T23:50:48.492493: step 1776, loss 0.396902, acc 0.86\n",
      "2018-04-21T23:50:48.536323: step 1777, loss 0.404971, acc 0.82\n",
      "2018-04-21T23:50:48.580823: step 1778, loss 0.256922, acc 0.92\n",
      "2018-04-21T23:50:48.624391: step 1779, loss 0.312064, acc 0.86\n",
      "2018-04-21T23:50:48.671388: step 1780, loss 0.30217, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:49.305281: step 1780, loss 0.417944, acc 0.858148, rec 0.770833, pre 0.932493, f1 0.843992\n",
      "\n",
      "2018-04-21T23:50:49.348254: step 1781, loss 0.271087, acc 0.88\n",
      "2018-04-21T23:50:49.391665: step 1782, loss 0.249735, acc 0.94\n",
      "2018-04-21T23:50:49.435780: step 1783, loss 0.426627, acc 0.9\n",
      "2018-04-21T23:50:49.479598: step 1784, loss 0.313962, acc 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:50:49.526861: step 1785, loss 0.317451, acc 0.88\n",
      "2018-04-21T23:50:49.570865: step 1786, loss 0.265419, acc 0.94\n",
      "2018-04-21T23:50:49.615395: step 1787, loss 0.624644, acc 0.88\n",
      "2018-04-21T23:50:49.659622: step 1788, loss 0.276208, acc 0.88\n",
      "2018-04-21T23:50:49.703446: step 1789, loss 0.406602, acc 0.82\n",
      "2018-04-21T23:50:49.753076: step 1790, loss 0.390728, acc 0.82\n",
      "2018-04-21T23:50:49.796563: step 1791, loss 0.426104, acc 0.74\n",
      "2018-04-21T23:50:49.840663: step 1792, loss 0.357584, acc 0.84\n",
      "2018-04-21T23:50:49.884152: step 1793, loss 0.222847, acc 0.94\n",
      "2018-04-21T23:50:49.928489: step 1794, loss 0.386789, acc 0.8\n",
      "2018-04-21T23:50:49.975128: step 1795, loss 0.290509, acc 0.86\n",
      "2018-04-21T23:50:50.018250: step 1796, loss 0.24655, acc 0.94\n",
      "2018-04-21T23:50:50.061488: step 1797, loss 0.377776, acc 0.84\n",
      "2018-04-21T23:50:50.104893: step 1798, loss 0.411072, acc 0.84\n",
      "2018-04-21T23:50:50.148623: step 1799, loss 0.374611, acc 0.84\n",
      "2018-04-21T23:50:50.195838: step 1800, loss 0.341812, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:50.879519: step 1800, loss 0.408526, acc 0.864815, rec 0.877976, pre 0.854453, f1 0.866055\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-1800\n",
      "\n",
      "2018-04-21T23:50:50.966637: step 1801, loss 0.256477, acc 0.92\n",
      "2018-04-21T23:50:51.009816: step 1802, loss 0.303387, acc 0.86\n",
      "2018-04-21T23:50:51.053508: step 1803, loss 0.424678, acc 0.8\n",
      "2018-04-21T23:50:51.100354: step 1804, loss 0.361085, acc 0.88\n",
      "2018-04-21T23:50:51.144112: step 1805, loss 0.45102, acc 0.84\n",
      "2018-04-21T23:50:51.188077: step 1806, loss 0.248686, acc 0.9\n",
      "2018-04-21T23:50:51.232243: step 1807, loss 0.289273, acc 0.9\n",
      "2018-04-21T23:50:51.276358: step 1808, loss 0.510379, acc 0.9\n",
      "2018-04-21T23:50:51.324592: step 1809, loss 0.201041, acc 0.96\n",
      "2018-04-21T23:50:51.368940: step 1810, loss 0.269702, acc 0.88\n",
      "2018-04-21T23:50:51.412335: step 1811, loss 0.567187, acc 0.8\n",
      "2018-04-21T23:50:51.457083: step 1812, loss 0.263975, acc 0.86\n",
      "2018-04-21T23:50:51.501758: step 1813, loss 0.328793, acc 0.86\n",
      "2018-04-21T23:50:51.549163: step 1814, loss 0.355016, acc 0.94\n",
      "2018-04-21T23:50:51.593695: step 1815, loss 0.289077, acc 0.88\n",
      "2018-04-21T23:50:51.637562: step 1816, loss 0.300004, acc 0.92\n",
      "2018-04-21T23:50:51.682140: step 1817, loss 0.323476, acc 0.84\n",
      "2018-04-21T23:50:51.726236: step 1818, loss 0.312292, acc 0.9\n",
      "2018-04-21T23:50:51.776547: step 1819, loss 0.269208, acc 0.92\n",
      "2018-04-21T23:50:51.821924: step 1820, loss 0.312798, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:52.506296: step 1820, loss 0.411735, acc 0.85037, rec 0.893601, pre 0.821477, f1 0.856023\n",
      "\n",
      "2018-04-21T23:50:52.551105: step 1821, loss 0.369041, acc 0.86\n",
      "2018-04-21T23:50:52.596140: step 1822, loss 0.24663, acc 0.92\n",
      "2018-04-21T23:50:52.641129: step 1823, loss 0.178506, acc 0.94\n",
      "2018-04-21T23:50:52.686465: step 1824, loss 0.686813, acc 0.8\n",
      "2018-04-21T23:50:52.737548: step 1825, loss 0.445333, acc 0.74\n",
      "2018-04-21T23:50:52.783733: step 1826, loss 0.285859, acc 0.86\n",
      "2018-04-21T23:50:52.829986: step 1827, loss 0.241452, acc 0.9\n",
      "2018-04-21T23:50:52.875405: step 1828, loss 0.313311, acc 0.88\n",
      "2018-04-21T23:50:52.922131: step 1829, loss 0.534817, acc 0.88\n",
      "2018-04-21T23:50:52.971339: step 1830, loss 0.226635, acc 0.94\n",
      "2018-04-21T23:50:53.016587: step 1831, loss 0.323544, acc 0.86\n",
      "2018-04-21T23:50:53.062660: step 1832, loss 0.321008, acc 0.88\n",
      "2018-04-21T23:50:53.109341: step 1833, loss 0.299838, acc 0.9\n",
      "2018-04-21T23:50:53.154367: step 1834, loss 0.286534, acc 0.96\n",
      "2018-04-21T23:50:53.203170: step 1835, loss 0.394135, acc 0.86\n",
      "2018-04-21T23:50:53.248475: step 1836, loss 0.34766, acc 0.86\n",
      "2018-04-21T23:50:53.293175: step 1837, loss 0.497362, acc 0.84\n",
      "2018-04-21T23:50:53.338495: step 1838, loss 0.324927, acc 0.86\n",
      "2018-04-21T23:50:53.382702: step 1839, loss 0.261878, acc 0.88\n",
      "2018-04-21T23:50:53.431580: step 1840, loss 0.219438, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:54.112154: step 1840, loss 0.437242, acc 0.797037, rec 0.93006, pre 0.733568, f1 0.82021\n",
      "\n",
      "2018-04-21T23:50:54.156344: step 1841, loss 0.305088, acc 0.82\n",
      "2018-04-21T23:50:54.200516: step 1842, loss 0.29922, acc 0.82\n",
      "2018-04-21T23:50:54.244556: step 1843, loss 0.228641, acc 0.9\n",
      "2018-04-21T23:50:54.288476: step 1844, loss 0.308633, acc 0.84\n",
      "2018-04-21T23:50:54.335935: step 1845, loss 0.395102, acc 0.86\n",
      "2018-04-21T23:50:54.380126: step 1846, loss 0.56674, acc 0.84\n",
      "2018-04-21T23:50:54.424784: step 1847, loss 0.280771, acc 0.9\n",
      "2018-04-21T23:50:54.470816: step 1848, loss 0.360133, acc 0.88\n",
      "2018-04-21T23:50:54.517597: step 1849, loss 0.175157, acc 0.98\n",
      "2018-04-21T23:50:54.566210: step 1850, loss 0.380432, acc 0.84\n",
      "2018-04-21T23:50:54.612343: step 1851, loss 0.287549, acc 0.9\n",
      "2018-04-21T23:50:54.658912: step 1852, loss 0.440633, acc 0.84\n",
      "2018-04-21T23:50:54.703139: step 1853, loss 0.452065, acc 0.84\n",
      "2018-04-21T23:50:54.747734: step 1854, loss 0.356701, acc 0.92\n",
      "2018-04-21T23:50:54.797137: step 1855, loss 0.569998, acc 0.74\n",
      "2018-04-21T23:50:54.841132: step 1856, loss 0.222482, acc 0.94\n",
      "2018-04-21T23:50:54.884525: step 1857, loss 0.180526, acc 0.96\n",
      "2018-04-21T23:50:54.927996: step 1858, loss 0.173474, acc 0.96\n",
      "2018-04-21T23:50:54.972718: step 1859, loss 0.631541, acc 0.88\n",
      "2018-04-21T23:50:55.019846: step 1860, loss 0.241667, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:55.656632: step 1860, loss 0.695664, acc 0.575556, rec 0.985119, pre 0.540408, f1 0.697944\n",
      "\n",
      "2018-04-21T23:50:55.700499: step 1861, loss 0.326501, acc 0.82\n",
      "2018-04-21T23:50:55.744946: step 1862, loss 0.234987, acc 0.92\n",
      "2018-04-21T23:50:55.791013: step 1863, loss 0.281941, acc 0.9\n",
      "2018-04-21T23:50:55.836069: step 1864, loss 0.72183, acc 0.84\n",
      "2018-04-21T23:50:55.884728: step 1865, loss 0.19562, acc 0.94\n",
      "2018-04-21T23:50:55.928828: step 1866, loss 0.484569, acc 0.82\n",
      "2018-04-21T23:50:55.972859: step 1867, loss 0.32906, acc 0.82\n",
      "2018-04-21T23:50:56.016595: step 1868, loss 0.491134, acc 0.8\n",
      "2018-04-21T23:50:56.060344: step 1869, loss 0.426859, acc 0.76\n",
      "2018-04-21T23:50:56.107665: step 1870, loss 0.322656, acc 0.8\n",
      "2018-04-21T23:50:56.151815: step 1871, loss 0.31076, acc 0.86\n",
      "2018-04-21T23:50:56.195832: step 1872, loss 0.256445, acc 0.88\n",
      "2018-04-21T23:50:56.239177: step 1873, loss 0.332258, acc 0.86\n",
      "2018-04-21T23:50:56.282788: step 1874, loss 0.200254, acc 0.94\n",
      "2018-04-21T23:50:56.330427: step 1875, loss 0.210802, acc 0.94\n",
      "2018-04-21T23:50:56.374241: step 1876, loss 0.487458, acc 0.9\n",
      "2018-04-21T23:50:56.417812: step 1877, loss 0.268795, acc 0.94\n",
      "2018-04-21T23:50:56.461365: step 1878, loss 0.266306, acc 0.92\n",
      "2018-04-21T23:50:56.505819: step 1879, loss 0.218439, acc 0.88\n",
      "2018-04-21T23:50:56.552823: step 1880, loss 0.595867, acc 0.8\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:57.183461: step 1880, loss 0.421314, acc 0.838519, rec 0.718006, pre 0.944227, f1 0.815723\n",
      "\n",
      "2018-04-21T23:50:57.226427: step 1881, loss 0.341018, acc 0.9\n",
      "2018-04-21T23:50:57.269754: step 1882, loss 0.371541, acc 0.82\n",
      "2018-04-21T23:50:57.313177: step 1883, loss 0.178125, acc 0.98\n",
      "2018-04-21T23:50:57.356863: step 1884, loss 0.440877, acc 0.74\n",
      "2018-04-21T23:50:57.404031: step 1885, loss 0.317093, acc 0.86\n",
      "2018-04-21T23:50:57.447078: step 1886, loss 0.236237, acc 0.96\n",
      "2018-04-21T23:50:57.490056: step 1887, loss 0.291356, acc 0.92\n",
      "2018-04-21T23:50:57.533703: step 1888, loss 0.286025, acc 0.88\n",
      "2018-04-21T23:50:57.577441: step 1889, loss 0.347943, acc 0.78\n",
      "2018-04-21T23:50:57.625072: step 1890, loss 0.367163, acc 0.86\n",
      "2018-04-21T23:50:57.670281: step 1891, loss 0.346833, acc 0.84\n",
      "2018-04-21T23:50:57.714599: step 1892, loss 0.240743, acc 0.88\n",
      "2018-04-21T23:50:57.757939: step 1893, loss 0.55918, acc 0.76\n",
      "2018-04-21T23:50:57.801714: step 1894, loss 0.431641, acc 0.84\n",
      "2018-04-21T23:50:57.848324: step 1895, loss 0.297399, acc 0.9\n",
      "2018-04-21T23:50:57.892407: step 1896, loss 0.22291, acc 0.9\n",
      "2018-04-21T23:50:57.935799: step 1897, loss 0.381428, acc 0.86\n",
      "2018-04-21T23:50:57.980004: step 1898, loss 0.233602, acc 0.92\n",
      "2018-04-21T23:50:58.023603: step 1899, loss 0.275173, acc 0.9\n",
      "2018-04-21T23:50:58.070838: step 1900, loss 0.210031, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:50:58.701082: step 1900, loss 0.469996, acc 0.752222, rec 0.965774, pre 0.67569, f1 0.7951\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-1900\n",
      "\n",
      "2018-04-21T23:50:58.790530: step 1901, loss 0.242453, acc 0.88\n",
      "2018-04-21T23:50:58.834225: step 1902, loss 0.634418, acc 0.88\n",
      "2018-04-21T23:50:58.878683: step 1903, loss 0.454065, acc 0.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:50:58.925095: step 1904, loss 0.319489, acc 0.84\n",
      "2018-04-21T23:50:58.968578: step 1905, loss 0.276183, acc 0.86\n",
      "2018-04-21T23:50:59.011812: step 1906, loss 0.155586, acc 0.96\n",
      "2018-04-21T23:50:59.055515: step 1907, loss 0.244913, acc 0.92\n",
      "2018-04-21T23:50:59.099389: step 1908, loss 0.287861, acc 0.86\n",
      "2018-04-21T23:50:59.147963: step 1909, loss 0.465263, acc 0.9\n",
      "2018-04-21T23:50:59.192051: step 1910, loss 0.176542, acc 0.96\n",
      "2018-04-21T23:50:59.235988: step 1911, loss 0.406089, acc 0.86\n",
      "2018-04-21T23:50:59.280008: step 1912, loss 0.27448, acc 0.9\n",
      "2018-04-21T23:50:59.324504: step 1913, loss 0.303423, acc 0.86\n",
      "2018-04-21T23:50:59.371095: step 1914, loss 0.351171, acc 0.86\n",
      "2018-04-21T23:50:59.414569: step 1915, loss 0.474132, acc 0.8\n",
      "2018-04-21T23:50:59.457941: step 1916, loss 0.378098, acc 0.82\n",
      "2018-04-21T23:50:59.502629: step 1917, loss 0.227824, acc 0.94\n",
      "2018-04-21T23:50:59.546357: step 1918, loss 0.193614, acc 0.92\n",
      "2018-04-21T23:50:59.593935: step 1919, loss 0.448791, acc 0.84\n",
      "2018-04-21T23:50:59.637927: step 1920, loss 0.395367, acc 0.8\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:00.269708: step 1920, loss 0.56371, acc 0.680741, rec 0.36756, pre 0.976285, f1 0.534054\n",
      "\n",
      "2018-04-21T23:51:00.314028: step 1921, loss 0.521517, acc 0.72\n",
      "2018-04-21T23:51:00.359975: step 1922, loss 0.335713, acc 0.86\n",
      "2018-04-21T23:51:00.403625: step 1923, loss 0.225661, acc 0.94\n",
      "2018-04-21T23:51:00.447947: step 1924, loss 0.218405, acc 0.96\n",
      "2018-04-21T23:51:00.493956: step 1925, loss 0.319014, acc 0.86\n",
      "2018-04-21T23:51:00.537573: step 1926, loss 0.35483, acc 0.88\n",
      "2018-04-21T23:51:00.581692: step 1927, loss 0.356475, acc 0.78\n",
      "2018-04-21T23:51:00.625469: step 1928, loss 0.183341, acc 0.92\n",
      "2018-04-21T23:51:00.668863: step 1929, loss 0.365404, acc 0.88\n",
      "2018-04-21T23:51:00.716069: step 1930, loss 0.285382, acc 0.88\n",
      "2018-04-21T23:51:00.759706: step 1931, loss 0.500554, acc 0.86\n",
      "2018-04-21T23:51:00.802988: step 1932, loss 0.553775, acc 0.88\n",
      "2018-04-21T23:51:00.846673: step 1933, loss 0.302989, acc 0.88\n",
      "2018-04-21T23:51:00.891654: step 1934, loss 0.272687, acc 0.92\n",
      "2018-04-21T23:51:00.938045: step 1935, loss 0.433797, acc 0.82\n",
      "2018-04-21T23:51:00.982325: step 1936, loss 0.280482, acc 0.86\n",
      "2018-04-21T23:51:01.025527: step 1937, loss 0.261496, acc 0.94\n",
      "2018-04-21T23:51:01.068923: step 1938, loss 0.254657, acc 0.9\n",
      "2018-04-21T23:51:01.112874: step 1939, loss 0.29453, acc 0.88\n",
      "2018-04-21T23:51:01.159161: step 1940, loss 0.69846, acc 0.8\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:01.807408: step 1940, loss 0.627771, acc 0.615556, rec 0.980655, pre 0.565665, f1 0.717474\n",
      "\n",
      "2018-04-21T23:51:01.851426: step 1941, loss 0.248718, acc 0.9\n",
      "2018-04-21T23:51:01.895122: step 1942, loss 0.431735, acc 0.82\n",
      "2018-04-21T23:51:01.942861: step 1943, loss 0.349781, acc 0.82\n",
      "2018-04-21T23:51:01.985965: step 1944, loss 0.276712, acc 0.88\n",
      "2018-04-21T23:51:02.034251: step 1945, loss 0.335361, acc 0.88\n",
      "2018-04-21T23:51:02.079761: step 1946, loss 0.305201, acc 0.82\n",
      "2018-04-21T23:51:02.123924: step 1947, loss 0.277281, acc 0.84\n",
      "2018-04-21T23:51:02.167958: step 1948, loss 0.242965, acc 0.92\n",
      "2018-04-21T23:51:02.211434: step 1949, loss 0.242274, acc 0.92\n",
      "2018-04-21T23:51:02.259512: step 1950, loss 0.206923, acc 0.96\n",
      "2018-04-21T23:51:02.303277: step 1951, loss 0.364469, acc 0.82\n",
      "2018-04-21T23:51:02.347792: step 1952, loss 0.296106, acc 0.84\n",
      "2018-04-21T23:51:02.391635: step 1953, loss 0.270026, acc 0.84\n",
      "2018-04-21T23:51:02.435594: step 1954, loss 0.196834, acc 0.94\n",
      "2018-04-21T23:51:02.483409: step 1955, loss 0.378503, acc 0.88\n",
      "2018-04-21T23:51:02.527158: step 1956, loss 0.203499, acc 0.92\n",
      "2018-04-21T23:51:02.572488: step 1957, loss 0.301106, acc 0.84\n",
      "2018-04-21T23:51:02.616640: step 1958, loss 0.464823, acc 0.8\n",
      "2018-04-21T23:51:02.661240: step 1959, loss 0.219878, acc 0.92\n",
      "2018-04-21T23:51:02.708510: step 1960, loss 0.243789, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:03.340176: step 1960, loss 0.513791, acc 0.7, rec 0.97619, pre 0.627751, f1 0.764123\n",
      "\n",
      "2018-04-21T23:51:03.383550: step 1961, loss 0.280092, acc 0.92\n",
      "2018-04-21T23:51:03.427200: step 1962, loss 0.285885, acc 0.9\n",
      "2018-04-21T23:51:03.471006: step 1963, loss 0.523707, acc 0.88\n",
      "2018-04-21T23:51:03.514318: step 1964, loss 0.415756, acc 0.96\n",
      "2018-04-21T23:51:03.560665: step 1965, loss 0.306285, acc 0.9\n",
      "2018-04-21T23:51:03.604311: step 1966, loss 0.621619, acc 0.9\n",
      "2018-04-21T23:51:03.647777: step 1967, loss 0.248287, acc 0.9\n",
      "2018-04-21T23:51:03.693413: step 1968, loss 0.303372, acc 0.86\n",
      "2018-04-21T23:51:03.738347: step 1969, loss 0.361243, acc 0.9\n",
      "2018-04-21T23:51:03.784484: step 1970, loss 0.285904, acc 0.88\n",
      "2018-04-21T23:51:03.828026: step 1971, loss 0.337888, acc 0.86\n",
      "2018-04-21T23:51:03.871444: step 1972, loss 0.400128, acc 0.86\n",
      "2018-04-21T23:51:03.914699: step 1973, loss 0.479135, acc 0.72\n",
      "2018-04-21T23:51:03.958393: step 1974, loss 0.265218, acc 0.88\n",
      "2018-04-21T23:51:04.005522: step 1975, loss 0.382451, acc 0.9\n",
      "2018-04-21T23:51:04.049847: step 1976, loss 0.345818, acc 0.88\n",
      "2018-04-21T23:51:04.093425: step 1977, loss 0.277857, acc 0.88\n",
      "2018-04-21T23:51:04.136780: step 1978, loss 0.231452, acc 0.94\n",
      "2018-04-21T23:51:04.180137: step 1979, loss 0.308181, acc 0.86\n",
      "2018-04-21T23:51:04.226553: step 1980, loss 0.619698, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:04.857709: step 1980, loss 0.424624, acc 0.811481, rec 0.921131, pre 0.754418, f1 0.829481\n",
      "\n",
      "2018-04-21T23:51:04.901386: step 1981, loss 0.281452, acc 0.9\n",
      "2018-04-21T23:51:04.945299: step 1982, loss 0.344942, acc 0.92\n",
      "2018-04-21T23:51:04.989059: step 1983, loss 0.233636, acc 0.88\n",
      "2018-04-21T23:51:05.033396: step 1984, loss 0.477726, acc 0.9\n",
      "2018-04-21T23:51:05.080837: step 1985, loss 0.261138, acc 0.92\n",
      "2018-04-21T23:51:05.125163: step 1986, loss 0.250417, acc 0.9\n",
      "2018-04-21T23:51:05.169116: step 1987, loss 0.182845, acc 0.96\n",
      "2018-04-21T23:51:05.212800: step 1988, loss 0.309793, acc 0.88\n",
      "2018-04-21T23:51:05.256749: step 1989, loss 0.276069, acc 0.82\n",
      "2018-04-21T23:51:05.303512: step 1990, loss 0.266678, acc 0.84\n",
      "2018-04-21T23:51:05.347604: step 1991, loss 0.369901, acc 0.88\n",
      "2018-04-21T23:51:05.392566: step 1992, loss 0.272845, acc 0.9\n",
      "2018-04-21T23:51:05.436406: step 1993, loss 0.209774, acc 0.9\n",
      "2018-04-21T23:51:05.480589: step 1994, loss 0.573414, acc 0.84\n",
      "2018-04-21T23:51:05.528265: step 1995, loss 0.432114, acc 0.78\n",
      "2018-04-21T23:51:05.574154: step 1996, loss 0.267814, acc 0.92\n",
      "2018-04-21T23:51:05.618140: step 1997, loss 0.291821, acc 0.88\n",
      "2018-04-21T23:51:05.661348: step 1998, loss 0.289377, acc 0.9\n",
      "2018-04-21T23:51:05.705419: step 1999, loss 0.336831, acc 0.88\n",
      "2018-04-21T23:51:05.753310: step 2000, loss 0.650933, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:06.384843: step 2000, loss 0.402163, acc 0.863333, rec 0.873512, pre 0.855062, f1 0.864188\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-2000\n",
      "\n",
      "2018-04-21T23:51:06.470829: step 2001, loss 0.286114, acc 0.88\n",
      "2018-04-21T23:51:06.514663: step 2002, loss 0.250164, acc 0.88\n",
      "2018-04-21T23:51:06.558748: step 2003, loss 0.277453, acc 0.9\n",
      "2018-04-21T23:51:06.605995: step 2004, loss 0.403933, acc 0.88\n",
      "2018-04-21T23:51:06.649780: step 2005, loss 0.491702, acc 0.86\n",
      "2018-04-21T23:51:06.694985: step 2006, loss 0.301734, acc 0.84\n",
      "2018-04-21T23:51:06.739237: step 2007, loss 0.635823, acc 0.8\n",
      "2018-04-21T23:51:06.784513: step 2008, loss 0.255814, acc 0.86\n",
      "2018-04-21T23:51:06.835137: step 2009, loss 0.248361, acc 0.88\n",
      "2018-04-21T23:51:06.879416: step 2010, loss 0.284108, acc 0.88\n",
      "2018-04-21T23:51:06.923370: step 2011, loss 0.202392, acc 0.94\n",
      "2018-04-21T23:51:06.968389: step 2012, loss 0.493525, acc 0.76\n",
      "2018-04-21T23:51:07.013096: step 2013, loss 0.532675, acc 0.72\n",
      "2018-04-21T23:51:07.060770: step 2014, loss 0.284368, acc 0.86\n",
      "2018-04-21T23:51:07.104572: step 2015, loss 0.322913, acc 0.82\n",
      "2018-04-21T23:51:07.149014: step 2016, loss 0.244919, acc 0.94\n",
      "2018-04-21T23:51:07.192068: step 2017, loss 0.174549, acc 0.96\n",
      "2018-04-21T23:51:07.235716: step 2018, loss 0.256021, acc 0.9\n",
      "2018-04-21T23:51:07.282611: step 2019, loss 0.306344, acc 0.86\n",
      "2018-04-21T23:51:07.326717: step 2020, loss 0.316937, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:07.956560: step 2020, loss 0.762095, acc 0.564074, rec 0.985119, pre 0.533656, f1 0.692288\n",
      "\n",
      "2018-04-21T23:51:07.999609: step 2021, loss 0.199293, acc 0.94\n",
      "2018-04-21T23:51:08.043979: step 2022, loss 0.252653, acc 0.92\n",
      "2018-04-21T23:51:08.087654: step 2023, loss 0.413953, acc 0.92\n",
      "2018-04-21T23:51:08.131549: step 2024, loss 0.282471, acc 0.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:51:08.178970: step 2025, loss 0.335542, acc 0.86\n",
      "2018-04-21T23:51:08.223659: step 2026, loss 0.324076, acc 0.88\n",
      "2018-04-21T23:51:08.267788: step 2027, loss 0.22627, acc 0.96\n",
      "2018-04-21T23:51:08.312767: step 2028, loss 0.447154, acc 0.92\n",
      "2018-04-21T23:51:08.356349: step 2029, loss 0.148606, acc 0.94\n",
      "2018-04-21T23:51:08.403481: step 2030, loss 0.357503, acc 0.84\n",
      "2018-04-21T23:51:08.447124: step 2031, loss 0.459093, acc 0.96\n",
      "2018-04-21T23:51:08.491371: step 2032, loss 0.253182, acc 0.94\n",
      "2018-04-21T23:51:08.535431: step 2033, loss 0.305936, acc 0.86\n",
      "2018-04-21T23:51:08.580478: step 2034, loss 0.304015, acc 0.9\n",
      "2018-04-21T23:51:08.628805: step 2035, loss 0.519264, acc 0.78\n",
      "2018-04-21T23:51:08.672368: step 2036, loss 0.385035, acc 0.82\n",
      "2018-04-21T23:51:08.716827: step 2037, loss 0.22553, acc 0.94\n",
      "2018-04-21T23:51:08.764450: step 2038, loss 0.324057, acc 0.86\n",
      "2018-04-21T23:51:08.810043: step 2039, loss 0.311563, acc 0.9\n",
      "2018-04-21T23:51:08.860533: step 2040, loss 0.269283, acc 0.82\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:09.542827: step 2040, loss 0.65603, acc 0.605185, rec 0.982887, pre 0.558799, f1 0.712513\n",
      "\n",
      "2018-04-21T23:51:09.588337: step 2041, loss 0.386103, acc 0.82\n",
      "2018-04-21T23:51:09.634673: step 2042, loss 0.415832, acc 0.84\n",
      "2018-04-21T23:51:09.682562: step 2043, loss 0.316937, acc 0.9\n",
      "2018-04-21T23:51:09.727811: step 2044, loss 0.38049, acc 0.84\n",
      "2018-04-21T23:51:09.777099: step 2045, loss 0.330026, acc 0.86\n",
      "2018-04-21T23:51:09.823944: step 2046, loss 0.577648, acc 0.88\n",
      "2018-04-21T23:51:09.869644: step 2047, loss 0.423879, acc 0.78\n",
      "2018-04-21T23:51:09.914296: step 2048, loss 0.241057, acc 0.9\n",
      "2018-04-21T23:51:09.960150: step 2049, loss 0.276141, acc 0.88\n",
      "2018-04-21T23:51:10.010394: step 2050, loss 0.281817, acc 0.9\n",
      "2018-04-21T23:51:10.055555: step 2051, loss 0.288269, acc 0.9\n",
      "2018-04-21T23:51:10.100473: step 2052, loss 0.235556, acc 0.88\n",
      "2018-04-21T23:51:10.145366: step 2053, loss 0.569296, acc 0.84\n",
      "2018-04-21T23:51:10.190129: step 2054, loss 0.444204, acc 0.82\n",
      "2018-04-21T23:51:10.237807: step 2055, loss 0.319506, acc 0.86\n",
      "2018-04-21T23:51:10.281853: step 2056, loss 0.277648, acc 0.86\n",
      "2018-04-21T23:51:10.325107: step 2057, loss 0.224713, acc 0.94\n",
      "2018-04-21T23:51:10.369668: step 2058, loss 0.326117, acc 0.9\n",
      "2018-04-21T23:51:10.415638: step 2059, loss 0.237113, acc 0.9\n",
      "2018-04-21T23:51:10.463837: step 2060, loss 0.16419, acc 0.98\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:11.298724: step 2060, loss 0.453729, acc 0.766667, rec 0.951637, pre 0.693601, f1 0.802384\n",
      "\n",
      "2018-04-21T23:51:11.372458: step 2061, loss 0.492759, acc 0.94\n",
      "2018-04-21T23:51:11.438502: step 2062, loss 0.29514, acc 0.9\n",
      "2018-04-21T23:51:11.513869: step 2063, loss 0.2505, acc 0.9\n",
      "2018-04-21T23:51:11.585257: step 2064, loss 0.30312, acc 0.92\n",
      "2018-04-21T23:51:11.643452: step 2065, loss 0.458949, acc 0.84\n",
      "2018-04-21T23:51:11.688543: step 2066, loss 0.323164, acc 0.82\n",
      "2018-04-21T23:51:11.737002: step 2067, loss 0.317836, acc 0.88\n",
      "2018-04-21T23:51:11.783092: step 2068, loss 0.26756, acc 0.86\n",
      "2018-04-21T23:51:11.827904: step 2069, loss 0.330052, acc 0.86\n",
      "2018-04-21T23:51:11.871465: step 2070, loss 0.235628, acc 0.92\n",
      "2018-04-21T23:51:11.915927: step 2071, loss 0.206148, acc 0.96\n",
      "2018-04-21T23:51:11.962483: step 2072, loss 0.226074, acc 0.94\n",
      "2018-04-21T23:51:12.006613: step 2073, loss 0.159973, acc 0.94\n",
      "2018-04-21T23:51:12.050864: step 2074, loss 0.299397, acc 0.88\n",
      "2018-04-21T23:51:12.094594: step 2075, loss 0.527586, acc 0.82\n",
      "2018-04-21T23:51:12.138596: step 2076, loss 0.310406, acc 0.84\n",
      "2018-04-21T23:51:12.186504: step 2077, loss 0.428699, acc 0.76\n",
      "2018-04-21T23:51:12.230758: step 2078, loss 0.410951, acc 0.88\n",
      "2018-04-21T23:51:12.274387: step 2079, loss 0.245454, acc 0.92\n",
      "2018-04-21T23:51:12.318869: step 2080, loss 0.253582, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:12.954496: step 2080, loss 0.408457, acc 0.836667, rec 0.903274, pre 0.796066, f1 0.846288\n",
      "\n",
      "2018-04-21T23:51:12.998389: step 2081, loss 0.286773, acc 0.88\n",
      "2018-04-21T23:51:13.043005: step 2082, loss 0.247619, acc 0.92\n",
      "2018-04-21T23:51:13.087401: step 2083, loss 0.247362, acc 0.86\n",
      "2018-04-21T23:51:13.131977: step 2084, loss 0.339207, acc 0.88\n",
      "2018-04-21T23:51:13.180827: step 2085, loss 0.378156, acc 0.84\n",
      "2018-04-21T23:51:13.225130: step 2086, loss 0.25109, acc 0.92\n",
      "2018-04-21T23:51:13.269103: step 2087, loss 0.493084, acc 0.82\n",
      "2018-04-21T23:51:13.313460: step 2088, loss 0.293253, acc 0.9\n",
      "2018-04-21T23:51:13.357785: step 2089, loss 0.261545, acc 0.88\n",
      "2018-04-21T23:51:13.404915: step 2090, loss 0.351009, acc 0.86\n",
      "2018-04-21T23:51:13.449194: step 2091, loss 0.599761, acc 0.66\n",
      "2018-04-21T23:51:13.493114: step 2092, loss 0.566379, acc 0.7\n",
      "2018-04-21T23:51:13.536889: step 2093, loss 0.462283, acc 0.74\n",
      "2018-04-21T23:51:13.581259: step 2094, loss 0.351186, acc 0.78\n",
      "2018-04-21T23:51:13.634235: step 2095, loss 0.35341, acc 0.94\n",
      "2018-04-21T23:51:13.678288: step 2096, loss 0.264584, acc 0.92\n",
      "2018-04-21T23:51:13.722350: step 2097, loss 0.195352, acc 0.98\n",
      "2018-04-21T23:51:13.766588: step 2098, loss 0.160656, acc 0.98\n",
      "2018-04-21T23:51:13.810258: step 2099, loss 0.655626, acc 0.84\n",
      "2018-04-21T23:51:13.856730: step 2100, loss 0.295009, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:14.488611: step 2100, loss 0.521146, acc 0.69037, rec 0.975446, pre 0.620151, f1 0.758242\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-2100\n",
      "\n",
      "2018-04-21T23:51:14.578318: step 2101, loss 0.313293, acc 0.88\n",
      "2018-04-21T23:51:14.624022: step 2102, loss 0.257907, acc 0.82\n",
      "2018-04-21T23:51:14.668391: step 2103, loss 0.419327, acc 0.9\n",
      "2018-04-21T23:51:14.716801: step 2104, loss 0.315515, acc 0.86\n",
      "2018-04-21T23:51:14.760818: step 2105, loss 0.223913, acc 0.9\n",
      "2018-04-21T23:51:14.804564: step 2106, loss 0.41504, acc 0.9\n",
      "2018-04-21T23:51:14.848546: step 2107, loss 0.33204, acc 0.82\n",
      "2018-04-21T23:51:14.892597: step 2108, loss 0.353468, acc 0.86\n",
      "2018-04-21T23:51:14.938931: step 2109, loss 0.275291, acc 0.84\n",
      "2018-04-21T23:51:14.982584: step 2110, loss 0.456364, acc 0.92\n",
      "2018-04-21T23:51:15.026259: step 2111, loss 0.623315, acc 0.76\n",
      "2018-04-21T23:51:15.070889: step 2112, loss 0.219049, acc 0.94\n",
      "2018-04-21T23:51:15.115030: step 2113, loss 0.277143, acc 0.92\n",
      "2018-04-21T23:51:15.163317: step 2114, loss 0.37217, acc 0.86\n",
      "2018-04-21T23:51:15.207011: step 2115, loss 0.265335, acc 0.9\n",
      "2018-04-21T23:51:15.250557: step 2116, loss 0.341247, acc 0.96\n",
      "2018-04-21T23:51:15.294552: step 2117, loss 0.299785, acc 0.9\n",
      "2018-04-21T23:51:15.338815: step 2118, loss 0.284718, acc 0.9\n",
      "2018-04-21T23:51:15.386101: step 2119, loss 0.243951, acc 0.94\n",
      "2018-04-21T23:51:15.432016: step 2120, loss 0.324366, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:16.067114: step 2120, loss 0.506421, acc 0.705556, rec 0.973958, pre 0.632673, f1 0.767067\n",
      "\n",
      "2018-04-21T23:51:16.110031: step 2121, loss 0.293993, acc 0.82\n",
      "2018-04-21T23:51:16.154120: step 2122, loss 0.293929, acc 0.86\n",
      "2018-04-21T23:51:16.197863: step 2123, loss 0.282205, acc 0.86\n",
      "2018-04-21T23:51:16.242654: step 2124, loss 0.269333, acc 0.92\n",
      "2018-04-21T23:51:16.290194: step 2125, loss 0.37488, acc 0.9\n",
      "2018-04-21T23:51:16.333890: step 2126, loss 0.212112, acc 0.94\n",
      "2018-04-21T23:51:16.377380: step 2127, loss 0.375551, acc 0.86\n",
      "2018-04-21T23:51:16.421071: step 2128, loss 0.253648, acc 0.88\n",
      "2018-04-21T23:51:16.464932: step 2129, loss 0.350848, acc 0.88\n",
      "2018-04-21T23:51:16.512064: step 2130, loss 0.192435, acc 0.9\n",
      "2018-04-21T23:51:16.557912: step 2131, loss 0.264225, acc 0.9\n",
      "2018-04-21T23:51:16.601976: step 2132, loss 0.294234, acc 0.88\n",
      "2018-04-21T23:51:16.646051: step 2133, loss 0.192876, acc 0.96\n",
      "2018-04-21T23:51:16.690210: step 2134, loss 0.196234, acc 0.92\n",
      "2018-04-21T23:51:16.738101: step 2135, loss 0.306993, acc 0.9\n",
      "2018-04-21T23:51:16.783068: step 2136, loss 0.334995, acc 0.92\n",
      "2018-04-21T23:51:16.827784: step 2137, loss 0.124464, acc 1\n",
      "2018-04-21T23:51:16.871916: step 2138, loss 0.225154, acc 0.92\n",
      "2018-04-21T23:51:16.916647: step 2139, loss 0.186886, acc 0.94\n",
      "2018-04-21T23:51:16.965032: step 2140, loss 0.286493, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:17.602226: step 2140, loss 0.504144, acc 0.712593, rec 0.973958, pre 0.638537, f1 0.771361\n",
      "\n",
      "2018-04-21T23:51:17.645424: step 2141, loss 0.219022, acc 0.94\n",
      "2018-04-21T23:51:17.688830: step 2142, loss 0.260235, acc 0.88\n",
      "2018-04-21T23:51:17.732975: step 2143, loss 0.339935, acc 0.9\n",
      "2018-04-21T23:51:17.778151: step 2144, loss 0.251038, acc 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:51:17.830610: step 2145, loss 0.302575, acc 0.86\n",
      "2018-04-21T23:51:17.903836: step 2146, loss 0.942131, acc 0.76\n",
      "2018-04-21T23:51:17.978459: step 2147, loss 0.507921, acc 0.74\n",
      "2018-04-21T23:51:18.057922: step 2148, loss 0.374947, acc 0.84\n",
      "2018-04-21T23:51:18.134061: step 2149, loss 0.479687, acc 0.84\n",
      "2018-04-21T23:51:18.188207: step 2150, loss 0.274498, acc 0.92\n",
      "2018-04-21T23:51:18.236973: step 2151, loss 0.325866, acc 0.9\n",
      "2018-04-21T23:51:18.305768: step 2152, loss 0.432075, acc 0.9\n",
      "2018-04-21T23:51:18.380158: step 2153, loss 0.320794, acc 0.88\n",
      "2018-04-21T23:51:18.454492: step 2154, loss 0.251982, acc 0.84\n",
      "2018-04-21T23:51:18.500627: step 2155, loss 0.598949, acc 0.74\n",
      "2018-04-21T23:51:18.550734: step 2156, loss 0.81539, acc 0.62\n",
      "2018-04-21T23:51:18.620349: step 2157, loss 0.301374, acc 0.8\n",
      "2018-04-21T23:51:18.670645: step 2158, loss 0.321618, acc 0.86\n",
      "2018-04-21T23:51:18.717820: step 2159, loss 0.296378, acc 0.9\n",
      "2018-04-21T23:51:18.767574: step 2160, loss 0.344318, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:19.451082: step 2160, loss 0.485845, acc 0.75963, rec 0.536458, pre 0.965194, f1 0.689622\n",
      "\n",
      "2018-04-21T23:51:19.496275: step 2161, loss 0.405421, acc 0.84\n",
      "2018-04-21T23:51:19.540879: step 2162, loss 0.440271, acc 0.82\n",
      "2018-04-21T23:51:19.585239: step 2163, loss 0.238623, acc 0.92\n",
      "2018-04-21T23:51:19.629436: step 2164, loss 0.414469, acc 0.84\n",
      "2018-04-21T23:51:19.678554: step 2165, loss 0.26881, acc 0.94\n",
      "2018-04-21T23:51:19.724097: step 2166, loss 0.277109, acc 0.88\n",
      "2018-04-21T23:51:19.769371: step 2167, loss 0.334643, acc 0.86\n",
      "2018-04-21T23:51:19.814768: step 2168, loss 0.229929, acc 0.9\n",
      "2018-04-21T23:51:19.860519: step 2169, loss 0.325766, acc 0.88\n",
      "2018-04-21T23:51:19.909537: step 2170, loss 0.243547, acc 0.92\n",
      "2018-04-21T23:51:19.955071: step 2171, loss 0.380397, acc 0.86\n",
      "2018-04-21T23:51:20.000879: step 2172, loss 0.19801, acc 0.92\n",
      "2018-04-21T23:51:20.046836: step 2173, loss 0.273182, acc 0.88\n",
      "2018-04-21T23:51:20.091773: step 2174, loss 0.225365, acc 0.92\n",
      "2018-04-21T23:51:20.140740: step 2175, loss 0.217689, acc 0.9\n",
      "2018-04-21T23:51:20.186340: step 2176, loss 0.234727, acc 0.96\n",
      "2018-04-21T23:51:20.231288: step 2177, loss 0.179839, acc 0.96\n",
      "2018-04-21T23:51:20.276937: step 2178, loss 0.46138, acc 0.84\n",
      "2018-04-21T23:51:20.322981: step 2179, loss 0.342969, acc 0.82\n",
      "2018-04-21T23:51:20.372589: step 2180, loss 0.271262, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:21.056193: step 2180, loss 0.562913, acc 0.659259, rec 0.978423, pre 0.596102, f1 0.740845\n",
      "\n",
      "2018-04-21T23:51:21.100539: step 2181, loss 0.152558, acc 0.96\n",
      "2018-04-21T23:51:21.147460: step 2182, loss 0.658923, acc 0.86\n",
      "2018-04-21T23:51:21.192403: step 2183, loss 0.370514, acc 0.86\n",
      "2018-04-21T23:51:21.237421: step 2184, loss 0.401547, acc 0.86\n",
      "2018-04-21T23:51:21.287325: step 2185, loss 0.342927, acc 0.92\n",
      "2018-04-21T23:51:21.331800: step 2186, loss 0.589083, acc 0.9\n",
      "2018-04-21T23:51:21.376775: step 2187, loss 0.209121, acc 0.94\n",
      "2018-04-21T23:51:21.422764: step 2188, loss 0.206468, acc 0.96\n",
      "2018-04-21T23:51:21.467462: step 2189, loss 0.433441, acc 0.82\n",
      "2018-04-21T23:51:21.518171: step 2190, loss 0.282415, acc 0.84\n",
      "2018-04-21T23:51:21.565023: step 2191, loss 0.356984, acc 0.82\n",
      "2018-04-21T23:51:21.610998: step 2192, loss 0.326432, acc 0.92\n",
      "2018-04-21T23:51:21.656289: step 2193, loss 0.208662, acc 0.96\n",
      "2018-04-21T23:51:21.703729: step 2194, loss 0.290037, acc 0.84\n",
      "2018-04-21T23:51:21.753072: step 2195, loss 0.451213, acc 0.8\n",
      "2018-04-21T23:51:21.798452: step 2196, loss 0.373599, acc 0.82\n",
      "2018-04-21T23:51:21.843567: step 2197, loss 0.405991, acc 0.8\n",
      "2018-04-21T23:51:21.887773: step 2198, loss 0.273166, acc 0.82\n",
      "2018-04-21T23:51:21.932226: step 2199, loss 0.270965, acc 0.9\n",
      "2018-04-21T23:51:21.981947: step 2200, loss 0.221093, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:22.664433: step 2200, loss 0.391301, acc 0.866296, rec 0.873512, pre 0.860073, f1 0.86674\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-2200\n",
      "\n",
      "2018-04-21T23:51:22.754374: step 2201, loss 0.576999, acc 0.9\n",
      "2018-04-21T23:51:22.799152: step 2202, loss 0.277595, acc 0.92\n",
      "2018-04-21T23:51:22.845395: step 2203, loss 0.319097, acc 0.86\n",
      "2018-04-21T23:51:22.895160: step 2204, loss 0.295937, acc 0.88\n",
      "2018-04-21T23:51:22.940621: step 2205, loss 0.261857, acc 0.9\n",
      "2018-04-21T23:51:22.984502: step 2206, loss 0.441989, acc 0.86\n",
      "2018-04-21T23:51:23.029459: step 2207, loss 0.188787, acc 0.98\n",
      "2018-04-21T23:51:23.074989: step 2208, loss 0.307801, acc 0.92\n",
      "2018-04-21T23:51:23.124162: step 2209, loss 0.244271, acc 0.88\n",
      "2018-04-21T23:51:23.169827: step 2210, loss 0.266444, acc 0.9\n",
      "2018-04-21T23:51:23.214394: step 2211, loss 0.250987, acc 0.92\n",
      "2018-04-21T23:51:23.259044: step 2212, loss 0.15608, acc 0.96\n",
      "2018-04-21T23:51:23.304152: step 2213, loss 0.255982, acc 0.9\n",
      "2018-04-21T23:51:23.354593: step 2214, loss 0.466202, acc 0.92\n",
      "2018-04-21T23:51:23.399264: step 2215, loss 0.220337, acc 0.94\n",
      "2018-04-21T23:51:23.443416: step 2216, loss 0.124845, acc 1\n",
      "2018-04-21T23:51:23.488985: step 2217, loss 0.312135, acc 0.9\n",
      "2018-04-21T23:51:23.534252: step 2218, loss 0.319099, acc 0.86\n",
      "2018-04-21T23:51:23.582376: step 2219, loss 0.423722, acc 0.88\n",
      "2018-04-21T23:51:23.628311: step 2220, loss 0.136947, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:24.309588: step 2220, loss 0.393368, acc 0.865185, rec 0.866815, pre 0.862963, f1 0.864885\n",
      "\n",
      "2018-04-21T23:51:24.355097: step 2221, loss 0.183734, acc 0.94\n",
      "2018-04-21T23:51:24.401313: step 2222, loss 0.271207, acc 0.88\n",
      "2018-04-21T23:51:24.446698: step 2223, loss 0.352173, acc 0.88\n",
      "2018-04-21T23:51:24.492065: step 2224, loss 0.285645, acc 0.86\n",
      "2018-04-21T23:51:24.542032: step 2225, loss 0.260948, acc 0.92\n",
      "2018-04-21T23:51:24.587601: step 2226, loss 0.576058, acc 0.86\n",
      "2018-04-21T23:51:24.633662: step 2227, loss 0.197862, acc 0.94\n",
      "2018-04-21T23:51:24.680667: step 2228, loss 0.353301, acc 0.88\n",
      "2018-04-21T23:51:24.727234: step 2229, loss 0.302074, acc 0.92\n",
      "2018-04-21T23:51:24.777149: step 2230, loss 0.182586, acc 0.94\n",
      "2018-04-21T23:51:24.823052: step 2231, loss 0.190456, acc 0.96\n",
      "2018-04-21T23:51:24.868799: step 2232, loss 0.187226, acc 0.94\n",
      "2018-04-21T23:51:24.914894: step 2233, loss 0.211261, acc 0.92\n",
      "2018-04-21T23:51:24.961958: step 2234, loss 0.257079, acc 0.9\n",
      "2018-04-21T23:51:25.011592: step 2235, loss 0.23018, acc 0.9\n",
      "2018-04-21T23:51:25.056822: step 2236, loss 0.804534, acc 0.82\n",
      "2018-04-21T23:51:25.102825: step 2237, loss 0.314876, acc 0.82\n",
      "2018-04-21T23:51:25.148335: step 2238, loss 0.277508, acc 0.94\n",
      "2018-04-21T23:51:25.193724: step 2239, loss 0.445536, acc 0.82\n",
      "2018-04-21T23:51:25.244413: step 2240, loss 0.250479, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:25.926749: step 2240, loss 0.429375, acc 0.799259, rec 0.944196, pre 0.730991, f1 0.824026\n",
      "\n",
      "2018-04-21T23:51:25.971076: step 2241, loss 0.251656, acc 0.94\n",
      "2018-04-21T23:51:26.018670: step 2242, loss 0.286264, acc 0.88\n",
      "2018-04-21T23:51:26.064965: step 2243, loss 0.337119, acc 0.9\n",
      "2018-04-21T23:51:26.110538: step 2244, loss 0.743522, acc 0.9\n",
      "2018-04-21T23:51:26.163640: step 2245, loss 0.169571, acc 0.94\n",
      "2018-04-21T23:51:26.208558: step 2246, loss 0.205709, acc 0.92\n",
      "2018-04-21T23:51:26.254005: step 2247, loss 0.318644, acc 0.86\n",
      "2018-04-21T23:51:26.299143: step 2248, loss 0.399952, acc 0.82\n",
      "2018-04-21T23:51:26.344812: step 2249, loss 0.24515, acc 0.86\n",
      "2018-04-21T23:51:26.394327: step 2250, loss 0.190206, acc 0.96\n",
      "2018-04-21T23:51:26.438685: step 2251, loss 0.196631, acc 0.92\n",
      "2018-04-21T23:51:26.484116: step 2252, loss 0.352982, acc 0.9\n",
      "2018-04-21T23:51:26.528709: step 2253, loss 0.284971, acc 0.88\n",
      "2018-04-21T23:51:26.573877: step 2254, loss 0.204751, acc 0.92\n",
      "2018-04-21T23:51:26.622433: step 2255, loss 0.312087, acc 0.84\n",
      "2018-04-21T23:51:26.668485: step 2256, loss 0.279206, acc 0.88\n",
      "2018-04-21T23:51:26.713897: step 2257, loss 0.180236, acc 0.96\n",
      "2018-04-21T23:51:26.758540: step 2258, loss 0.188822, acc 0.94\n",
      "2018-04-21T23:51:26.803139: step 2259, loss 0.33439, acc 0.82\n",
      "2018-04-21T23:51:26.852312: step 2260, loss 0.344828, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:27.527826: step 2260, loss 0.756188, acc 0.575556, rec 0.985119, pre 0.540408, f1 0.697944\n",
      "\n",
      "2018-04-21T23:51:27.572509: step 2261, loss 0.406242, acc 0.86\n",
      "2018-04-21T23:51:27.618682: step 2262, loss 0.545877, acc 0.88\n",
      "2018-04-21T23:51:27.665821: step 2263, loss 0.281494, acc 0.9\n",
      "2018-04-21T23:51:27.712568: step 2264, loss 0.299098, acc 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:51:27.762938: step 2265, loss 0.243205, acc 0.92\n",
      "2018-04-21T23:51:27.808240: step 2266, loss 0.401035, acc 0.86\n",
      "2018-04-21T23:51:27.854076: step 2267, loss 0.216093, acc 0.92\n",
      "2018-04-21T23:51:27.899832: step 2268, loss 0.199763, acc 0.96\n",
      "2018-04-21T23:51:27.946616: step 2269, loss 0.284025, acc 0.88\n",
      "2018-04-21T23:51:27.996059: step 2270, loss 0.269349, acc 0.92\n",
      "2018-04-21T23:51:28.042625: step 2271, loss 0.254096, acc 0.9\n",
      "2018-04-21T23:51:28.088372: step 2272, loss 0.47398, acc 0.84\n",
      "2018-04-21T23:51:28.132911: step 2273, loss 0.240238, acc 0.92\n",
      "2018-04-21T23:51:28.177663: step 2274, loss 0.259557, acc 0.9\n",
      "2018-04-21T23:51:28.226529: step 2275, loss 0.240297, acc 0.92\n",
      "2018-04-21T23:51:28.271380: step 2276, loss 0.193573, acc 0.94\n",
      "2018-04-21T23:51:28.316486: step 2277, loss 0.228157, acc 0.92\n",
      "2018-04-21T23:51:28.360786: step 2278, loss 0.208795, acc 0.9\n",
      "2018-04-21T23:51:28.405981: step 2279, loss 0.319783, acc 0.9\n",
      "2018-04-21T23:51:28.454594: step 2280, loss 0.23477, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:29.133886: step 2280, loss 0.395055, acc 0.84, rec 0.897321, pre 0.804, f1 0.848101\n",
      "\n",
      "2018-04-21T23:51:29.178608: step 2281, loss 0.221009, acc 0.94\n",
      "2018-04-21T23:51:29.223238: step 2282, loss 0.150788, acc 0.98\n",
      "2018-04-21T23:51:29.269558: step 2283, loss 0.483967, acc 0.86\n",
      "2018-04-21T23:51:29.315477: step 2284, loss 0.269096, acc 0.86\n",
      "2018-04-21T23:51:29.365556: step 2285, loss 0.26746, acc 0.86\n",
      "2018-04-21T23:51:29.410099: step 2286, loss 0.33361, acc 0.86\n",
      "2018-04-21T23:51:29.454625: step 2287, loss 0.241039, acc 0.96\n",
      "2018-04-21T23:51:29.501141: step 2288, loss 0.38372, acc 0.88\n",
      "2018-04-21T23:51:29.546554: step 2289, loss 0.50272, acc 0.84\n",
      "2018-04-21T23:51:29.597888: step 2290, loss 0.366563, acc 0.86\n",
      "2018-04-21T23:51:29.644641: step 2291, loss 0.38073, acc 0.82\n",
      "2018-04-21T23:51:29.690857: step 2292, loss 0.569861, acc 0.8\n",
      "2018-04-21T23:51:29.737160: step 2293, loss 0.339882, acc 0.86\n",
      "2018-04-21T23:51:29.782762: step 2294, loss 0.265803, acc 0.86\n",
      "2018-04-21T23:51:29.831038: step 2295, loss 0.250012, acc 0.86\n",
      "2018-04-21T23:51:29.876588: step 2296, loss 0.228257, acc 0.88\n",
      "2018-04-21T23:51:29.921806: step 2297, loss 0.222159, acc 0.94\n",
      "2018-04-21T23:51:29.966512: step 2298, loss 0.224109, acc 0.94\n",
      "2018-04-21T23:51:30.012430: step 2299, loss 0.133371, acc 0.96\n",
      "2018-04-21T23:51:30.063580: step 2300, loss 0.187826, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:30.747283: step 2300, loss 0.584963, acc 0.650741, rec 0.979911, pre 0.58979, f1 0.736371\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-2300\n",
      "\n",
      "2018-04-21T23:51:30.837401: step 2301, loss 0.247938, acc 0.92\n",
      "2018-04-21T23:51:30.882232: step 2302, loss 0.200445, acc 0.94\n",
      "2018-04-21T23:51:30.926224: step 2303, loss 0.227856, acc 0.94\n",
      "2018-04-21T23:51:30.973531: step 2304, loss 0.199447, acc 0.96\n",
      "2018-04-21T23:51:31.017667: step 2305, loss 0.604433, acc 0.84\n",
      "2018-04-21T23:51:31.062691: step 2306, loss 0.233479, acc 0.9\n",
      "2018-04-21T23:51:31.107765: step 2307, loss 0.438764, acc 0.8\n",
      "2018-04-21T23:51:31.153511: step 2308, loss 0.276095, acc 0.86\n",
      "2018-04-21T23:51:31.200342: step 2309, loss 0.465053, acc 0.8\n",
      "2018-04-21T23:51:31.245633: step 2310, loss 0.175516, acc 0.94\n",
      "2018-04-21T23:51:31.291322: step 2311, loss 0.268018, acc 0.9\n",
      "2018-04-21T23:51:31.335894: step 2312, loss 0.192045, acc 0.96\n",
      "2018-04-21T23:51:31.380784: step 2313, loss 0.282958, acc 0.88\n",
      "2018-04-21T23:51:31.429767: step 2314, loss 0.414342, acc 0.9\n",
      "2018-04-21T23:51:31.475312: step 2315, loss 0.311492, acc 0.86\n",
      "2018-04-21T23:51:31.519842: step 2316, loss 0.494702, acc 0.84\n",
      "2018-04-21T23:51:31.564318: step 2317, loss 0.238954, acc 0.88\n",
      "2018-04-21T23:51:31.610708: step 2318, loss 0.297402, acc 0.88\n",
      "2018-04-21T23:51:31.659602: step 2319, loss 0.28344, acc 0.9\n",
      "2018-04-21T23:51:31.704673: step 2320, loss 0.523283, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:32.384713: step 2320, loss 0.50128, acc 0.714444, rec 0.973958, pre 0.640098, f1 0.772499\n",
      "\n",
      "2018-04-21T23:51:32.430071: step 2321, loss 0.230295, acc 0.92\n",
      "2018-04-21T23:51:32.475324: step 2322, loss 0.329545, acc 0.88\n",
      "2018-04-21T23:51:32.521709: step 2323, loss 0.161417, acc 0.96\n",
      "2018-04-21T23:51:32.567825: step 2324, loss 0.318053, acc 0.92\n",
      "2018-04-21T23:51:32.616753: step 2325, loss 0.332124, acc 0.9\n",
      "2018-04-21T23:51:32.663793: step 2326, loss 0.665394, acc 0.7\n",
      "2018-04-21T23:51:32.710377: step 2327, loss 0.847328, acc 0.7\n",
      "2018-04-21T23:51:32.755662: step 2328, loss 0.486928, acc 0.78\n",
      "2018-04-21T23:51:32.801477: step 2329, loss 0.256122, acc 0.9\n",
      "2018-04-21T23:51:32.848982: step 2330, loss 0.252246, acc 0.9\n",
      "2018-04-21T23:51:32.893358: step 2331, loss 0.193804, acc 0.92\n",
      "2018-04-21T23:51:32.937285: step 2332, loss 0.223408, acc 0.86\n",
      "2018-04-21T23:51:32.981747: step 2333, loss 0.382245, acc 0.88\n",
      "2018-04-21T23:51:33.025816: step 2334, loss 0.230024, acc 0.88\n",
      "2018-04-21T23:51:33.073715: step 2335, loss 0.148406, acc 0.96\n",
      "2018-04-21T23:51:33.117905: step 2336, loss 0.373404, acc 0.8\n",
      "2018-04-21T23:51:33.162276: step 2337, loss 0.665991, acc 0.7\n",
      "2018-04-21T23:51:33.206752: step 2338, loss 0.275578, acc 0.88\n",
      "2018-04-21T23:51:33.250388: step 2339, loss 0.243585, acc 0.92\n",
      "2018-04-21T23:51:33.296955: step 2340, loss 0.19202, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:33.935501: step 2340, loss 0.386726, acc 0.866667, rec 0.850446, pre 0.87788, f1 0.863946\n",
      "\n",
      "2018-04-21T23:51:33.978738: step 2341, loss 0.249437, acc 0.96\n",
      "2018-04-21T23:51:34.022566: step 2342, loss 0.204641, acc 0.92\n",
      "2018-04-21T23:51:34.065946: step 2343, loss 0.296238, acc 0.9\n",
      "2018-04-21T23:51:34.110062: step 2344, loss 0.367497, acc 0.86\n",
      "2018-04-21T23:51:34.157603: step 2345, loss 0.306881, acc 0.88\n",
      "2018-04-21T23:51:34.202873: step 2346, loss 0.42317, acc 0.76\n",
      "2018-04-21T23:51:34.246337: step 2347, loss 0.32639, acc 0.86\n",
      "2018-04-21T23:51:34.289785: step 2348, loss 0.39214, acc 0.82\n",
      "2018-04-21T23:51:34.333775: step 2349, loss 0.215447, acc 0.92\n",
      "2018-04-21T23:51:34.381178: step 2350, loss 0.365003, acc 0.8\n",
      "2018-04-21T23:51:34.424806: step 2351, loss 0.192306, acc 0.94\n",
      "2018-04-21T23:51:34.468117: step 2352, loss 0.291345, acc 0.9\n",
      "2018-04-21T23:51:34.511272: step 2353, loss 0.1888, acc 0.96\n",
      "2018-04-21T23:51:34.555246: step 2354, loss 0.218092, acc 0.88\n",
      "2018-04-21T23:51:34.603763: step 2355, loss 0.240093, acc 0.92\n",
      "2018-04-21T23:51:34.647683: step 2356, loss 0.198061, acc 0.96\n",
      "2018-04-21T23:51:34.691852: step 2357, loss 0.267663, acc 0.92\n",
      "2018-04-21T23:51:34.736580: step 2358, loss 0.426379, acc 0.78\n",
      "2018-04-21T23:51:34.780697: step 2359, loss 0.504894, acc 0.88\n",
      "2018-04-21T23:51:34.827405: step 2360, loss 0.283046, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:35.458417: step 2360, loss 0.430201, acc 0.792593, rec 0.947173, pre 0.722474, f1 0.819704\n",
      "\n",
      "2018-04-21T23:51:35.502231: step 2361, loss 0.243416, acc 0.9\n",
      "2018-04-21T23:51:35.545438: step 2362, loss 0.187539, acc 0.94\n",
      "2018-04-21T23:51:35.589608: step 2363, loss 0.289167, acc 0.92\n",
      "2018-04-21T23:51:35.633415: step 2364, loss 0.277839, acc 0.88\n",
      "2018-04-21T23:51:35.679758: step 2365, loss 0.175152, acc 0.96\n",
      "2018-04-21T23:51:35.723956: step 2366, loss 0.174853, acc 0.96\n",
      "2018-04-21T23:51:35.768379: step 2367, loss 0.187317, acc 0.9\n",
      "2018-04-21T23:51:35.812315: step 2368, loss 0.248803, acc 0.88\n",
      "2018-04-21T23:51:35.855734: step 2369, loss 0.457195, acc 0.82\n",
      "2018-04-21T23:51:35.901738: step 2370, loss 0.455149, acc 0.86\n",
      "2018-04-21T23:51:35.945873: step 2371, loss 0.291575, acc 0.84\n",
      "2018-04-21T23:51:35.989409: step 2372, loss 0.550357, acc 0.88\n",
      "2018-04-21T23:51:36.032798: step 2373, loss 0.285559, acc 0.92\n",
      "2018-04-21T23:51:36.076623: step 2374, loss 0.290985, acc 0.9\n",
      "2018-04-21T23:51:36.122781: step 2375, loss 0.461431, acc 0.88\n",
      "2018-04-21T23:51:36.167113: step 2376, loss 0.246306, acc 0.92\n",
      "2018-04-21T23:51:36.210814: step 2377, loss 0.279373, acc 0.88\n",
      "2018-04-21T23:51:36.254336: step 2378, loss 0.325163, acc 0.88\n",
      "2018-04-21T23:51:36.298077: step 2379, loss 0.257161, acc 0.92\n",
      "2018-04-21T23:51:36.345176: step 2380, loss 0.259873, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:36.978827: step 2380, loss 0.390786, acc 0.864074, rec 0.796875, pre 0.919313, f1 0.853727\n",
      "\n",
      "2018-04-21T23:51:37.022328: step 2381, loss 0.320781, acc 0.9\n",
      "2018-04-21T23:51:37.066723: step 2382, loss 0.222538, acc 0.88\n",
      "2018-04-21T23:51:37.111193: step 2383, loss 0.443492, acc 0.8\n",
      "2018-04-21T23:51:37.155357: step 2384, loss 0.289564, acc 0.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:51:37.203527: step 2385, loss 0.311965, acc 0.86\n",
      "2018-04-21T23:51:37.247466: step 2386, loss 0.240744, acc 0.88\n",
      "2018-04-21T23:51:37.290951: step 2387, loss 0.233067, acc 0.94\n",
      "2018-04-21T23:51:37.335269: step 2388, loss 0.276996, acc 0.94\n",
      "2018-04-21T23:51:37.379753: step 2389, loss 0.315771, acc 0.86\n",
      "2018-04-21T23:51:37.426914: step 2390, loss 0.208082, acc 0.96\n",
      "2018-04-21T23:51:37.471039: step 2391, loss 0.546265, acc 0.92\n",
      "2018-04-21T23:51:37.515349: step 2392, loss 0.325065, acc 0.78\n",
      "2018-04-21T23:51:37.562531: step 2393, loss 0.277486, acc 0.92\n",
      "2018-04-21T23:51:37.606727: step 2394, loss 0.393683, acc 0.78\n",
      "2018-04-21T23:51:37.653624: step 2395, loss 0.302641, acc 0.82\n",
      "2018-04-21T23:51:37.697857: step 2396, loss 0.254027, acc 0.9\n",
      "2018-04-21T23:51:37.742534: step 2397, loss 0.745839, acc 0.78\n",
      "2018-04-21T23:51:37.790685: step 2398, loss 0.290348, acc 0.88\n",
      "2018-04-21T23:51:37.836235: step 2399, loss 0.310333, acc 0.86\n",
      "2018-04-21T23:51:37.883668: step 2400, loss 0.341394, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:38.560774: step 2400, loss 0.386612, acc 0.868889, rec 0.810268, pre 0.916667, f1 0.86019\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-2400\n",
      "\n",
      "2018-04-21T23:51:38.646256: step 2401, loss 0.411519, acc 0.82\n",
      "2018-04-21T23:51:38.690469: step 2402, loss 0.31971, acc 0.88\n",
      "2018-04-21T23:51:38.735667: step 2403, loss 0.248316, acc 0.94\n",
      "2018-04-21T23:51:38.783860: step 2404, loss 0.467181, acc 0.88\n",
      "2018-04-21T23:51:38.828033: step 2405, loss 0.430182, acc 0.86\n",
      "2018-04-21T23:51:38.872039: step 2406, loss 0.195529, acc 0.98\n",
      "2018-04-21T23:51:38.916038: step 2407, loss 0.287605, acc 0.88\n",
      "2018-04-21T23:51:38.960015: step 2408, loss 0.243268, acc 0.88\n",
      "2018-04-21T23:51:39.008471: step 2409, loss 0.245194, acc 0.96\n",
      "2018-04-21T23:51:39.053466: step 2410, loss 0.421748, acc 0.9\n",
      "2018-04-21T23:51:39.097466: step 2411, loss 0.282093, acc 0.88\n",
      "2018-04-21T23:51:39.140927: step 2412, loss 0.259191, acc 0.92\n",
      "2018-04-21T23:51:39.184524: step 2413, loss 0.478526, acc 0.86\n",
      "2018-04-21T23:51:39.232106: step 2414, loss 0.216321, acc 0.92\n",
      "2018-04-21T23:51:39.276232: step 2415, loss 0.382149, acc 0.84\n",
      "2018-04-21T23:51:39.319958: step 2416, loss 0.311437, acc 0.88\n",
      "2018-04-21T23:51:39.363236: step 2417, loss 0.204877, acc 0.96\n",
      "2018-04-21T23:51:39.406929: step 2418, loss 0.253303, acc 0.86\n",
      "2018-04-21T23:51:39.453163: step 2419, loss 0.138491, acc 0.98\n",
      "2018-04-21T23:51:39.497512: step 2420, loss 0.233122, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:40.130532: step 2420, loss 0.421702, acc 0.800741, rec 0.939732, pre 0.734302, f1 0.824413\n",
      "\n",
      "2018-04-21T23:51:40.173621: step 2421, loss 0.23104, acc 0.94\n",
      "2018-04-21T23:51:40.216956: step 2422, loss 0.217623, acc 0.88\n",
      "2018-04-21T23:51:40.260021: step 2423, loss 0.243523, acc 0.94\n",
      "2018-04-21T23:51:40.303759: step 2424, loss 0.278172, acc 0.9\n",
      "2018-04-21T23:51:40.350705: step 2425, loss 0.221913, acc 0.94\n",
      "2018-04-21T23:51:40.394307: step 2426, loss 0.434446, acc 0.78\n",
      "2018-04-21T23:51:40.437736: step 2427, loss 0.170045, acc 0.96\n",
      "2018-04-21T23:51:40.480880: step 2428, loss 0.160229, acc 0.94\n",
      "2018-04-21T23:51:40.524039: step 2429, loss 0.208775, acc 0.94\n",
      "2018-04-21T23:51:40.571808: step 2430, loss 0.32883, acc 0.88\n",
      "2018-04-21T23:51:40.616075: step 2431, loss 0.250146, acc 0.9\n",
      "2018-04-21T23:51:40.659896: step 2432, loss 0.350844, acc 0.84\n",
      "2018-04-21T23:51:40.702924: step 2433, loss 0.457515, acc 0.86\n",
      "2018-04-21T23:51:40.746382: step 2434, loss 0.236951, acc 0.92\n",
      "2018-04-21T23:51:40.793189: step 2435, loss 0.194471, acc 0.92\n",
      "2018-04-21T23:51:40.836354: step 2436, loss 0.331889, acc 0.88\n",
      "2018-04-21T23:51:40.879819: step 2437, loss 0.214638, acc 0.92\n",
      "2018-04-21T23:51:40.923635: step 2438, loss 0.269877, acc 0.94\n",
      "2018-04-21T23:51:40.966997: step 2439, loss 0.358682, acc 0.9\n",
      "2018-04-21T23:51:41.013032: step 2440, loss 0.159058, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:41.646371: step 2440, loss 0.393881, acc 0.848518, rec 0.74628, pre 0.936508, f1 0.830642\n",
      "\n",
      "2018-04-21T23:51:41.690388: step 2441, loss 0.476858, acc 0.9\n",
      "2018-04-21T23:51:41.734694: step 2442, loss 0.268333, acc 0.92\n",
      "2018-04-21T23:51:41.778736: step 2443, loss 0.431308, acc 0.86\n",
      "2018-04-21T23:51:41.822483: step 2444, loss 0.375742, acc 0.84\n",
      "2018-04-21T23:51:41.868913: step 2445, loss 0.46548, acc 0.88\n",
      "2018-04-21T23:51:41.912117: step 2446, loss 0.295428, acc 0.92\n",
      "2018-04-21T23:51:41.955689: step 2447, loss 0.225347, acc 0.92\n",
      "2018-04-21T23:51:41.999120: step 2448, loss 0.296514, acc 0.88\n",
      "2018-04-21T23:51:42.042747: step 2449, loss 0.215046, acc 0.94\n",
      "2018-04-21T23:51:42.089498: step 2450, loss 0.216297, acc 0.88\n",
      "2018-04-21T23:51:42.133693: step 2451, loss 0.208007, acc 0.9\n",
      "2018-04-21T23:51:42.178212: step 2452, loss 0.549759, acc 0.82\n",
      "2018-04-21T23:51:42.222254: step 2453, loss 0.169908, acc 0.96\n",
      "2018-04-21T23:51:42.266125: step 2454, loss 0.237045, acc 0.94\n",
      "2018-04-21T23:51:42.312791: step 2455, loss 0.212024, acc 0.94\n",
      "2018-04-21T23:51:42.357114: step 2456, loss 0.287127, acc 0.84\n",
      "2018-04-21T23:51:42.400301: step 2457, loss 0.309659, acc 0.88\n",
      "2018-04-21T23:51:42.443966: step 2458, loss 0.187979, acc 0.96\n",
      "2018-04-21T23:51:42.487248: step 2459, loss 0.301077, acc 0.86\n",
      "2018-04-21T23:51:42.533513: step 2460, loss 0.249523, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:43.168515: step 2460, loss 0.49967, acc 0.72, rec 0.973958, pre 0.644828, f1 0.775934\n",
      "\n",
      "2018-04-21T23:51:43.212880: step 2461, loss 0.294072, acc 0.88\n",
      "2018-04-21T23:51:43.256729: step 2462, loss 0.634019, acc 0.86\n",
      "2018-04-21T23:51:43.301133: step 2463, loss 0.170127, acc 0.92\n",
      "2018-04-21T23:51:43.344586: step 2464, loss 0.187175, acc 0.92\n",
      "2018-04-21T23:51:43.390972: step 2465, loss 0.22087, acc 0.92\n",
      "2018-04-21T23:51:43.434446: step 2466, loss 0.220842, acc 0.92\n",
      "2018-04-21T23:51:43.477847: step 2467, loss 0.278863, acc 0.9\n",
      "2018-04-21T23:51:43.521217: step 2468, loss 0.26493, acc 0.92\n",
      "2018-04-21T23:51:43.565759: step 2469, loss 0.356746, acc 0.9\n",
      "2018-04-21T23:51:43.612165: step 2470, loss 0.26814, acc 0.86\n",
      "2018-04-21T23:51:43.655432: step 2471, loss 0.468634, acc 0.88\n",
      "2018-04-21T23:51:43.699133: step 2472, loss 0.245737, acc 0.94\n",
      "2018-04-21T23:51:43.743304: step 2473, loss 0.209465, acc 0.86\n",
      "2018-04-21T23:51:43.787055: step 2474, loss 0.289357, acc 0.9\n",
      "2018-04-21T23:51:43.833098: step 2475, loss 0.17656, acc 0.94\n",
      "2018-04-21T23:51:43.876357: step 2476, loss 0.225358, acc 0.92\n",
      "2018-04-21T23:51:43.919409: step 2477, loss 0.380252, acc 0.82\n",
      "2018-04-21T23:51:43.962580: step 2478, loss 0.52625, acc 0.9\n",
      "2018-04-21T23:51:44.005393: step 2479, loss 0.261526, acc 0.88\n",
      "2018-04-21T23:51:44.052141: step 2480, loss 0.284578, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:44.689636: step 2480, loss 0.379953, acc 0.863704, rec 0.86756, pre 0.859882, f1 0.863704\n",
      "\n",
      "2018-04-21T23:51:44.735008: step 2481, loss 0.343659, acc 0.88\n",
      "2018-04-21T23:51:44.781538: step 2482, loss 0.158445, acc 0.94\n",
      "2018-04-21T23:51:44.828896: step 2483, loss 0.212976, acc 0.86\n",
      "2018-04-21T23:51:44.875148: step 2484, loss 0.17823, acc 0.96\n",
      "2018-04-21T23:51:44.925236: step 2485, loss 0.199028, acc 0.94\n",
      "2018-04-21T23:51:44.970540: step 2486, loss 0.286425, acc 0.9\n",
      "2018-04-21T23:51:45.015686: step 2487, loss 0.237096, acc 0.9\n",
      "2018-04-21T23:51:45.061294: step 2488, loss 0.329707, acc 0.84\n",
      "2018-04-21T23:51:45.106785: step 2489, loss 0.295182, acc 0.84\n",
      "2018-04-21T23:51:45.156569: step 2490, loss 0.226665, acc 0.9\n",
      "2018-04-21T23:51:45.202204: step 2491, loss 0.2874, acc 0.82\n",
      "2018-04-21T23:51:45.247289: step 2492, loss 0.205255, acc 0.9\n",
      "2018-04-21T23:51:45.293044: step 2493, loss 0.266498, acc 0.92\n",
      "2018-04-21T23:51:45.338802: step 2494, loss 0.328528, acc 0.84\n",
      "2018-04-21T23:51:45.389656: step 2495, loss 0.341421, acc 0.9\n",
      "2018-04-21T23:51:45.435446: step 2496, loss 0.213164, acc 0.92\n",
      "2018-04-21T23:51:45.480330: step 2497, loss 0.333065, acc 0.82\n",
      "2018-04-21T23:51:45.526255: step 2498, loss 0.22466, acc 0.92\n",
      "2018-04-21T23:51:45.571930: step 2499, loss 0.523382, acc 0.86\n",
      "2018-04-21T23:51:45.622819: step 2500, loss 0.156338, acc 0.96\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:46.306961: step 2500, loss 0.513652, acc 0.702593, rec 0.975446, pre 0.629986, f1 0.765547\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-2500\n",
      "\n",
      "2018-04-21T23:51:46.397406: step 2501, loss 0.347177, acc 0.86\n",
      "2018-04-21T23:51:46.443205: step 2502, loss 0.453007, acc 0.88\n",
      "2018-04-21T23:51:46.489795: step 2503, loss 0.316518, acc 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:51:46.539671: step 2504, loss 0.31563, acc 0.84\n",
      "2018-04-21T23:51:46.584986: step 2505, loss 0.354759, acc 0.82\n",
      "2018-04-21T23:51:46.629490: step 2506, loss 0.409877, acc 0.76\n",
      "2018-04-21T23:51:46.673978: step 2507, loss 0.308322, acc 0.88\n",
      "2018-04-21T23:51:46.718773: step 2508, loss 0.414155, acc 0.86\n",
      "2018-04-21T23:51:46.768482: step 2509, loss 0.310789, acc 0.86\n",
      "2018-04-21T23:51:46.814217: step 2510, loss 0.205646, acc 0.9\n",
      "2018-04-21T23:51:46.859382: step 2511, loss 0.354348, acc 0.84\n",
      "2018-04-21T23:51:46.904739: step 2512, loss 0.28172, acc 0.88\n",
      "2018-04-21T23:51:46.950251: step 2513, loss 0.252919, acc 0.92\n",
      "2018-04-21T23:51:46.999218: step 2514, loss 0.531427, acc 0.9\n",
      "2018-04-21T23:51:47.044586: step 2515, loss 0.329803, acc 0.88\n",
      "2018-04-21T23:51:47.089418: step 2516, loss 0.36046, acc 0.88\n",
      "2018-04-21T23:51:47.134090: step 2517, loss 0.205849, acc 0.92\n",
      "2018-04-21T23:51:47.179012: step 2518, loss 0.305366, acc 0.88\n",
      "2018-04-21T23:51:47.226894: step 2519, loss 0.331934, acc 0.88\n",
      "2018-04-21T23:51:47.272080: step 2520, loss 0.295858, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:47.955196: step 2520, loss 0.394954, acc 0.832963, rec 0.90253, pre 0.791259, f1 0.843239\n",
      "\n",
      "2018-04-21T23:51:47.999440: step 2521, loss 0.236021, acc 0.92\n",
      "2018-04-21T23:51:48.044446: step 2522, loss 0.237152, acc 0.86\n",
      "2018-04-21T23:51:48.089302: step 2523, loss 0.305286, acc 0.86\n",
      "2018-04-21T23:51:48.134915: step 2524, loss 0.259448, acc 0.9\n",
      "2018-04-21T23:51:48.184584: step 2525, loss 0.218407, acc 0.94\n",
      "2018-04-21T23:51:48.230204: step 2526, loss 0.362551, acc 0.86\n",
      "2018-04-21T23:51:48.276040: step 2527, loss 0.202881, acc 0.9\n",
      "2018-04-21T23:51:48.321452: step 2528, loss 0.299522, acc 0.88\n",
      "2018-04-21T23:51:48.366660: step 2529, loss 0.240861, acc 0.92\n",
      "2018-04-21T23:51:48.415850: step 2530, loss 0.366564, acc 0.82\n",
      "2018-04-21T23:51:48.462954: step 2531, loss 0.286918, acc 0.86\n",
      "2018-04-21T23:51:48.508522: step 2532, loss 0.348298, acc 0.88\n",
      "2018-04-21T23:51:48.553445: step 2533, loss 0.311905, acc 0.86\n",
      "2018-04-21T23:51:48.600201: step 2534, loss 0.309038, acc 0.88\n",
      "2018-04-21T23:51:48.650770: step 2535, loss 0.232378, acc 0.9\n",
      "2018-04-21T23:51:48.696806: step 2536, loss 0.227355, acc 0.94\n",
      "2018-04-21T23:51:48.742674: step 2537, loss 0.36359, acc 0.94\n",
      "2018-04-21T23:51:48.788114: step 2538, loss 0.200181, acc 0.96\n",
      "2018-04-21T23:51:48.833415: step 2539, loss 0.370479, acc 0.92\n",
      "2018-04-21T23:51:48.882606: step 2540, loss 0.264011, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:49.563160: step 2540, loss 0.375868, acc 0.866296, rec 0.854911, pre 0.873764, f1 0.864235\n",
      "\n",
      "2018-04-21T23:51:49.606554: step 2541, loss 0.287645, acc 0.96\n",
      "2018-04-21T23:51:49.652076: step 2542, loss 0.179039, acc 0.96\n",
      "2018-04-21T23:51:49.699301: step 2543, loss 0.197104, acc 0.96\n",
      "2018-04-21T23:51:49.744454: step 2544, loss 0.256461, acc 0.88\n",
      "2018-04-21T23:51:49.792340: step 2545, loss 0.191675, acc 0.94\n",
      "2018-04-21T23:51:49.837673: step 2546, loss 0.261941, acc 0.88\n",
      "2018-04-21T23:51:49.882967: step 2547, loss 0.180212, acc 0.92\n",
      "2018-04-21T23:51:49.928636: step 2548, loss 0.216844, acc 0.92\n",
      "2018-04-21T23:51:49.974426: step 2549, loss 0.224685, acc 0.96\n",
      "2018-04-21T23:51:50.024661: step 2550, loss 0.260505, acc 0.86\n",
      "2018-04-21T23:51:50.070611: step 2551, loss 0.234404, acc 0.9\n",
      "2018-04-21T23:51:50.115793: step 2552, loss 0.241629, acc 0.9\n",
      "2018-04-21T23:51:50.161356: step 2553, loss 0.305384, acc 0.9\n",
      "2018-04-21T23:51:50.206815: step 2554, loss 0.444805, acc 0.86\n",
      "2018-04-21T23:51:50.256462: step 2555, loss 0.216863, acc 0.9\n",
      "2018-04-21T23:51:50.301916: step 2556, loss 0.214771, acc 0.94\n",
      "2018-04-21T23:51:50.347236: step 2557, loss 0.302034, acc 0.84\n",
      "2018-04-21T23:51:50.393401: step 2558, loss 0.417281, acc 0.78\n",
      "2018-04-21T23:51:50.439501: step 2559, loss 0.361825, acc 0.82\n",
      "2018-04-21T23:51:50.490754: step 2560, loss 0.320519, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:51.172915: step 2560, loss 0.39643, acc 0.842963, rec 0.725446, pre 0.946602, f1 0.821398\n",
      "\n",
      "2018-04-21T23:51:51.218354: step 2561, loss 0.358325, acc 0.9\n",
      "2018-04-21T23:51:51.263387: step 2562, loss 0.4252, acc 0.88\n",
      "2018-04-21T23:51:51.315067: step 2563, loss 0.418113, acc 0.9\n",
      "2018-04-21T23:51:51.361575: step 2564, loss 0.277771, acc 0.9\n",
      "2018-04-21T23:51:51.410258: step 2565, loss 0.220102, acc 0.92\n",
      "2018-04-21T23:51:51.455477: step 2566, loss 0.43755, acc 0.74\n",
      "2018-04-21T23:51:51.501382: step 2567, loss 0.397942, acc 0.8\n",
      "2018-04-21T23:51:51.548016: step 2568, loss 0.363298, acc 0.96\n",
      "2018-04-21T23:51:51.593863: step 2569, loss 0.397762, acc 0.88\n",
      "2018-04-21T23:51:51.643643: step 2570, loss 0.178429, acc 0.94\n",
      "2018-04-21T23:51:51.692336: step 2571, loss 0.164559, acc 0.94\n",
      "2018-04-21T23:51:51.738087: step 2572, loss 0.324243, acc 0.9\n",
      "2018-04-21T23:51:51.783456: step 2573, loss 0.256381, acc 0.92\n",
      "2018-04-21T23:51:51.828907: step 2574, loss 0.505958, acc 0.84\n",
      "2018-04-21T23:51:51.878850: step 2575, loss 0.251351, acc 0.94\n",
      "2018-04-21T23:51:51.925002: step 2576, loss 0.463985, acc 0.86\n",
      "2018-04-21T23:51:51.972456: step 2577, loss 0.277173, acc 0.92\n",
      "2018-04-21T23:51:52.017382: step 2578, loss 0.439922, acc 0.82\n",
      "2018-04-21T23:51:52.062914: step 2579, loss 0.388788, acc 0.78\n",
      "2018-04-21T23:51:52.112920: step 2580, loss 0.353541, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:52.798868: step 2580, loss 0.67952, acc 0.605556, rec 0.982143, pre 0.559085, f1 0.712551\n",
      "\n",
      "2018-04-21T23:51:52.843914: step 2581, loss 0.186238, acc 0.94\n",
      "2018-04-21T23:51:52.889412: step 2582, loss 0.243424, acc 0.88\n",
      "2018-04-21T23:51:52.936966: step 2583, loss 0.275723, acc 0.86\n",
      "2018-04-21T23:51:52.982957: step 2584, loss 0.231359, acc 0.9\n",
      "2018-04-21T23:51:53.032922: step 2585, loss 0.341051, acc 0.84\n",
      "2018-04-21T23:51:53.079125: step 2586, loss 0.403286, acc 0.8\n",
      "2018-04-21T23:51:53.125770: step 2587, loss 0.175299, acc 0.94\n",
      "2018-04-21T23:51:53.172148: step 2588, loss 0.188912, acc 0.9\n",
      "2018-04-21T23:51:53.218877: step 2589, loss 0.165242, acc 0.92\n",
      "2018-04-21T23:51:53.268247: step 2590, loss 0.228651, acc 0.92\n",
      "2018-04-21T23:51:53.315515: step 2591, loss 0.131365, acc 0.98\n",
      "2018-04-21T23:51:53.360608: step 2592, loss 0.288042, acc 0.92\n",
      "2018-04-21T23:51:53.406417: step 2593, loss 0.341546, acc 0.92\n",
      "2018-04-21T23:51:53.451065: step 2594, loss 0.319348, acc 0.88\n",
      "2018-04-21T23:51:53.500348: step 2595, loss 0.639855, acc 0.84\n",
      "2018-04-21T23:51:53.545199: step 2596, loss 0.192312, acc 0.94\n",
      "2018-04-21T23:51:53.589713: step 2597, loss 0.199142, acc 0.96\n",
      "2018-04-21T23:51:53.636673: step 2598, loss 0.280037, acc 0.86\n",
      "2018-04-21T23:51:53.681181: step 2599, loss 0.282536, acc 0.92\n",
      "2018-04-21T23:51:53.730491: step 2600, loss 0.231976, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:54.410073: step 2600, loss 0.467091, acc 0.745926, rec 0.966518, pre 0.669588, f1 0.791108\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-2600\n",
      "\n",
      "2018-04-21T23:51:54.502485: step 2601, loss 0.177739, acc 0.94\n",
      "2018-04-21T23:51:54.548321: step 2602, loss 0.270858, acc 0.88\n",
      "2018-04-21T23:51:54.593457: step 2603, loss 0.234791, acc 0.94\n",
      "2018-04-21T23:51:54.642612: step 2604, loss 0.259109, acc 0.92\n",
      "2018-04-21T23:51:54.690207: step 2605, loss 0.179561, acc 0.94\n",
      "2018-04-21T23:51:54.735619: step 2606, loss 0.30188, acc 0.88\n",
      "2018-04-21T23:51:54.780358: step 2607, loss 0.462939, acc 0.8\n",
      "2018-04-21T23:51:54.826310: step 2608, loss 0.255069, acc 0.86\n",
      "2018-04-21T23:51:54.875878: step 2609, loss 0.230459, acc 0.9\n",
      "2018-04-21T23:51:54.921739: step 2610, loss 0.321717, acc 0.84\n",
      "2018-04-21T23:51:54.967820: step 2611, loss 0.325892, acc 0.84\n",
      "2018-04-21T23:51:55.012616: step 2612, loss 0.527657, acc 0.88\n",
      "2018-04-21T23:51:55.058426: step 2613, loss 0.259205, acc 0.86\n",
      "2018-04-21T23:51:55.106780: step 2614, loss 0.294468, acc 0.88\n",
      "2018-04-21T23:51:55.151998: step 2615, loss 0.398481, acc 0.84\n",
      "2018-04-21T23:51:55.197561: step 2616, loss 0.239924, acc 0.9\n",
      "2018-04-21T23:51:55.243016: step 2617, loss 0.190063, acc 0.94\n",
      "2018-04-21T23:51:55.289366: step 2618, loss 0.251978, acc 0.9\n",
      "2018-04-21T23:51:55.340204: step 2619, loss 0.255518, acc 0.9\n",
      "2018-04-21T23:51:55.388646: step 2620, loss 0.479182, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:56.070892: step 2620, loss 0.466744, acc 0.747037, rec 0.965774, pre 0.670801, f1 0.791705\n",
      "\n",
      "2018-04-21T23:51:56.115246: step 2621, loss 0.144429, acc 0.96\n",
      "2018-04-21T23:51:56.160614: step 2622, loss 0.392327, acc 0.8\n",
      "2018-04-21T23:51:56.205678: step 2623, loss 0.432153, acc 0.88\n",
      "2018-04-21T23:51:56.250753: step 2624, loss 0.221014, acc 0.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:51:56.300117: step 2625, loss 0.200001, acc 0.94\n",
      "2018-04-21T23:51:56.345723: step 2626, loss 0.195977, acc 0.92\n",
      "2018-04-21T23:51:56.392236: step 2627, loss 0.338492, acc 0.9\n",
      "2018-04-21T23:51:56.439286: step 2628, loss 0.347223, acc 0.86\n",
      "2018-04-21T23:51:56.484729: step 2629, loss 0.245772, acc 0.9\n",
      "2018-04-21T23:51:56.533786: step 2630, loss 0.477913, acc 0.86\n",
      "2018-04-21T23:51:56.577756: step 2631, loss 0.232418, acc 0.94\n",
      "2018-04-21T23:51:56.622768: step 2632, loss 0.256914, acc 0.88\n",
      "2018-04-21T23:51:56.669227: step 2633, loss 0.239374, acc 0.9\n",
      "2018-04-21T23:51:56.714759: step 2634, loss 0.276041, acc 0.9\n",
      "2018-04-21T23:51:56.763231: step 2635, loss 0.229226, acc 0.88\n",
      "2018-04-21T23:51:56.808037: step 2636, loss 0.215014, acc 0.88\n",
      "2018-04-21T23:51:56.852981: step 2637, loss 0.423496, acc 0.9\n",
      "2018-04-21T23:51:56.897831: step 2638, loss 0.166272, acc 0.96\n",
      "2018-04-21T23:51:56.943701: step 2639, loss 0.14749, acc 0.98\n",
      "2018-04-21T23:51:56.992466: step 2640, loss 0.207701, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:57.673105: step 2640, loss 0.658258, acc 0.618519, rec 0.982887, pre 0.56744, f1 0.719499\n",
      "\n",
      "2018-04-21T23:51:57.717355: step 2641, loss 0.227205, acc 0.9\n",
      "2018-04-21T23:51:57.763413: step 2642, loss 0.185853, acc 0.92\n",
      "2018-04-21T23:51:57.808826: step 2643, loss 0.514377, acc 0.84\n",
      "2018-04-21T23:51:57.853597: step 2644, loss 0.392726, acc 0.8\n",
      "2018-04-21T23:51:57.902918: step 2645, loss 0.404716, acc 0.84\n",
      "2018-04-21T23:51:57.950178: step 2646, loss 0.300146, acc 0.9\n",
      "2018-04-21T23:51:57.996526: step 2647, loss 0.518586, acc 0.82\n",
      "2018-04-21T23:51:58.042878: step 2648, loss 0.270268, acc 0.86\n",
      "2018-04-21T23:51:58.089306: step 2649, loss 0.496811, acc 0.74\n",
      "2018-04-21T23:51:58.139272: step 2650, loss 0.49242, acc 0.8\n",
      "2018-04-21T23:51:58.185027: step 2651, loss 0.249476, acc 0.88\n",
      "2018-04-21T23:51:58.230744: step 2652, loss 0.335214, acc 0.86\n",
      "2018-04-21T23:51:58.277047: step 2653, loss 0.257604, acc 0.92\n",
      "2018-04-21T23:51:58.322859: step 2654, loss 0.344014, acc 0.86\n",
      "2018-04-21T23:51:58.373640: step 2655, loss 0.170534, acc 0.96\n",
      "2018-04-21T23:51:58.419876: step 2656, loss 0.202852, acc 0.94\n",
      "2018-04-21T23:51:58.466050: step 2657, loss 0.173848, acc 0.94\n",
      "2018-04-21T23:51:58.512286: step 2658, loss 0.344797, acc 0.9\n",
      "2018-04-21T23:51:58.557678: step 2659, loss 0.17513, acc 0.92\n",
      "2018-04-21T23:51:58.607533: step 2660, loss 0.217176, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:51:59.288821: step 2660, loss 0.655344, acc 0.621481, rec 0.982887, pre 0.569397, f1 0.72107\n",
      "\n",
      "2018-04-21T23:51:59.334656: step 2661, loss 0.18722, acc 0.96\n",
      "2018-04-21T23:51:59.380585: step 2662, loss 0.278828, acc 0.9\n",
      "2018-04-21T23:51:59.426451: step 2663, loss 0.225039, acc 0.92\n",
      "2018-04-21T23:51:59.474430: step 2664, loss 0.279026, acc 0.9\n",
      "2018-04-21T23:51:59.523913: step 2665, loss 0.184127, acc 0.96\n",
      "2018-04-21T23:51:59.568587: step 2666, loss 0.205371, acc 0.94\n",
      "2018-04-21T23:51:59.614030: step 2667, loss 0.231914, acc 0.9\n",
      "2018-04-21T23:51:59.660605: step 2668, loss 0.237591, acc 0.94\n",
      "2018-04-21T23:51:59.706462: step 2669, loss 0.442504, acc 0.88\n",
      "2018-04-21T23:51:59.755509: step 2670, loss 0.329677, acc 0.88\n",
      "2018-04-21T23:51:59.801774: step 2671, loss 0.170415, acc 0.92\n",
      "2018-04-21T23:51:59.846468: step 2672, loss 0.203907, acc 0.96\n",
      "2018-04-21T23:51:59.890618: step 2673, loss 0.328012, acc 0.88\n",
      "2018-04-21T23:51:59.935724: step 2674, loss 0.240935, acc 0.88\n",
      "2018-04-21T23:51:59.984989: step 2675, loss 0.512437, acc 0.88\n",
      "2018-04-21T23:52:00.030523: step 2676, loss 0.225529, acc 0.92\n",
      "2018-04-21T23:52:00.077456: step 2677, loss 0.167789, acc 0.9\n",
      "2018-04-21T23:52:00.123670: step 2678, loss 0.357063, acc 0.84\n",
      "2018-04-21T23:52:00.169841: step 2679, loss 0.262274, acc 0.9\n",
      "2018-04-21T23:52:00.219538: step 2680, loss 0.282367, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:00.902363: step 2680, loss 0.515839, acc 0.704815, rec 0.97619, pre 0.63168, f1 0.767027\n",
      "\n",
      "2018-04-21T23:52:00.946995: step 2681, loss 0.266999, acc 0.92\n",
      "2018-04-21T23:52:00.991958: step 2682, loss 0.300315, acc 0.88\n",
      "2018-04-21T23:52:01.036363: step 2683, loss 0.283677, acc 0.92\n",
      "2018-04-21T23:52:01.081262: step 2684, loss 0.183008, acc 0.94\n",
      "2018-04-21T23:52:01.131550: step 2685, loss 0.151994, acc 0.92\n",
      "2018-04-21T23:52:01.178510: step 2686, loss 0.480511, acc 0.86\n",
      "2018-04-21T23:52:01.224484: step 2687, loss 0.277099, acc 0.9\n",
      "2018-04-21T23:52:01.271137: step 2688, loss 0.250119, acc 0.88\n",
      "2018-04-21T23:52:01.317083: step 2689, loss 0.324277, acc 0.9\n",
      "2018-04-21T23:52:01.367273: step 2690, loss 0.231538, acc 0.88\n",
      "2018-04-21T23:52:01.414171: step 2691, loss 0.283546, acc 0.88\n",
      "2018-04-21T23:52:01.461291: step 2692, loss 0.371437, acc 0.82\n",
      "2018-04-21T23:52:01.508511: step 2693, loss 0.286019, acc 0.88\n",
      "2018-04-21T23:52:01.555196: step 2694, loss 0.269045, acc 0.9\n",
      "2018-04-21T23:52:01.604662: step 2695, loss 0.260098, acc 0.88\n",
      "2018-04-21T23:52:01.651146: step 2696, loss 0.17772, acc 0.96\n",
      "2018-04-21T23:52:01.699120: step 2697, loss 0.261298, acc 0.9\n",
      "2018-04-21T23:52:01.745551: step 2698, loss 0.186116, acc 0.96\n",
      "2018-04-21T23:52:01.793392: step 2699, loss 0.117591, acc 0.98\n",
      "2018-04-21T23:52:01.844548: step 2700, loss 0.380091, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:02.527671: step 2700, loss 0.590673, acc 0.654815, rec 0.982887, pre 0.592377, f1 0.739228\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-2700\n",
      "\n",
      "2018-04-21T23:52:02.620079: step 2701, loss 0.155005, acc 0.96\n",
      "2018-04-21T23:52:02.666267: step 2702, loss 0.184872, acc 0.94\n",
      "2018-04-21T23:52:02.713156: step 2703, loss 0.28288, acc 0.88\n",
      "2018-04-21T23:52:02.763877: step 2704, loss 0.282527, acc 0.84\n",
      "2018-04-21T23:52:02.809973: step 2705, loss 0.153076, acc 0.98\n",
      "2018-04-21T23:52:02.857711: step 2706, loss 0.185964, acc 0.92\n",
      "2018-04-21T23:52:02.904330: step 2707, loss 0.167613, acc 0.94\n",
      "2018-04-21T23:52:02.950013: step 2708, loss 0.264108, acc 0.88\n",
      "2018-04-21T23:52:02.999939: step 2709, loss 0.150511, acc 0.96\n",
      "2018-04-21T23:52:03.045896: step 2710, loss 0.209673, acc 0.88\n",
      "2018-04-21T23:52:03.095706: step 2711, loss 0.247624, acc 0.92\n",
      "2018-04-21T23:52:03.142736: step 2712, loss 0.22628, acc 0.88\n",
      "2018-04-21T23:52:03.191452: step 2713, loss 0.464625, acc 0.88\n",
      "2018-04-21T23:52:03.241784: step 2714, loss 0.166461, acc 0.96\n",
      "2018-04-21T23:52:03.287922: step 2715, loss 0.250545, acc 0.9\n",
      "2018-04-21T23:52:03.335961: step 2716, loss 0.28107, acc 0.86\n",
      "2018-04-21T23:52:03.384205: step 2717, loss 0.249476, acc 0.9\n",
      "2018-04-21T23:52:03.432885: step 2718, loss 0.428246, acc 0.9\n",
      "2018-04-21T23:52:03.509633: step 2719, loss 0.245852, acc 0.88\n",
      "2018-04-21T23:52:03.555446: step 2720, loss 0.429744, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:04.242244: step 2720, loss 0.498583, acc 0.72, rec 0.973958, pre 0.644828, f1 0.775934\n",
      "\n",
      "2018-04-21T23:52:04.287253: step 2721, loss 0.434485, acc 0.88\n",
      "2018-04-21T23:52:04.333119: step 2722, loss 0.256627, acc 0.94\n",
      "2018-04-21T23:52:04.378900: step 2723, loss 0.320722, acc 0.86\n",
      "2018-04-21T23:52:04.424338: step 2724, loss 0.233422, acc 0.9\n",
      "2018-04-21T23:52:04.475203: step 2725, loss 0.283377, acc 0.86\n",
      "2018-04-21T23:52:04.520972: step 2726, loss 0.330597, acc 0.88\n",
      "2018-04-21T23:52:04.566210: step 2727, loss 0.172959, acc 0.94\n",
      "2018-04-21T23:52:04.613282: step 2728, loss 0.379347, acc 0.8\n",
      "2018-04-21T23:52:04.661188: step 2729, loss 0.276651, acc 0.88\n",
      "2018-04-21T23:52:04.712607: step 2730, loss 0.196608, acc 0.92\n",
      "2018-04-21T23:52:04.758939: step 2731, loss 0.151022, acc 0.94\n",
      "2018-04-21T23:52:04.806457: step 2732, loss 0.473389, acc 0.82\n",
      "2018-04-21T23:52:04.852040: step 2733, loss 0.216805, acc 0.92\n",
      "2018-04-21T23:52:04.897602: step 2734, loss 0.490339, acc 0.88\n",
      "2018-04-21T23:52:04.947276: step 2735, loss 0.246058, acc 0.96\n",
      "2018-04-21T23:52:04.994384: step 2736, loss 0.210162, acc 0.92\n",
      "2018-04-21T23:52:05.040994: step 2737, loss 0.37905, acc 0.86\n",
      "2018-04-21T23:52:05.087039: step 2738, loss 0.469399, acc 0.82\n",
      "2018-04-21T23:52:05.132527: step 2739, loss 0.332928, acc 0.84\n",
      "2018-04-21T23:52:05.182500: step 2740, loss 0.202424, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:05.865106: step 2740, loss 0.444159, acc 0.772593, rec 0.959077, pre 0.697511, f1 0.807644\n",
      "\n",
      "2018-04-21T23:52:05.909786: step 2741, loss 0.346671, acc 0.9\n",
      "2018-04-21T23:52:05.955124: step 2742, loss 0.249374, acc 0.9\n",
      "2018-04-21T23:52:06.000356: step 2743, loss 0.356711, acc 0.78\n",
      "2018-04-21T23:52:06.046774: step 2744, loss 0.141418, acc 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:52:06.096993: step 2745, loss 0.207905, acc 0.92\n",
      "2018-04-21T23:52:06.142225: step 2746, loss 0.341078, acc 0.94\n",
      "2018-04-21T23:52:06.187816: step 2747, loss 0.270807, acc 0.9\n",
      "2018-04-21T23:52:06.233513: step 2748, loss 0.230428, acc 0.94\n",
      "2018-04-21T23:52:06.279740: step 2749, loss 0.231474, acc 0.92\n",
      "2018-04-21T23:52:06.329456: step 2750, loss 0.217509, acc 0.86\n",
      "2018-04-21T23:52:06.374754: step 2751, loss 0.181488, acc 0.92\n",
      "2018-04-21T23:52:06.419972: step 2752, loss 0.283543, acc 0.86\n",
      "2018-04-21T23:52:06.465517: step 2753, loss 0.397348, acc 0.82\n",
      "2018-04-21T23:52:06.510562: step 2754, loss 0.217224, acc 0.92\n",
      "2018-04-21T23:52:06.559943: step 2755, loss 0.275315, acc 0.88\n",
      "2018-04-21T23:52:06.605059: step 2756, loss 0.164606, acc 0.94\n",
      "2018-04-21T23:52:06.650226: step 2757, loss 0.220069, acc 0.92\n",
      "2018-04-21T23:52:06.695768: step 2758, loss 0.223546, acc 0.88\n",
      "2018-04-21T23:52:06.740673: step 2759, loss 0.288269, acc 0.84\n",
      "2018-04-21T23:52:06.789419: step 2760, loss 0.313622, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:07.470801: step 2760, loss 0.615237, acc 0.643704, rec 0.980655, pre 0.584738, f1 0.732629\n",
      "\n",
      "2018-04-21T23:52:07.515943: step 2761, loss 0.34095, acc 0.92\n",
      "2018-04-21T23:52:07.560836: step 2762, loss 0.270457, acc 0.88\n",
      "2018-04-21T23:52:07.606403: step 2763, loss 0.303601, acc 0.86\n",
      "2018-04-21T23:52:07.652093: step 2764, loss 0.265567, acc 0.88\n",
      "2018-04-21T23:52:07.702071: step 2765, loss 0.358691, acc 0.84\n",
      "2018-04-21T23:52:07.748060: step 2766, loss 0.206315, acc 0.96\n",
      "2018-04-21T23:52:07.797822: step 2767, loss 0.557741, acc 0.82\n",
      "2018-04-21T23:52:07.846464: step 2768, loss 0.219682, acc 0.94\n",
      "2018-04-21T23:52:07.895267: step 2769, loss 0.203181, acc 0.96\n",
      "2018-04-21T23:52:07.946536: step 2770, loss 0.239497, acc 0.92\n",
      "2018-04-21T23:52:07.993496: step 2771, loss 0.38585, acc 0.84\n",
      "2018-04-21T23:52:08.041150: step 2772, loss 0.265929, acc 0.84\n",
      "2018-04-21T23:52:08.086807: step 2773, loss 0.461248, acc 0.84\n",
      "2018-04-21T23:52:08.132049: step 2774, loss 0.284363, acc 0.88\n",
      "2018-04-21T23:52:08.181968: step 2775, loss 0.160075, acc 0.96\n",
      "2018-04-21T23:52:08.228003: step 2776, loss 0.18664, acc 0.94\n",
      "2018-04-21T23:52:08.273392: step 2777, loss 0.327998, acc 0.88\n",
      "2018-04-21T23:52:08.319910: step 2778, loss 0.195262, acc 0.94\n",
      "2018-04-21T23:52:08.365312: step 2779, loss 0.202518, acc 0.94\n",
      "2018-04-21T23:52:08.413944: step 2780, loss 0.25196, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:09.092818: step 2780, loss 0.455035, acc 0.760741, rec 0.960565, pre 0.685244, f1 0.799876\n",
      "\n",
      "2018-04-21T23:52:09.137389: step 2781, loss 0.18271, acc 0.92\n",
      "2018-04-21T23:52:09.184787: step 2782, loss 0.409749, acc 0.86\n",
      "2018-04-21T23:52:09.231381: step 2783, loss 0.179132, acc 0.94\n",
      "2018-04-21T23:52:09.277989: step 2784, loss 0.180145, acc 0.98\n",
      "2018-04-21T23:52:09.326680: step 2785, loss 0.309394, acc 0.88\n",
      "2018-04-21T23:52:09.372692: step 2786, loss 0.145366, acc 0.96\n",
      "2018-04-21T23:52:09.418598: step 2787, loss 0.272387, acc 0.88\n",
      "2018-04-21T23:52:09.463998: step 2788, loss 0.154144, acc 0.96\n",
      "2018-04-21T23:52:09.508917: step 2789, loss 0.224074, acc 0.92\n",
      "2018-04-21T23:52:09.557818: step 2790, loss 0.230321, acc 0.9\n",
      "2018-04-21T23:52:09.603513: step 2791, loss 0.175576, acc 0.98\n",
      "2018-04-21T23:52:09.650740: step 2792, loss 0.187286, acc 0.94\n",
      "2018-04-21T23:52:09.697609: step 2793, loss 0.279615, acc 0.96\n",
      "2018-04-21T23:52:09.742942: step 2794, loss 0.401318, acc 0.88\n",
      "2018-04-21T23:52:09.793803: step 2795, loss 0.482215, acc 0.78\n",
      "2018-04-21T23:52:09.838996: step 2796, loss 0.609366, acc 0.8\n",
      "2018-04-21T23:52:09.883357: step 2797, loss 0.550101, acc 0.8\n",
      "2018-04-21T23:52:09.929545: step 2798, loss 0.481002, acc 0.76\n",
      "2018-04-21T23:52:09.976137: step 2799, loss 0.367946, acc 0.86\n",
      "2018-04-21T23:52:10.026654: step 2800, loss 0.288397, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:10.710278: step 2800, loss 0.379328, acc 0.86037, rec 0.795387, pre 0.912895, f1 0.850099\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-2800\n",
      "\n",
      "2018-04-21T23:52:10.803212: step 2801, loss 0.287261, acc 0.88\n",
      "2018-04-21T23:52:10.849235: step 2802, loss 0.227073, acc 0.92\n",
      "2018-04-21T23:52:10.893345: step 2803, loss 0.14746, acc 0.94\n",
      "2018-04-21T23:52:10.942367: step 2804, loss 0.306336, acc 0.88\n",
      "2018-04-21T23:52:10.987726: step 2805, loss 0.276707, acc 0.88\n",
      "2018-04-21T23:52:11.033584: step 2806, loss 0.198003, acc 0.92\n",
      "2018-04-21T23:52:11.079821: step 2807, loss 0.269468, acc 0.88\n",
      "2018-04-21T23:52:11.125929: step 2808, loss 0.409819, acc 0.82\n",
      "2018-04-21T23:52:11.175681: step 2809, loss 0.186359, acc 0.92\n",
      "2018-04-21T23:52:11.233721: step 2810, loss 0.24225, acc 0.92\n",
      "2018-04-21T23:52:11.279413: step 2811, loss 0.320345, acc 0.92\n",
      "2018-04-21T23:52:11.327039: step 2812, loss 0.241936, acc 0.9\n",
      "2018-04-21T23:52:11.373625: step 2813, loss 0.392758, acc 0.86\n",
      "2018-04-21T23:52:11.422145: step 2814, loss 0.23311, acc 0.92\n",
      "2018-04-21T23:52:11.468312: step 2815, loss 0.25345, acc 0.9\n",
      "2018-04-21T23:52:11.515549: step 2816, loss 0.282866, acc 0.94\n",
      "2018-04-21T23:52:11.561830: step 2817, loss 0.304989, acc 0.86\n",
      "2018-04-21T23:52:11.607778: step 2818, loss 0.110593, acc 0.98\n",
      "2018-04-21T23:52:11.657112: step 2819, loss 0.125556, acc 0.96\n",
      "2018-04-21T23:52:11.703539: step 2820, loss 0.14139, acc 0.98\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:12.383883: step 2820, loss 0.535381, acc 0.694074, rec 0.977679, pre 0.622749, f1 0.760857\n",
      "\n",
      "2018-04-21T23:52:12.429367: step 2821, loss 0.200103, acc 0.94\n",
      "2018-04-21T23:52:12.475385: step 2822, loss 0.44396, acc 0.78\n",
      "2018-04-21T23:52:12.525632: step 2823, loss 0.519101, acc 0.76\n",
      "2018-04-21T23:52:12.570163: step 2824, loss 0.421424, acc 0.82\n",
      "2018-04-21T23:52:12.620315: step 2825, loss 0.264157, acc 0.88\n",
      "2018-04-21T23:52:12.667217: step 2826, loss 0.135349, acc 0.96\n",
      "2018-04-21T23:52:12.713739: step 2827, loss 0.325269, acc 0.86\n",
      "2018-04-21T23:52:12.758978: step 2828, loss 0.369068, acc 0.88\n",
      "2018-04-21T23:52:12.804793: step 2829, loss 0.250468, acc 0.92\n",
      "2018-04-21T23:52:12.853975: step 2830, loss 0.178367, acc 0.96\n",
      "2018-04-21T23:52:12.898917: step 2831, loss 0.24431, acc 0.92\n",
      "2018-04-21T23:52:12.944881: step 2832, loss 0.226804, acc 0.94\n",
      "2018-04-21T23:52:12.991065: step 2833, loss 0.266757, acc 0.88\n",
      "2018-04-21T23:52:13.037184: step 2834, loss 0.455126, acc 0.88\n",
      "2018-04-21T23:52:13.088047: step 2835, loss 0.258016, acc 0.84\n",
      "2018-04-21T23:52:13.134298: step 2836, loss 0.290892, acc 0.88\n",
      "2018-04-21T23:52:13.180858: step 2837, loss 0.181349, acc 0.94\n",
      "2018-04-21T23:52:13.227164: step 2838, loss 0.176113, acc 0.94\n",
      "2018-04-21T23:52:13.273505: step 2839, loss 0.131231, acc 0.96\n",
      "2018-04-21T23:52:13.323329: step 2840, loss 0.31842, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:13.986593: step 2840, loss 0.423913, acc 0.786296, rec 0.946429, pre 0.715813, f1 0.815123\n",
      "\n",
      "2018-04-21T23:52:14.030756: step 2841, loss 0.222828, acc 0.9\n",
      "2018-04-21T23:52:14.074876: step 2842, loss 0.115309, acc 1\n",
      "2018-04-21T23:52:14.119119: step 2843, loss 0.225646, acc 0.86\n",
      "2018-04-21T23:52:14.163093: step 2844, loss 0.452777, acc 0.86\n",
      "2018-04-21T23:52:14.212828: step 2845, loss 0.37655, acc 0.82\n",
      "2018-04-21T23:52:14.256638: step 2846, loss 0.42075, acc 0.8\n",
      "2018-04-21T23:52:14.301678: step 2847, loss 0.351153, acc 0.84\n",
      "2018-04-21T23:52:14.347331: step 2848, loss 0.319401, acc 0.96\n",
      "2018-04-21T23:52:14.391215: step 2849, loss 0.539589, acc 0.76\n",
      "2018-04-21T23:52:14.438797: step 2850, loss 0.363993, acc 0.9\n",
      "2018-04-21T23:52:14.483369: step 2851, loss 0.254293, acc 0.94\n",
      "2018-04-21T23:52:14.527181: step 2852, loss 0.11267, acc 0.98\n",
      "2018-04-21T23:52:14.572450: step 2853, loss 0.230383, acc 0.92\n",
      "2018-04-21T23:52:14.616599: step 2854, loss 0.146328, acc 0.94\n",
      "2018-04-21T23:52:14.664249: step 2855, loss 0.283887, acc 0.92\n",
      "2018-04-21T23:52:14.708740: step 2856, loss 0.315171, acc 0.94\n",
      "2018-04-21T23:52:14.753591: step 2857, loss 0.249448, acc 0.88\n",
      "2018-04-21T23:52:14.797533: step 2858, loss 0.344696, acc 0.86\n",
      "2018-04-21T23:52:14.842210: step 2859, loss 0.151931, acc 0.96\n",
      "2018-04-21T23:52:14.889162: step 2860, loss 0.388632, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:15.522938: step 2860, loss 0.408897, acc 0.809259, rec 0.935268, pre 0.745994, f1 0.829977\n",
      "\n",
      "2018-04-21T23:52:15.567422: step 2861, loss 0.198581, acc 0.94\n",
      "2018-04-21T23:52:15.612746: step 2862, loss 0.265197, acc 0.88\n",
      "2018-04-21T23:52:15.658888: step 2863, loss 0.236128, acc 0.9\n",
      "2018-04-21T23:52:15.702999: step 2864, loss 0.260232, acc 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:52:15.750545: step 2865, loss 0.30142, acc 0.86\n",
      "2018-04-21T23:52:15.795189: step 2866, loss 0.247441, acc 0.94\n",
      "2018-04-21T23:52:15.839320: step 2867, loss 0.314082, acc 0.84\n",
      "2018-04-21T23:52:15.882934: step 2868, loss 0.20163, acc 0.9\n",
      "2018-04-21T23:52:15.926966: step 2869, loss 0.293101, acc 0.9\n",
      "2018-04-21T23:52:15.974383: step 2870, loss 0.524773, acc 0.9\n",
      "2018-04-21T23:52:16.018308: step 2871, loss 0.196829, acc 0.9\n",
      "2018-04-21T23:52:16.062189: step 2872, loss 0.17074, acc 0.96\n",
      "2018-04-21T23:52:16.106102: step 2873, loss 0.210966, acc 0.94\n",
      "2018-04-21T23:52:16.149717: step 2874, loss 0.495958, acc 0.84\n",
      "2018-04-21T23:52:16.197158: step 2875, loss 0.149357, acc 0.96\n",
      "2018-04-21T23:52:16.240754: step 2876, loss 0.180438, acc 0.96\n",
      "2018-04-21T23:52:16.283961: step 2877, loss 0.227207, acc 0.9\n",
      "2018-04-21T23:52:16.328833: step 2878, loss 0.366695, acc 0.9\n",
      "2018-04-21T23:52:16.374033: step 2879, loss 0.219015, acc 0.9\n",
      "2018-04-21T23:52:16.420840: step 2880, loss 0.184535, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:17.060175: step 2880, loss 0.371954, acc 0.864815, rec 0.817708, pre 0.901559, f1 0.857589\n",
      "\n",
      "2018-04-21T23:52:17.104778: step 2881, loss 0.24913, acc 0.92\n",
      "2018-04-21T23:52:17.148653: step 2882, loss 0.158293, acc 0.94\n",
      "2018-04-21T23:52:17.192325: step 2883, loss 0.145769, acc 0.94\n",
      "2018-04-21T23:52:17.236536: step 2884, loss 0.283993, acc 0.96\n",
      "2018-04-21T23:52:17.284102: step 2885, loss 0.211077, acc 0.92\n",
      "2018-04-21T23:52:17.332067: step 2886, loss 0.296855, acc 0.9\n",
      "2018-04-21T23:52:17.375337: step 2887, loss 0.140509, acc 0.94\n",
      "2018-04-21T23:52:17.418792: step 2888, loss 0.254651, acc 0.88\n",
      "2018-04-21T23:52:17.462489: step 2889, loss 0.261844, acc 0.92\n",
      "2018-04-21T23:52:17.510729: step 2890, loss 0.272258, acc 0.86\n",
      "2018-04-21T23:52:17.554354: step 2891, loss 0.443115, acc 0.86\n",
      "2018-04-21T23:52:17.598046: step 2892, loss 0.143141, acc 0.96\n",
      "2018-04-21T23:52:17.641585: step 2893, loss 0.232737, acc 0.9\n",
      "2018-04-21T23:52:17.685390: step 2894, loss 0.139822, acc 0.94\n",
      "2018-04-21T23:52:17.732144: step 2895, loss 0.188508, acc 0.92\n",
      "2018-04-21T23:52:17.776070: step 2896, loss 0.163664, acc 0.94\n",
      "2018-04-21T23:52:17.820181: step 2897, loss 0.408806, acc 0.86\n",
      "2018-04-21T23:52:17.864110: step 2898, loss 0.219601, acc 0.92\n",
      "2018-04-21T23:52:17.908268: step 2899, loss 0.174285, acc 0.94\n",
      "2018-04-21T23:52:17.956323: step 2900, loss 0.153835, acc 0.98\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:18.595826: step 2900, loss 0.383774, acc 0.841111, rec 0.901786, pre 0.803181, f1 0.849632\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-2900\n",
      "\n",
      "2018-04-21T23:52:18.682032: step 2901, loss 0.177536, acc 0.94\n",
      "2018-04-21T23:52:18.725935: step 2902, loss 0.310782, acc 0.86\n",
      "2018-04-21T23:52:18.771896: step 2903, loss 0.149424, acc 0.96\n",
      "2018-04-21T23:52:18.821848: step 2904, loss 0.262414, acc 0.92\n",
      "2018-04-21T23:52:18.867145: step 2905, loss 0.295286, acc 0.86\n",
      "2018-04-21T23:52:18.911669: step 2906, loss 0.316695, acc 0.86\n",
      "2018-04-21T23:52:18.956608: step 2907, loss 0.308105, acc 0.88\n",
      "2018-04-21T23:52:19.002087: step 2908, loss 0.203387, acc 0.92\n",
      "2018-04-21T23:52:19.052461: step 2909, loss 0.637565, acc 0.8\n",
      "2018-04-21T23:52:19.098076: step 2910, loss 0.356486, acc 0.8\n",
      "2018-04-21T23:52:19.142942: step 2911, loss 0.173716, acc 0.94\n",
      "2018-04-21T23:52:19.187542: step 2912, loss 0.194615, acc 0.98\n",
      "2018-04-21T23:52:19.233714: step 2913, loss 0.160798, acc 0.94\n",
      "2018-04-21T23:52:19.283669: step 2914, loss 0.273516, acc 0.86\n",
      "2018-04-21T23:52:19.328543: step 2915, loss 0.222009, acc 0.94\n",
      "2018-04-21T23:52:19.372697: step 2916, loss 0.410136, acc 0.84\n",
      "2018-04-21T23:52:19.417342: step 2917, loss 0.170177, acc 0.94\n",
      "2018-04-21T23:52:19.462600: step 2918, loss 0.227587, acc 0.92\n",
      "2018-04-21T23:52:19.511083: step 2919, loss 0.245302, acc 0.9\n",
      "2018-04-21T23:52:19.555971: step 2920, loss 0.250884, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:20.240378: step 2920, loss 0.368809, acc 0.864815, rec 0.833333, pre 0.888184, f1 0.859885\n",
      "\n",
      "2018-04-21T23:52:20.286196: step 2921, loss 0.344612, acc 0.92\n",
      "2018-04-21T23:52:20.332177: step 2922, loss 0.233451, acc 0.94\n",
      "2018-04-21T23:52:20.377167: step 2923, loss 0.427786, acc 0.88\n",
      "2018-04-21T23:52:20.421482: step 2924, loss 0.297356, acc 0.8\n",
      "2018-04-21T23:52:20.471483: step 2925, loss 0.17376, acc 0.96\n",
      "2018-04-21T23:52:20.517255: step 2926, loss 0.271986, acc 0.88\n",
      "2018-04-21T23:52:20.561999: step 2927, loss 0.244542, acc 0.88\n",
      "2018-04-21T23:52:20.606631: step 2928, loss 0.288765, acc 0.88\n",
      "2018-04-21T23:52:20.651064: step 2929, loss 0.255941, acc 0.9\n",
      "2018-04-21T23:52:20.699541: step 2930, loss 0.202179, acc 0.94\n",
      "2018-04-21T23:52:20.744153: step 2931, loss 0.167763, acc 0.96\n",
      "2018-04-21T23:52:20.788858: step 2932, loss 0.237697, acc 0.9\n",
      "2018-04-21T23:52:20.833260: step 2933, loss 0.250252, acc 0.9\n",
      "2018-04-21T23:52:20.878726: step 2934, loss 0.189444, acc 0.94\n",
      "2018-04-21T23:52:20.928495: step 2935, loss 0.263515, acc 0.9\n",
      "2018-04-21T23:52:20.972997: step 2936, loss 0.265715, acc 0.92\n",
      "2018-04-21T23:52:21.017838: step 2937, loss 0.264442, acc 0.9\n",
      "2018-04-21T23:52:21.063059: step 2938, loss 0.296384, acc 0.9\n",
      "2018-04-21T23:52:21.108940: step 2939, loss 0.273487, acc 0.88\n",
      "2018-04-21T23:52:21.158044: step 2940, loss 0.275276, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:21.838960: step 2940, loss 0.551093, acc 0.684074, rec 0.979911, pre 0.614559, f1 0.755377\n",
      "\n",
      "2018-04-21T23:52:21.884169: step 2941, loss 0.208644, acc 0.9\n",
      "2018-04-21T23:52:21.929347: step 2942, loss 0.186225, acc 0.94\n",
      "2018-04-21T23:52:21.975333: step 2943, loss 0.131541, acc 1\n",
      "2018-04-21T23:52:22.021223: step 2944, loss 0.229162, acc 0.9\n",
      "2018-04-21T23:52:22.070340: step 2945, loss 0.191918, acc 0.98\n",
      "2018-04-21T23:52:22.115540: step 2946, loss 0.291751, acc 0.94\n",
      "2018-04-21T23:52:22.161403: step 2947, loss 0.235887, acc 0.9\n",
      "2018-04-21T23:52:22.206130: step 2948, loss 0.450265, acc 0.86\n",
      "2018-04-21T23:52:22.252848: step 2949, loss 0.149119, acc 0.96\n",
      "2018-04-21T23:52:22.301162: step 2950, loss 0.16542, acc 0.96\n",
      "2018-04-21T23:52:22.345655: step 2951, loss 0.267977, acc 0.9\n",
      "2018-04-21T23:52:22.390067: step 2952, loss 0.194495, acc 0.96\n",
      "2018-04-21T23:52:22.434501: step 2953, loss 0.105451, acc 1\n",
      "2018-04-21T23:52:22.479298: step 2954, loss 0.222908, acc 0.92\n",
      "2018-04-21T23:52:22.528012: step 2955, loss 0.244156, acc 0.88\n",
      "2018-04-21T23:52:22.573473: step 2956, loss 0.471615, acc 0.8\n",
      "2018-04-21T23:52:22.617630: step 2957, loss 0.236222, acc 0.92\n",
      "2018-04-21T23:52:22.663014: step 2958, loss 0.130343, acc 1\n",
      "2018-04-21T23:52:22.709296: step 2959, loss 0.224819, acc 0.88\n",
      "2018-04-21T23:52:22.759144: step 2960, loss 0.447597, acc 0.76\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:23.440293: step 2960, loss 0.437813, acc 0.798519, rec 0.622024, pre 0.958716, f1 0.754513\n",
      "\n",
      "2018-04-21T23:52:23.485322: step 2961, loss 0.314525, acc 0.92\n",
      "2018-04-21T23:52:23.530472: step 2962, loss 0.330968, acc 0.86\n",
      "2018-04-21T23:52:23.575051: step 2963, loss 0.322435, acc 0.92\n",
      "2018-04-21T23:52:23.619332: step 2964, loss 0.265691, acc 0.9\n",
      "2018-04-21T23:52:23.667397: step 2965, loss 0.209595, acc 0.92\n",
      "2018-04-21T23:52:23.713273: step 2966, loss 0.165754, acc 0.92\n",
      "2018-04-21T23:52:23.757463: step 2967, loss 0.280273, acc 0.84\n",
      "2018-04-21T23:52:23.802384: step 2968, loss 0.080136, acc 1\n",
      "2018-04-21T23:52:23.846856: step 2969, loss 0.420362, acc 0.92\n",
      "2018-04-21T23:52:23.895901: step 2970, loss 0.281498, acc 0.88\n",
      "2018-04-21T23:52:23.941535: step 2971, loss 0.401119, acc 0.88\n",
      "2018-04-21T23:52:23.985879: step 2972, loss 0.231537, acc 0.9\n",
      "2018-04-21T23:52:24.030752: step 2973, loss 0.184701, acc 0.94\n",
      "2018-04-21T23:52:24.075439: step 2974, loss 0.247587, acc 0.86\n",
      "2018-04-21T23:52:24.124098: step 2975, loss 0.33012, acc 0.92\n",
      "2018-04-21T23:52:24.169183: step 2976, loss 0.137348, acc 0.94\n",
      "2018-04-21T23:52:24.214750: step 2977, loss 0.196313, acc 0.92\n",
      "2018-04-21T23:52:24.261907: step 2978, loss 0.220806, acc 0.92\n",
      "2018-04-21T23:52:24.307453: step 2979, loss 0.195366, acc 0.9\n",
      "2018-04-21T23:52:24.357639: step 2980, loss 0.370951, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:25.039467: step 2980, loss 0.575302, acc 0.669259, rec 0.981399, pre 0.603109, f1 0.747097\n",
      "\n",
      "2018-04-21T23:52:25.083910: step 2981, loss 0.16527, acc 0.96\n",
      "2018-04-21T23:52:25.129425: step 2982, loss 0.139108, acc 0.94\n",
      "2018-04-21T23:52:25.174394: step 2983, loss 0.1461, acc 0.96\n",
      "2018-04-21T23:52:25.219666: step 2984, loss 0.11545, acc 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:52:25.268468: step 2985, loss 0.250116, acc 0.88\n",
      "2018-04-21T23:52:25.312718: step 2986, loss 0.474327, acc 0.82\n",
      "2018-04-21T23:52:25.356528: step 2987, loss 0.542786, acc 0.9\n",
      "2018-04-21T23:52:25.401299: step 2988, loss 0.341618, acc 0.86\n",
      "2018-04-21T23:52:25.445637: step 2989, loss 0.2369, acc 0.9\n",
      "2018-04-21T23:52:25.494350: step 2990, loss 0.288062, acc 0.9\n",
      "2018-04-21T23:52:25.539958: step 2991, loss 0.162518, acc 0.94\n",
      "2018-04-21T23:52:25.585499: step 2992, loss 0.400154, acc 0.8\n",
      "2018-04-21T23:52:25.630551: step 2993, loss 0.340858, acc 0.86\n",
      "2018-04-21T23:52:25.674826: step 2994, loss 0.336757, acc 0.88\n",
      "2018-04-21T23:52:25.724599: step 2995, loss 0.210433, acc 0.92\n",
      "2018-04-21T23:52:25.769952: step 2996, loss 0.182758, acc 0.96\n",
      "2018-04-21T23:52:25.815239: step 2997, loss 0.254587, acc 0.9\n",
      "2018-04-21T23:52:25.859680: step 2998, loss 0.251707, acc 0.88\n",
      "2018-04-21T23:52:25.904798: step 2999, loss 0.176956, acc 0.94\n",
      "2018-04-21T23:52:25.954071: step 3000, loss 0.35782, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:26.636238: step 3000, loss 0.412119, acc 0.803333, rec 0.940476, pre 0.737026, f1 0.826414\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-3000\n",
      "\n",
      "2018-04-21T23:52:26.727591: step 3001, loss 0.176593, acc 0.94\n",
      "2018-04-21T23:52:26.772022: step 3002, loss 0.444235, acc 0.86\n",
      "2018-04-21T23:52:26.816273: step 3003, loss 0.316623, acc 0.92\n",
      "2018-04-21T23:52:26.863244: step 3004, loss 0.235742, acc 0.88\n",
      "2018-04-21T23:52:26.906847: step 3005, loss 0.175518, acc 0.96\n",
      "2018-04-21T23:52:26.950536: step 3006, loss 0.205253, acc 0.96\n",
      "2018-04-21T23:52:26.993376: step 3007, loss 0.145138, acc 0.96\n",
      "2018-04-21T23:52:27.036736: step 3008, loss 0.212166, acc 0.96\n",
      "2018-04-21T23:52:27.083208: step 3009, loss 0.216817, acc 0.92\n",
      "2018-04-21T23:52:27.126991: step 3010, loss 0.420553, acc 0.76\n",
      "2018-04-21T23:52:27.170384: step 3011, loss 0.436138, acc 0.78\n",
      "2018-04-21T23:52:27.214491: step 3012, loss 0.273298, acc 0.96\n",
      "2018-04-21T23:52:27.258247: step 3013, loss 0.347822, acc 0.84\n",
      "2018-04-21T23:52:27.305342: step 3014, loss 0.27439, acc 0.9\n",
      "2018-04-21T23:52:27.348739: step 3015, loss 0.18931, acc 0.92\n",
      "2018-04-21T23:52:27.392090: step 3016, loss 0.226239, acc 0.94\n",
      "2018-04-21T23:52:27.436469: step 3017, loss 0.250307, acc 0.9\n",
      "2018-04-21T23:52:27.480052: step 3018, loss 0.229773, acc 0.88\n",
      "2018-04-21T23:52:27.526350: step 3019, loss 0.308451, acc 0.82\n",
      "2018-04-21T23:52:27.569834: step 3020, loss 0.315335, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:28.204170: step 3020, loss 0.893947, acc 0.565556, rec 0.991071, pre 0.534296, f1 0.694292\n",
      "\n",
      "2018-04-21T23:52:28.247013: step 3021, loss 0.338165, acc 0.88\n",
      "2018-04-21T23:52:28.290799: step 3022, loss 0.324253, acc 0.88\n",
      "2018-04-21T23:52:28.334886: step 3023, loss 0.277678, acc 0.92\n",
      "2018-04-21T23:52:28.378085: step 3024, loss 0.359992, acc 0.9\n",
      "2018-04-21T23:52:28.425304: step 3025, loss 0.212872, acc 0.94\n",
      "2018-04-21T23:52:28.468772: step 3026, loss 0.324514, acc 0.94\n",
      "2018-04-21T23:52:28.512484: step 3027, loss 0.199871, acc 0.94\n",
      "2018-04-21T23:52:28.556283: step 3028, loss 0.231183, acc 0.9\n",
      "2018-04-21T23:52:28.601307: step 3029, loss 0.396843, acc 0.82\n",
      "2018-04-21T23:52:28.648690: step 3030, loss 0.182798, acc 0.92\n",
      "2018-04-21T23:52:28.692667: step 3031, loss 0.555022, acc 0.82\n",
      "2018-04-21T23:52:28.737710: step 3032, loss 0.363632, acc 0.84\n",
      "2018-04-21T23:52:28.783346: step 3033, loss 0.313153, acc 0.9\n",
      "2018-04-21T23:52:28.827377: step 3034, loss 0.259545, acc 0.9\n",
      "2018-04-21T23:52:28.874180: step 3035, loss 0.16282, acc 0.9\n",
      "2018-04-21T23:52:28.917784: step 3036, loss 0.255815, acc 0.92\n",
      "2018-04-21T23:52:28.961697: step 3037, loss 0.266501, acc 0.86\n",
      "2018-04-21T23:52:29.005613: step 3038, loss 0.187839, acc 0.96\n",
      "2018-04-21T23:52:29.049493: step 3039, loss 0.293313, acc 0.88\n",
      "2018-04-21T23:52:29.097278: step 3040, loss 0.267207, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:29.730685: step 3040, loss 0.402364, acc 0.812593, rec 0.93006, pre 0.752106, f1 0.83167\n",
      "\n",
      "2018-04-21T23:52:29.776551: step 3041, loss 0.212568, acc 0.9\n",
      "2018-04-21T23:52:29.820703: step 3042, loss 0.162377, acc 0.96\n",
      "2018-04-21T23:52:29.864797: step 3043, loss 0.312267, acc 0.86\n",
      "2018-04-21T23:52:29.909079: step 3044, loss 0.121331, acc 0.96\n",
      "2018-04-21T23:52:29.956732: step 3045, loss 0.35078, acc 0.88\n",
      "2018-04-21T23:52:30.000052: step 3046, loss 0.284374, acc 0.84\n",
      "2018-04-21T23:52:30.044139: step 3047, loss 0.344082, acc 0.84\n",
      "2018-04-21T23:52:30.088794: step 3048, loss 0.317732, acc 0.86\n",
      "2018-04-21T23:52:30.132909: step 3049, loss 0.271051, acc 0.9\n",
      "2018-04-21T23:52:30.180655: step 3050, loss 0.231581, acc 0.9\n",
      "2018-04-21T23:52:30.224777: step 3051, loss 0.221743, acc 0.96\n",
      "2018-04-21T23:52:30.269932: step 3052, loss 0.242231, acc 0.9\n",
      "2018-04-21T23:52:30.315198: step 3053, loss 0.193953, acc 0.94\n",
      "2018-04-21T23:52:30.359700: step 3054, loss 0.221441, acc 0.88\n",
      "2018-04-21T23:52:30.407861: step 3055, loss 0.258277, acc 0.9\n",
      "2018-04-21T23:52:30.452131: step 3056, loss 0.536498, acc 0.86\n",
      "2018-04-21T23:52:30.496285: step 3057, loss 0.588847, acc 0.76\n",
      "2018-04-21T23:52:30.540528: step 3058, loss 0.275206, acc 0.82\n",
      "2018-04-21T23:52:30.585097: step 3059, loss 0.269848, acc 0.82\n",
      "2018-04-21T23:52:30.633324: step 3060, loss 0.155489, acc 0.98\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:31.265255: step 3060, loss 0.370535, acc 0.862222, rec 0.813244, pre 0.900329, f1 0.854574\n",
      "\n",
      "2018-04-21T23:52:31.307877: step 3061, loss 0.170253, acc 0.92\n",
      "2018-04-21T23:52:31.352068: step 3062, loss 0.151336, acc 0.96\n",
      "2018-04-21T23:52:31.396511: step 3063, loss 0.13176, acc 0.98\n",
      "2018-04-21T23:52:31.441111: step 3064, loss 0.177846, acc 0.94\n",
      "2018-04-21T23:52:31.488407: step 3065, loss 0.221855, acc 0.94\n",
      "2018-04-21T23:52:31.531523: step 3066, loss 0.274517, acc 0.86\n",
      "2018-04-21T23:52:31.574856: step 3067, loss 0.262169, acc 0.88\n",
      "2018-04-21T23:52:31.618943: step 3068, loss 0.314228, acc 0.94\n",
      "2018-04-21T23:52:31.662688: step 3069, loss 0.349217, acc 0.88\n",
      "2018-04-21T23:52:31.709885: step 3070, loss 0.22383, acc 0.96\n",
      "2018-04-21T23:52:31.753891: step 3071, loss 0.248836, acc 0.88\n",
      "2018-04-21T23:52:31.798779: step 3072, loss 0.19248, acc 0.92\n",
      "2018-04-21T23:52:31.842021: step 3073, loss 0.172751, acc 0.96\n",
      "2018-04-21T23:52:31.885093: step 3074, loss 0.309721, acc 0.9\n",
      "2018-04-21T23:52:31.931628: step 3075, loss 0.185863, acc 0.98\n",
      "2018-04-21T23:52:31.974802: step 3076, loss 0.190124, acc 0.9\n",
      "2018-04-21T23:52:32.018442: step 3077, loss 0.34546, acc 0.84\n",
      "2018-04-21T23:52:32.062387: step 3078, loss 0.300071, acc 0.88\n",
      "2018-04-21T23:52:32.107823: step 3079, loss 0.176547, acc 0.96\n",
      "2018-04-21T23:52:32.154801: step 3080, loss 0.157304, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:32.812986: step 3080, loss 0.41163, acc 0.801481, rec 0.940476, pre 0.734884, f1 0.825065\n",
      "\n",
      "2018-04-21T23:52:32.858318: step 3081, loss 0.239111, acc 0.88\n",
      "2018-04-21T23:52:32.903552: step 3082, loss 0.1862, acc 0.9\n",
      "2018-04-21T23:52:32.947830: step 3083, loss 0.25531, acc 0.88\n",
      "2018-04-21T23:52:32.991651: step 3084, loss 0.281901, acc 0.86\n",
      "2018-04-21T23:52:33.040019: step 3085, loss 0.559509, acc 0.84\n",
      "2018-04-21T23:52:33.085239: step 3086, loss 0.150262, acc 0.94\n",
      "2018-04-21T23:52:33.130487: step 3087, loss 0.360726, acc 0.9\n",
      "2018-04-21T23:52:33.174366: step 3088, loss 0.145121, acc 0.94\n",
      "2018-04-21T23:52:33.218634: step 3089, loss 0.20556, acc 0.9\n",
      "2018-04-21T23:52:33.266421: step 3090, loss 0.193414, acc 0.94\n",
      "2018-04-21T23:52:33.311583: step 3091, loss 0.274426, acc 0.88\n",
      "2018-04-21T23:52:33.356304: step 3092, loss 0.363926, acc 0.84\n",
      "2018-04-21T23:52:33.401286: step 3093, loss 0.308488, acc 0.86\n",
      "2018-04-21T23:52:33.446554: step 3094, loss 0.357078, acc 0.88\n",
      "2018-04-21T23:52:33.495908: step 3095, loss 0.378353, acc 0.78\n",
      "2018-04-21T23:52:33.541186: step 3096, loss 0.206569, acc 0.94\n",
      "2018-04-21T23:52:33.585987: step 3097, loss 0.137626, acc 0.94\n",
      "2018-04-21T23:52:33.631130: step 3098, loss 0.216083, acc 0.96\n",
      "2018-04-21T23:52:33.679143: step 3099, loss 0.268779, acc 0.92\n",
      "2018-04-21T23:52:33.728536: step 3100, loss 0.1802, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:34.406096: step 3100, loss 0.389948, acc 0.828148, rec 0.918155, pre 0.777078, f1 0.841746\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-3100\n",
      "\n",
      "2018-04-21T23:52:34.496405: step 3101, loss 0.205373, acc 0.92\n",
      "2018-04-21T23:52:34.540616: step 3102, loss 0.296594, acc 0.88\n",
      "2018-04-21T23:52:34.585170: step 3103, loss 0.254866, acc 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:52:34.635142: step 3104, loss 0.165309, acc 0.94\n",
      "2018-04-21T23:52:34.681937: step 3105, loss 0.187807, acc 0.94\n",
      "2018-04-21T23:52:34.728148: step 3106, loss 0.244869, acc 0.84\n",
      "2018-04-21T23:52:34.773270: step 3107, loss 0.181446, acc 0.92\n",
      "2018-04-21T23:52:34.818556: step 3108, loss 0.165439, acc 0.96\n",
      "2018-04-21T23:52:34.869093: step 3109, loss 0.22392, acc 0.9\n",
      "2018-04-21T23:52:34.913589: step 3110, loss 0.443258, acc 0.84\n",
      "2018-04-21T23:52:34.957939: step 3111, loss 0.293234, acc 0.9\n",
      "2018-04-21T23:52:35.002354: step 3112, loss 0.381756, acc 0.84\n",
      "2018-04-21T23:52:35.046898: step 3113, loss 0.491334, acc 0.92\n",
      "2018-04-21T23:52:35.095933: step 3114, loss 0.288649, acc 0.88\n",
      "2018-04-21T23:52:35.140814: step 3115, loss 0.20475, acc 0.88\n",
      "2018-04-21T23:52:35.186135: step 3116, loss 0.155891, acc 0.96\n",
      "2018-04-21T23:52:35.232079: step 3117, loss 0.142729, acc 0.98\n",
      "2018-04-21T23:52:35.278306: step 3118, loss 0.200546, acc 0.92\n",
      "2018-04-21T23:52:35.328716: step 3119, loss 0.113021, acc 0.96\n",
      "2018-04-21T23:52:35.373113: step 3120, loss 0.175798, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:36.052113: step 3120, loss 0.440875, acc 0.768889, rec 0.955357, pre 0.694805, f1 0.804511\n",
      "\n",
      "2018-04-21T23:52:36.096424: step 3121, loss 0.248974, acc 0.92\n",
      "2018-04-21T23:52:36.141083: step 3122, loss 0.251211, acc 0.9\n",
      "2018-04-21T23:52:36.185227: step 3123, loss 0.168379, acc 0.92\n",
      "2018-04-21T23:52:36.231829: step 3124, loss 0.224142, acc 0.92\n",
      "2018-04-21T23:52:36.281517: step 3125, loss 0.273103, acc 0.9\n",
      "2018-04-21T23:52:36.326648: step 3126, loss 0.320026, acc 0.82\n",
      "2018-04-21T23:52:36.370520: step 3127, loss 0.366677, acc 0.84\n",
      "2018-04-21T23:52:36.414418: step 3128, loss 0.207672, acc 0.94\n",
      "2018-04-21T23:52:36.459115: step 3129, loss 0.105699, acc 0.96\n",
      "2018-04-21T23:52:36.507627: step 3130, loss 0.238017, acc 0.92\n",
      "2018-04-21T23:52:36.553472: step 3131, loss 0.246761, acc 0.9\n",
      "2018-04-21T23:52:36.598066: step 3132, loss 0.261044, acc 0.88\n",
      "2018-04-21T23:52:36.643379: step 3133, loss 0.185508, acc 0.94\n",
      "2018-04-21T23:52:36.689138: step 3134, loss 0.355233, acc 0.86\n",
      "2018-04-21T23:52:36.738263: step 3135, loss 0.43497, acc 0.76\n",
      "2018-04-21T23:52:36.783117: step 3136, loss 0.469343, acc 0.86\n",
      "2018-04-21T23:52:36.829108: step 3137, loss 0.360822, acc 0.86\n",
      "2018-04-21T23:52:36.875406: step 3138, loss 0.196274, acc 0.96\n",
      "2018-04-21T23:52:36.921462: step 3139, loss 0.141351, acc 0.96\n",
      "2018-04-21T23:52:36.971803: step 3140, loss 0.129575, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:37.650289: step 3140, loss 0.429738, acc 0.784444, rec 0.949405, pre 0.712849, f1 0.814295\n",
      "\n",
      "2018-04-21T23:52:37.696167: step 3141, loss 0.189923, acc 0.94\n",
      "2018-04-21T23:52:37.742244: step 3142, loss 0.188009, acc 0.9\n",
      "2018-04-21T23:52:37.787238: step 3143, loss 0.216353, acc 0.92\n",
      "2018-04-21T23:52:37.838669: step 3144, loss 0.190791, acc 0.92\n",
      "2018-04-21T23:52:37.889882: step 3145, loss 0.228529, acc 0.96\n",
      "2018-04-21T23:52:37.935846: step 3146, loss 0.312159, acc 0.86\n",
      "2018-04-21T23:52:37.980165: step 3147, loss 0.266544, acc 0.88\n",
      "2018-04-21T23:52:38.024857: step 3148, loss 0.288843, acc 0.94\n",
      "2018-04-21T23:52:38.069468: step 3149, loss 0.207202, acc 0.88\n",
      "2018-04-21T23:52:38.118729: step 3150, loss 0.141066, acc 0.96\n",
      "2018-04-21T23:52:38.164347: step 3151, loss 0.331911, acc 0.9\n",
      "2018-04-21T23:52:38.209657: step 3152, loss 0.134214, acc 0.96\n",
      "2018-04-21T23:52:38.255639: step 3153, loss 0.290764, acc 0.88\n",
      "2018-04-21T23:52:38.300960: step 3154, loss 0.132666, acc 0.96\n",
      "2018-04-21T23:52:38.350604: step 3155, loss 0.543084, acc 0.84\n",
      "2018-04-21T23:52:38.395111: step 3156, loss 0.498261, acc 0.82\n",
      "2018-04-21T23:52:38.438647: step 3157, loss 0.331589, acc 0.82\n",
      "2018-04-21T23:52:38.483669: step 3158, loss 0.157562, acc 0.96\n",
      "2018-04-21T23:52:38.528317: step 3159, loss 0.168043, acc 0.94\n",
      "2018-04-21T23:52:38.576311: step 3160, loss 0.302853, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:39.256919: step 3160, loss 0.533024, acc 0.696667, rec 0.97619, pre 0.62506, f1 0.762126\n",
      "\n",
      "2018-04-21T23:52:39.301061: step 3161, loss 0.248975, acc 0.86\n",
      "2018-04-21T23:52:39.345921: step 3162, loss 0.288276, acc 0.9\n",
      "2018-04-21T23:52:39.389762: step 3163, loss 0.207673, acc 0.94\n",
      "2018-04-21T23:52:39.434634: step 3164, loss 0.254317, acc 0.88\n",
      "2018-04-21T23:52:39.484302: step 3165, loss 0.182649, acc 0.9\n",
      "2018-04-21T23:52:39.528970: step 3166, loss 0.216759, acc 0.92\n",
      "2018-04-21T23:52:39.573072: step 3167, loss 0.420296, acc 0.86\n",
      "2018-04-21T23:52:39.618336: step 3168, loss 0.245521, acc 0.92\n",
      "2018-04-21T23:52:39.663593: step 3169, loss 0.22965, acc 0.88\n",
      "2018-04-21T23:52:39.714848: step 3170, loss 0.305211, acc 0.84\n",
      "2018-04-21T23:52:39.761565: step 3171, loss 0.225427, acc 0.9\n",
      "2018-04-21T23:52:39.807343: step 3172, loss 0.151058, acc 0.94\n",
      "2018-04-21T23:52:39.854595: step 3173, loss 0.183579, acc 0.92\n",
      "2018-04-21T23:52:39.902639: step 3174, loss 0.15748, acc 0.94\n",
      "2018-04-21T23:52:39.952895: step 3175, loss 0.26523, acc 0.9\n",
      "2018-04-21T23:52:39.999861: step 3176, loss 0.201813, acc 0.9\n",
      "2018-04-21T23:52:40.046799: step 3177, loss 0.206921, acc 0.94\n",
      "2018-04-21T23:52:40.093765: step 3178, loss 0.316608, acc 0.86\n",
      "2018-04-21T23:52:40.139534: step 3179, loss 0.194235, acc 0.9\n",
      "2018-04-21T23:52:40.188786: step 3180, loss 0.251244, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:40.873384: step 3180, loss 0.667711, acc 0.633333, rec 0.983631, pre 0.577293, f1 0.727573\n",
      "\n",
      "2018-04-21T23:52:40.919097: step 3181, loss 0.313248, acc 0.92\n",
      "2018-04-21T23:52:40.964120: step 3182, loss 0.135263, acc 0.98\n",
      "2018-04-21T23:52:41.008987: step 3183, loss 0.23388, acc 0.88\n",
      "2018-04-21T23:52:41.055500: step 3184, loss 0.259322, acc 0.84\n",
      "2018-04-21T23:52:41.105719: step 3185, loss 0.477562, acc 0.88\n",
      "2018-04-21T23:52:41.152339: step 3186, loss 0.328776, acc 0.84\n",
      "2018-04-21T23:52:41.198862: step 3187, loss 0.164871, acc 0.94\n",
      "2018-04-21T23:52:41.245125: step 3188, loss 0.2296, acc 0.94\n",
      "2018-04-21T23:52:41.290260: step 3189, loss 0.31456, acc 0.86\n",
      "2018-04-21T23:52:41.340606: step 3190, loss 0.183511, acc 0.98\n",
      "2018-04-21T23:52:41.386827: step 3191, loss 0.273152, acc 0.86\n",
      "2018-04-21T23:52:41.432870: step 3192, loss 0.251573, acc 0.88\n",
      "2018-04-21T23:52:41.478997: step 3193, loss 0.169293, acc 0.94\n",
      "2018-04-21T23:52:41.525810: step 3194, loss 0.218024, acc 0.9\n",
      "2018-04-21T23:52:41.577041: step 3195, loss 0.338711, acc 0.82\n",
      "2018-04-21T23:52:41.622235: step 3196, loss 0.100164, acc 1\n",
      "2018-04-21T23:52:41.667900: step 3197, loss 0.267137, acc 0.92\n",
      "2018-04-21T23:52:41.713367: step 3198, loss 0.428068, acc 0.88\n",
      "2018-04-21T23:52:41.758660: step 3199, loss 0.241613, acc 0.88\n",
      "2018-04-21T23:52:41.807897: step 3200, loss 0.376915, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:42.484857: step 3200, loss 0.723007, acc 0.61037, rec 0.985863, pre 0.561917, f1 0.715829\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-3200\n",
      "\n",
      "2018-04-21T23:52:42.573734: step 3201, loss 0.212897, acc 0.94\n",
      "2018-04-21T23:52:42.619605: step 3202, loss 0.250496, acc 0.9\n",
      "2018-04-21T23:52:42.665284: step 3203, loss 0.164916, acc 0.94\n",
      "2018-04-21T23:52:42.715253: step 3204, loss 0.162586, acc 0.96\n",
      "2018-04-21T23:52:42.763619: step 3205, loss 0.234652, acc 0.94\n",
      "2018-04-21T23:52:42.808510: step 3206, loss 0.2484, acc 0.86\n",
      "2018-04-21T23:52:42.853890: step 3207, loss 0.197995, acc 0.92\n",
      "2018-04-21T23:52:42.900061: step 3208, loss 0.197602, acc 0.96\n",
      "2018-04-21T23:52:42.949789: step 3209, loss 0.152453, acc 0.94\n",
      "2018-04-21T23:52:42.999165: step 3210, loss 0.208972, acc 0.9\n",
      "2018-04-21T23:52:43.044283: step 3211, loss 0.22891, acc 0.92\n",
      "2018-04-21T23:52:43.089363: step 3212, loss 0.137927, acc 0.96\n",
      "2018-04-21T23:52:43.134959: step 3213, loss 0.410982, acc 0.9\n",
      "2018-04-21T23:52:43.185987: step 3214, loss 0.128734, acc 0.96\n",
      "2018-04-21T23:52:43.231455: step 3215, loss 0.248605, acc 0.9\n",
      "2018-04-21T23:52:43.277760: step 3216, loss 0.170949, acc 0.92\n",
      "2018-04-21T23:52:43.324460: step 3217, loss 0.250112, acc 0.88\n",
      "2018-04-21T23:52:43.369645: step 3218, loss 0.268476, acc 0.94\n",
      "2018-04-21T23:52:43.419183: step 3219, loss 0.395819, acc 0.82\n",
      "2018-04-21T23:52:43.465527: step 3220, loss 0.32206, acc 0.82\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:44.147627: step 3220, loss 0.789573, acc 0.595556, rec 0.987351, pre 0.552456, f1 0.708489\n",
      "\n",
      "2018-04-21T23:52:44.192592: step 3221, loss 0.319055, acc 0.86\n",
      "2018-04-21T23:52:44.238924: step 3222, loss 0.209069, acc 0.96\n",
      "2018-04-21T23:52:44.285398: step 3223, loss 0.171528, acc 0.94\n",
      "2018-04-21T23:52:44.331848: step 3224, loss 0.324993, acc 0.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:52:44.382380: step 3225, loss 0.298517, acc 0.9\n",
      "2018-04-21T23:52:44.428842: step 3226, loss 0.268974, acc 0.86\n",
      "2018-04-21T23:52:44.473915: step 3227, loss 0.170888, acc 0.94\n",
      "2018-04-21T23:52:44.519866: step 3228, loss 0.324059, acc 0.88\n",
      "2018-04-21T23:52:44.565606: step 3229, loss 0.148642, acc 0.96\n",
      "2018-04-21T23:52:44.615152: step 3230, loss 0.157395, acc 0.96\n",
      "2018-04-21T23:52:44.660306: step 3231, loss 0.218753, acc 0.9\n",
      "2018-04-21T23:52:44.705746: step 3232, loss 0.122709, acc 0.96\n",
      "2018-04-21T23:52:44.751033: step 3233, loss 0.184115, acc 0.98\n",
      "2018-04-21T23:52:44.795387: step 3234, loss 0.190744, acc 0.92\n",
      "2018-04-21T23:52:44.844515: step 3235, loss 0.154849, acc 0.96\n",
      "2018-04-21T23:52:44.889441: step 3236, loss 0.284687, acc 0.88\n",
      "2018-04-21T23:52:44.936273: step 3237, loss 0.199431, acc 0.9\n",
      "2018-04-21T23:52:44.981912: step 3238, loss 0.227719, acc 0.88\n",
      "2018-04-21T23:52:45.027487: step 3239, loss 0.139828, acc 0.94\n",
      "2018-04-21T23:52:45.077101: step 3240, loss 0.280464, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:45.758447: step 3240, loss 0.438959, acc 0.774074, rec 0.952381, pre 0.700986, f1 0.807571\n",
      "\n",
      "2018-04-21T23:52:45.806793: step 3241, loss 0.223607, acc 0.92\n",
      "2018-04-21T23:52:45.853486: step 3242, loss 0.256916, acc 0.88\n",
      "2018-04-21T23:52:45.899011: step 3243, loss 0.182712, acc 0.92\n",
      "2018-04-21T23:52:45.944865: step 3244, loss 0.140504, acc 0.96\n",
      "2018-04-21T23:52:45.993793: step 3245, loss 0.358662, acc 0.88\n",
      "2018-04-21T23:52:46.039899: step 3246, loss 0.255561, acc 0.94\n",
      "2018-04-21T23:52:46.085878: step 3247, loss 0.315437, acc 0.88\n",
      "2018-04-21T23:52:46.130804: step 3248, loss 0.233691, acc 0.88\n",
      "2018-04-21T23:52:46.176033: step 3249, loss 0.234226, acc 0.94\n",
      "2018-04-21T23:52:46.225868: step 3250, loss 0.122127, acc 0.98\n",
      "2018-04-21T23:52:46.271415: step 3251, loss 0.187648, acc 0.9\n",
      "2018-04-21T23:52:46.317389: step 3252, loss 0.280568, acc 0.86\n",
      "2018-04-21T23:52:46.363140: step 3253, loss 0.154039, acc 0.96\n",
      "2018-04-21T23:52:46.407993: step 3254, loss 0.215263, acc 0.9\n",
      "2018-04-21T23:52:46.457461: step 3255, loss 0.254264, acc 0.86\n",
      "2018-04-21T23:52:46.504553: step 3256, loss 0.161433, acc 0.94\n",
      "2018-04-21T23:52:46.549940: step 3257, loss 0.29258, acc 0.88\n",
      "2018-04-21T23:52:46.594703: step 3258, loss 0.321789, acc 0.84\n",
      "2018-04-21T23:52:46.641338: step 3259, loss 0.122455, acc 0.98\n",
      "2018-04-21T23:52:46.690219: step 3260, loss 0.248417, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:47.370318: step 3260, loss 0.459227, acc 0.756296, rec 0.959077, pre 0.68129, f1 0.796663\n",
      "\n",
      "2018-04-21T23:52:47.415079: step 3261, loss 0.115884, acc 1\n",
      "2018-04-21T23:52:47.461063: step 3262, loss 0.333583, acc 0.94\n",
      "2018-04-21T23:52:47.506928: step 3263, loss 0.254853, acc 0.88\n",
      "2018-04-21T23:52:47.552534: step 3264, loss 0.194697, acc 0.94\n",
      "2018-04-21T23:52:47.601160: step 3265, loss 0.312046, acc 0.92\n",
      "2018-04-21T23:52:47.645794: step 3266, loss 0.186781, acc 0.94\n",
      "2018-04-21T23:52:47.691002: step 3267, loss 0.157236, acc 0.98\n",
      "2018-04-21T23:52:47.736313: step 3268, loss 0.16148, acc 0.96\n",
      "2018-04-21T23:52:47.781004: step 3269, loss 0.274516, acc 0.88\n",
      "2018-04-21T23:52:47.829430: step 3270, loss 0.216287, acc 0.86\n",
      "2018-04-21T23:52:47.875098: step 3271, loss 0.557977, acc 0.8\n",
      "2018-04-21T23:52:47.921006: step 3272, loss 0.382684, acc 0.8\n",
      "2018-04-21T23:52:47.966745: step 3273, loss 0.271827, acc 0.84\n",
      "2018-04-21T23:52:48.011614: step 3274, loss 0.15503, acc 0.96\n",
      "2018-04-21T23:52:48.060653: step 3275, loss 0.25602, acc 0.92\n",
      "2018-04-21T23:52:48.105780: step 3276, loss 0.264518, acc 0.86\n",
      "2018-04-21T23:52:48.150344: step 3277, loss 0.574102, acc 0.76\n",
      "2018-04-21T23:52:48.195299: step 3278, loss 0.621761, acc 0.74\n",
      "2018-04-21T23:52:48.240382: step 3279, loss 0.183162, acc 0.92\n",
      "2018-04-21T23:52:48.289548: step 3280, loss 0.155384, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:48.968742: step 3280, loss 0.470584, acc 0.747037, rec 0.964286, pre 0.671155, f1 0.79145\n",
      "\n",
      "2018-04-21T23:52:49.013154: step 3281, loss 0.171127, acc 0.98\n",
      "2018-04-21T23:52:49.057552: step 3282, loss 0.140476, acc 0.96\n",
      "2018-04-21T23:52:49.102367: step 3283, loss 0.121609, acc 0.94\n",
      "2018-04-21T23:52:49.147135: step 3284, loss 0.198233, acc 0.9\n",
      "2018-04-21T23:52:49.196773: step 3285, loss 0.153217, acc 0.94\n",
      "2018-04-21T23:52:49.244155: step 3286, loss 0.187537, acc 0.94\n",
      "2018-04-21T23:52:49.367106: step 3287, loss 0.16615, acc 0.9\n",
      "2018-04-21T23:52:49.416584: step 3288, loss 0.226723, acc 0.92\n",
      "2018-04-21T23:52:49.461190: step 3289, loss 0.253461, acc 0.92\n",
      "2018-04-21T23:52:49.506306: step 3290, loss 0.216622, acc 0.92\n",
      "2018-04-21T23:52:49.551648: step 3291, loss 0.232971, acc 0.92\n",
      "2018-04-21T23:52:49.595864: step 3292, loss 0.261584, acc 0.86\n",
      "2018-04-21T23:52:49.644957: step 3293, loss 0.342781, acc 0.9\n",
      "2018-04-21T23:52:49.689991: step 3294, loss 0.185668, acc 0.94\n",
      "2018-04-21T23:52:49.734997: step 3295, loss 0.175609, acc 0.9\n",
      "2018-04-21T23:52:49.781031: step 3296, loss 0.221963, acc 0.92\n",
      "2018-04-21T23:52:49.826402: step 3297, loss 0.34023, acc 0.84\n",
      "2018-04-21T23:52:49.875694: step 3298, loss 0.113409, acc 0.96\n",
      "2018-04-21T23:52:49.920908: step 3299, loss 0.200263, acc 0.92\n",
      "2018-04-21T23:52:49.966445: step 3300, loss 0.287384, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:50.647827: step 3300, loss 0.35726, acc 0.858889, rec 0.84747, pre 0.86616, f1 0.856713\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-3300\n",
      "\n",
      "2018-04-21T23:52:50.742460: step 3301, loss 0.209603, acc 0.94\n",
      "2018-04-21T23:52:50.788988: step 3302, loss 0.25167, acc 0.96\n",
      "2018-04-21T23:52:50.835603: step 3303, loss 0.186673, acc 0.94\n",
      "2018-04-21T23:52:50.886098: step 3304, loss 0.242941, acc 0.9\n",
      "2018-04-21T23:52:50.932643: step 3305, loss 0.452964, acc 0.84\n",
      "2018-04-21T23:52:50.978359: step 3306, loss 0.495384, acc 0.74\n",
      "2018-04-21T23:52:51.024660: step 3307, loss 0.253548, acc 0.94\n",
      "2018-04-21T23:52:51.071421: step 3308, loss 0.119349, acc 0.98\n",
      "2018-04-21T23:52:51.122290: step 3309, loss 0.234158, acc 0.9\n",
      "2018-04-21T23:52:51.167318: step 3310, loss 0.215482, acc 0.94\n",
      "2018-04-21T23:52:51.212670: step 3311, loss 0.30068, acc 0.84\n",
      "2018-04-21T23:52:51.258722: step 3312, loss 0.164381, acc 0.96\n",
      "2018-04-21T23:52:51.305519: step 3313, loss 0.150467, acc 0.98\n",
      "2018-04-21T23:52:51.356715: step 3314, loss 0.207513, acc 0.94\n",
      "2018-04-21T23:52:51.403544: step 3315, loss 0.15772, acc 0.96\n",
      "2018-04-21T23:52:51.450152: step 3316, loss 0.440722, acc 0.88\n",
      "2018-04-21T23:52:51.496359: step 3317, loss 0.17101, acc 0.92\n",
      "2018-04-21T23:52:51.541764: step 3318, loss 0.227645, acc 0.9\n",
      "2018-04-21T23:52:51.591748: step 3319, loss 0.145942, acc 0.98\n",
      "2018-04-21T23:52:51.636906: step 3320, loss 0.282769, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:52.321404: step 3320, loss 0.452155, acc 0.759259, rec 0.956845, pre 0.684771, f1 0.798262\n",
      "\n",
      "2018-04-21T23:52:52.367555: step 3321, loss 0.212492, acc 0.94\n",
      "2018-04-21T23:52:52.415239: step 3322, loss 0.3346, acc 0.82\n",
      "2018-04-21T23:52:52.461280: step 3323, loss 0.191037, acc 0.94\n",
      "2018-04-21T23:52:52.507105: step 3324, loss 0.219885, acc 0.94\n",
      "2018-04-21T23:52:52.556728: step 3325, loss 0.290029, acc 0.88\n",
      "2018-04-21T23:52:52.602631: step 3326, loss 0.407673, acc 0.88\n",
      "2018-04-21T23:52:52.648164: step 3327, loss 0.205707, acc 0.94\n",
      "2018-04-21T23:52:52.694236: step 3328, loss 0.18878, acc 0.92\n",
      "2018-04-21T23:52:52.742635: step 3329, loss 0.142677, acc 0.96\n",
      "2018-04-21T23:52:52.792937: step 3330, loss 0.210429, acc 0.92\n",
      "2018-04-21T23:52:52.840293: step 3331, loss 0.295405, acc 0.88\n",
      "2018-04-21T23:52:52.886375: step 3332, loss 0.243766, acc 0.9\n",
      "2018-04-21T23:52:52.932933: step 3333, loss 0.180367, acc 0.96\n",
      "2018-04-21T23:52:52.978438: step 3334, loss 0.199838, acc 0.9\n",
      "2018-04-21T23:52:53.027884: step 3335, loss 0.464876, acc 0.84\n",
      "2018-04-21T23:52:53.073752: step 3336, loss 0.242876, acc 0.92\n",
      "2018-04-21T23:52:53.119364: step 3337, loss 0.188038, acc 0.92\n",
      "2018-04-21T23:52:53.164287: step 3338, loss 0.230711, acc 0.9\n",
      "2018-04-21T23:52:53.210311: step 3339, loss 0.186417, acc 0.96\n",
      "2018-04-21T23:52:53.260892: step 3340, loss 0.213276, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:53.942944: step 3340, loss 0.357285, acc 0.857778, rec 0.863095, pre 0.852941, f1 0.857988\n",
      "\n",
      "2018-04-21T23:52:53.989160: step 3341, loss 0.326375, acc 0.92\n",
      "2018-04-21T23:52:54.034727: step 3342, loss 0.209553, acc 0.88\n",
      "2018-04-21T23:52:54.079759: step 3343, loss 0.335543, acc 0.9\n",
      "2018-04-21T23:52:54.125514: step 3344, loss 0.238358, acc 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:52:54.175695: step 3345, loss 0.174568, acc 0.92\n",
      "2018-04-21T23:52:54.221751: step 3346, loss 0.229711, acc 0.92\n",
      "2018-04-21T23:52:54.267038: step 3347, loss 0.255207, acc 0.94\n",
      "2018-04-21T23:52:54.313546: step 3348, loss 0.187369, acc 0.94\n",
      "2018-04-21T23:52:54.359249: step 3349, loss 0.151009, acc 0.94\n",
      "2018-04-21T23:52:54.408663: step 3350, loss 0.216136, acc 0.9\n",
      "2018-04-21T23:52:54.454573: step 3351, loss 0.39261, acc 0.84\n",
      "2018-04-21T23:52:54.499232: step 3352, loss 0.346246, acc 0.84\n",
      "2018-04-21T23:52:54.545271: step 3353, loss 0.150834, acc 0.98\n",
      "2018-04-21T23:52:54.591022: step 3354, loss 0.169093, acc 0.96\n",
      "2018-04-21T23:52:54.640620: step 3355, loss 0.217908, acc 0.92\n",
      "2018-04-21T23:52:54.687477: step 3356, loss 0.145235, acc 0.94\n",
      "2018-04-21T23:52:54.735008: step 3357, loss 0.228152, acc 0.9\n",
      "2018-04-21T23:52:54.780102: step 3358, loss 0.364416, acc 0.96\n",
      "2018-04-21T23:52:54.825139: step 3359, loss 0.19286, acc 0.94\n",
      "2018-04-21T23:52:54.873953: step 3360, loss 0.169277, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:55.553767: step 3360, loss 0.364308, acc 0.862593, rec 0.799851, pre 0.913339, f1 0.852836\n",
      "\n",
      "2018-04-21T23:52:55.597616: step 3361, loss 0.183366, acc 0.94\n",
      "2018-04-21T23:52:55.644236: step 3362, loss 0.177713, acc 0.96\n",
      "2018-04-21T23:52:55.689568: step 3363, loss 0.303967, acc 0.94\n",
      "2018-04-21T23:52:55.734862: step 3364, loss 0.244301, acc 0.9\n",
      "2018-04-21T23:52:55.784133: step 3365, loss 0.220301, acc 0.92\n",
      "2018-04-21T23:52:55.830200: step 3366, loss 0.281639, acc 0.84\n",
      "2018-04-21T23:52:55.876094: step 3367, loss 0.586477, acc 0.74\n",
      "2018-04-21T23:52:55.921288: step 3368, loss 0.470424, acc 0.82\n",
      "2018-04-21T23:52:55.966897: step 3369, loss 0.154212, acc 0.94\n",
      "2018-04-21T23:52:56.016569: step 3370, loss 0.172137, acc 0.92\n",
      "2018-04-21T23:52:56.062114: step 3371, loss 0.238671, acc 0.9\n",
      "2018-04-21T23:52:56.108366: step 3372, loss 0.2576, acc 0.86\n",
      "2018-04-21T23:52:56.153385: step 3373, loss 0.235975, acc 0.9\n",
      "2018-04-21T23:52:56.197845: step 3374, loss 0.218997, acc 0.9\n",
      "2018-04-21T23:52:56.247374: step 3375, loss 0.171898, acc 0.96\n",
      "2018-04-21T23:52:56.293265: step 3376, loss 0.248338, acc 0.92\n",
      "2018-04-21T23:52:56.339435: step 3377, loss 0.298357, acc 0.88\n",
      "2018-04-21T23:52:56.386102: step 3378, loss 0.113082, acc 1\n",
      "2018-04-21T23:52:56.431917: step 3379, loss 0.193692, acc 0.9\n",
      "2018-04-21T23:52:56.480800: step 3380, loss 0.332645, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:57.146344: step 3380, loss 0.630211, acc 0.656667, rec 0.982887, pre 0.593708, f1 0.740263\n",
      "\n",
      "2018-04-21T23:52:57.190575: step 3381, loss 0.137472, acc 0.92\n",
      "2018-04-21T23:52:57.234379: step 3382, loss 0.186077, acc 0.9\n",
      "2018-04-21T23:52:57.278194: step 3383, loss 0.251335, acc 0.92\n",
      "2018-04-21T23:52:57.322210: step 3384, loss 0.213386, acc 0.96\n",
      "2018-04-21T23:52:57.370217: step 3385, loss 0.161895, acc 0.96\n",
      "2018-04-21T23:52:57.414102: step 3386, loss 0.200486, acc 0.92\n",
      "2018-04-21T23:52:57.457728: step 3387, loss 0.170565, acc 0.9\n",
      "2018-04-21T23:52:57.502105: step 3388, loss 0.121369, acc 0.96\n",
      "2018-04-21T23:52:57.545945: step 3389, loss 0.336551, acc 0.86\n",
      "2018-04-21T23:52:57.593139: step 3390, loss 0.168881, acc 0.92\n",
      "2018-04-21T23:52:57.637695: step 3391, loss 0.413521, acc 0.84\n",
      "2018-04-21T23:52:57.682195: step 3392, loss 0.224254, acc 0.9\n",
      "2018-04-21T23:52:57.725890: step 3393, loss 0.370164, acc 0.88\n",
      "2018-04-21T23:52:57.769812: step 3394, loss 0.328251, acc 0.88\n",
      "2018-04-21T23:52:57.816792: step 3395, loss 0.320232, acc 0.82\n",
      "2018-04-21T23:52:57.861266: step 3396, loss 0.483839, acc 0.8\n",
      "2018-04-21T23:52:57.905419: step 3397, loss 0.264881, acc 0.84\n",
      "2018-04-21T23:52:57.950028: step 3398, loss 0.0949059, acc 0.98\n",
      "2018-04-21T23:52:57.994431: step 3399, loss 0.168863, acc 0.94\n",
      "2018-04-21T23:52:58.042336: step 3400, loss 0.289388, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:52:58.678234: step 3400, loss 0.500183, acc 0.731111, rec 0.970982, pre 0.65512, f1 0.782374\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-3400\n",
      "\n",
      "2018-04-21T23:52:58.767273: step 3401, loss 0.144987, acc 0.94\n",
      "2018-04-21T23:52:58.813141: step 3402, loss 0.197601, acc 0.94\n",
      "2018-04-21T23:52:58.858935: step 3403, loss 0.276277, acc 0.9\n",
      "2018-04-21T23:52:58.908153: step 3404, loss 0.293575, acc 0.88\n",
      "2018-04-21T23:52:58.953497: step 3405, loss 0.213169, acc 0.86\n",
      "2018-04-21T23:52:58.999200: step 3406, loss 0.120994, acc 0.94\n",
      "2018-04-21T23:52:59.045425: step 3407, loss 0.23408, acc 0.94\n",
      "2018-04-21T23:52:59.092105: step 3408, loss 0.147578, acc 0.96\n",
      "2018-04-21T23:52:59.140558: step 3409, loss 0.175913, acc 0.94\n",
      "2018-04-21T23:52:59.186857: step 3410, loss 0.22694, acc 0.88\n",
      "2018-04-21T23:52:59.232721: step 3411, loss 0.183596, acc 0.92\n",
      "2018-04-21T23:52:59.277491: step 3412, loss 0.283274, acc 0.86\n",
      "2018-04-21T23:52:59.322906: step 3413, loss 0.0804434, acc 1\n",
      "2018-04-21T23:52:59.377235: step 3414, loss 0.145672, acc 0.96\n",
      "2018-04-21T23:52:59.421939: step 3415, loss 0.245906, acc 0.92\n",
      "2018-04-21T23:52:59.468366: step 3416, loss 0.205354, acc 0.88\n",
      "2018-04-21T23:52:59.515129: step 3417, loss 0.355753, acc 0.84\n",
      "2018-04-21T23:52:59.560796: step 3418, loss 0.32195, acc 0.84\n",
      "2018-04-21T23:52:59.610651: step 3419, loss 0.175685, acc 0.92\n",
      "2018-04-21T23:52:59.657998: step 3420, loss 0.230197, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:53:00.340995: step 3420, loss 0.430506, acc 0.783704, rec 0.949405, pre 0.712054, f1 0.813776\n",
      "\n",
      "2018-04-21T23:53:00.385855: step 3421, loss 0.29684, acc 0.94\n",
      "2018-04-21T23:53:00.430993: step 3422, loss 0.497445, acc 0.88\n",
      "2018-04-21T23:53:00.475691: step 3423, loss 0.158456, acc 0.94\n",
      "2018-04-21T23:53:00.521661: step 3424, loss 0.178038, acc 0.92\n",
      "2018-04-21T23:53:00.571318: step 3425, loss 0.189879, acc 0.94\n",
      "2018-04-21T23:53:00.617913: step 3426, loss 0.23566, acc 0.88\n",
      "2018-04-21T23:53:00.667949: step 3427, loss 0.147993, acc 0.98\n",
      "2018-04-21T23:53:00.712791: step 3428, loss 0.184251, acc 0.96\n",
      "2018-04-21T23:53:00.757427: step 3429, loss 0.21301, acc 0.92\n",
      "2018-04-21T23:53:00.805277: step 3430, loss 0.250302, acc 0.9\n",
      "2018-04-21T23:53:00.850210: step 3431, loss 0.214946, acc 0.88\n",
      "2018-04-21T23:53:00.894559: step 3432, loss 0.13727, acc 0.98\n",
      "2018-04-21T23:53:00.938617: step 3433, loss 0.190519, acc 0.96\n",
      "2018-04-21T23:53:00.983721: step 3434, loss 0.209226, acc 0.92\n",
      "2018-04-21T23:53:01.031053: step 3435, loss 0.140371, acc 0.96\n",
      "2018-04-21T23:53:01.074789: step 3436, loss 0.188036, acc 0.9\n",
      "2018-04-21T23:53:01.118627: step 3437, loss 0.463615, acc 0.82\n",
      "2018-04-21T23:53:01.162668: step 3438, loss 0.242576, acc 0.88\n",
      "2018-04-21T23:53:01.206269: step 3439, loss 0.327555, acc 0.9\n",
      "2018-04-21T23:53:01.253946: step 3440, loss 0.181726, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:53:01.899948: step 3440, loss 0.411833, acc 0.79963, rec 0.940476, pre 0.732754, f1 0.823721\n",
      "\n",
      "2018-04-21T23:53:01.946938: step 3441, loss 0.346727, acc 0.94\n",
      "2018-04-21T23:53:01.994499: step 3442, loss 0.301276, acc 0.94\n",
      "2018-04-21T23:53:02.041541: step 3443, loss 0.137054, acc 0.96\n",
      "2018-04-21T23:53:02.089132: step 3444, loss 0.127941, acc 0.94\n",
      "2018-04-21T23:53:02.140080: step 3445, loss 0.102036, acc 0.98\n",
      "2018-04-21T23:53:02.186220: step 3446, loss 0.17687, acc 0.94\n",
      "2018-04-21T23:53:02.234243: step 3447, loss 0.272246, acc 0.86\n",
      "2018-04-21T23:53:02.280377: step 3448, loss 0.146443, acc 0.96\n",
      "2018-04-21T23:53:02.327760: step 3449, loss 0.149323, acc 0.92\n",
      "2018-04-21T23:53:02.377786: step 3450, loss 0.279763, acc 0.9\n",
      "2018-04-21T23:53:02.423831: step 3451, loss 0.177257, acc 0.9\n",
      "2018-04-21T23:53:02.471559: step 3452, loss 0.22242, acc 0.9\n",
      "2018-04-21T23:53:02.519977: step 3453, loss 0.341139, acc 0.84\n",
      "2018-04-21T23:53:02.567256: step 3454, loss 0.103393, acc 1\n",
      "2018-04-21T23:53:02.617208: step 3455, loss 0.124423, acc 0.98\n",
      "2018-04-21T23:53:02.662120: step 3456, loss 0.201991, acc 0.9\n",
      "2018-04-21T23:53:02.708273: step 3457, loss 0.477308, acc 0.84\n",
      "2018-04-21T23:53:02.754529: step 3458, loss 0.184662, acc 0.94\n",
      "2018-04-21T23:53:02.800819: step 3459, loss 0.158387, acc 0.94\n",
      "2018-04-21T23:53:02.851641: step 3460, loss 0.18948, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:53:03.535140: step 3460, loss 0.361593, acc 0.855556, rec 0.873512, pre 0.842181, f1 0.85756\n",
      "\n",
      "2018-04-21T23:53:03.579713: step 3461, loss 0.207672, acc 0.92\n",
      "2018-04-21T23:53:03.625572: step 3462, loss 0.121949, acc 0.96\n",
      "2018-04-21T23:53:03.674428: step 3463, loss 0.254105, acc 0.92\n",
      "2018-04-21T23:53:03.723146: step 3464, loss 0.163074, acc 0.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:53:03.772738: step 3465, loss 0.116823, acc 0.98\n",
      "2018-04-21T23:53:03.819379: step 3466, loss 0.192296, acc 0.94\n",
      "2018-04-21T23:53:03.865527: step 3467, loss 0.246374, acc 0.88\n",
      "2018-04-21T23:53:03.911200: step 3468, loss 0.215536, acc 0.92\n",
      "2018-04-21T23:53:03.957786: step 3469, loss 0.220923, acc 0.9\n",
      "2018-04-21T23:53:04.007945: step 3470, loss 0.136651, acc 0.96\n",
      "2018-04-21T23:53:04.054601: step 3471, loss 0.231051, acc 0.92\n",
      "2018-04-21T23:53:04.101027: step 3472, loss 0.114795, acc 0.96\n",
      "2018-04-21T23:53:04.146622: step 3473, loss 0.162539, acc 0.94\n",
      "2018-04-21T23:53:04.192093: step 3474, loss 0.157211, acc 0.94\n",
      "2018-04-21T23:53:04.242838: step 3475, loss 0.2941, acc 0.9\n",
      "2018-04-21T23:53:04.288786: step 3476, loss 0.33334, acc 0.82\n",
      "2018-04-21T23:53:04.334657: step 3477, loss 0.203326, acc 0.88\n",
      "2018-04-21T23:53:04.380689: step 3478, loss 0.228637, acc 0.88\n",
      "2018-04-21T23:53:04.426000: step 3479, loss 0.21982, acc 0.9\n",
      "2018-04-21T23:53:04.475810: step 3480, loss 0.105191, acc 0.96\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:53:05.169698: step 3480, loss 0.370596, acc 0.841852, rec 0.90253, pre 0.803844, f1 0.850333\n",
      "\n",
      "2018-04-21T23:53:05.215335: step 3481, loss 0.152285, acc 0.94\n",
      "2018-04-21T23:53:05.262744: step 3482, loss 0.250893, acc 0.92\n",
      "2018-04-21T23:53:05.309779: step 3483, loss 0.291074, acc 0.92\n",
      "2018-04-21T23:53:05.357630: step 3484, loss 0.21275, acc 0.92\n",
      "2018-04-21T23:53:05.408895: step 3485, loss 0.195836, acc 0.94\n",
      "2018-04-21T23:53:05.456964: step 3486, loss 0.108459, acc 0.98\n",
      "2018-04-21T23:53:05.504349: step 3487, loss 0.233907, acc 0.92\n",
      "2018-04-21T23:53:05.551731: step 3488, loss 0.143174, acc 0.98\n",
      "2018-04-21T23:53:05.597107: step 3489, loss 0.164793, acc 0.94\n",
      "2018-04-21T23:53:05.647264: step 3490, loss 0.104317, acc 0.98\n",
      "2018-04-21T23:53:05.696095: step 3491, loss 0.187335, acc 0.92\n",
      "2018-04-21T23:53:05.742125: step 3492, loss 0.157787, acc 0.92\n",
      "2018-04-21T23:53:05.788111: step 3493, loss 0.435014, acc 0.88\n",
      "2018-04-21T23:53:05.834368: step 3494, loss 0.121879, acc 0.96\n",
      "2018-04-21T23:53:05.884689: step 3495, loss 0.146807, acc 0.96\n",
      "2018-04-21T23:53:05.933001: step 3496, loss 0.312872, acc 0.94\n",
      "2018-04-21T23:53:05.978999: step 3497, loss 0.236291, acc 0.88\n",
      "2018-04-21T23:53:06.026188: step 3498, loss 0.105029, acc 0.98\n",
      "2018-04-21T23:53:06.072782: step 3499, loss 0.103785, acc 0.98\n",
      "2018-04-21T23:53:06.122927: step 3500, loss 0.253199, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:53:06.806235: step 3500, loss 0.371388, acc 0.84037, rec 0.906994, pre 0.799344, f1 0.849773\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-3500\n",
      "\n",
      "2018-04-21T23:53:06.898315: step 3501, loss 0.254768, acc 0.92\n",
      "2018-04-21T23:53:06.944265: step 3502, loss 0.167287, acc 0.94\n",
      "2018-04-21T23:53:06.991103: step 3503, loss 0.189769, acc 0.94\n",
      "2018-04-21T23:53:07.039947: step 3504, loss 0.334595, acc 0.88\n",
      "2018-04-21T23:53:07.084860: step 3505, loss 0.28543, acc 0.86\n",
      "2018-04-21T23:53:07.130308: step 3506, loss 0.216586, acc 0.92\n",
      "2018-04-21T23:53:07.174953: step 3507, loss 0.16766, acc 0.92\n",
      "2018-04-21T23:53:07.220361: step 3508, loss 0.0920537, acc 0.98\n",
      "2018-04-21T23:53:07.270193: step 3509, loss 0.0585207, acc 1\n",
      "2018-04-21T23:53:07.316122: step 3510, loss 0.271098, acc 0.92\n",
      "2018-04-21T23:53:07.362118: step 3511, loss 0.185532, acc 0.94\n",
      "2018-04-21T23:53:07.407471: step 3512, loss 0.163698, acc 0.94\n",
      "2018-04-21T23:53:07.452857: step 3513, loss 0.151382, acc 0.94\n",
      "2018-04-21T23:53:07.502386: step 3514, loss 0.345135, acc 0.92\n",
      "2018-04-21T23:53:07.547520: step 3515, loss 0.276363, acc 0.88\n",
      "2018-04-21T23:53:07.592631: step 3516, loss 0.239076, acc 0.84\n",
      "2018-04-21T23:53:07.637577: step 3517, loss 0.282123, acc 0.92\n",
      "2018-04-21T23:53:07.684317: step 3518, loss 0.398199, acc 0.8\n",
      "2018-04-21T23:53:07.733015: step 3519, loss 0.294009, acc 0.82\n",
      "2018-04-21T23:53:07.778264: step 3520, loss 0.253451, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:53:08.459251: step 3520, loss 0.391196, acc 0.818148, rec 0.927083, pre 0.76022, f1 0.835401\n",
      "\n",
      "2018-04-21T23:53:08.505119: step 3521, loss 0.198632, acc 0.94\n",
      "2018-04-21T23:53:08.550312: step 3522, loss 0.22268, acc 0.94\n",
      "2018-04-21T23:53:08.595410: step 3523, loss 0.220824, acc 0.94\n",
      "2018-04-21T23:53:08.640057: step 3524, loss 0.159694, acc 0.94\n",
      "2018-04-21T23:53:08.689634: step 3525, loss 0.124389, acc 0.96\n",
      "2018-04-21T23:53:08.734529: step 3526, loss 0.116679, acc 0.96\n",
      "2018-04-21T23:53:08.780669: step 3527, loss 0.132641, acc 0.94\n",
      "2018-04-21T23:53:08.825666: step 3528, loss 0.162373, acc 0.94\n",
      "2018-04-21T23:53:08.870143: step 3529, loss 0.295826, acc 0.9\n",
      "2018-04-21T23:53:08.919035: step 3530, loss 0.418181, acc 0.84\n",
      "2018-04-21T23:53:08.963870: step 3531, loss 0.117581, acc 0.94\n",
      "2018-04-21T23:53:09.010436: step 3532, loss 0.12982, acc 0.96\n",
      "2018-04-21T23:53:09.055960: step 3533, loss 0.292647, acc 0.86\n",
      "2018-04-21T23:53:09.102621: step 3534, loss 0.459877, acc 0.8\n",
      "2018-04-21T23:53:09.153036: step 3535, loss 0.174473, acc 0.92\n",
      "2018-04-21T23:53:09.197716: step 3536, loss 0.126524, acc 1\n",
      "2018-04-21T23:53:09.243287: step 3537, loss 0.344059, acc 0.9\n",
      "2018-04-21T23:53:09.288673: step 3538, loss 0.2704, acc 0.9\n",
      "2018-04-21T23:53:09.333803: step 3539, loss 0.152689, acc 0.94\n",
      "2018-04-21T23:53:09.383550: step 3540, loss 0.381403, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:53:10.065796: step 3540, loss 1.04377, acc 0.560741, rec 0.994048, pre 0.531424, f1 0.692587\n",
      "\n",
      "2018-04-21T23:53:10.110747: step 3541, loss 0.317819, acc 0.82\n",
      "2018-04-21T23:53:10.156069: step 3542, loss 0.316855, acc 0.84\n",
      "2018-04-21T23:53:10.201499: step 3543, loss 0.244505, acc 0.9\n",
      "2018-04-21T23:53:10.246879: step 3544, loss 0.124121, acc 0.96\n",
      "2018-04-21T23:53:10.296352: step 3545, loss 0.144935, acc 0.96\n",
      "2018-04-21T23:53:10.342176: step 3546, loss 0.116046, acc 0.98\n",
      "2018-04-21T23:53:10.388123: step 3547, loss 0.1828, acc 0.92\n",
      "2018-04-21T23:53:10.435075: step 3548, loss 0.153555, acc 0.92\n",
      "2018-04-21T23:53:10.480745: step 3549, loss 0.31661, acc 0.94\n",
      "2018-04-21T23:53:10.529819: step 3550, loss 0.22293, acc 0.92\n",
      "2018-04-21T23:53:10.576314: step 3551, loss 0.2304, acc 0.9\n",
      "2018-04-21T23:53:10.621819: step 3552, loss 0.227383, acc 0.9\n",
      "2018-04-21T23:53:10.667601: step 3553, loss 0.238968, acc 0.9\n",
      "2018-04-21T23:53:10.714651: step 3554, loss 0.155586, acc 0.96\n",
      "2018-04-21T23:53:10.763338: step 3555, loss 0.161549, acc 0.94\n",
      "2018-04-21T23:53:10.808871: step 3556, loss 0.189129, acc 0.94\n",
      "2018-04-21T23:53:10.854282: step 3557, loss 0.217163, acc 0.9\n",
      "2018-04-21T23:53:10.901777: step 3558, loss 0.268787, acc 0.88\n",
      "2018-04-21T23:53:10.946073: step 3559, loss 0.206849, acc 0.94\n",
      "2018-04-21T23:53:10.995521: step 3560, loss 0.288203, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:53:11.675245: step 3560, loss 0.356116, acc 0.854815, rec 0.872768, pre 0.841463, f1 0.85683\n",
      "\n",
      "2018-04-21T23:53:11.719734: step 3561, loss 0.124379, acc 0.94\n",
      "2018-04-21T23:53:11.764513: step 3562, loss 0.179903, acc 0.92\n",
      "2018-04-21T23:53:11.808353: step 3563, loss 0.122068, acc 0.98\n",
      "2018-04-21T23:53:11.852129: step 3564, loss 0.215688, acc 0.96\n",
      "2018-04-21T23:53:11.899659: step 3565, loss 0.174659, acc 0.96\n",
      "2018-04-21T23:53:11.943365: step 3566, loss 0.260547, acc 0.88\n",
      "2018-04-21T23:53:11.986720: step 3567, loss 0.134565, acc 0.92\n",
      "2018-04-21T23:53:12.029927: step 3568, loss 0.161542, acc 0.96\n",
      "2018-04-21T23:53:12.073110: step 3569, loss 0.253282, acc 0.92\n",
      "2018-04-21T23:53:12.119893: step 3570, loss 0.107311, acc 0.96\n",
      "2018-04-21T23:53:12.163481: step 3571, loss 0.126233, acc 0.92\n",
      "2018-04-21T23:53:12.207157: step 3572, loss 0.136276, acc 0.98\n",
      "2018-04-21T23:53:12.250465: step 3573, loss 0.190191, acc 0.92\n",
      "2018-04-21T23:53:12.294389: step 3574, loss 0.248446, acc 0.92\n",
      "2018-04-21T23:53:12.341966: step 3575, loss 0.173977, acc 0.92\n",
      "2018-04-21T23:53:12.386148: step 3576, loss 0.221386, acc 0.86\n",
      "2018-04-21T23:53:12.430959: step 3577, loss 0.367799, acc 0.94\n",
      "2018-04-21T23:53:12.475024: step 3578, loss 0.272638, acc 0.94\n",
      "2018-04-21T23:53:12.518124: step 3579, loss 0.407155, acc 0.86\n",
      "2018-04-21T23:53:12.565259: step 3580, loss 0.391038, acc 0.76\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:53:13.202744: step 3580, loss 0.574971, acc 0.715185, rec 0.441964, pre 0.969005, f1 0.607052\n",
      "\n",
      "2018-04-21T23:53:13.247481: step 3581, loss 0.561615, acc 0.8\n",
      "2018-04-21T23:53:13.291962: step 3582, loss 0.203073, acc 0.9\n",
      "2018-04-21T23:53:13.336612: step 3583, loss 0.185831, acc 0.94\n",
      "2018-04-21T23:53:13.381389: step 3584, loss 0.149796, acc 0.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:53:13.429372: step 3585, loss 0.091573, acc 1\n",
      "2018-04-21T23:53:13.473857: step 3586, loss 0.263172, acc 0.94\n",
      "2018-04-21T23:53:13.517798: step 3587, loss 0.136231, acc 0.96\n",
      "2018-04-21T23:53:13.561801: step 3588, loss 0.136682, acc 0.94\n",
      "2018-04-21T23:53:13.607617: step 3589, loss 0.178896, acc 0.92\n",
      "2018-04-21T23:53:13.655669: step 3590, loss 0.226771, acc 0.94\n",
      "2018-04-21T23:53:13.699474: step 3591, loss 0.242998, acc 0.9\n",
      "2018-04-21T23:53:13.743665: step 3592, loss 0.25939, acc 0.84\n",
      "2018-04-21T23:53:13.787775: step 3593, loss 0.249442, acc 0.92\n",
      "2018-04-21T23:53:13.831458: step 3594, loss 0.195383, acc 0.96\n",
      "2018-04-21T23:53:13.878051: step 3595, loss 0.19766, acc 0.9\n",
      "2018-04-21T23:53:13.922768: step 3596, loss 0.139018, acc 0.96\n",
      "2018-04-21T23:53:13.967197: step 3597, loss 0.120305, acc 0.96\n",
      "2018-04-21T23:53:14.011148: step 3598, loss 0.14206, acc 0.96\n",
      "2018-04-21T23:53:14.054770: step 3599, loss 0.25943, acc 0.88\n",
      "2018-04-21T23:53:14.101944: step 3600, loss 0.267607, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:53:14.739608: step 3600, loss 0.398764, acc 0.812963, rec 0.935268, pre 0.750448, f1 0.832726\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-3600\n",
      "\n",
      "2018-04-21T23:53:14.830690: step 3601, loss 0.0828244, acc 0.98\n",
      "2018-04-21T23:53:14.875811: step 3602, loss 0.223168, acc 0.94\n",
      "2018-04-21T23:53:14.920533: step 3603, loss 0.111464, acc 0.96\n",
      "2018-04-21T23:53:14.967535: step 3604, loss 0.15845, acc 0.96\n",
      "2018-04-21T23:53:15.012918: step 3605, loss 0.224657, acc 0.92\n",
      "2018-04-21T23:53:15.058048: step 3606, loss 0.101399, acc 1\n",
      "2018-04-21T23:53:15.102321: step 3607, loss 0.255529, acc 0.92\n",
      "2018-04-21T23:53:15.146232: step 3608, loss 0.287187, acc 0.84\n",
      "2018-04-21T23:53:15.193293: step 3609, loss 0.430114, acc 0.84\n",
      "2018-04-21T23:53:15.237770: step 3610, loss 0.192078, acc 0.96\n",
      "2018-04-21T23:53:15.282351: step 3611, loss 0.171287, acc 0.92\n",
      "2018-04-21T23:53:15.326741: step 3612, loss 0.447427, acc 0.92\n",
      "2018-04-21T23:53:15.370459: step 3613, loss 0.19051, acc 0.94\n",
      "2018-04-21T23:53:15.417923: step 3614, loss 0.133986, acc 0.94\n",
      "2018-04-21T23:53:15.462156: step 3615, loss 0.189032, acc 0.96\n",
      "2018-04-21T23:53:15.506816: step 3616, loss 0.248385, acc 0.94\n",
      "2018-04-21T23:53:15.550950: step 3617, loss 0.273609, acc 0.9\n",
      "2018-04-21T23:53:15.595653: step 3618, loss 0.198344, acc 0.96\n",
      "2018-04-21T23:53:15.642239: step 3619, loss 0.195635, acc 0.92\n",
      "2018-04-21T23:53:15.687932: step 3620, loss 0.104537, acc 0.98\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:53:16.325902: step 3620, loss 0.46057, acc 0.760741, rec 0.954613, pre 0.686831, f1 0.798879\n",
      "\n",
      "2018-04-21T23:53:16.369486: step 3621, loss 0.226698, acc 0.88\n",
      "2018-04-21T23:53:16.414297: step 3622, loss 0.49776, acc 0.9\n",
      "2018-04-21T23:53:16.458682: step 3623, loss 0.220525, acc 0.9\n",
      "2018-04-21T23:53:16.503337: step 3624, loss 0.129908, acc 0.94\n",
      "2018-04-21T23:53:16.551708: step 3625, loss 0.250534, acc 0.88\n",
      "2018-04-21T23:53:16.596372: step 3626, loss 0.246221, acc 0.94\n",
      "2018-04-21T23:53:16.640333: step 3627, loss 0.356973, acc 0.9\n",
      "2018-04-21T23:53:16.684551: step 3628, loss 0.140514, acc 0.94\n",
      "2018-04-21T23:53:16.729308: step 3629, loss 0.109346, acc 0.98\n",
      "2018-04-21T23:53:16.778043: step 3630, loss 0.0931059, acc 0.98\n",
      "2018-04-21T23:53:16.822663: step 3631, loss 0.20353, acc 0.94\n",
      "2018-04-21T23:53:16.866988: step 3632, loss 0.164372, acc 0.94\n",
      "2018-04-21T23:53:16.912002: step 3633, loss 0.138304, acc 0.94\n",
      "2018-04-21T23:53:16.956314: step 3634, loss 0.176766, acc 0.96\n",
      "2018-04-21T23:53:17.004565: step 3635, loss 0.128828, acc 0.94\n",
      "2018-04-21T23:53:17.048566: step 3636, loss 0.224702, acc 0.92\n",
      "2018-04-21T23:53:17.092662: step 3637, loss 0.207026, acc 0.9\n",
      "2018-04-21T23:53:17.137374: step 3638, loss 0.112459, acc 0.96\n",
      "2018-04-21T23:53:17.181446: step 3639, loss 0.158619, acc 0.94\n",
      "2018-04-21T23:53:17.228897: step 3640, loss 0.38585, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:53:17.863378: step 3640, loss 0.789866, acc 0.647778, rec 0.299107, pre 0.978102, f1 0.45812\n",
      "\n",
      "2018-04-21T23:53:17.906875: step 3641, loss 0.262109, acc 0.9\n",
      "2018-04-21T23:53:17.950078: step 3642, loss 0.211243, acc 0.88\n",
      "2018-04-21T23:53:17.993180: step 3643, loss 0.212096, acc 0.94\n",
      "2018-04-21T23:53:18.036306: step 3644, loss 0.203815, acc 0.9\n",
      "2018-04-21T23:53:18.082776: step 3645, loss 0.155224, acc 0.94\n",
      "2018-04-21T23:53:18.126821: step 3646, loss 0.209005, acc 0.94\n",
      "2018-04-21T23:53:18.170783: step 3647, loss 0.119727, acc 0.96\n",
      "2018-04-21T23:53:18.214635: step 3648, loss 0.100059, acc 1\n",
      "2018-04-21T23:53:18.258228: step 3649, loss 0.0889551, acc 0.98\n",
      "2018-04-21T23:53:18.305289: step 3650, loss 0.0993436, acc 0.98\n",
      "2018-04-21T23:53:18.348782: step 3651, loss 0.169279, acc 0.94\n",
      "2018-04-21T23:53:18.392263: step 3652, loss 0.152929, acc 0.96\n",
      "2018-04-21T23:53:18.436061: step 3653, loss 0.225057, acc 0.9\n",
      "2018-04-21T23:53:18.480488: step 3654, loss 0.33775, acc 0.9\n",
      "2018-04-21T23:53:18.527936: step 3655, loss 0.198582, acc 0.92\n",
      "2018-04-21T23:53:18.571703: step 3656, loss 0.106002, acc 0.96\n",
      "2018-04-21T23:53:18.618256: step 3657, loss 0.257559, acc 0.9\n",
      "2018-04-21T23:53:18.665259: step 3658, loss 0.274991, acc 0.94\n",
      "2018-04-21T23:53:18.710312: step 3659, loss 0.25464, acc 0.9\n",
      "2018-04-21T23:53:18.759577: step 3660, loss 0.0995084, acc 0.98\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:53:19.450071: step 3660, loss 0.510127, acc 0.731481, rec 0.970238, pre 0.655606, f1 0.782478\n",
      "\n",
      "2018-04-21T23:53:19.495600: step 3661, loss 0.161847, acc 0.96\n",
      "2018-04-21T23:53:19.539766: step 3662, loss 0.13603, acc 0.96\n",
      "2018-04-21T23:53:19.583063: step 3663, loss 0.287363, acc 0.84\n",
      "2018-04-21T23:53:19.627275: step 3664, loss 0.256844, acc 0.92\n",
      "2018-04-21T23:53:19.677254: step 3665, loss 0.392974, acc 0.84\n",
      "2018-04-21T23:53:19.723110: step 3666, loss 0.247771, acc 0.88\n",
      "2018-04-21T23:53:19.768689: step 3667, loss 0.348246, acc 0.84\n",
      "2018-04-21T23:53:19.813937: step 3668, loss 0.216569, acc 0.96\n",
      "2018-04-21T23:53:19.859127: step 3669, loss 0.17073, acc 0.96\n",
      "2018-04-21T23:53:19.907825: step 3670, loss 0.177555, acc 0.96\n",
      "2018-04-21T23:53:19.953763: step 3671, loss 0.246992, acc 0.9\n",
      "2018-04-21T23:53:20.024575: step 3672, loss 0.332964, acc 0.94\n",
      "2018-04-21T23:53:20.069918: step 3673, loss 0.0887611, acc 0.96\n",
      "2018-04-21T23:53:20.119072: step 3674, loss 0.340404, acc 0.88\n",
      "2018-04-21T23:53:20.163648: step 3675, loss 0.187317, acc 0.94\n",
      "2018-04-21T23:53:20.208597: step 3676, loss 0.185392, acc 0.94\n",
      "2018-04-21T23:53:20.253696: step 3677, loss 0.200477, acc 0.92\n",
      "2018-04-21T23:53:20.299081: step 3678, loss 0.274072, acc 0.86\n",
      "2018-04-21T23:53:20.350092: step 3679, loss 0.215605, acc 0.9\n",
      "2018-04-21T23:53:20.394829: step 3680, loss 0.156092, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:53:21.077818: step 3680, loss 0.360415, acc 0.858889, rec 0.787946, pre 0.916883, f1 0.847539\n",
      "\n",
      "2018-04-21T23:53:21.122309: step 3681, loss 0.450222, acc 0.88\n",
      "2018-04-21T23:53:21.168473: step 3682, loss 0.42248, acc 0.8\n",
      "2018-04-21T23:53:21.213787: step 3683, loss 0.224209, acc 0.92\n",
      "2018-04-21T23:53:21.259253: step 3684, loss 0.153462, acc 0.92\n",
      "2018-04-21T23:53:21.308135: step 3685, loss 0.124044, acc 0.94\n",
      "2018-04-21T23:53:21.354411: step 3686, loss 0.539603, acc 0.8\n",
      "2018-04-21T23:53:21.400027: step 3687, loss 0.45325, acc 0.76\n",
      "2018-04-21T23:53:21.445189: step 3688, loss 0.417983, acc 0.82\n",
      "2018-04-21T23:53:21.492348: step 3689, loss 0.289235, acc 0.9\n",
      "2018-04-21T23:53:21.542644: step 3690, loss 0.167051, acc 0.94\n",
      "2018-04-21T23:53:21.588166: step 3691, loss 0.171403, acc 0.94\n",
      "2018-04-21T23:53:21.634278: step 3692, loss 0.269638, acc 0.94\n",
      "2018-04-21T23:53:21.680699: step 3693, loss 0.108661, acc 0.98\n",
      "2018-04-21T23:53:21.725659: step 3694, loss 0.110874, acc 0.98\n",
      "2018-04-21T23:53:21.775796: step 3695, loss 0.240901, acc 0.9\n",
      "2018-04-21T23:53:21.821399: step 3696, loss 0.178153, acc 0.94\n",
      "2018-04-21T23:53:21.866993: step 3697, loss 0.176399, acc 0.96\n",
      "2018-04-21T23:53:21.910982: step 3698, loss 0.151076, acc 0.96\n",
      "2018-04-21T23:53:21.956876: step 3699, loss 0.244224, acc 0.86\n",
      "2018-04-21T23:53:22.005820: step 3700, loss 0.279917, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:53:22.686686: step 3700, loss 0.349969, acc 0.87, rec 0.822173, pre 0.90797, f1 0.862944\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-3700\n",
      "\n",
      "2018-04-21T23:53:22.776662: step 3701, loss 0.217024, acc 0.86\n",
      "2018-04-21T23:53:22.820912: step 3702, loss 0.337171, acc 0.88\n",
      "2018-04-21T23:53:22.867044: step 3703, loss 0.212919, acc 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:53:22.917236: step 3704, loss 0.441775, acc 0.84\n",
      "2018-04-21T23:53:22.963432: step 3705, loss 0.313205, acc 0.86\n",
      "2018-04-21T23:53:23.008414: step 3706, loss 0.243189, acc 0.92\n",
      "2018-04-21T23:53:23.053206: step 3707, loss 0.127812, acc 0.96\n",
      "2018-04-21T23:53:23.098090: step 3708, loss 0.214786, acc 0.94\n",
      "2018-04-21T23:53:23.145932: step 3709, loss 0.190671, acc 0.9\n",
      "2018-04-21T23:53:23.189686: step 3710, loss 0.280958, acc 0.88\n",
      "2018-04-21T23:53:23.234014: step 3711, loss 0.161961, acc 0.92\n",
      "2018-04-21T23:53:23.277865: step 3712, loss 0.0891755, acc 0.98\n",
      "2018-04-21T23:53:23.322530: step 3713, loss 0.186829, acc 0.92\n",
      "2018-04-21T23:53:23.370333: step 3714, loss 0.193074, acc 0.94\n",
      "2018-04-21T23:53:23.415698: step 3715, loss 0.410153, acc 0.88\n",
      "2018-04-21T23:53:23.460419: step 3716, loss 0.210701, acc 0.9\n",
      "2018-04-21T23:53:23.505667: step 3717, loss 0.184105, acc 0.92\n",
      "2018-04-21T23:53:23.549721: step 3718, loss 0.208357, acc 0.94\n",
      "2018-04-21T23:53:23.599149: step 3719, loss 0.141072, acc 0.96\n",
      "2018-04-21T23:53:23.643752: step 3720, loss 0.147982, acc 0.96\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:53:24.327426: step 3720, loss 0.511294, acc 0.730741, rec 0.970982, pre 0.654792, f1 0.78214\n",
      "\n",
      "2018-04-21T23:53:24.372131: step 3721, loss 0.19147, acc 0.96\n",
      "2018-04-21T23:53:24.416578: step 3722, loss 0.153267, acc 0.92\n",
      "2018-04-21T23:53:24.460789: step 3723, loss 0.258988, acc 0.86\n",
      "2018-04-21T23:53:24.506035: step 3724, loss 0.152737, acc 0.96\n",
      "2018-04-21T23:53:24.555883: step 3725, loss 0.233222, acc 0.92\n",
      "2018-04-21T23:53:24.601107: step 3726, loss 0.199159, acc 0.9\n",
      "2018-04-21T23:53:24.646745: step 3727, loss 0.0929203, acc 1\n",
      "2018-04-21T23:53:24.693347: step 3728, loss 0.140478, acc 0.94\n",
      "2018-04-21T23:53:24.739288: step 3729, loss 0.175607, acc 0.92\n",
      "2018-04-21T23:53:24.789278: step 3730, loss 0.395522, acc 0.82\n",
      "2018-04-21T23:53:24.834882: step 3731, loss 0.38181, acc 0.82\n",
      "2018-04-21T23:53:24.880830: step 3732, loss 0.352031, acc 0.84\n",
      "2018-04-21T23:53:24.926477: step 3733, loss 0.44132, acc 0.82\n",
      "2018-04-21T23:53:24.972504: step 3734, loss 0.174521, acc 0.9\n",
      "2018-04-21T23:53:25.022549: step 3735, loss 0.132922, acc 0.98\n",
      "2018-04-21T23:53:25.069696: step 3736, loss 0.0853, acc 0.98\n",
      "2018-04-21T23:53:25.116934: step 3737, loss 0.307195, acc 0.88\n",
      "2018-04-21T23:53:25.162074: step 3738, loss 0.266062, acc 0.92\n",
      "2018-04-21T23:53:25.207276: step 3739, loss 0.216788, acc 0.9\n",
      "2018-04-21T23:53:25.257365: step 3740, loss 0.215914, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:53:25.935402: step 3740, loss 0.354714, acc 0.851481, rec 0.890625, pre 0.824948, f1 0.85653\n",
      "\n",
      "2018-04-21T23:53:25.980167: step 3741, loss 0.227641, acc 0.9\n",
      "2018-04-21T23:53:26.025270: step 3742, loss 0.165944, acc 0.92\n",
      "2018-04-21T23:53:26.070634: step 3743, loss 0.126208, acc 0.96\n",
      "2018-04-21T23:53:26.115129: step 3744, loss 0.214979, acc 0.96\n",
      "2018-04-21T23:53:26.163952: step 3745, loss 0.20618, acc 0.94\n",
      "2018-04-21T23:53:26.211101: step 3746, loss 0.217404, acc 0.92\n",
      "2018-04-21T23:53:26.256288: step 3747, loss 0.224382, acc 0.92\n",
      "2018-04-21T23:53:26.302162: step 3748, loss 0.145356, acc 0.96\n",
      "2018-04-21T23:53:26.348632: step 3749, loss 0.239044, acc 0.86\n",
      "2018-04-21T23:53:26.396737: step 3750, loss 0.0933963, acc 0.98\n",
      "2018-04-21T23:53:26.441759: step 3751, loss 0.166561, acc 0.94\n",
      "2018-04-21T23:53:26.487478: step 3752, loss 0.373419, acc 0.94\n",
      "2018-04-21T23:53:26.533037: step 3753, loss 0.317532, acc 0.86\n",
      "2018-04-21T23:53:26.578684: step 3754, loss 0.382552, acc 0.88\n",
      "2018-04-21T23:53:26.629005: step 3755, loss 0.139309, acc 0.96\n",
      "2018-04-21T23:53:26.674579: step 3756, loss 0.156097, acc 0.94\n",
      "2018-04-21T23:53:26.719619: step 3757, loss 0.136468, acc 0.94\n",
      "2018-04-21T23:53:26.771514: step 3758, loss 0.143822, acc 0.96\n",
      "2018-04-21T23:53:26.816231: step 3759, loss 0.120776, acc 0.96\n",
      "2018-04-21T23:53:26.864968: step 3760, loss 0.15398, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:53:27.541659: step 3760, loss 0.484164, acc 0.748148, rec 0.964286, pre 0.672199, f1 0.792176\n",
      "\n",
      "2018-04-21T23:53:27.586771: step 3761, loss 0.1956, acc 0.94\n",
      "2018-04-21T23:53:27.631589: step 3762, loss 0.168459, acc 0.92\n",
      "2018-04-21T23:53:27.677949: step 3763, loss 0.133132, acc 0.98\n",
      "2018-04-21T23:53:27.724952: step 3764, loss 0.178546, acc 0.94\n",
      "2018-04-21T23:53:27.772364: step 3765, loss 0.0990282, acc 0.98\n",
      "2018-04-21T23:53:27.817747: step 3766, loss 0.123116, acc 0.96\n",
      "2018-04-21T23:53:27.862507: step 3767, loss 0.0942787, acc 0.98\n",
      "2018-04-21T23:53:27.908096: step 3768, loss 0.123574, acc 0.96\n",
      "2018-04-21T23:53:27.953651: step 3769, loss 0.228401, acc 0.94\n",
      "2018-04-21T23:53:28.003525: step 3770, loss 0.216101, acc 0.88\n",
      "2018-04-21T23:53:28.049006: step 3771, loss 0.349112, acc 0.92\n",
      "2018-04-21T23:53:28.094019: step 3772, loss 0.215848, acc 0.92\n",
      "2018-04-21T23:53:28.138740: step 3773, loss 0.231366, acc 0.92\n",
      "2018-04-21T23:53:28.183654: step 3774, loss 0.25569, acc 0.88\n",
      "2018-04-21T23:53:28.233003: step 3775, loss 0.284168, acc 0.9\n",
      "2018-04-21T23:53:28.279208: step 3776, loss 0.260791, acc 0.88\n",
      "2018-04-21T23:53:28.324041: step 3777, loss 0.351757, acc 0.84\n",
      "2018-04-21T23:53:28.369758: step 3778, loss 0.29564, acc 0.86\n",
      "2018-04-21T23:53:28.414827: step 3779, loss 0.156673, acc 0.96\n",
      "2018-04-21T23:53:28.463729: step 3780, loss 0.188238, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:53:29.147469: step 3780, loss 0.464557, acc 0.757407, rec 0.956845, pre 0.682953, f1 0.797025\n",
      "\n",
      "2018-04-21T23:53:29.192670: step 3781, loss 0.172875, acc 0.94\n",
      "2018-04-21T23:53:29.239453: step 3782, loss 0.110564, acc 0.96\n",
      "2018-04-21T23:53:29.286731: step 3783, loss 0.44331, acc 0.92\n",
      "2018-04-21T23:53:29.333057: step 3784, loss 0.171834, acc 0.92\n",
      "2018-04-21T23:53:29.381792: step 3785, loss 0.0972016, acc 1\n",
      "2018-04-21T23:53:29.427824: step 3786, loss 0.114521, acc 0.98\n",
      "2018-04-21T23:53:29.473154: step 3787, loss 0.196916, acc 0.92\n",
      "2018-04-21T23:53:29.519411: step 3788, loss 0.127864, acc 0.94\n",
      "2018-04-21T23:53:29.564478: step 3789, loss 0.225361, acc 0.92\n",
      "2018-04-21T23:53:29.613234: step 3790, loss 0.177623, acc 0.9\n",
      "2018-04-21T23:53:29.658208: step 3791, loss 0.133807, acc 0.96\n",
      "2018-04-21T23:53:29.703687: step 3792, loss 0.277431, acc 0.84\n",
      "2018-04-21T23:53:29.748805: step 3793, loss 0.405847, acc 0.84\n",
      "2018-04-21T23:53:29.794899: step 3794, loss 0.399294, acc 0.84\n",
      "2018-04-21T23:53:29.845209: step 3795, loss 0.308182, acc 0.9\n",
      "2018-04-21T23:53:29.892353: step 3796, loss 0.251524, acc 0.92\n",
      "2018-04-21T23:53:29.938176: step 3797, loss 0.196695, acc 0.96\n",
      "2018-04-21T23:53:29.984324: step 3798, loss 0.0824341, acc 0.98\n",
      "2018-04-21T23:53:30.031083: step 3799, loss 0.140418, acc 0.96\n",
      "2018-04-21T23:53:30.080498: step 3800, loss 0.175812, acc 0.96\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:53:30.765558: step 3800, loss 0.39957, acc 0.81, rec 0.935268, pre 0.746881, f1 0.830525\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524354489/checkpoints/model-3800\n",
      "\n",
      "2018-04-21T23:53:30.857278: step 3801, loss 0.0749602, acc 1\n",
      "2018-04-21T23:53:30.902436: step 3802, loss 0.161964, acc 0.98\n",
      "2018-04-21T23:53:30.947306: step 3803, loss 0.32492, acc 0.86\n",
      "2018-04-21T23:53:30.995976: step 3804, loss 0.170499, acc 0.96\n",
      "2018-04-21T23:53:31.040340: step 3805, loss 0.30312, acc 0.92\n",
      "2018-04-21T23:53:31.086076: step 3806, loss 0.401724, acc 0.88\n",
      "2018-04-21T23:53:31.131593: step 3807, loss 0.123449, acc 0.96\n",
      "2018-04-21T23:53:31.177031: step 3808, loss 0.147963, acc 0.94\n",
      "2018-04-21T23:53:31.229031: step 3809, loss 0.133996, acc 0.96\n",
      "2018-04-21T23:53:31.274444: step 3810, loss 0.313247, acc 0.9\n",
      "2018-04-21T23:53:31.319470: step 3811, loss 0.130682, acc 0.94\n",
      "2018-04-21T23:53:31.366279: step 3812, loss 0.360052, acc 0.92\n",
      "2018-04-21T23:53:31.412697: step 3813, loss 0.221579, acc 0.86\n",
      "2018-04-21T23:53:31.461574: step 3814, loss 0.432621, acc 0.8\n",
      "2018-04-21T23:53:31.508272: step 3815, loss 0.142853, acc 0.96\n",
      "2018-04-21T23:53:31.553393: step 3816, loss 0.208755, acc 0.9\n",
      "2018-04-21T23:53:31.598686: step 3817, loss 0.287789, acc 0.84\n",
      "2018-04-21T23:53:31.644434: step 3818, loss 0.312891, acc 0.92\n",
      "2018-04-21T23:53:31.694647: step 3819, loss 0.127891, acc 0.98\n",
      "2018-04-21T23:53:31.739837: step 3820, loss 0.0886659, acc 0.98\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:53:32.421611: step 3820, loss 0.400099, acc 0.810741, rec 0.936012, pre 0.747475, f1 0.831186\n",
      "\n",
      "2018-04-21T23:53:32.465823: step 3821, loss 0.141393, acc 0.96\n",
      "2018-04-21T23:53:32.512769: step 3822, loss 0.184224, acc 0.94\n",
      "2018-04-21T23:53:32.558743: step 3823, loss 0.270899, acc 0.92\n",
      "2018-04-21T23:53:32.606256: step 3824, loss 0.282009, acc 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-21T23:53:32.654853: step 3825, loss 0.0754596, acc 0.96\n",
      "2018-04-21T23:53:32.699633: step 3826, loss 0.201616, acc 0.94\n",
      "2018-04-21T23:53:32.744312: step 3827, loss 0.116933, acc 0.96\n",
      "2018-04-21T23:53:32.789160: step 3828, loss 0.135002, acc 0.96\n",
      "2018-04-21T23:53:32.834511: step 3829, loss 0.222897, acc 0.9\n",
      "2018-04-21T23:53:32.885705: step 3830, loss 0.335654, acc 0.9\n",
      "2018-04-21T23:53:32.932187: step 3831, loss 0.205687, acc 0.94\n",
      "2018-04-21T23:53:32.977981: step 3832, loss 0.137586, acc 0.94\n",
      "2018-04-21T23:53:33.023685: step 3833, loss 0.19487, acc 0.92\n",
      "2018-04-21T23:53:33.068982: step 3834, loss 0.155586, acc 0.94\n",
      "2018-04-21T23:53:33.118939: step 3835, loss 0.14418, acc 0.94\n",
      "2018-04-21T23:53:33.163668: step 3836, loss 0.18517, acc 0.94\n",
      "2018-04-21T23:53:33.208627: step 3837, loss 0.0846846, acc 0.96\n",
      "2018-04-21T23:53:33.253774: step 3838, loss 0.430781, acc 0.92\n",
      "2018-04-21T23:53:33.298144: step 3839, loss 0.143628, acc 0.92\n",
      "2018-04-21T23:53:33.347610: step 3840, loss 0.138694, acc 0.96\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:53:34.008514: step 3840, loss 0.480299, acc 0.742222, rec 0.96131, pre 0.667355, f1 0.787805\n",
      "\n",
      "2018-04-21T23:53:34.052473: step 3841, loss 0.148149, acc 0.94\n",
      "2018-04-21T23:53:34.096786: step 3842, loss 0.139391, acc 0.94\n",
      "2018-04-21T23:53:34.140345: step 3843, loss 0.14453, acc 0.94\n",
      "2018-04-21T23:53:34.184739: step 3844, loss 0.249087, acc 0.9\n",
      "2018-04-21T23:53:34.241139: step 3845, loss 0.165637, acc 0.94\n",
      "2018-04-21T23:53:34.285090: step 3846, loss 0.219796, acc 0.92\n",
      "2018-04-21T23:53:34.328596: step 3847, loss 0.218775, acc 0.9\n",
      "2018-04-21T23:53:34.372617: step 3848, loss 0.187833, acc 0.94\n",
      "2018-04-21T23:53:34.417420: step 3849, loss 0.269799, acc 0.92\n",
      "2018-04-21T23:53:34.464952: step 3850, loss 0.166212, acc 0.92\n",
      "2018-04-21T23:53:34.509433: step 3851, loss 0.154936, acc 0.94\n",
      "2018-04-21T23:53:34.553524: step 3852, loss 0.13142, acc 0.96\n",
      "2018-04-21T23:53:34.597957: step 3853, loss 0.214076, acc 0.86\n",
      "2018-04-21T23:53:34.641756: step 3854, loss 0.276097, acc 0.9\n",
      "2018-04-21T23:53:34.688718: step 3855, loss 0.366687, acc 0.8\n",
      "2018-04-21T23:53:34.733445: step 3856, loss 0.395596, acc 0.78\n",
      "2018-04-21T23:53:34.778889: step 3857, loss 0.470975, acc 0.76\n",
      "2018-04-21T23:53:34.823971: step 3858, loss 0.267397, acc 0.92\n",
      "2018-04-21T23:53:34.868160: step 3859, loss 0.14802, acc 0.94\n",
      "2018-04-21T23:53:34.915353: step 3860, loss 0.246204, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:53:35.544986: step 3860, loss 0.355656, acc 0.85, rec 0.892857, pre 0.821355, f1 0.855615\n",
      "\n",
      "2018-04-21T23:53:35.588260: step 3861, loss 0.164823, acc 0.96\n",
      "2018-04-21T23:53:35.631839: step 3862, loss 0.126246, acc 0.96\n",
      "2018-04-21T23:53:35.674928: step 3863, loss 0.432679, acc 0.84\n",
      "2018-04-21T23:53:35.718080: step 3864, loss 0.224217, acc 0.92\n",
      "2018-04-21T23:53:35.763811: step 3865, loss 0.234421, acc 0.94\n",
      "2018-04-21T23:53:35.806935: step 3866, loss 0.140727, acc 0.94\n",
      "2018-04-21T23:53:35.849810: step 3867, loss 0.273873, acc 0.94\n",
      "2018-04-21T23:53:35.893079: step 3868, loss 0.170353, acc 0.92\n",
      "2018-04-21T23:53:35.936603: step 3869, loss 0.16433, acc 0.96\n",
      "2018-04-21T23:53:35.982214: step 3870, loss 0.150466, acc 0.98\n",
      "2018-04-21T23:53:36.025253: step 3871, loss 0.0652142, acc 1\n",
      "2018-04-21T23:53:36.068069: step 3872, loss 0.147481, acc 0.94\n",
      "2018-04-21T23:53:36.111386: step 3873, loss 0.128549, acc 0.98\n",
      "2018-04-21T23:53:36.154082: step 3874, loss 0.188347, acc 0.96\n",
      "2018-04-21T23:53:36.199693: step 3875, loss 0.218011, acc 0.94\n",
      "2018-04-21T23:53:36.243949: step 3876, loss 0.0730957, acc 1\n",
      "2018-04-21T23:53:36.287200: step 3877, loss 0.211563, acc 0.9\n",
      "2018-04-21T23:53:36.330314: step 3878, loss 0.270594, acc 0.88\n",
      "2018-04-21T23:53:36.374553: step 3879, loss 0.102658, acc 0.96\n",
      "2018-04-21T23:53:36.420401: step 3880, loss 0.144386, acc 0.96\n",
      "\n",
      "Evaluation:\n",
      "2018-04-21T23:53:37.106939: step 3880, loss 0.597377, acc 0.686667, rec 0.978423, pre 0.616792, f1 0.756617\n",
      "\n",
      "2018-04-21T23:53:37.150710: step 3881, loss 0.174434, acc 0.92\n",
      "2018-04-21T23:53:37.194193: step 3882, loss 0.123655, acc 0.94\n",
      "2018-04-21T23:53:37.237931: step 3883, loss 0.118886, acc 0.96\n",
      "2018-04-21T23:53:37.282053: step 3884, loss 0.182822, acc 0.94\n",
      "2018-04-21T23:53:37.328888: step 3885, loss 0.295375, acc 0.88\n",
      "2018-04-21T23:53:37.372591: step 3886, loss 0.13432, acc 0.94\n",
      "2018-04-21T23:53:37.416609: step 3887, loss 0.20553, acc 0.96\n",
      "2018-04-21T23:53:37.460032: step 3888, loss 0.44349, acc 0.82\n",
      "2018-04-21T23:53:37.503969: step 3889, loss 0.229043, acc 0.92\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", 0.75, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 100, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"1\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\",100, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.25, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 50\n",
    "tf.flags.DEFINE_integer(\"batch_size\", batch_size, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 250, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 20, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=train_s.shape[1],\n",
    "            num_classes=2,\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "        \n",
    "         # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        #vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch1, x_batch2, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch1, x_batch2, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            #r \n",
    "            step, summaries, loss, accuracy, recall, precision, f1 = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy,cnn.recall, cnn.precision, cnn.f1_score],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, rec {:g}, pre {:g}, f1 {:g}\".format(time_str, step, loss, accuracy, recall, precision, f1))# recall, precision, f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(train_s, train_c,  expanded_train_labels)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch1, x_batch2, y_batch = zip(*batch)\n",
    "            train_step(x_batch1, x_batch1, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(validation_s, validation_c, expanded_validation_labels, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "print(\"\\nTest Set:\")\n",
    "dev_step(test_s, test_c, expanded_test_labels, writer=dev_summary_writer)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
