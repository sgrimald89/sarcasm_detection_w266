{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saulgrimaldo1/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from w266_common import utils, vocabulary\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "np.random.seed(266)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "tokenizer = TweetTokenizer()\n",
    "x_data = []\n",
    "x_contexts = []\n",
    "labels = []\n",
    "sentences  = []\n",
    "contexts = []\n",
    "with open('dta/merged_data_v3.csv', 'r') as csvfile:\n",
    "    linereader = csv.reader(csvfile, delimiter = '|')\n",
    "    for i, row in enumerate(linereader):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        sentence, context, sarcasm = row\n",
    "        sentence = re.sub(\"RT @[^\\s]+:\", \"retweet\", sentence)\n",
    "        sentences.append(sentence)\n",
    "        contexts.append(context)\n",
    "        x_tokens = utils.canonicalize_words(tokenizer.tokenize(sentence), hashtags = True)\n",
    "        context_tokens = utils.canonicalize_words(tokenizer.tokenize(context), hashtags = True)\n",
    "        x_data.append(x_tokens)\n",
    "        x_contexts.append(context_tokens)\n",
    "        labels.append(int(sarcasm))\n",
    "\n",
    "\n",
    "\n",
    "shuffle_indices = np.random.permutation(np.arange(len(labels)))\n",
    "train_split_idx = int(0.7 * len(labels))\n",
    "test_split_idx  = int(0.9 * len(labels))\n",
    "\n",
    "train_indices = shuffle_indices[:train_split_idx]\n",
    "validation_indices = shuffle_indices[train_split_idx:test_split_idx]\n",
    "test_indices = shuffle_indices[test_split_idx:]\n",
    "\n",
    "\n",
    "train_sentences = np.array(x_data)[train_indices]\n",
    "train_contexts = np.array(x_contexts)[train_indices]\n",
    "train_labels= np.array(labels)[train_indices] \n",
    "validation_sentences = np.array(x_data)[validation_indices]\n",
    "validation_labels = np.array(labels)[validation_indices]\n",
    "validation_contexts = np.array(x_contexts)[validation_indices]\n",
    "test_sentences = np.array(x_data)[test_indices]  \n",
    "test_contexts =  np.array(x_contexts)[test_indices]\n",
    "test_labels = np.array(labels)[test_indices]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 6, 2]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [2]*4\n",
    "a[2] = 6\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_labels(raw_label_set, size):\n",
    "    label_set = []\n",
    "    for label in raw_label_set:\n",
    "        labels = [0] * size\n",
    "        labels[label] = 1\n",
    "        label_set.append(labels)\n",
    "    return np.array(label_set)\n",
    "\n",
    "expanded_train_labels = transform_labels(train_labels, 2)\n",
    "expanded_validation_labels = transform_labels(validation_labels,2)\n",
    "expanded_test_labels = transform_labels(test_labels,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,3,4,5,6,7,8,8,1,5,6,7]\n",
    "a + [\"<PADDING>\"]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'angry',\n",
       " 'man',\n",
       " 'is',\n",
       " 'angry',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PaddingAndTruncating:\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def pad_or_truncate(self, sentence):\n",
    "        sen_len = len(sentence)\n",
    "        paddings = self.max_len - sen_len\n",
    "        if paddings >=0:\n",
    "            return list(sentence) + [\"<PADDING>\"] * paddings\n",
    "        return sentence[0:paddings]\n",
    "        \n",
    "PadAndTrunc = PaddingAndTruncating(10)\n",
    "        \n",
    "        \n",
    "PadAndTrunc.pad_or_truncate([\"the\",\"angry\",\"man\",\"is\",\"angry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  13,   82,   75, ...,    3,    3,    3],\n",
       "       [  13, 4248,   17, ...,    3,    3,    3],\n",
       "       [  13, 4250,  296, ...,    3,    3,    3],\n",
       "       ..., \n",
       "       [  33,  140,   19, ...,    3,    3,    3],\n",
       "       [   4,    9,  607, ...,    3,    3,    3],\n",
       "       [  13,  256,   11, ...,    3,    3,    3]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.contrib import learn\n",
    "vocab_size = 5000\n",
    "\n",
    "#max_len = max([len(sent) for sent  in train_sentences])\n",
    "PadAndTrunc =PaddingAndTruncating(40)\n",
    "train_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, train_sentences))\n",
    "train_context_padded = list(map(PadAndTrunc.pad_or_truncate, train_contexts))\n",
    "validation_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, validation_sentences))\n",
    "validation_context_padded = list(map(PadAndTrunc.pad_or_truncate, validation_contexts))\n",
    "test_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, test_sentences))\n",
    "test_context_padded = list(map(PadAndTrunc.pad_or_truncate, test_contexts))\n",
    "\n",
    "vocab = vocabulary.Vocabulary(utils.flatten(list(train_sentences_padded) + list(train_context_padded)), vocab_size)\n",
    "train_s = np.array(list(map(vocab.words_to_ids, train_sentences_padded)))\n",
    "train_c = np.array(list(map(vocab.words_to_ids, train_context_padded)))\n",
    "validation_s = np.array(list(map(vocab.words_to_ids, validation_sentences_padded)))\n",
    "validation_c = np.array(list(map(vocab.words_to_ids, validation_context_padded)))\n",
    "test_s = np.array(list(map(vocab.words_to_ids, test_sentences_padded)))\n",
    "test_c = np.array(list(map(vocab.words_to_ids, test_context_padded)))\n",
    "train_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv-maxpool-3'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 3\n",
    "(\"conv-maxpool-%s\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size, \n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x1 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x1\")\n",
    "        self.input_x2 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x2\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars1 = tf.nn.embedding_lookup(self.W, self.input_x1)\n",
    "            self.embedded_chars_expanded1 = tf.expand_dims(self.embedded_chars1, -1)\n",
    "            self.embedded_chars2 = tf.nn.embedding_lookup(self.W, self.input_x2)\n",
    "            self.embedded_chars_expanded2 = tf.expand_dims(self.embedded_chars2, -1)\n",
    "\n",
    "        # Create a convolution + avgpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.variable_scope(\"conv-avgpool-%s\" % i) as scope:\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "              \n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded1,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h1 = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"tanh1\")\n",
    "               \n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.avg_pool(\n",
    "                    h1,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                \n",
    "                pooled_outputs.append(pooled)\n",
    "                #pooled_outputs.append(pooled)\n",
    "                scope.reuse_variables()\n",
    "                 \n",
    "                \n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded2,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h2 = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"tanh2\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled2 = tf.nn.avg_pool(\n",
    "                    h2,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled2)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "\n",
    "        num_filters_total = num_filters * len(filter_sizes) * 2\n",
    "\n",
    "        self.h_pool = tf.concat(pooled_outputs, 1)\n",
    "\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "        \n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "           \n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.labels = tf.argmax(self.input_y, 1)\n",
    "            TP = tf.count_nonzero(self.predictions * self.labels)\n",
    "            TN = tf.count_nonzero((self.predictions - 1) * (self.labels - 1))\n",
    "            FP = tf.count_nonzero(self.predictions * (self.labels - 1))\n",
    "            FN = tf.count_nonzero((self.predictions - 1) * self.labels)\n",
    "            self.correct_predictions = tf.equal(self.predictions, self.labels)\n",
    "            self.precision = TP / (TP + FP)\n",
    "            self.recall = TP / (TP + FN)\n",
    "            self.f1_score = 2 * self.precision * self.recall / (self.precision + self.recall)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=200\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.75\n",
      "DROPOUT_KEEP_PROB=0.4\n",
      "EMBEDDING_DIM=40\n",
      "EVALUATE_EVERY=20\n",
      "FILTER_SIZES=1\n",
      "L2_REG_LAMBDA=0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.neg\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=10\n",
      "NUM_FILTERS=40\n",
      "POSITIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.pos\n",
      "\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/W:0/grad/hist is illegal; using conv-avgpool-0/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/W:0/grad/sparsity is illegal; using conv-avgpool-0/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/b:0/grad/hist is illegal; using conv-avgpool-0/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/b:0/grad/sparsity is illegal; using conv-avgpool-0/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524457042\n",
      "\n",
      "2018-04-23T04:17:22.308707: step 1, loss 0.766557, acc 0.45\n",
      "2018-04-23T04:17:22.350265: step 2, loss 2.46834, acc 0.53\n",
      "2018-04-23T04:17:22.388251: step 3, loss 1.02614, acc 0.52\n",
      "2018-04-23T04:17:22.425486: step 4, loss 1.39134, acc 0.525\n",
      "2018-04-23T04:17:22.462659: step 5, loss 0.897728, acc 0.56\n",
      "2018-04-23T04:17:22.500460: step 6, loss 0.720883, acc 0.57\n",
      "2018-04-23T04:17:22.541553: step 7, loss 0.770722, acc 0.56\n",
      "2018-04-23T04:17:22.579657: step 8, loss 0.828969, acc 0.585\n",
      "2018-04-23T04:17:22.618734: step 9, loss 0.727337, acc 0.635\n",
      "2018-04-23T04:17:22.656381: step 10, loss 0.718736, acc 0.58\n",
      "2018-04-23T04:17:22.694230: step 11, loss 0.703115, acc 0.615\n",
      "2018-04-23T04:17:22.732721: step 12, loss 0.61083, acc 0.705\n",
      "2018-04-23T04:17:22.774382: step 13, loss 0.726158, acc 0.625\n",
      "2018-04-23T04:17:22.813263: step 14, loss 0.665475, acc 0.665\n",
      "2018-04-23T04:17:22.851212: step 15, loss 0.56279, acc 0.73\n",
      "2018-04-23T04:17:22.888841: step 16, loss 0.484383, acc 0.765\n",
      "2018-04-23T04:17:22.926896: step 17, loss 0.548553, acc 0.74\n",
      "2018-04-23T04:17:22.963347: step 18, loss 0.46873, acc 0.795\n",
      "2018-04-23T04:17:23.003801: step 19, loss 0.53269, acc 0.745\n",
      "2018-04-23T04:17:23.041777: step 20, loss 0.481072, acc 0.79\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T04:17:23.156272: step 20, loss 0.615343, acc 0.691117, rec 0.378378, pre 0.96988, f1 0.544379\n",
      "\n",
      "2018-04-23T04:17:23.197705: step 21, loss 0.62117, acc 0.705\n",
      "2018-04-23T04:17:23.239741: step 22, loss 0.484431, acc 0.78\n",
      "2018-04-23T04:17:23.277186: step 23, loss 0.606993, acc 0.72\n",
      "2018-04-23T04:17:23.314419: step 24, loss 0.502415, acc 0.77\n",
      "2018-04-23T04:17:23.351955: step 25, loss 0.396069, acc 0.825\n",
      "2018-04-23T04:17:23.389526: step 26, loss 0.395829, acc 0.85\n",
      "2018-04-23T04:17:23.426947: step 27, loss 0.417339, acc 0.82\n",
      "2018-04-23T04:17:23.468739: step 28, loss 0.569348, acc 0.695\n",
      "2018-04-23T04:17:23.506901: step 29, loss 0.502736, acc 0.78\n",
      "2018-04-23T04:17:23.545096: step 30, loss 0.40213, acc 0.815\n",
      "2018-04-23T04:17:23.569008: step 31, loss 0.570599, acc 0.768519\n",
      "2018-04-23T04:17:23.607801: step 32, loss 0.448111, acc 0.805\n",
      "2018-04-23T04:17:23.644995: step 33, loss 0.322556, acc 0.865\n",
      "2018-04-23T04:17:23.686367: step 34, loss 0.434418, acc 0.785\n",
      "2018-04-23T04:17:23.726321: step 35, loss 0.458776, acc 0.81\n",
      "2018-04-23T04:17:23.763842: step 36, loss 0.335822, acc 0.865\n",
      "2018-04-23T04:17:23.802396: step 37, loss 0.366389, acc 0.86\n",
      "2018-04-23T04:17:23.839939: step 38, loss 0.412124, acc 0.795\n",
      "2018-04-23T04:17:23.877953: step 39, loss 0.428704, acc 0.79\n",
      "2018-04-23T04:17:23.918455: step 40, loss 0.444362, acc 0.795\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T04:17:24.022539: step 40, loss 0.57831, acc 0.64298, rec 0.978848, pre 0.579277, f1 0.727829\n",
      "\n",
      "2018-04-23T04:17:24.063364: step 41, loss 0.40985, acc 0.795\n",
      "2018-04-23T04:17:24.100749: step 42, loss 0.387112, acc 0.86\n",
      "2018-04-23T04:17:24.142365: step 43, loss 0.393297, acc 0.825\n",
      "2018-04-23T04:17:24.179908: step 44, loss 0.345738, acc 0.855\n",
      "2018-04-23T04:17:24.217921: step 45, loss 0.417284, acc 0.83\n",
      "2018-04-23T04:17:24.255229: step 46, loss 0.432316, acc 0.805\n",
      "2018-04-23T04:17:24.291867: step 47, loss 0.367174, acc 0.865\n",
      "2018-04-23T04:17:24.329095: step 48, loss 0.412505, acc 0.82\n",
      "2018-04-23T04:17:24.369750: step 49, loss 0.306806, acc 0.885\n",
      "2018-04-23T04:17:24.407670: step 50, loss 0.399166, acc 0.795\n",
      "2018-04-23T04:17:24.444763: step 51, loss 0.385024, acc 0.825\n",
      "2018-04-23T04:17:24.481864: step 52, loss 0.333808, acc 0.84\n",
      "2018-04-23T04:17:24.519652: step 53, loss 0.325183, acc 0.88\n",
      "2018-04-23T04:17:24.557318: step 54, loss 0.501777, acc 0.76\n",
      "2018-04-23T04:17:24.600353: step 55, loss 0.423158, acc 0.82\n",
      "2018-04-23T04:17:24.639425: step 56, loss 0.342228, acc 0.845\n",
      "2018-04-23T04:17:24.678192: step 57, loss 0.376004, acc 0.84\n",
      "2018-04-23T04:17:24.716612: step 58, loss 0.435666, acc 0.845\n",
      "2018-04-23T04:17:24.754661: step 59, loss 0.356162, acc 0.86\n",
      "2018-04-23T04:17:24.793532: step 60, loss 0.34919, acc 0.845\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T04:17:24.901115: step 60, loss 0.423406, acc 0.812034, rec 0.917744, pre 0.751684, f1 0.826455\n",
      "\n",
      "2018-04-23T04:17:24.941698: step 61, loss 0.304059, acc 0.875\n",
      "2018-04-23T04:17:24.965545: step 62, loss 0.302101, acc 0.87963\n",
      "2018-04-23T04:17:25.004496: step 63, loss 0.226549, acc 0.88\n",
      "2018-04-23T04:17:25.043312: step 64, loss 0.323726, acc 0.87\n",
      "2018-04-23T04:17:25.081382: step 65, loss 0.271792, acc 0.89\n",
      "2018-04-23T04:17:25.122823: step 66, loss 0.32028, acc 0.855\n",
      "2018-04-23T04:17:25.161404: step 67, loss 0.348264, acc 0.84\n",
      "2018-04-23T04:17:25.199817: step 68, loss 0.277242, acc 0.875\n",
      "2018-04-23T04:17:25.237583: step 69, loss 0.393366, acc 0.825\n",
      "2018-04-23T04:17:25.275857: step 70, loss 0.326932, acc 0.865\n",
      "2018-04-23T04:17:25.313961: step 71, loss 0.329512, acc 0.85\n",
      "2018-04-23T04:17:25.356151: step 72, loss 0.389647, acc 0.845\n",
      "2018-04-23T04:17:25.394522: step 73, loss 0.330365, acc 0.86\n",
      "2018-04-23T04:17:25.432001: step 74, loss 0.454461, acc 0.79\n",
      "2018-04-23T04:17:25.469275: step 75, loss 0.356658, acc 0.88\n",
      "2018-04-23T04:17:25.506734: step 76, loss 0.384427, acc 0.865\n",
      "2018-04-23T04:17:25.545081: step 77, loss 0.229637, acc 0.905\n",
      "2018-04-23T04:17:25.586134: step 78, loss 0.356104, acc 0.875\n",
      "2018-04-23T04:17:25.624107: step 79, loss 0.375346, acc 0.84\n",
      "2018-04-23T04:17:25.661942: step 80, loss 0.412396, acc 0.81\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T04:17:25.767239: step 80, loss 0.385976, acc 0.854441, rec 0.86839, pre 0.83882, f1 0.853349\n",
      "\n",
      "2018-04-23T04:17:25.813388: step 81, loss 0.393309, acc 0.86\n",
      "2018-04-23T04:17:25.850987: step 82, loss 0.438818, acc 0.795\n",
      "2018-04-23T04:17:25.889496: step 83, loss 0.377226, acc 0.875\n",
      "2018-04-23T04:17:25.928141: step 84, loss 0.306902, acc 0.905\n",
      "2018-04-23T04:17:25.965499: step 85, loss 0.346469, acc 0.86\n",
      "2018-04-23T04:17:26.002984: step 86, loss 0.307312, acc 0.885\n",
      "2018-04-23T04:17:26.043682: step 87, loss 0.330789, acc 0.855\n",
      "2018-04-23T04:17:26.080932: step 88, loss 0.3499, acc 0.845\n",
      "2018-04-23T04:17:26.121766: step 89, loss 0.427085, acc 0.805\n",
      "2018-04-23T04:17:26.158643: step 90, loss 0.30866, acc 0.87\n",
      "2018-04-23T04:17:26.197815: step 91, loss 0.337732, acc 0.835\n",
      "2018-04-23T04:17:26.237089: step 92, loss 0.257395, acc 0.88\n",
      "2018-04-23T04:17:26.264615: step 93, loss 0.414054, acc 0.805556\n",
      "2018-04-23T04:17:26.303808: step 94, loss 0.371693, acc 0.85\n",
      "2018-04-23T04:17:26.341808: step 95, loss 0.288051, acc 0.885\n",
      "2018-04-23T04:17:26.378980: step 96, loss 0.364667, acc 0.83\n",
      "2018-04-23T04:17:26.416132: step 97, loss 0.290946, acc 0.89\n",
      "2018-04-23T04:17:26.453575: step 98, loss 0.395944, acc 0.86\n",
      "2018-04-23T04:17:26.493818: step 99, loss 0.310328, acc 0.87\n",
      "2018-04-23T04:17:26.531292: step 100, loss 0.257564, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T04:17:26.633865: step 100, loss 0.815326, acc 0.573066, rec 0.971798, pre 0.534238, f1 0.689454\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524457042/checkpoints/model-100\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-23T04:17:26.728460: step 101, loss 0.424785, acc 0.825\n",
      "2018-04-23T04:17:26.765773: step 102, loss 0.33465, acc 0.87\n",
      "2018-04-23T04:17:26.807383: step 103, loss 0.316422, acc 0.86\n",
      "2018-04-23T04:17:26.845296: step 104, loss 0.404167, acc 0.83\n",
      "2018-04-23T04:17:26.883434: step 105, loss 0.394577, acc 0.865\n",
      "2018-04-23T04:17:26.921906: step 106, loss 0.366283, acc 0.845\n",
      "2018-04-23T04:17:26.962474: step 107, loss 0.296332, acc 0.885\n",
      "2018-04-23T04:17:26.999430: step 108, loss 0.308156, acc 0.885\n",
      "2018-04-23T04:17:27.038280: step 109, loss 0.437419, acc 0.8\n",
      "2018-04-23T04:17:27.076080: step 110, loss 0.408308, acc 0.855\n",
      "2018-04-23T04:17:27.113644: step 111, loss 0.405456, acc 0.845\n",
      "2018-04-23T04:17:27.150894: step 112, loss 0.337789, acc 0.84\n",
      "2018-04-23T04:17:27.200594: step 113, loss 0.300077, acc 0.875\n",
      "2018-04-23T04:17:27.261779: step 114, loss 0.378629, acc 0.83\n",
      "2018-04-23T04:17:27.321342: step 115, loss 0.30569, acc 0.845\n",
      "2018-04-23T04:17:27.378452: step 116, loss 0.305983, acc 0.87\n",
      "2018-04-23T04:17:27.421942: step 117, loss 0.263376, acc 0.87\n",
      "2018-04-23T04:17:27.459582: step 118, loss 0.303242, acc 0.865\n",
      "2018-04-23T04:17:27.497161: step 119, loss 0.313442, acc 0.88\n",
      "2018-04-23T04:17:27.535553: step 120, loss 0.337574, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T04:17:27.648193: step 120, loss 0.760386, acc 0.569628, rec 0.957697, pre 0.53268, f1 0.684586\n",
      "\n",
      "2018-04-23T04:17:27.688811: step 121, loss 0.286945, acc 0.885\n",
      "2018-04-23T04:17:27.726855: step 122, loss 0.342404, acc 0.86\n",
      "2018-04-23T04:17:27.765031: step 123, loss 0.363265, acc 0.845\n",
      "2018-04-23T04:17:27.789634: step 124, loss 0.444383, acc 0.842593\n",
      "2018-04-23T04:17:27.829542: step 125, loss 0.33162, acc 0.85\n",
      "2018-04-23T04:17:27.870575: step 126, loss 0.358876, acc 0.86\n",
      "2018-04-23T04:17:27.909754: step 127, loss 0.31117, acc 0.88\n",
      "2018-04-23T04:17:27.948975: step 128, loss 0.30998, acc 0.9\n",
      "2018-04-23T04:17:27.987252: step 129, loss 0.29745, acc 0.87\n",
      "2018-04-23T04:17:28.025823: step 130, loss 0.482798, acc 0.835\n",
      "2018-04-23T04:17:28.065088: step 131, loss 0.364568, acc 0.865\n",
      "2018-04-23T04:17:28.107555: step 132, loss 0.30823, acc 0.89\n",
      "2018-04-23T04:17:28.145662: step 133, loss 0.494321, acc 0.83\n",
      "2018-04-23T04:17:28.185118: step 134, loss 0.241595, acc 0.905\n",
      "2018-04-23T04:17:28.222646: step 135, loss 0.289826, acc 0.865\n",
      "2018-04-23T04:17:28.260448: step 136, loss 0.386885, acc 0.855\n",
      "2018-04-23T04:17:28.298179: step 137, loss 0.370101, acc 0.865\n",
      "2018-04-23T04:17:28.340091: step 138, loss 0.353319, acc 0.855\n",
      "2018-04-23T04:17:28.379932: step 139, loss 0.295861, acc 0.88\n",
      "2018-04-23T04:17:28.419468: step 140, loss 0.392398, acc 0.845\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T04:17:28.528374: step 140, loss 0.335607, acc 0.866476, rec 0.850764, pre 0.872289, f1 0.861392\n",
      "\n",
      "2018-04-23T04:17:28.574359: step 141, loss 0.387779, acc 0.845\n",
      "2018-04-23T04:17:28.612441: step 142, loss 0.360278, acc 0.895\n",
      "2018-04-23T04:17:28.649962: step 143, loss 0.257932, acc 0.905\n",
      "2018-04-23T04:17:28.687509: step 144, loss 0.268855, acc 0.885\n",
      "2018-04-23T04:17:28.724790: step 145, loss 0.275605, acc 0.895\n",
      "2018-04-23T04:17:28.762079: step 146, loss 0.298681, acc 0.89\n",
      "2018-04-23T04:17:28.803362: step 147, loss 0.31065, acc 0.895\n",
      "2018-04-23T04:17:28.842464: step 148, loss 0.241719, acc 0.92\n",
      "2018-04-23T04:17:28.881131: step 149, loss 0.400695, acc 0.855\n",
      "2018-04-23T04:17:28.920565: step 150, loss 0.277286, acc 0.9\n",
      "2018-04-23T04:17:28.958955: step 151, loss 0.282778, acc 0.91\n",
      "2018-04-23T04:17:28.997476: step 152, loss 0.424632, acc 0.835\n",
      "2018-04-23T04:17:29.039672: step 153, loss 0.296458, acc 0.875\n",
      "2018-04-23T04:17:29.077420: step 154, loss 0.387523, acc 0.84\n",
      "2018-04-23T04:17:29.101563: step 155, loss 0.269314, acc 0.87963\n",
      "2018-04-23T04:17:29.140414: step 156, loss 0.345951, acc 0.825\n",
      "2018-04-23T04:17:29.177829: step 157, loss 0.292144, acc 0.885\n",
      "2018-04-23T04:17:29.215727: step 158, loss 0.413617, acc 0.83\n",
      "2018-04-23T04:17:29.257258: step 159, loss 0.329255, acc 0.885\n",
      "2018-04-23T04:17:29.295279: step 160, loss 0.233535, acc 0.905\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T04:17:29.401154: step 160, loss 0.485475, acc 0.738109, rec 0.960047, pre 0.658871, f1 0.781444\n",
      "\n",
      "2018-04-23T04:17:29.442331: step 161, loss 0.27896, acc 0.865\n",
      "2018-04-23T04:17:29.486233: step 162, loss 0.411577, acc 0.85\n",
      "2018-04-23T04:17:29.525154: step 163, loss 0.350525, acc 0.86\n",
      "2018-04-23T04:17:29.563360: step 164, loss 0.265485, acc 0.885\n",
      "2018-04-23T04:17:29.601541: step 165, loss 0.352835, acc 0.865\n",
      "2018-04-23T04:17:29.641478: step 166, loss 0.328157, acc 0.86\n",
      "2018-04-23T04:17:29.679984: step 167, loss 0.543855, acc 0.825\n",
      "2018-04-23T04:17:29.720679: step 168, loss 0.356331, acc 0.87\n",
      "2018-04-23T04:17:29.758525: step 169, loss 0.31668, acc 0.89\n",
      "2018-04-23T04:17:29.796704: step 170, loss 0.245261, acc 0.895\n",
      "2018-04-23T04:17:29.834617: step 171, loss 0.25032, acc 0.905\n",
      "2018-04-23T04:17:29.872038: step 172, loss 0.302462, acc 0.885\n",
      "2018-04-23T04:17:29.909883: step 173, loss 0.270492, acc 0.875\n",
      "2018-04-23T04:17:29.950555: step 174, loss 0.279375, acc 0.865\n",
      "2018-04-23T04:17:29.987681: step 175, loss 0.338699, acc 0.855\n",
      "2018-04-23T04:17:30.025344: step 176, loss 0.28983, acc 0.92\n",
      "2018-04-23T04:17:30.062107: step 177, loss 0.325835, acc 0.865\n",
      "2018-04-23T04:17:30.099680: step 178, loss 0.338116, acc 0.855\n",
      "2018-04-23T04:17:30.137049: step 179, loss 0.246995, acc 0.885\n",
      "2018-04-23T04:17:30.177978: step 180, loss 0.350307, acc 0.845\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T04:17:30.281438: step 180, loss 0.483607, acc 0.730086, rec 0.956522, pre 0.652244, f1 0.775607\n",
      "\n",
      "2018-04-23T04:17:30.323088: step 181, loss 0.400731, acc 0.85\n",
      "2018-04-23T04:17:30.360960: step 182, loss 0.283341, acc 0.89\n",
      "2018-04-23T04:17:30.401578: step 183, loss 0.258957, acc 0.895\n",
      "2018-04-23T04:17:30.439210: step 184, loss 0.300334, acc 0.87\n",
      "2018-04-23T04:17:30.476921: step 185, loss 0.356383, acc 0.855\n",
      "2018-04-23T04:17:30.500468: step 186, loss 0.377932, acc 0.851852\n",
      "2018-04-23T04:17:30.538778: step 187, loss 0.315069, acc 0.9\n",
      "2018-04-23T04:17:30.575547: step 188, loss 0.452752, acc 0.87\n",
      "2018-04-23T04:17:30.617220: step 189, loss 0.278422, acc 0.87\n",
      "2018-04-23T04:17:30.654202: step 190, loss 0.266568, acc 0.905\n",
      "2018-04-23T04:17:30.695069: step 191, loss 0.378948, acc 0.86\n",
      "2018-04-23T04:17:30.734846: step 192, loss 0.316312, acc 0.905\n",
      "2018-04-23T04:17:30.772284: step 193, loss 0.303729, acc 0.87\n",
      "2018-04-23T04:17:30.813638: step 194, loss 0.327414, acc 0.89\n",
      "2018-04-23T04:17:30.855443: step 195, loss 0.330285, acc 0.87\n",
      "2018-04-23T04:17:30.896039: step 196, loss 0.356281, acc 0.875\n",
      "2018-04-23T04:17:30.936398: step 197, loss 0.265176, acc 0.875\n",
      "2018-04-23T04:17:30.996496: step 198, loss 0.352616, acc 0.85\n",
      "2018-04-23T04:17:31.047789: step 199, loss 0.442913, acc 0.81\n",
      "2018-04-23T04:17:31.090544: step 200, loss 0.278418, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T04:17:31.199123: step 200, loss 0.38961, acc 0.802865, rec 0.638073, pre 0.937824, f1 0.759441\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524457042/checkpoints/model-200\n",
      "\n",
      "2018-04-23T04:17:31.290756: step 201, loss 0.330952, acc 0.895\n",
      "2018-04-23T04:17:31.333387: step 202, loss 0.395701, acc 0.885\n",
      "2018-04-23T04:17:31.371228: step 203, loss 0.345972, acc 0.88\n",
      "2018-04-23T04:17:31.410231: step 204, loss 0.366055, acc 0.86\n",
      "2018-04-23T04:17:31.448133: step 205, loss 0.332692, acc 0.89\n",
      "2018-04-23T04:17:31.486601: step 206, loss 0.415657, acc 0.86\n",
      "2018-04-23T04:17:31.525324: step 207, loss 0.303816, acc 0.88\n",
      "2018-04-23T04:17:31.567440: step 208, loss 0.398768, acc 0.855\n",
      "2018-04-23T04:17:31.606441: step 209, loss 0.354969, acc 0.855\n",
      "2018-04-23T04:17:31.645676: step 210, loss 0.410512, acc 0.875\n",
      "2018-04-23T04:17:31.684102: step 211, loss 0.274478, acc 0.89\n",
      "2018-04-23T04:17:31.722530: step 212, loss 0.278227, acc 0.875\n",
      "2018-04-23T04:17:31.759568: step 213, loss 0.525051, acc 0.83\n",
      "2018-04-23T04:17:31.802263: step 214, loss 0.410889, acc 0.85\n",
      "2018-04-23T04:17:31.839906: step 215, loss 0.271678, acc 0.88\n",
      "2018-04-23T04:17:31.877342: step 216, loss 0.371762, acc 0.825\n",
      "2018-04-23T04:17:31.902228: step 217, loss 0.238667, acc 0.898148\n",
      "2018-04-23T04:17:31.940917: step 218, loss 0.482606, acc 0.795\n",
      "2018-04-23T04:17:31.978126: step 219, loss 0.28306, acc 0.895\n",
      "2018-04-23T04:17:32.019101: step 220, loss 0.314077, acc 0.89\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T04:17:32.127323: step 220, loss 0.57389, acc 0.710602, rec 0.902468, pre 0.645378, f1 0.752572\n",
      "\n",
      "2018-04-23T04:17:32.168382: step 221, loss 0.465336, acc 0.84\n",
      "2018-04-23T04:17:32.207473: step 222, loss 0.349845, acc 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-23T04:17:32.250111: step 223, loss 0.335897, acc 0.885\n",
      "2018-04-23T04:17:32.288457: step 224, loss 0.284744, acc 0.9\n",
      "2018-04-23T04:17:32.326816: step 225, loss 0.234962, acc 0.9\n",
      "2018-04-23T04:17:32.364255: step 226, loss 0.358427, acc 0.86\n",
      "2018-04-23T04:17:32.401843: step 227, loss 0.262265, acc 0.905\n",
      "2018-04-23T04:17:32.439881: step 228, loss 0.321765, acc 0.865\n",
      "2018-04-23T04:17:32.481654: step 229, loss 0.246836, acc 0.9\n",
      "2018-04-23T04:17:32.519102: step 230, loss 0.331257, acc 0.87\n",
      "2018-04-23T04:17:32.557128: step 231, loss 0.344104, acc 0.835\n",
      "2018-04-23T04:17:32.594925: step 232, loss 0.238079, acc 0.9\n",
      "2018-04-23T04:17:32.633704: step 233, loss 0.287134, acc 0.89\n",
      "2018-04-23T04:17:32.673100: step 234, loss 0.278406, acc 0.89\n",
      "2018-04-23T04:17:32.715432: step 235, loss 0.289929, acc 0.88\n",
      "2018-04-23T04:17:32.753807: step 236, loss 0.218165, acc 0.915\n",
      "2018-04-23T04:17:32.790907: step 237, loss 0.427806, acc 0.865\n",
      "2018-04-23T04:17:32.829024: step 238, loss 0.363039, acc 0.89\n",
      "2018-04-23T04:17:32.867453: step 239, loss 0.366494, acc 0.87\n",
      "2018-04-23T04:17:32.905253: step 240, loss 0.280726, acc 0.895\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T04:17:33.015347: step 240, loss 0.380522, acc 0.809169, rec 0.643948, pre 0.948097, f1 0.76697\n",
      "\n",
      "2018-04-23T04:17:33.057083: step 241, loss 0.320592, acc 0.88\n",
      "2018-04-23T04:17:33.095564: step 242, loss 0.287384, acc 0.88\n",
      "2018-04-23T04:17:33.133880: step 243, loss 0.169874, acc 0.92\n",
      "2018-04-23T04:17:33.173263: step 244, loss 0.268283, acc 0.9\n",
      "2018-04-23T04:17:33.212454: step 245, loss 0.336017, acc 0.88\n",
      "2018-04-23T04:17:33.255066: step 246, loss 0.224126, acc 0.885\n",
      "2018-04-23T04:17:33.293595: step 247, loss 0.317875, acc 0.885\n",
      "2018-04-23T04:17:33.318473: step 248, loss 0.236853, acc 0.87963\n",
      "2018-04-23T04:17:33.358134: step 249, loss 0.286269, acc 0.885\n",
      "2018-04-23T04:17:33.396259: step 250, loss 0.362217, acc 0.87\n",
      "2018-04-23T04:17:33.433002: step 251, loss 0.364499, acc 0.875\n",
      "2018-04-23T04:17:33.474238: step 252, loss 0.313411, acc 0.87\n",
      "2018-04-23T04:17:33.512832: step 253, loss 0.306019, acc 0.885\n",
      "2018-04-23T04:17:33.549967: step 254, loss 0.295411, acc 0.91\n",
      "2018-04-23T04:17:33.588500: step 255, loss 0.291878, acc 0.88\n",
      "2018-04-23T04:17:33.626715: step 256, loss 0.332991, acc 0.89\n",
      "2018-04-23T04:17:33.665662: step 257, loss 0.295038, acc 0.885\n",
      "2018-04-23T04:17:33.707993: step 258, loss 0.287796, acc 0.885\n",
      "2018-04-23T04:17:33.746555: step 259, loss 0.339437, acc 0.85\n",
      "2018-04-23T04:17:33.785615: step 260, loss 0.266643, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T04:17:33.892529: step 260, loss 0.451055, acc 0.763897, rec 0.958872, pre 0.68399, f1 0.798434\n",
      "\n",
      "2018-04-23T04:17:33.937726: step 261, loss 0.179719, acc 0.93\n",
      "2018-04-23T04:17:33.976883: step 262, loss 0.333752, acc 0.89\n",
      "2018-04-23T04:17:34.015675: step 263, loss 0.408655, acc 0.835\n",
      "2018-04-23T04:17:34.053757: step 264, loss 0.260434, acc 0.91\n",
      "2018-04-23T04:17:34.092315: step 265, loss 0.34814, acc 0.875\n",
      "2018-04-23T04:17:34.130418: step 266, loss 0.302061, acc 0.88\n",
      "2018-04-23T04:17:34.171695: step 267, loss 0.256123, acc 0.885\n",
      "2018-04-23T04:17:34.210125: step 268, loss 0.274348, acc 0.89\n",
      "2018-04-23T04:17:34.249227: step 269, loss 0.278957, acc 0.875\n",
      "2018-04-23T04:17:34.287713: step 270, loss 0.360308, acc 0.85\n",
      "2018-04-23T04:17:34.325819: step 271, loss 0.250528, acc 0.905\n",
      "2018-04-23T04:17:34.363263: step 272, loss 0.304339, acc 0.895\n",
      "2018-04-23T04:17:34.404434: step 273, loss 0.269649, acc 0.9\n",
      "2018-04-23T04:17:34.441829: step 274, loss 0.328371, acc 0.875\n",
      "2018-04-23T04:17:34.479741: step 275, loss 0.356826, acc 0.85\n",
      "2018-04-23T04:17:34.517527: step 276, loss 0.330276, acc 0.88\n",
      "2018-04-23T04:17:34.555001: step 277, loss 0.358082, acc 0.86\n",
      "2018-04-23T04:17:34.593048: step 278, loss 0.299308, acc 0.875\n",
      "2018-04-23T04:17:34.620564: step 279, loss 0.32952, acc 0.842593\n",
      "2018-04-23T04:17:34.660384: step 280, loss 0.261798, acc 0.885\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T04:17:34.766369: step 280, loss 0.502338, acc 0.722063, rec 0.965922, pre 0.643192, f1 0.772194\n",
      "\n",
      "2018-04-23T04:17:34.807663: step 281, loss 0.360879, acc 0.87\n",
      "2018-04-23T04:17:34.849581: step 282, loss 0.330573, acc 0.865\n",
      "2018-04-23T04:17:34.886416: step 283, loss 0.334592, acc 0.89\n",
      "2018-04-23T04:17:34.923848: step 284, loss 0.260707, acc 0.9\n",
      "2018-04-23T04:17:34.961171: step 285, loss 0.292465, acc 0.895\n",
      "2018-04-23T04:17:34.998202: step 286, loss 0.270168, acc 0.9\n",
      "2018-04-23T04:17:35.035876: step 287, loss 0.286675, acc 0.915\n",
      "2018-04-23T04:17:35.077694: step 288, loss 0.336229, acc 0.875\n",
      "2018-04-23T04:17:35.115043: step 289, loss 0.162999, acc 0.96\n",
      "2018-04-23T04:17:35.153114: step 290, loss 0.417058, acc 0.85\n",
      "2018-04-23T04:17:35.191178: step 291, loss 0.356061, acc 0.875\n",
      "2018-04-23T04:17:35.229655: step 292, loss 0.268868, acc 0.905\n",
      "2018-04-23T04:17:35.267711: step 293, loss 0.329816, acc 0.88\n",
      "2018-04-23T04:17:35.311201: step 294, loss 0.328728, acc 0.87\n",
      "2018-04-23T04:17:35.350322: step 295, loss 0.212991, acc 0.915\n",
      "2018-04-23T04:17:35.388938: step 296, loss 0.312742, acc 0.88\n",
      "2018-04-23T04:17:35.427743: step 297, loss 0.283831, acc 0.895\n",
      "2018-04-23T04:17:35.465489: step 298, loss 0.390174, acc 0.865\n",
      "2018-04-23T04:17:35.505713: step 299, loss 0.373365, acc 0.85\n",
      "2018-04-23T04:17:35.547169: step 300, loss 0.266773, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T04:17:35.653010: step 300, loss 0.514984, acc 0.727221, rec 0.964747, pre 0.647987, f1 0.77526\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524457042/checkpoints/model-300\n",
      "\n",
      "2018-04-23T04:17:35.779290: step 301, loss 0.353372, acc 0.855\n",
      "2018-04-23T04:17:35.842179: step 302, loss 0.255498, acc 0.89\n",
      "2018-04-23T04:17:35.902723: step 303, loss 0.243859, acc 0.91\n",
      "2018-04-23T04:17:35.962636: step 304, loss 0.182654, acc 0.93\n",
      "2018-04-23T04:17:36.020427: step 305, loss 0.329893, acc 0.845\n",
      "2018-04-23T04:17:36.066636: step 306, loss 0.341431, acc 0.89\n",
      "2018-04-23T04:17:36.114222: step 307, loss 0.331993, acc 0.88\n",
      "2018-04-23T04:17:36.172779: step 308, loss 0.425155, acc 0.865\n",
      "2018-04-23T04:17:36.238654: step 309, loss 0.306191, acc 0.89\n",
      "2018-04-23T04:17:36.276420: step 310, loss 0.30454, acc 0.87963\n",
      "\n",
      "Test Set:\n",
      "2018-04-23T04:17:36.345355: step 310, loss 0.394417, acc 0.805269, rec 0.65358, pre 0.933993, f1 0.769022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", 0.75, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 40, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"1\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 40, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.4, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 200\n",
    "tf.flags.DEFINE_integer(\"batch_size\", batch_size, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 10, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 20, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=train_s.shape[1],\n",
    "            num_classes=2,\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "        \n",
    "         # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        #vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch1, x_batch2, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch1, x_batch2, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            #r \n",
    "            step, summaries, loss, accuracy, recall, precision, f1 = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy,cnn.recall, cnn.precision, cnn.f1_score],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, rec {:g}, pre {:g}, f1 {:g}\".format(time_str, step, loss, accuracy, recall, precision, f1))# recall, precision, f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "        \n",
    "        def error_analysis(x_batch1, x_batch2, y_batch, writer=None):\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            #r \n",
    "            step, summaries, loss, accuracy, recall, precision, f1, correct, scores,predictions  = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy,cnn.recall, cnn.precision, cnn.f1_score, cnn.correct_predictions, cnn.scores, cnn.predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, rec {:g}, pre {:g}, f1 {:g}\".format(time_str, step, loss, accuracy, recall, precision, f1))# recall, precision, f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "            return correct, scores, predictions\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(train_s, train_c,  expanded_train_labels)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch1, x_batch2, y_batch = zip(*batch)\n",
    "            train_step(x_batch1, x_batch1, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(validation_s, validation_c, expanded_validation_labels, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "print(\"\\nTest Set:\")\n",
    "correct, logits, predictions = error_analysis(test_s, test_c, expanded_test_labels, writer=dev_summary_writer)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3.9692254, 585],\n",
       " [3.6889453, 499],\n",
       " [3.4967997, 21],\n",
       " [3.3064656, 342],\n",
       " [3.2465987, 122],\n",
       " [3.1841993, 847],\n",
       " [3.1125093, 500],\n",
       " [3.0225759, 423],\n",
       " [2.9111867, 599],\n",
       " [2.5301166, 569],\n",
       " [2.3669016, 776],\n",
       " [2.2934806, 727],\n",
       " [2.2127571, 428],\n",
       " [2.207355, 208],\n",
       " [2.2011037, 493],\n",
       " [2.1007051, 473],\n",
       " [1.9627103, 314],\n",
       " [1.9453565, 71],\n",
       " [1.9207044, 450],\n",
       " [1.8842003, 30],\n",
       " [1.8573475, 409],\n",
       " [1.8101934, 422],\n",
       " [1.7824445, 835],\n",
       " [1.7614505, 19],\n",
       " [1.6806912, 526],\n",
       " [1.6610124, 133],\n",
       " [1.6342477, 697],\n",
       " [1.5654123, 406],\n",
       " [1.4898311, 402],\n",
       " [1.449121, 579],\n",
       " [1.4169533, 794],\n",
       " [1.3894911, 259],\n",
       " [1.3894911, 293],\n",
       " [1.3894911, 471],\n",
       " [1.3805683, 729],\n",
       " [1.3546317, 775],\n",
       " [1.3247739, 84],\n",
       " [1.3198525, 470],\n",
       " [1.2961091, 652],\n",
       " [1.2777147, 288],\n",
       " [1.2588717, 465],\n",
       " [1.2387588, 223],\n",
       " [1.2354015, 80],\n",
       " [1.2300875, 165],\n",
       " [1.22452, 327],\n",
       " [1.1979649, 678],\n",
       " [1.1748334, 258],\n",
       " [1.1276263, 743],\n",
       " [1.119957, 395],\n",
       " [1.1013198, 11],\n",
       " [1.0924248, 100],\n",
       " [1.0760601, 442],\n",
       " [1.0331972, 390],\n",
       " [1.029422, 630],\n",
       " [1.0276014, 431],\n",
       " [1.0237194, 576],\n",
       " [1.0222585, 75],\n",
       " [1.0078558, 255],\n",
       " [0.96828842, 636],\n",
       " [0.95950228, 821],\n",
       " [0.9580186, 20],\n",
       " [0.91395521, 403],\n",
       " [0.90825838, 407],\n",
       " [0.90463549, 34],\n",
       " [0.90267658, 207],\n",
       " [0.89620209, 715],\n",
       " [0.88468724, 724],\n",
       " [0.86670661, 755],\n",
       " [0.8656553, 270],\n",
       " [0.8656553, 290],\n",
       " [0.86248893, 574],\n",
       " [0.85803962, 99],\n",
       " [0.84979218, 519],\n",
       " [0.8177554, 146],\n",
       " [0.8169691, 568],\n",
       " [0.80298907, 566],\n",
       " [0.79120755, 230],\n",
       " [0.76467586, 115],\n",
       " [0.7629056, 478],\n",
       " [0.7507745, 169],\n",
       " [0.73464102, 252],\n",
       " [0.73203605, 644],\n",
       " [0.72106743, 131],\n",
       " [0.71436447, 352],\n",
       " [0.71316099, 466],\n",
       " [0.71188337, 98],\n",
       " [0.70610696, 94],\n",
       " [0.70539111, 121],\n",
       " [0.69119167, 281],\n",
       " [0.68430728, 570],\n",
       " [0.66587991, 451],\n",
       " [0.66373968, 278],\n",
       " [0.66081208, 761],\n",
       " [0.65341902, 722],\n",
       " [0.6469121, 858],\n",
       " [0.63115996, 114],\n",
       " [0.60988045, 837],\n",
       " [0.60087216, 367],\n",
       " [0.56988388, 118],\n",
       " [0.56150568, 705],\n",
       " [0.55914515, 59],\n",
       " [0.54555225, 483],\n",
       " [0.5441578, 448],\n",
       " [0.53767025, 565],\n",
       " [0.53559333, 27],\n",
       " [0.53482985, 391],\n",
       " [0.53320354, 419],\n",
       " [0.52538085, 869],\n",
       " [0.5093196, 517],\n",
       " [0.4990173, 347],\n",
       " [0.48354688, 22],\n",
       " [0.47861904, 78],\n",
       " [0.47269326, 188],\n",
       " [0.46086162, 147],\n",
       " [0.42276257, 803],\n",
       " [0.41263676, 250],\n",
       " [0.41092509, 184],\n",
       " [0.38336027, 545],\n",
       " [0.38336027, 767],\n",
       " [0.37140185, 764],\n",
       " [0.35217518, 510],\n",
       " [0.33368975, 309],\n",
       " [0.3243933, 769],\n",
       " [0.31723315, 547],\n",
       " [0.31534332, 239],\n",
       " [0.30952626, 464],\n",
       " [0.30622709, 554],\n",
       " [0.30482048, 89],\n",
       " [0.30262333, 453],\n",
       " [0.29524809, 572],\n",
       " [0.27726519, 195],\n",
       " [0.27326688, 163],\n",
       " [0.27253157, 413],\n",
       " [0.27126741, 386],\n",
       " [0.26251239, 32],\n",
       " [0.26251239, 463],\n",
       " [0.26221448, 354],\n",
       " [0.25113624, 712],\n",
       " [0.24828935, 54],\n",
       " [0.23350823, 485],\n",
       " [0.22342879, 360],\n",
       " [0.22294366, 812],\n",
       " [0.22207338, 658],\n",
       " [0.21894825, 540],\n",
       " [0.21768019, 645],\n",
       " [0.21467981, 206],\n",
       " [0.19661778, 0],\n",
       " [0.19661778, 241],\n",
       " [0.19661778, 672],\n",
       " [0.18032706, 651],\n",
       " [0.17981547, 695],\n",
       " [0.15708509, 702],\n",
       " [0.15307403, 455],\n",
       " [0.15208843, 141],\n",
       " [0.14844224, 720],\n",
       " [0.13625246, 679],\n",
       " [0.12691072, 87],\n",
       " [0.12162942, 811],\n",
       " [0.11445442, 101],\n",
       " [0.10610676, 582],\n",
       " [0.097944885, 532],\n",
       " [0.076670289, 734],\n",
       " [0.075637877, 454],\n",
       " [0.058322549, 655],\n",
       " [0.048644453, 841],\n",
       " [0.037262619, 798],\n",
       " [0.034209311, 226],\n",
       " [0.027563125, 534],\n",
       " [0.01410687, 804],\n",
       " [0.0065432191, 537]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong = correct == False\n",
    "def incorrect_confidence(wrong, logits, predictions):\n",
    "    indeces = np.where(wrong)\n",
    "    wrong_predictions = predictions[indeces]\n",
    "    wrong_logits = logits[indeces]\n",
    "    \n",
    "    return [[wrong_logits[i][value] - wrong_logits[i][1-value], indeces[0][i]] for i, value in enumerate(wrong_predictions)]\n",
    "\n",
    "\n",
    "sorted(incorrect_confidence(wrong, logits, predictions), key = lambda logit: -logit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LINK', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'once', 'upon', 'a', 'time', '(', 'spin', ')', 'featuring', 'ACCOUNT', 'of', 'd12', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG']\n",
      "['nan']\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "index = 499\n",
    "print(test_sentences[index])\n",
    "print(test_contexts[index])\n",
    "print(test_labels[index])\n",
    "print(predictions[index])\n",
    "#definitely sarcastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'one', 'enter', 'gan', '!', '!', '!', '-', '-', '-', 'follow', 'ACCOUNT', '-', '-', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'HASHTAG', 'LINK']\n",
      "['nan']\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "index = 122\n",
    "print(test_sentences[index])\n",
    "print(test_contexts[index])\n",
    "print(test_labels[index])\n",
    "print(predictions[index])\n",
    "\n",
    "# hard to say what's going on here. seems to be slightly sarcastic, not really sure though. Throwing me off too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['march', 'for', 'our', 'lives', 'brings', 'out', 'mlks', 'granddaughter', 'and', 'demands', 'for', 'a', 'gun', 'free', 'world', '.', 'yes', 'lets', 'all', 'rally', 'behind', 'a', 'DG', 'year', 'olds', 'beliefs', '.', 'HASHTAG']\n",
      "['nan']\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "index = 423\n",
    "print(test_sentences[index])\n",
    "print(test_contexts[index])\n",
    "print(test_labels[index])\n",
    "print(predictions[index])\n",
    "# heavy use of hashtags is throwing this one off too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['these', 'kids', 'have', 'managed', 'to', 'score', 'the', 'kind', 'of', 'extracurricular', 'HASHTAG', 'we', '', 've', 'been', 'eviscerating', 'for', 'decades', 'in', 'the', 'united', 'states', '.', 'these', 'kids', 'aren', '', 't', 'prodigiously', 'gifted', '.', 'they', '', 've', 'just', 'had', 'the', 'gift', 'of', 'the', 'kind', 'of', 'education', 'we', 'no', 'longer', 'value', '.', 'HASHTAG', 'HASHTAG']\n",
      "['kasi', 'c', '.', 'on', 'twitter', ':', 'the', 'students', 'of', 'HASHTAG', 'have', 'been', 'the', 'beneficiaries', 'of', 'the', 'kind', 'of', '1950s', '-', 'style', 'public', 'HASHTAG', 'that', 'has', 'all', 'but', 'vanished', 'in', 'america', '&', 'that', 'is', 'being', 'dismantled', 'with', 'great', 'deliberation', 'as', 'funding', 'for', 'things', 'like', 'the', 'arts', ',', 'civics', ',', '&', 'enrichment', 'are', 'zeroed', 'out', '.', 'HASHTAG']\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "index = 84\n",
    "print(test_sentences[index])\n",
    "print(test_contexts[index])\n",
    "print(test_labels[index])\n",
    "print(predictions[index])\n",
    "# emoji is throwing it off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['retweet', 'ACCOUNT', 'ACCOUNT', '', '', '', '.', '&', '(', '', '_', '', ')', '_', 'HASHTAG', '.', 'des', 'HASHTAG', '!', '[', '!', 'HASHTAG', '*', ']', '(', '*', 'HASHTAG', 'of', ',', 'sad', 'as', '']\n",
      "['nan']\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "index = 395\n",
    "print(test_sentences[index])\n",
    "print(test_contexts[index])\n",
    "print(test_labels[index])\n",
    "print(predictions[index])\n",
    "\n",
    "# heavy use of hashtags is throwing this one off. Majority of tweets with heavy hashtag usage (as decribed in EDA)\n",
    "# are non-sarcatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
