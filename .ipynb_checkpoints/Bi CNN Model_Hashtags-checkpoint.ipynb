{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saulgrimaldo1/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from w266_common import utils, vocabulary\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "np.random.seed(266)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "tokenizer = TweetTokenizer()\n",
    "x_data = []\n",
    "x_contexts = []\n",
    "labels = []\n",
    "sentences  = []\n",
    "contexts = []\n",
    "with open('merged_data_v3.csv', 'r') as csvfile:\n",
    "    linereader = csv.reader(csvfile, delimiter = '|')\n",
    "    for i, row in enumerate(linereader):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        sentence, context, sarcasm = row\n",
    "        sentence = re.sub(\"RT @[^\\s]+:\", \"retweet\", sentence)\n",
    "        sentences.append(sentence)\n",
    "        contexts.append(context)\n",
    "        x_tokens = utils.canonicalize_words(tokenizer.tokenize(sentence), hashtags = False)\n",
    "        context_tokens = utils.canonicalize_words(tokenizer.tokenize(context))\n",
    "        x_data.append(x_tokens)\n",
    "        x_contexts.append(context_tokens)\n",
    "        labels.append(int(sarcasm))\n",
    "\n",
    "\n",
    "#rng = np.random.RandomState(5)\n",
    "#rng.shuffle(x_data)  # in-place\n",
    "#train_split_idx = int(0.7 * len(labels))\n",
    "#test_split_idx  = int(0.9 * len(labels))\n",
    "\n",
    "shuffle_indices = np.random.permutation(np.arange(len(labels)))\n",
    "train_split_idx = int(0.7 * len(labels))\n",
    "test_split_idx  = int(0.9 * len(labels))\n",
    "\n",
    "train_indices = shuffle_indices[:train_split_idx]\n",
    "validation_indices = shuffle_indices[train_split_idx:test_split_idx]\n",
    "test_indices = shuffle_indices[test_split_idx:]\n",
    "\n",
    "\n",
    "train_sentences = np.array(x_data)[train_indices]\n",
    "train_contexts = np.array(x_contexts)[train_indices]\n",
    "train_labels= np.array(labels)[train_indices] \n",
    "validation_sentences = np.array(x_data)[validation_indices]\n",
    "validation_labels = np.array(labels)[validation_indices]\n",
    "validation_contexts = np.array(x_contexts)[validation_indices]\n",
    "test_sentences = np.array(x_data)[test_indices]  \n",
    "test_contexts = np.array(x_contexts)[test_indices]\n",
    "test_labels = np.array(labels)[test_indices]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6108,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_contexts)[train_indices].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6108,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_labels(raw_label_set, size):\n",
    "    label_set = []\n",
    "    for label in raw_label_set:\n",
    "        labels = [0] * size\n",
    "        labels[label] = 1\n",
    "        label_set.append(labels)\n",
    "    return np.array(label_set)\n",
    "\n",
    "expanded_train_labels = transform_labels(train_labels, 2)\n",
    "expanded_validation_labels = transform_labels(validation_labels,2)\n",
    "expanded_test_labels = transform_labels(test_labels,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4, 5, 6, 7, 8, 8, 1, 5, 6, 7]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,3,4,5,6,7,8,8,1,5,6,7]\n",
    "a + [\"<PADDING>\"]*0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'angry',\n",
       " 'man',\n",
       " 'is',\n",
       " 'angry',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>',\n",
       " '<PADDING>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PaddingAndTruncating:\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def pad_or_truncate(self, sentence):\n",
    "        sen_len = len(sentence)\n",
    "        paddings = self.max_len - sen_len\n",
    "        if paddings >=0:\n",
    "            return list(sentence) + [\"<PADDING>\"] * paddings\n",
    "        return sentence[0:paddings]\n",
    "        \n",
    "PadAndTrunc = PaddingAndTruncating(10)\n",
    "        \n",
    "        \n",
    "PadAndTrunc.pad_or_truncate([\"the\",\"angry\",\"man\",\"is\",\"angry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 12,  83,  76, ...,   3,   3,   3],\n",
       "       [ 12,   2,  16, ...,   3,   3,   3],\n",
       "       [ 12,   2, 324, ...,   3,   3,   3],\n",
       "       ..., \n",
       "       [ 32, 148,  18, ...,   3,   3,   3],\n",
       "       [240,   8, 686, ...,   3,   3,   3],\n",
       "       [ 12, 279,  10, ...,   3,   3,   3]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.contrib import learn\n",
    "vocab_size = 5000\n",
    "#max_len = max([len(sent) for sent  in train_sentences])\n",
    "PadAndTrunc =PaddingAndTruncating(40)\n",
    "train_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, train_sentences))\n",
    "train_context_padded = list(map(PadAndTrunc.pad_or_truncate, train_contexts))\n",
    "validation_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, validation_sentences))\n",
    "validation_context_padded = list(map(PadAndTrunc.pad_or_truncate, validation_contexts))\n",
    "test_sentences_padded = list(map(PadAndTrunc.pad_or_truncate, test_sentences))\n",
    "test_context_padded = list(map(PadAndTrunc.pad_or_truncate, test_contexts))\n",
    "\n",
    "\n",
    "vocab = vocabulary.Vocabulary(utils.flatten(list(train_sentences_padded) + list(train_context_padded)), vocab_size)\n",
    "train_s = np.array(list(map(vocab.words_to_ids, train_sentences_padded)))\n",
    "train_c = np.array(list(map(vocab.words_to_ids, train_context_padded)))\n",
    "validation_s = np.array(list(map(vocab.words_to_ids, validation_sentences_padded)))\n",
    "validation_c = np.array(list(map(vocab.words_to_ids, validation_context_padded)))\n",
    "test_s = np.array(list(map(vocab.words_to_ids, test_sentences_padded)))\n",
    "test_c = np.array(list(map(vocab.words_to_ids, test_context_padded)))\n",
    "train_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv-maxpool-3'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 3\n",
    "(\"conv-maxpool-%s\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size, \n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x1 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x1\")\n",
    "        self.input_x2 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x2\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars1 = tf.nn.embedding_lookup(self.W, self.input_x1)\n",
    "            self.embedded_chars_expanded1 = tf.expand_dims(self.embedded_chars1, -1)\n",
    "            self.embedded_chars2 = tf.nn.embedding_lookup(self.W, self.input_x2)\n",
    "            self.embedded_chars_expanded2 = tf.expand_dims(self.embedded_chars2, -1)\n",
    "\n",
    "        # Create a convolution + avgpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.variable_scope(\"conv-avgpool-%s\" % i) as scope:\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "              \n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded1,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h1 = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"tanh1\")\n",
    "               \n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.avg_pool(\n",
    "                    h1,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                \n",
    "                pooled_outputs.append(pooled)\n",
    "                #pooled_outputs.append(pooled)\n",
    "                scope.reuse_variables()\n",
    "                 \n",
    "                \n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded2,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h2 = tf.nn.tanh(tf.nn.bias_add(conv, b), name=\"tanh2\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled2 = tf.nn.avg_pool(\n",
    "                    h2,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled2)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "\n",
    "        num_filters_total = num_filters * len(filter_sizes) * 2\n",
    "\n",
    "        self.h_pool = tf.concat(pooled_outputs, 1)\n",
    "\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "        \n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "           \n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.labels = tf.argmax(self.input_y, 1)\n",
    "            TP = tf.count_nonzero(self.predictions * self.labels)\n",
    "            TN = tf.count_nonzero((self.predictions - 1) * (self.labels - 1))\n",
    "            FP = tf.count_nonzero(self.predictions * (self.labels - 1))\n",
    "            FN = tf.count_nonzero((self.predictions - 1) * self.labels)\n",
    "            self.correct_predictions = tf.equal(self.predictions, self.labels)\n",
    "            self.precision = TP / (TP + FP)\n",
    "            self.recall = TP / (TP + FN)\n",
    "            self.f1_score = 2 * self.precision * self.recall / (self.precision + self.recall)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=200\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.75\n",
      "DROPOUT_KEEP_PROB=0.4\n",
      "EMBEDDING_DIM=60\n",
      "EVALUATE_EVERY=20\n",
      "FILTER_SIZES=1\n",
      "L2_REG_LAMBDA=0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.neg\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=10\n",
      "NUM_FILTERS=60\n",
      "POSITIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.pos\n",
      "\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/W:0/grad/hist is illegal; using conv-avgpool-0/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/W:0/grad/sparsity is illegal; using conv-avgpool-0/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/b:0/grad/hist is illegal; using conv-avgpool-0/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-avgpool-0/b:0/grad/sparsity is illegal; using conv-avgpool-0/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524455611\n",
      "\n",
      "2018-04-23T03:53:31.988734: step 1, loss 0.727564, acc 0.515\n",
      "2018-04-23T03:53:32.061398: step 2, loss 1.40747, acc 0.475\n",
      "2018-04-23T03:53:32.126561: step 3, loss 3.33782, acc 0.46\n",
      "2018-04-23T03:53:32.199124: step 4, loss 1.38997, acc 0.515\n",
      "2018-04-23T03:53:32.263457: step 5, loss 0.782295, acc 0.685\n",
      "2018-04-23T03:53:32.329409: step 6, loss 1.22791, acc 0.59\n",
      "2018-04-23T03:53:32.396026: step 7, loss 1.08662, acc 0.59\n",
      "2018-04-23T03:53:32.467279: step 8, loss 0.850519, acc 0.62\n",
      "2018-04-23T03:53:32.534142: step 9, loss 0.664051, acc 0.72\n",
      "2018-04-23T03:53:32.601934: step 10, loss 0.697297, acc 0.685\n",
      "2018-04-23T03:53:32.676167: step 11, loss 0.958696, acc 0.7\n",
      "2018-04-23T03:53:32.741803: step 12, loss 0.707285, acc 0.745\n",
      "2018-04-23T03:53:32.808268: step 13, loss 0.517027, acc 0.82\n",
      "2018-04-23T03:53:32.874582: step 14, loss 0.590088, acc 0.79\n",
      "2018-04-23T03:53:32.945439: step 15, loss 0.792661, acc 0.73\n",
      "2018-04-23T03:53:33.011292: step 16, loss 0.625061, acc 0.775\n",
      "2018-04-23T03:53:33.077172: step 17, loss 0.506607, acc 0.82\n",
      "2018-04-23T03:53:33.143644: step 18, loss 0.488903, acc 0.845\n",
      "2018-04-23T03:53:33.215384: step 19, loss 0.665857, acc 0.79\n",
      "2018-04-23T03:53:33.282219: step 20, loss 0.694852, acc 0.765\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T03:53:33.468407: step 20, loss 0.36107, acc 0.824642, rec 0.903643, pre 0.774421, f1 0.834056\n",
      "\n",
      "2018-04-23T03:53:33.548383: step 21, loss 0.479128, acc 0.825\n",
      "2018-04-23T03:53:33.613334: step 22, loss 0.412988, acc 0.805\n",
      "2018-04-23T03:53:33.681186: step 23, loss 0.362235, acc 0.865\n",
      "2018-04-23T03:53:33.744125: step 24, loss 0.563877, acc 0.795\n",
      "2018-04-23T03:53:33.804226: step 25, loss 0.453445, acc 0.785\n",
      "2018-04-23T03:53:33.864398: step 26, loss 0.342013, acc 0.875\n",
      "2018-04-23T03:53:33.929397: step 27, loss 0.401911, acc 0.835\n",
      "2018-04-23T03:53:33.990092: step 28, loss 0.487385, acc 0.81\n",
      "2018-04-23T03:53:34.049682: step 29, loss 0.599323, acc 0.76\n",
      "2018-04-23T03:53:34.109307: step 30, loss 0.350334, acc 0.855\n",
      "2018-04-23T03:53:34.150247: step 31, loss 0.377956, acc 0.814815\n",
      "2018-04-23T03:53:34.213314: step 32, loss 0.385883, acc 0.845\n",
      "2018-04-23T03:53:34.274169: step 33, loss 0.36385, acc 0.855\n",
      "2018-04-23T03:53:34.333639: step 34, loss 0.363026, acc 0.86\n",
      "2018-04-23T03:53:34.396308: step 35, loss 0.275659, acc 0.885\n",
      "2018-04-23T03:53:34.455343: step 36, loss 0.321735, acc 0.845\n",
      "2018-04-23T03:53:34.514021: step 37, loss 0.284285, acc 0.875\n",
      "2018-04-23T03:53:34.577121: step 38, loss 0.348295, acc 0.855\n",
      "2018-04-23T03:53:34.641206: step 39, loss 0.274233, acc 0.885\n",
      "2018-04-23T03:53:34.703091: step 40, loss 0.284375, acc 0.885\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T03:53:34.877201: step 40, loss 0.302898, acc 0.881375, rec 0.804935, pre 0.943526, f1 0.868738\n",
      "\n",
      "2018-04-23T03:53:34.942538: step 41, loss 0.347071, acc 0.855\n",
      "2018-04-23T03:53:35.001845: step 42, loss 0.214105, acc 0.905\n",
      "2018-04-23T03:53:35.061282: step 43, loss 0.287068, acc 0.88\n",
      "2018-04-23T03:53:35.124882: step 44, loss 0.264402, acc 0.885\n",
      "2018-04-23T03:53:35.185163: step 45, loss 0.295069, acc 0.9\n",
      "2018-04-23T03:53:35.245954: step 46, loss 0.37891, acc 0.865\n",
      "2018-04-23T03:53:35.306025: step 47, loss 0.377924, acc 0.86\n",
      "2018-04-23T03:53:35.370184: step 48, loss 0.312695, acc 0.855\n",
      "2018-04-23T03:53:35.430444: step 49, loss 0.181343, acc 0.93\n",
      "2018-04-23T03:53:35.490188: step 50, loss 0.226502, acc 0.92\n",
      "2018-04-23T03:53:35.550490: step 51, loss 0.245292, acc 0.905\n",
      "2018-04-23T03:53:35.616202: step 52, loss 0.235978, acc 0.9\n",
      "2018-04-23T03:53:35.677176: step 53, loss 0.214516, acc 0.92\n",
      "2018-04-23T03:53:35.738081: step 54, loss 0.283624, acc 0.88\n",
      "2018-04-23T03:53:35.800818: step 55, loss 0.271975, acc 0.875\n",
      "2018-04-23T03:53:35.866422: step 56, loss 0.194542, acc 0.935\n",
      "2018-04-23T03:53:35.928174: step 57, loss 0.181079, acc 0.925\n",
      "2018-04-23T03:53:35.988970: step 58, loss 0.251718, acc 0.91\n",
      "2018-04-23T03:53:36.050610: step 59, loss 0.20705, acc 0.905\n",
      "2018-04-23T03:53:36.116672: step 60, loss 0.23135, acc 0.885\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T03:53:36.286940: step 60, loss 0.324174, acc 0.864183, rec 0.964747, pre 0.798638, f1 0.873869\n",
      "\n",
      "2018-04-23T03:53:36.357841: step 61, loss 0.207908, acc 0.915\n",
      "2018-04-23T03:53:36.397818: step 62, loss 0.245864, acc 0.87037\n",
      "2018-04-23T03:53:36.459184: step 63, loss 0.167745, acc 0.935\n",
      "2018-04-23T03:53:36.520280: step 64, loss 0.251256, acc 0.915\n",
      "2018-04-23T03:53:36.584565: step 65, loss 0.145906, acc 0.94\n",
      "2018-04-23T03:53:36.645263: step 66, loss 0.220912, acc 0.915\n",
      "2018-04-23T03:53:36.708090: step 67, loss 0.246494, acc 0.875\n",
      "2018-04-23T03:53:36.768670: step 68, loss 0.199402, acc 0.955\n",
      "2018-04-23T03:53:36.834260: step 69, loss 0.204083, acc 0.92\n",
      "2018-04-23T03:53:36.896427: step 70, loss 0.288509, acc 0.885\n",
      "2018-04-23T03:53:36.957819: step 71, loss 0.296205, acc 0.9\n",
      "2018-04-23T03:53:37.018754: step 72, loss 0.143072, acc 0.95\n",
      "2018-04-23T03:53:37.083151: step 73, loss 0.287212, acc 0.895\n",
      "2018-04-23T03:53:37.143298: step 74, loss 0.233368, acc 0.925\n",
      "2018-04-23T03:53:37.204766: step 75, loss 0.245802, acc 0.915\n",
      "2018-04-23T03:53:37.267516: step 76, loss 0.41368, acc 0.845\n",
      "2018-04-23T03:53:37.333514: step 77, loss 0.264021, acc 0.9\n",
      "2018-04-23T03:53:37.394732: step 78, loss 0.174841, acc 0.94\n",
      "2018-04-23T03:53:37.455965: step 79, loss 0.267233, acc 0.925\n",
      "2018-04-23T03:53:37.517117: step 80, loss 0.38803, acc 0.86\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T03:53:37.693442: step 80, loss 0.792767, acc 0.647564, rec 0.970623, pre 0.583333, f1 0.728716\n",
      "\n",
      "2018-04-23T03:53:37.760719: step 81, loss 0.334527, acc 0.875\n",
      "2018-04-23T03:53:37.827298: step 82, loss 0.301709, acc 0.88\n",
      "2018-04-23T03:53:37.888008: step 83, loss 0.193356, acc 0.91\n",
      "2018-04-23T03:53:37.952464: step 84, loss 0.209265, acc 0.92\n",
      "2018-04-23T03:53:38.013517: step 85, loss 0.277867, acc 0.895\n",
      "2018-04-23T03:53:38.074834: step 86, loss 0.242995, acc 0.88\n",
      "2018-04-23T03:53:38.134804: step 87, loss 0.220028, acc 0.92\n",
      "2018-04-23T03:53:38.199533: step 88, loss 0.318329, acc 0.885\n",
      "2018-04-23T03:53:38.262401: step 89, loss 0.331963, acc 0.895\n",
      "2018-04-23T03:53:38.322956: step 90, loss 0.174074, acc 0.915\n",
      "2018-04-23T03:53:38.382371: step 91, loss 0.234075, acc 0.895\n",
      "2018-04-23T03:53:38.444160: step 92, loss 0.332361, acc 0.89\n",
      "2018-04-23T03:53:38.482701: step 93, loss 0.325487, acc 0.916667\n",
      "2018-04-23T03:53:38.544135: step 94, loss 0.313204, acc 0.89\n",
      "2018-04-23T03:53:38.603996: step 95, loss 0.252547, acc 0.915\n",
      "2018-04-23T03:53:38.670060: step 96, loss 0.472014, acc 0.825\n",
      "2018-04-23T03:53:38.731082: step 97, loss 0.313827, acc 0.875\n",
      "2018-04-23T03:53:38.791846: step 98, loss 0.188757, acc 0.94\n",
      "2018-04-23T03:53:38.853554: step 99, loss 0.210049, acc 0.91\n",
      "2018-04-23T03:53:38.917947: step 100, loss 0.246723, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T03:53:39.089875: step 100, loss 0.307051, acc 0.849857, rec 0.733255, pre 0.946889, f1 0.82649\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524455611/checkpoints/model-100\n",
      "\n",
      "2018-04-23T03:53:39.212025: step 101, loss 0.412204, acc 0.865\n",
      "2018-04-23T03:53:39.272339: step 102, loss 0.168087, acc 0.955\n",
      "2018-04-23T03:53:39.332699: step 103, loss 0.244442, acc 0.905\n",
      "2018-04-23T03:53:39.395515: step 104, loss 0.402113, acc 0.9\n",
      "2018-04-23T03:53:39.455242: step 105, loss 0.315952, acc 0.905\n",
      "2018-04-23T03:53:39.516587: step 106, loss 0.185671, acc 0.93\n",
      "2018-04-23T03:53:39.577712: step 107, loss 0.241141, acc 0.91\n",
      "2018-04-23T03:53:39.642757: step 108, loss 0.17575, acc 0.935\n",
      "2018-04-23T03:53:39.704397: step 109, loss 0.272382, acc 0.88\n",
      "2018-04-23T03:53:39.764324: step 110, loss 0.240277, acc 0.925\n",
      "2018-04-23T03:53:39.824775: step 111, loss 0.255546, acc 0.925\n",
      "2018-04-23T03:53:39.889879: step 112, loss 0.132339, acc 0.94\n",
      "2018-04-23T03:53:39.950729: step 113, loss 0.264832, acc 0.9\n",
      "2018-04-23T03:53:40.012242: step 114, loss 0.214915, acc 0.93\n",
      "2018-04-23T03:53:40.073725: step 115, loss 0.123271, acc 0.96\n",
      "2018-04-23T03:53:40.138268: step 116, loss 0.212224, acc 0.92\n",
      "2018-04-23T03:53:40.199729: step 117, loss 0.258915, acc 0.91\n",
      "2018-04-23T03:53:40.259671: step 118, loss 0.238827, acc 0.885\n",
      "2018-04-23T03:53:40.320482: step 119, loss 0.222331, acc 0.94\n",
      "2018-04-23T03:53:40.388085: step 120, loss 0.184205, acc 0.945\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T03:53:40.562366: step 120, loss 0.5919, acc 0.724928, rec 0.958872, pre 0.647105, f1 0.772727\n",
      "\n",
      "2018-04-23T03:53:40.632283: step 121, loss 0.246784, acc 0.895\n",
      "2018-04-23T03:53:40.694497: step 122, loss 0.341877, acc 0.88\n",
      "2018-04-23T03:53:40.757420: step 123, loss 0.181992, acc 0.945\n",
      "2018-04-23T03:53:40.795964: step 124, loss 0.197893, acc 0.898148\n",
      "2018-04-23T03:53:40.860683: step 125, loss 0.456475, acc 0.855\n",
      "2018-04-23T03:53:40.922246: step 126, loss 0.235471, acc 0.895\n",
      "2018-04-23T03:53:40.982767: step 127, loss 0.144345, acc 0.96\n",
      "2018-04-23T03:53:41.043649: step 128, loss 0.287296, acc 0.895\n",
      "2018-04-23T03:53:41.110012: step 129, loss 0.152991, acc 0.945\n",
      "2018-04-23T03:53:41.172231: step 130, loss 0.276201, acc 0.88\n",
      "2018-04-23T03:53:41.233997: step 131, loss 0.257872, acc 0.905\n",
      "2018-04-23T03:53:41.293650: step 132, loss 0.20688, acc 0.935\n",
      "2018-04-23T03:53:41.356168: step 133, loss 0.259997, acc 0.9\n",
      "2018-04-23T03:53:41.416139: step 134, loss 0.376469, acc 0.87\n",
      "2018-04-23T03:53:41.476756: step 135, loss 0.209543, acc 0.94\n",
      "2018-04-23T03:53:41.536825: step 136, loss 0.234793, acc 0.915\n",
      "2018-04-23T03:53:41.600252: step 137, loss 0.179753, acc 0.93\n",
      "2018-04-23T03:53:41.660797: step 138, loss 0.337634, acc 0.875\n",
      "2018-04-23T03:53:41.720100: step 139, loss 0.247626, acc 0.91\n",
      "2018-04-23T03:53:41.779209: step 140, loss 0.211269, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T03:53:41.953433: step 140, loss 0.248793, acc 0.89914, rec 0.86839, pre 0.920299, f1 0.893591\n",
      "\n",
      "2018-04-23T03:53:42.021691: step 141, loss 0.216806, acc 0.925\n",
      "2018-04-23T03:53:42.081733: step 142, loss 0.190032, acc 0.92\n",
      "2018-04-23T03:53:42.142474: step 143, loss 0.138646, acc 0.945\n",
      "2018-04-23T03:53:42.207288: step 144, loss 0.253764, acc 0.915\n",
      "2018-04-23T03:53:42.267151: step 145, loss 0.201659, acc 0.91\n",
      "2018-04-23T03:53:42.327423: step 146, loss 0.360708, acc 0.89\n",
      "2018-04-23T03:53:42.387093: step 147, loss 0.216055, acc 0.915\n",
      "2018-04-23T03:53:42.450402: step 148, loss 0.220895, acc 0.93\n",
      "2018-04-23T03:53:42.509874: step 149, loss 0.241673, acc 0.92\n",
      "2018-04-23T03:53:42.569953: step 150, loss 0.253327, acc 0.915\n",
      "2018-04-23T03:53:42.629351: step 151, loss 0.179203, acc 0.94\n",
      "2018-04-23T03:53:42.697560: step 152, loss 0.332833, acc 0.885\n",
      "2018-04-23T03:53:42.760719: step 153, loss 0.0816977, acc 0.97\n",
      "2018-04-23T03:53:42.822230: step 154, loss 0.241286, acc 0.93\n",
      "2018-04-23T03:53:42.862425: step 155, loss 0.154661, acc 0.907407\n",
      "2018-04-23T03:53:42.928988: step 156, loss 0.231378, acc 0.915\n",
      "2018-04-23T03:53:42.991310: step 157, loss 0.205477, acc 0.925\n",
      "2018-04-23T03:53:43.052727: step 158, loss 0.267104, acc 0.905\n",
      "2018-04-23T03:53:43.116153: step 159, loss 0.199076, acc 0.905\n",
      "2018-04-23T03:53:43.183510: step 160, loss 0.172778, acc 0.93\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T03:53:43.371057: step 160, loss 0.490888, acc 0.758166, rec 0.983549, pre 0.672289, f1 0.798664\n",
      "\n",
      "2018-04-23T03:53:43.443386: step 161, loss 0.19053, acc 0.935\n",
      "2018-04-23T03:53:43.506236: step 162, loss 0.267635, acc 0.91\n",
      "2018-04-23T03:53:43.566566: step 163, loss 0.197192, acc 0.94\n",
      "2018-04-23T03:53:43.629511: step 164, loss 0.186855, acc 0.92\n",
      "2018-04-23T03:53:43.693331: step 165, loss 0.298083, acc 0.87\n",
      "2018-04-23T03:53:43.755354: step 166, loss 0.150568, acc 0.94\n",
      "2018-04-23T03:53:43.816716: step 167, loss 0.224205, acc 0.925\n",
      "2018-04-23T03:53:43.878186: step 168, loss 0.335201, acc 0.875\n",
      "2018-04-23T03:53:43.942618: step 169, loss 0.248214, acc 0.895\n",
      "2018-04-23T03:53:44.003033: step 170, loss 0.183742, acc 0.94\n",
      "2018-04-23T03:53:44.064023: step 171, loss 0.107102, acc 0.955\n",
      "2018-04-23T03:53:44.123397: step 172, loss 0.170516, acc 0.925\n",
      "2018-04-23T03:53:44.185917: step 173, loss 0.231686, acc 0.91\n",
      "2018-04-23T03:53:44.245954: step 174, loss 0.170121, acc 0.95\n",
      "2018-04-23T03:53:44.307072: step 175, loss 0.193082, acc 0.91\n",
      "2018-04-23T03:53:44.366986: step 176, loss 0.173155, acc 0.94\n",
      "2018-04-23T03:53:44.431880: step 177, loss 0.279071, acc 0.92\n",
      "2018-04-23T03:53:44.492837: step 178, loss 0.205519, acc 0.91\n",
      "2018-04-23T03:53:44.552606: step 179, loss 0.214565, acc 0.93\n",
      "2018-04-23T03:53:44.612798: step 180, loss 0.138512, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T03:53:44.787925: step 180, loss 0.752854, acc 0.64298, rec 0.975323, pre 0.579609, f1 0.727113\n",
      "\n",
      "2018-04-23T03:53:44.854082: step 181, loss 0.235938, acc 0.92\n",
      "2018-04-23T03:53:44.915552: step 182, loss 0.212947, acc 0.895\n",
      "2018-04-23T03:53:44.976181: step 183, loss 0.208294, acc 0.93\n",
      "2018-04-23T03:53:45.041128: step 184, loss 0.248982, acc 0.92\n",
      "2018-04-23T03:53:45.101922: step 185, loss 0.157661, acc 0.915\n",
      "2018-04-23T03:53:45.139912: step 186, loss 0.167324, acc 0.925926\n",
      "2018-04-23T03:53:45.200956: step 187, loss 0.25123, acc 0.905\n",
      "2018-04-23T03:53:45.265672: step 188, loss 0.267153, acc 0.91\n",
      "2018-04-23T03:53:45.325966: step 189, loss 0.154065, acc 0.925\n",
      "2018-04-23T03:53:45.387285: step 190, loss 0.236734, acc 0.915\n",
      "2018-04-23T03:53:45.447360: step 191, loss 0.288207, acc 0.905\n",
      "2018-04-23T03:53:45.511023: step 192, loss 0.166335, acc 0.93\n",
      "2018-04-23T03:53:45.570933: step 193, loss 0.186516, acc 0.925\n",
      "2018-04-23T03:53:45.630408: step 194, loss 0.241034, acc 0.915\n",
      "2018-04-23T03:53:45.692698: step 195, loss 0.156674, acc 0.945\n",
      "2018-04-23T03:53:45.755206: step 196, loss 0.275598, acc 0.91\n",
      "2018-04-23T03:53:45.814366: step 197, loss 0.168848, acc 0.935\n",
      "2018-04-23T03:53:45.875230: step 198, loss 0.218067, acc 0.905\n",
      "2018-04-23T03:53:45.935387: step 199, loss 0.404928, acc 0.875\n",
      "2018-04-23T03:53:45.998853: step 200, loss 0.145532, acc 0.96\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T03:53:46.168605: step 200, loss 0.322547, acc 0.85043, rec 0.768508, pre 0.910864, f1 0.833652\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524455611/checkpoints/model-200\n",
      "\n",
      "2018-04-23T03:53:46.285805: step 201, loss 0.302388, acc 0.875\n",
      "2018-04-23T03:53:46.345776: step 202, loss 0.241568, acc 0.91\n",
      "2018-04-23T03:53:46.406186: step 203, loss 0.146556, acc 0.96\n",
      "2018-04-23T03:53:46.472963: step 204, loss 0.382301, acc 0.875\n",
      "2018-04-23T03:53:46.534229: step 205, loss 0.23508, acc 0.92\n",
      "2018-04-23T03:53:46.594257: step 206, loss 0.2379, acc 0.925\n",
      "2018-04-23T03:53:46.653332: step 207, loss 0.170319, acc 0.945\n",
      "2018-04-23T03:53:46.717511: step 208, loss 0.287659, acc 0.92\n",
      "2018-04-23T03:53:46.777923: step 209, loss 0.193048, acc 0.93\n",
      "2018-04-23T03:53:46.838892: step 210, loss 0.182904, acc 0.925\n",
      "2018-04-23T03:53:46.899981: step 211, loss 0.144412, acc 0.925\n",
      "2018-04-23T03:53:46.966133: step 212, loss 0.235516, acc 0.935\n",
      "2018-04-23T03:53:47.027968: step 213, loss 0.297156, acc 0.9\n",
      "2018-04-23T03:53:47.088879: step 214, loss 0.3155, acc 0.9\n",
      "2018-04-23T03:53:47.147939: step 215, loss 0.252485, acc 0.93\n",
      "2018-04-23T03:53:47.209911: step 216, loss 0.404656, acc 0.86\n",
      "2018-04-23T03:53:47.246394: step 217, loss 0.195822, acc 0.935185\n",
      "2018-04-23T03:53:47.306128: step 218, loss 0.318307, acc 0.875\n",
      "2018-04-23T03:53:47.364282: step 219, loss 0.312751, acc 0.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-23T03:53:47.426914: step 220, loss 0.311032, acc 0.89\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T03:53:47.600238: step 220, loss 0.487697, acc 0.767908, rec 0.962397, pre 0.687081, f1 0.801762\n",
      "\n",
      "2018-04-23T03:53:47.669552: step 221, loss 0.224841, acc 0.925\n",
      "2018-04-23T03:53:47.730458: step 222, loss 0.246643, acc 0.9\n",
      "2018-04-23T03:53:47.789535: step 223, loss 0.235345, acc 0.905\n",
      "2018-04-23T03:53:47.850763: step 224, loss 0.200088, acc 0.915\n",
      "2018-04-23T03:53:47.915152: step 225, loss 0.171452, acc 0.93\n",
      "2018-04-23T03:53:47.976012: step 226, loss 0.274884, acc 0.91\n",
      "2018-04-23T03:53:48.038599: step 227, loss 0.261572, acc 0.9\n",
      "2018-04-23T03:53:48.101100: step 228, loss 0.141005, acc 0.95\n",
      "2018-04-23T03:53:48.165801: step 229, loss 0.197012, acc 0.93\n",
      "2018-04-23T03:53:48.227538: step 230, loss 0.179757, acc 0.945\n",
      "2018-04-23T03:53:48.288969: step 231, loss 0.277951, acc 0.88\n",
      "2018-04-23T03:53:48.349342: step 232, loss 0.157172, acc 0.925\n",
      "2018-04-23T03:53:48.416341: step 233, loss 0.182412, acc 0.935\n",
      "2018-04-23T03:53:48.477450: step 234, loss 0.264889, acc 0.9\n",
      "2018-04-23T03:53:48.541148: step 235, loss 0.214345, acc 0.95\n",
      "2018-04-23T03:53:48.602381: step 236, loss 0.234292, acc 0.935\n",
      "2018-04-23T03:53:48.666441: step 237, loss 0.224408, acc 0.935\n",
      "2018-04-23T03:53:48.728604: step 238, loss 0.212176, acc 0.9\n",
      "2018-04-23T03:53:48.791104: step 239, loss 0.228375, acc 0.915\n",
      "2018-04-23T03:53:48.854537: step 240, loss 0.218557, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T03:53:49.030032: step 240, loss 0.342416, acc 0.832665, rec 0.931845, pre 0.772152, f1 0.844515\n",
      "\n",
      "2018-04-23T03:53:49.095377: step 241, loss 0.215896, acc 0.94\n",
      "2018-04-23T03:53:49.154473: step 242, loss 0.314399, acc 0.915\n",
      "2018-04-23T03:53:49.214389: step 243, loss 0.19262, acc 0.93\n",
      "2018-04-23T03:53:49.276885: step 244, loss 0.238021, acc 0.92\n",
      "2018-04-23T03:53:49.336480: step 245, loss 0.178297, acc 0.935\n",
      "2018-04-23T03:53:49.396063: step 246, loss 0.263016, acc 0.9\n",
      "2018-04-23T03:53:49.455551: step 247, loss 0.230078, acc 0.915\n",
      "2018-04-23T03:53:49.496871: step 248, loss 0.158645, acc 0.972222\n",
      "2018-04-23T03:53:49.559189: step 249, loss 0.238618, acc 0.91\n",
      "2018-04-23T03:53:49.620254: step 250, loss 0.245416, acc 0.92\n",
      "2018-04-23T03:53:49.681867: step 251, loss 0.270549, acc 0.93\n",
      "2018-04-23T03:53:49.746011: step 252, loss 0.183357, acc 0.91\n",
      "2018-04-23T03:53:49.807224: step 253, loss 0.177951, acc 0.965\n",
      "2018-04-23T03:53:49.868053: step 254, loss 0.198214, acc 0.935\n",
      "2018-04-23T03:53:49.928846: step 255, loss 0.17493, acc 0.93\n",
      "2018-04-23T03:53:49.993434: step 256, loss 0.251782, acc 0.915\n",
      "2018-04-23T03:53:50.054233: step 257, loss 0.26493, acc 0.915\n",
      "2018-04-23T03:53:50.114684: step 258, loss 0.181207, acc 0.925\n",
      "2018-04-23T03:53:50.175103: step 259, loss 0.271295, acc 0.89\n",
      "2018-04-23T03:53:50.239294: step 260, loss 0.249821, acc 0.905\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T03:53:50.408621: step 260, loss 0.334407, acc 0.844126, rec 0.940071, pre 0.783546, f1 0.854701\n",
      "\n",
      "2018-04-23T03:53:50.477410: step 261, loss 0.171735, acc 0.94\n",
      "2018-04-23T03:53:50.537669: step 262, loss 0.176173, acc 0.925\n",
      "2018-04-23T03:53:50.598148: step 263, loss 0.325434, acc 0.9\n",
      "2018-04-23T03:53:50.660299: step 264, loss 0.169375, acc 0.945\n",
      "2018-04-23T03:53:50.727572: step 265, loss 0.313244, acc 0.875\n",
      "2018-04-23T03:53:50.787910: step 266, loss 0.27636, acc 0.88\n",
      "2018-04-23T03:53:50.848903: step 267, loss 0.138721, acc 0.935\n",
      "2018-04-23T03:53:50.909453: step 268, loss 0.178273, acc 0.94\n",
      "2018-04-23T03:53:50.973516: step 269, loss 0.203242, acc 0.91\n",
      "2018-04-23T03:53:51.034827: step 270, loss 0.436328, acc 0.87\n",
      "2018-04-23T03:53:51.094720: step 271, loss 0.262664, acc 0.91\n",
      "2018-04-23T03:53:51.154809: step 272, loss 0.274086, acc 0.915\n",
      "2018-04-23T03:53:51.219144: step 273, loss 0.243703, acc 0.895\n",
      "2018-04-23T03:53:51.280318: step 274, loss 0.186257, acc 0.93\n",
      "2018-04-23T03:53:51.340166: step 275, loss 0.246221, acc 0.915\n",
      "2018-04-23T03:53:51.401386: step 276, loss 0.215391, acc 0.915\n",
      "2018-04-23T03:53:51.466110: step 277, loss 0.281738, acc 0.885\n",
      "2018-04-23T03:53:51.528408: step 278, loss 0.146134, acc 0.935\n",
      "2018-04-23T03:53:51.565069: step 279, loss 0.483788, acc 0.87963\n",
      "2018-04-23T03:53:51.624865: step 280, loss 0.150418, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T03:53:51.801151: step 280, loss 0.340347, acc 0.847564, rec 0.970623, pre 0.774133, f1 0.861314\n",
      "\n",
      "2018-04-23T03:53:51.866586: step 281, loss 0.209384, acc 0.925\n",
      "2018-04-23T03:53:51.926617: step 282, loss 0.234086, acc 0.925\n",
      "2018-04-23T03:53:51.985981: step 283, loss 0.489132, acc 0.875\n",
      "2018-04-23T03:53:52.049671: step 284, loss 0.193812, acc 0.935\n",
      "2018-04-23T03:53:52.109685: step 285, loss 0.238978, acc 0.92\n",
      "2018-04-23T03:53:52.168890: step 286, loss 0.290713, acc 0.91\n",
      "2018-04-23T03:53:52.228190: step 287, loss 0.370819, acc 0.865\n",
      "2018-04-23T03:53:52.291110: step 288, loss 0.34809, acc 0.925\n",
      "2018-04-23T03:53:52.350089: step 289, loss 0.20401, acc 0.91\n",
      "2018-04-23T03:53:52.409155: step 290, loss 0.499344, acc 0.875\n",
      "2018-04-23T03:53:52.469034: step 291, loss 0.238364, acc 0.915\n",
      "2018-04-23T03:53:52.532881: step 292, loss 0.214287, acc 0.93\n",
      "2018-04-23T03:53:52.593063: step 293, loss 0.195989, acc 0.94\n",
      "2018-04-23T03:53:52.652830: step 294, loss 0.369671, acc 0.9\n",
      "2018-04-23T03:53:52.714029: step 295, loss 0.224396, acc 0.92\n",
      "2018-04-23T03:53:52.779853: step 296, loss 0.382645, acc 0.895\n",
      "2018-04-23T03:53:52.839310: step 297, loss 0.3871, acc 0.885\n",
      "2018-04-23T03:53:52.899066: step 298, loss 0.349163, acc 0.905\n",
      "2018-04-23T03:53:52.962674: step 299, loss 0.299221, acc 0.92\n",
      "2018-04-23T03:53:53.027330: step 300, loss 0.33993, acc 0.91\n",
      "\n",
      "Evaluation:\n",
      "2018-04-23T03:53:53.195610: step 300, loss 1.63107, acc 0.565616, rec 0.977673, pre 0.529599, f1 0.687036\n",
      "\n",
      "Saved model checkpoint to /home/saulgrimaldo1/sarcasm_detection_w266/runs/1524455611/checkpoints/model-300\n",
      "\n",
      "2018-04-23T03:53:53.311807: step 301, loss 0.274191, acc 0.93\n",
      "2018-04-23T03:53:53.371640: step 302, loss 0.245357, acc 0.91\n",
      "2018-04-23T03:53:53.430798: step 303, loss 0.227937, acc 0.925\n",
      "2018-04-23T03:53:53.493830: step 304, loss 0.264506, acc 0.915\n",
      "2018-04-23T03:53:53.552910: step 305, loss 0.329518, acc 0.895\n",
      "2018-04-23T03:53:53.613290: step 306, loss 0.190242, acc 0.93\n",
      "2018-04-23T03:53:53.673374: step 307, loss 0.268573, acc 0.925\n",
      "2018-04-23T03:53:53.736057: step 308, loss 0.49824, acc 0.875\n",
      "2018-04-23T03:53:53.798542: step 309, loss 0.138616, acc 0.945\n",
      "2018-04-23T03:53:53.835359: step 310, loss 0.20501, acc 0.935185\n",
      "\n",
      "Test Set:\n",
      "2018-04-23T03:53:53.920409: step 310, loss 0.297398, acc 0.884307, rec 0.942263, pre 0.842975, f1 0.889858\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", 0.75, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "\n",
    "# embedding of 60 is best so far\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\",60, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"1\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 60, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.4, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 200\n",
    "tf.flags.DEFINE_integer(\"batch_size\", batch_size, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 10, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 20, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=train_s.shape[1],\n",
    "            num_classes=2,\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "        \n",
    "         # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        #vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch1, x_batch2, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch1, x_batch2, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            #r \n",
    "            step, summaries, loss, accuracy, recall, precision, f1 = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy,cnn.recall, cnn.precision, cnn.f1_score],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, rec {:g}, pre {:g}, f1 {:g}\".format(time_str, step, loss, accuracy, recall, precision, f1))# recall, precision, f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "        def error_analysis(x_batch1, x_batch2, y_batch, writer=None):\n",
    "            feed_dict = {\n",
    "              cnn.input_x1: x_batch1,\n",
    "              cnn.input_x2: x_batch2,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            #r \n",
    "            step, summaries, loss, accuracy, recall, precision, f1, correct, scores,predictions  = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy,cnn.recall, cnn.precision, cnn.f1_score, cnn.correct_predictions, cnn.scores, cnn.predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, rec {:g}, pre {:g}, f1 {:g}\".format(time_str, step, loss, accuracy, recall, precision, f1))# recall, precision, f1))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "            return correct, scores, predictions\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(train_s, train_c,  expanded_train_labels)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch1, x_batch2, y_batch = zip(*batch)\n",
    "            train_step(x_batch1, x_batch1, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(validation_s, validation_c, expanded_validation_labels, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "print(\"\\nTest Set:\")\n",
    "correct, logits, predictions = error_analysis(test_s, test_c, expanded_test_labels, writer=dev_summary_writer)\n",
    "print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7.3791409, 585],\n",
       " [6.7787189, 342],\n",
       " [6.1188622, 409],\n",
       " [4.6671057, 775],\n",
       " [4.1234961, 847],\n",
       " [3.834964, 22],\n",
       " [3.1755302, 493],\n",
       " [3.0420289, 645],\n",
       " [3.0061371, 115],\n",
       " [3.0001233, 176],\n",
       " [2.8778098, 621],\n",
       " [2.5429554, 281],\n",
       " [2.5366409, 858],\n",
       " [2.4542432, 823],\n",
       " [2.3208306, 345],\n",
       " [2.2803125, 30],\n",
       " [2.1543279, 659],\n",
       " [2.0631256, 199],\n",
       " [2.042244, 314],\n",
       " [2.0207593, 1],\n",
       " [1.9649138, 776],\n",
       " [1.955812, 246],\n",
       " [1.9179856, 499],\n",
       " [1.8881249, 119],\n",
       " [1.8792014, 855],\n",
       " [1.794512, 413],\n",
       " [1.7524679, 655],\n",
       " [1.7229431, 423],\n",
       " [1.7122647, 378],\n",
       " [1.6944897, 727],\n",
       " [1.6551533, 740],\n",
       " [1.6353781, 839],\n",
       " [1.53804, 540],\n",
       " [1.5363944, 231],\n",
       " [1.5196466, 280],\n",
       " [1.4590036, 331],\n",
       " [1.3307029, 562],\n",
       " [1.275373, 343],\n",
       " [1.2068136, 101],\n",
       " [1.1911135, 729],\n",
       " [1.1890311, 804],\n",
       " [1.1883707, 421],\n",
       " [1.1687806, 625],\n",
       " [1.1432855, 355],\n",
       " [1.1354543, 402],\n",
       " [1.1266637, 242],\n",
       " [1.119264, 98],\n",
       " [1.1021111, 693],\n",
       " [1.0614793, 466],\n",
       " [0.97489673, 144],\n",
       " [0.9744426, 439],\n",
       " [0.96606225, 737],\n",
       " [0.91767579, 294],\n",
       " [0.91269451, 161],\n",
       " [0.89912778, 420],\n",
       " [0.89705449, 733],\n",
       " [0.88854897, 360],\n",
       " [0.87118036, 347],\n",
       " [0.87026864, 72],\n",
       " [0.86473542, 268],\n",
       " [0.83695132, 552],\n",
       " [0.80028909, 587],\n",
       " [0.79395753, 758],\n",
       " [0.76919836, 53],\n",
       " [0.76448143, 21],\n",
       " [0.73772699, 432],\n",
       " [0.70219105, 707],\n",
       " [0.68048853, 339],\n",
       " [0.66164511, 527],\n",
       " [0.65823781, 236],\n",
       " [0.64079958, 70],\n",
       " [0.57758093, 640],\n",
       " [0.56052274, 681],\n",
       " [0.54235977, 65],\n",
       " [0.47985038, 706],\n",
       " [0.42081964, 599],\n",
       " [0.39194334, 388],\n",
       " [0.39098251, 683],\n",
       " [0.38820839, 486],\n",
       " [0.36341059, 491],\n",
       " [0.35325217, 122],\n",
       " [0.23390573, 584],\n",
       " [0.22422677, 543],\n",
       " [0.22218728, 667],\n",
       " [0.21979028, 731],\n",
       " [0.20467043, 300],\n",
       " [0.19725382, 209],\n",
       " [0.19019687, 310],\n",
       " [0.18092072, 416],\n",
       " [0.17328203, 473],\n",
       " [0.11472261, 37],\n",
       " [0.10583267, 856],\n",
       " [0.091197312, 673],\n",
       " [0.083615959, 803],\n",
       " [0.060651898, 288],\n",
       " [0.053940654, 561],\n",
       " [0.052567303, 219],\n",
       " [0.051396966, 99],\n",
       " [0.043424606, 328],\n",
       " [0.032497644, 741],\n",
       " [0.0050211549, 395]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def incorrect_confidence(wrong, logits, predictions):\n",
    "    indeces = np.where(wrong)\n",
    "    wrong_predictions = predictions[indeces]\n",
    "    wrong_logits = logits[indeces]\n",
    "    \n",
    "    return [[wrong_logits[i][value] - wrong_logits[i][1-value], indeces[0][i]] for i, value in enumerate(wrong_predictions)]\n",
    "wrong = correct == False\n",
    "\n",
    "sorted(incorrect_confidence(wrong, logits, predictions), key = lambda logit: -logit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ACCOUNT', 'ACCOUNT', 'ACCOUNT', 'you', 'obviously', 'have', 'no', 'knowledge', 'of', '#disarmament', ',', 'being', 'fed', 'the', 'fuck', 'up', 'with', '#socialism', ',', 'being', 'under', 'constant', 'threat', 'of', 'force', '&', 'being', 'damn', 'near', 'killed', 'through', '#socialistoppression', ';', 'real', '#history', ',', 'and', 'enough', '=', 'enough', 'which', 'gave', 'birth', 'DG', '#constitution', '#secondamendment', '#liberty', '…']\n",
      "['wakandabarbie', '🌊', 'on', 'twitter', ':', 'you', 'obviously', 'haven', '’', 't', 'seen', 'who', '’', 's', 'marching', '.', 'it', '’', 's', 'not', 'just', 'teens', ',', 'sweetie', '.', 'millions', 'of', 'us', 'are', 'fed', 'the', 'fuck', 'up', '.', 'damn', 'near', 'everyone', 'knows', 'someone', 'or', 'has', 'had', 'a', 'family', 'member', 'or', 'friend', 'killed', 'through', 'gun', 'violence', '.', 'enough', 'is', 'enough', '.']\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "index = 585\n",
    "print(test_sentences[index])\n",
    "print(test_contexts[index])\n",
    "print(test_labels[index])\n",
    "print(predictions[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['finished', 'a', 'way', 'out', 'with', 'ACCOUNT', 'yesterday', 'night', '.', 'i', 'really', 'like', 'the', 'game', '.', 'it', '’', 's', 'a', 'story', 'drive', 'coop', 'game', 'with', 'cool', 'protagonists', '.', 'DG-DG', 'hours', 'of', 'good', 'fun', ',', 'tension', 'and', 'surprises', '.', 'especially', 'during', 'the', 'final', 'hours', ',', 'this', 'game', 'feels', 'like', 'a', 'blockbuster', 'movie', '.', '#awayout', '#gaming']\n",
      "['nan']\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "index = 342\n",
    "print(test_sentences[index])\n",
    "print(test_contexts[index])\n",
    "print(test_labels[index])\n",
    "print(predictions[index])\n",
    "# seems to tag compliments as sarcastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['luckily', ',', 'i', 'once', 'again', 'had', 'a', 'ACCOUNT', 'bar', 'in', 'my', 'camera', 'case', 'since', 'i', 'cant', 'trust', 'what', 'any', 'humans', 'say', '...', '😑', 'ACCOUNT', 'started', 'their', 'main', 'set', 'with', 'one', 'of', 'my', 'favorite', 'songs', 'on', 'their', 'new', 'album', 'the', 'north', 'star', 'called', 'endless', '!', '😀', '#saturdaynight', '#concert', '#event', '#songs', '#greatmusic']\n",
      "['nan']\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "index = 409\n",
    "print(test_sentences[index])\n",
    "print(test_contexts[index])\n",
    "print(test_labels[index])\n",
    "print(predictions[index])\n",
    "# seems to be another case of a compliment being tagged as sarcastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['always', 'knew', 'this', 'was', 'false.but', 'heres', 'the', 'question', ',', 'what', 'should', 'a', 'person', 'do', 'if', 'taken', 'to', 'an', 'atm', 'under', 'duress', '?', 'any', 'help', '?', 'LINK']\n",
      "['nan']\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "index = 775\n",
    "print(test_sentences[index])\n",
    "print(test_contexts[index])\n",
    "print(test_labels[index])\n",
    "print(predictions[index])\n",
    "# Not really sure why this is tagged as sarcasm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ACCOUNT', 'ACCOUNT', 'that', 'was', 'very', 'thoughtful', '&', 'supportive', 'of', 'mr', '.', 'kraft', '.', 'all', 'the', 'other', '#nfl', 'teams', 'should', 'have', 'done', 'the', 'same', 'thing', '!']\n",
      "['the', 'ACCOUNT', 'loaned', 'their', 'plane', 'to', 'parkland', 'students', 'so', 'they', 'could', 'get', 'to', 'the', 'HASHTAG', '.twitter.com/6gjhfcpu7i']\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "index = 847\n",
    "print(test_sentences[index])\n",
    "print(test_contexts[index])\n",
    "print(test_labels[index])\n",
    "print(predictions[index])\n",
    "# another compliment tagged as sarcastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LINK', '…', '…', '…', '…', '…', '…', '…', '…', '…', '…', '…', '…', '…', '…', '…', '…', '…', '…', '…', 'once', 'upon', 'a', 'time', '(', 'spin', ')', 'featuring', 'ACCOUNT', 'of', 'd12', '#newmusic', '#hot', '#brandnew', '#michigan', '#midwest', '#underground', '#spin', '#lol', '#lmao', '#bars', '#hot', '#swag', '#d12', '#rap']\n",
      "['nan']\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "index = 499\n",
    "print(test_sentences[index])\n",
    "print(test_contexts[index])\n",
    "print(test_labels[index])\n",
    "print(predictions[index])\n",
    "# too many hashtags?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
